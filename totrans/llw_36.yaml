- en: The Multi-Armed Bandit Problem and Its Solutions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://lilianweng.github.io/posts/2018-01-23-multi-armed-bandit/](https://lilianweng.github.io/posts/2018-01-23-multi-armed-bandit/)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The algorithms are implemented for Bernoulli bandit in [lilianweng/multi-armed-bandit](http://github.com/lilianweng/multi-armed-bandit).
  prefs: []
  type: TYPE_NORMAL
- en: Exploitation vs Exploration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The exploration vs exploitation dilemma exists in many aspects of our life.
    Say, your favorite restaurant is right around the corner. If you go there every
    day, you would be confident of what you will get, but miss the chances of discovering
    an even better option. If you try new places all the time, very likely you are
    gonna have to eat unpleasant food from time to time. Similarly, online advisors
    try to balance between the known most attractive ads and the new ads that might
    be even more successful.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4346c76dcfd6200509e34603c21987d5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 1\. A real-life example of the exploration vs exploitation dilemma: where
    to eat? (Image source: UC Berkeley AI course [slide](http://ai.berkeley.edu/lecture_slides.html),
    [lecture 11](http://ai.berkeley.edu/slides/Lecture%2011%20--%20Reinforcement%20Learning%20II/SP14%20CS188%20Lecture%2011%20--%20Reinforcement%20Learning%20II.pptx).)'
  prefs: []
  type: TYPE_NORMAL
- en: 'If we have learned all the information about the environment, we are able to
    find the best strategy by even just simulating brute-force, let alone many other
    smart approaches. The dilemma comes from the *incomplete* information: we need
    to gather enough information to make best overall decisions while keeping the
    risk under control. With exploitation, we take advantage of the best option we
    know. With exploration, we take some risk to collect information about unknown
    options. The best long-term strategy may involve short-term sacrifices. For example,
    one exploration trial could be a total failure, but it warns us of not taking
    that action too often in the future.'
  prefs: []
  type: TYPE_NORMAL
- en: What is Multi-Armed Bandit?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The [multi-armed bandit](https://en.wikipedia.org/wiki/Multi-armed_bandit)
    problem is a classic problem that well demonstrates the exploration vs exploitation
    dilemma. Imagine you are in a casino facing multiple slot machines and each is
    configured with an unknown probability of how likely you can get a reward at one
    play. The question is: *What is the best strategy to achieve highest long-term
    rewards?*'
  prefs: []
  type: TYPE_NORMAL
- en: In this post, we will only discuss the setting of having an infinite number
    of trials. The restriction on a finite number of trials introduces a new type
    of exploration problem. For instance, if the number of trials is smaller than
    the number of slot machines, we cannot even try every machine to estimate the
    reward probability (!) and hence we have to behave smartly w.r.t. a limited set
    of knowledge and resources (i.e. time).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3f371ed9866c1a75effe0fe91dbc5fc1.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig. 2\. An illustration of how a Bernoulli multi-armed bandit works. The reward
    probabilities are **unknown** to the player.
  prefs: []
  type: TYPE_NORMAL
- en: A naive approach can be that you continue to playing with one machine for many
    many rounds so as to eventually estimate the “true” reward probability according
    to the [law of large numbers](https://en.wikipedia.org/wiki/Law_of_large_numbers).
    However, this is quite wasteful and surely does not guarantee the best long-term
    reward.
  prefs: []
  type: TYPE_NORMAL
- en: Definition
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now let’s give it a scientific definition.
  prefs: []
  type: TYPE_NORMAL
- en: 'A Bernoulli multi-armed bandit can be described as a tuple of $\langle \mathcal{A},
    \mathcal{R} \rangle$, where:'
  prefs: []
  type: TYPE_NORMAL
- en: We have $K$ machines with reward probabilities, $\{ \theta_1, \dots, \theta_K
    \}$.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At each time step t, we take an action a on one slot machine and receive a reward
    r.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\mathcal{A}$ is a set of actions, each referring to the interaction with one
    slot machine. The value of action a is the expected reward, $Q(a) = \mathbb{E}
    [r \vert a] = \theta$. If action $a_t$ at the time step t is on the i-th machine,
    then $Q(a_t) = \theta_i$.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\mathcal{R}$ is a reward function. In the case of Bernoulli bandit, we observe
    a reward r in a *stochastic* fashion. At the time step t, $r_t = \mathcal{R}(a_t)$
    may return reward 1 with a probability $Q(a_t)$ or 0 otherwise.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is a simplified version of [Markov decision process](https://en.wikipedia.org/wiki/Markov_decision_process),
    as there is no state $\mathcal{S}$.
  prefs: []
  type: TYPE_NORMAL
- en: The goal is to maximize the cumulative reward $\sum_{t=1}^T r_t$. If we know
    the optimal action with the best reward, then the goal is same as to minimize
    the potential [regret](https://en.wikipedia.org/wiki/Regret_(decision_theory))
    or loss by not picking the optimal action.
  prefs: []
  type: TYPE_NORMAL
- en: 'The optimal reward probability $\theta^{*}$ of the optimal action $a^{*}$ is:'
  prefs: []
  type: TYPE_NORMAL
- en: $$ \theta^{*}=Q(a^{*})=\max_{a \in \mathcal{A}} Q(a) = \max_{1 \leq i \leq K}
    \theta_i $$
  prefs: []
  type: TYPE_NORMAL
- en: 'Our loss function is the total regret we might have by not selecting the optimal
    action up to the time step T:'
  prefs: []
  type: TYPE_NORMAL
- en: $$ \mathcal{L}_T = \mathbb{E} \Big[ \sum_{t=1}^T \big( \theta^{*} - Q(a_t) \big)
    \Big] $$
  prefs: []
  type: TYPE_NORMAL
- en: Bandit Strategies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Based on how we do exploration, there several ways to solve the multi-armed
    bandit.
  prefs: []
  type: TYPE_NORMAL
- en: 'No exploration: the most naive approach and a bad one.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploration at random
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploration smartly with preference to uncertainty
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ε-Greedy Algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The ε-greedy algorithm takes the best action most of the time, but does random
    exploration occasionally. The action value is estimated according to the past
    experience by averaging the rewards associated with the target action a that we
    have observed so far (up to the current time step t):'
  prefs: []
  type: TYPE_NORMAL
- en: $$ \hat{Q}_t(a) = \frac{1}{N_t(a)} \sum_{\tau=1}^t r_\tau \mathbb{1}[a_\tau
    = a] $$
  prefs: []
  type: TYPE_NORMAL
- en: where $\mathbb{1}$ is a binary indicator function and $N_t(a)$ is how many times
    the action a has been selected so far, $N_t(a) = \sum_{\tau=1}^t \mathbb{1}[a_\tau
    = a]$.
  prefs: []
  type: TYPE_NORMAL
- en: 'According to the ε-greedy algorithm, with a small probability $\epsilon$ we
    take a random action, but otherwise (which should be the most of the time, probability
    1-$\epsilon$) we pick the best action that we have learnt so far: $\hat{a}^{*}_t
    = \arg\max_{a \in \mathcal{A}} \hat{Q}_t(a)$.'
  prefs: []
  type: TYPE_NORMAL
- en: Check my toy implementation [here](https://github.com/lilianweng/multi-armed-bandit/blob/master/solvers.py#L45).
  prefs: []
  type: TYPE_NORMAL
- en: Upper Confidence Bounds
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Random exploration gives us an opportunity to try out options that we have not
    known much about. However, due to the randomness, it is possible we end up exploring
    a bad action which we have confirmed in the past (bad luck!). To avoid such inefficient
    exploration, one approach is to decrease the parameter ε in time and the other
    is to be optimistic about options with *high uncertainty* and thus to prefer actions
    for which we haven’t had a confident value estimation yet. Or in other words,
    we favor exploration of actions with a strong potential to have a optimal value.
  prefs: []
  type: TYPE_NORMAL
- en: The Upper Confidence Bounds (UCB) algorithm measures this potential by an upper
    confidence bound of the reward value, $\hat{U}_t(a)$, so that the true value is
    below with bound $Q(a) \leq \hat{Q}_t(a) + \hat{U}_t(a)$ with high probability.
    The upper bound $\hat{U}_t(a)$ is a function of $N_t(a)$; a larger number of trials
    $N_t(a)$ should give us a smaller bound $\hat{U}_t(a)$.
  prefs: []
  type: TYPE_NORMAL
- en: 'In UCB algorithm, we always select the greediest action to maximize the upper
    confidence bound:'
  prefs: []
  type: TYPE_NORMAL
- en: $$ a^{UCB}_t = argmax_{a \in \mathcal{A}} \hat{Q}_t(a) + \hat{U}_t(a) $$
  prefs: []
  type: TYPE_NORMAL
- en: Now, the question is *how to estimate the upper confidence bound*.
  prefs: []
  type: TYPE_NORMAL
- en: Hoeffding’s Inequality
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If we do not want to assign any prior knowledge on how the distribution looks
    like, we can get help from [“Hoeffding’s Inequality”](http://cs229.stanford.edu/extra-notes/hoeffding.pdf)
    — a theorem applicable to any bounded distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let $X_1, \dots, X_t$ be i.i.d. (independent and identically distributed) random
    variables and they are all bounded by the interval [0, 1]. The sample mean is
    $\overline{X}_t = \frac{1}{t}\sum_{\tau=1}^t X_\tau$. Then for u > 0, we have:'
  prefs: []
  type: TYPE_NORMAL
- en: $$ \mathbb{P} [ \mathbb{E}[X] > \overline{X}_t + u] \leq e^{-2tu^2} $$
  prefs: []
  type: TYPE_NORMAL
- en: 'Given one target action a, let us consider:'
  prefs: []
  type: TYPE_NORMAL
- en: $r_t(a)$ as the random variables,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $Q(a)$ as the true mean,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\hat{Q}_t(a)$ as the sample mean,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: And $u$ as the upper confidence bound, $u = U_t(a)$
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then we have,
  prefs: []
  type: TYPE_NORMAL
- en: $$ \mathbb{P} [ Q(a) > \hat{Q}_t(a) + U_t(a)] \leq e^{-2t{U_t(a)}^2} $$
  prefs: []
  type: TYPE_NORMAL
- en: 'We want to pick a bound so that with high chances the true mean is blow the
    sample mean + the upper confidence bound. Thus $e^{-2t U_t(a)^2}$ should be a
    small probability. Let’s say we are ok with a tiny threshold p:'
  prefs: []
  type: TYPE_NORMAL
- en: $$ e^{-2t U_t(a)^2} = p \text{ Thus, } U_t(a) = \sqrt{\frac{-\log p}{2 N_t(a)}}
    $$
  prefs: []
  type: TYPE_NORMAL
- en: UCB1
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One heuristic is to reduce the threshold p in time, as we want to make more
    confident bound estimation with more rewards observed. Set $p=t^{-4}$ we get **UCB1**
    algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: $$ U_t(a) = \sqrt{\frac{2 \log t}{N_t(a)}} \text{ and } a^{UCB1}_t = \arg\max_{a
    \in \mathcal{A}} Q(a) + \sqrt{\frac{2 \log t}{N_t(a)}} $$
  prefs: []
  type: TYPE_NORMAL
- en: Bayesian UCB
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In UCB or UCB1 algorithm, we do not assume any prior on the reward distribution
    and therefore we have to rely on the Hoeffding’s Inequality for a very generalize
    estimation. If we are able to know the distribution upfront, we would be able
    to make better bound estimation.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if we expect the mean reward of every slot machine to be Gaussian
    as in Fig 2, we can set the upper bound as 95% confidence interval by setting
    $\hat{U}_t(a)$ to be twice the standard deviation.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9b40be6abd4d5c5f5f186b8d3c5e6bf0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 3\. When the expected reward has a Gaussian distribution. $\sigma(a\_i)$
    is the standard deviation and $c\sigma(a\_i)$ is the upper confidence bound. The
    constant $c$ is a adjustable hyperparameter. (Image source: [UCL RL course lecture
    9''s slides](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/XX.pdf))'
  prefs: []
  type: TYPE_NORMAL
- en: Check my toy implementation of [UCB1](https://github.com/lilianweng/multi-armed-bandit/blob/master/solvers.py#L76)
    and [Bayesian UCB](https://github.com/lilianweng/multi-armed-bandit/blob/master/solvers.py#L99)
    with Beta prior on θ.
  prefs: []
  type: TYPE_NORMAL
- en: Thompson Sampling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Thompson sampling has a simple idea but it works great for solving the multi-armed
    bandit problem.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3eedd17dfeb10d3f934660fb1bc589d9.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig. 4\. Oops, I guess not this Thompson? (Credit goes to [Ben Taborsky](https://www.linkedin.com/in/benjamin-taborsky);
    he has a full theorem of how Thompson invented while pondering over who to pass
    the ball. Yes I stole his joke.)
  prefs: []
  type: TYPE_NORMAL
- en: 'At each time step, we want to select action a according to the probability
    that a is **optimal**:'
  prefs: []
  type: TYPE_NORMAL
- en: $$ \begin{aligned} \pi(a \; \vert \; h_t) &= \mathbb{P} [ Q(a) > Q(a'), \forall
    a' \neq a \; \vert \; h_t] \\ &= \mathbb{E}_{\mathcal{R} \vert h_t} [ \mathbb{1}(a
    = \arg\max_{a \in \mathcal{A}} Q(a)) ] \end{aligned} $$
  prefs: []
  type: TYPE_NORMAL
- en: where $\pi(a ; \vert ; h_t)$ is the probability of taking action a given the
    history $h_t$.
  prefs: []
  type: TYPE_NORMAL
- en: For the Bernoulli bandit, it is natural to assume that $Q(a)$ follows a [Beta](https://en.wikipedia.org/wiki/Beta_distribution)
    distribution, as $Q(a)$ is essentially the success probability θ in [Bernoulli](https://en.wikipedia.org/wiki/Bernoulli_distribution)
    distribution. The value of $\text{Beta}(\alpha, \beta)$ is within the interval
    [0, 1]; α and β correspond to the counts when we **succeeded** or **failed** to
    get a reward respectively.
  prefs: []
  type: TYPE_NORMAL
- en: First, let us initialize the Beta parameters α and β based on some prior knowledge
    or belief for every action. For example,
  prefs: []
  type: TYPE_NORMAL
- en: α = 1 and β = 1; we expect the reward probability to be 50% but we are not very
    confident.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: α = 1000 and β = 9000; we strongly believe that the reward probability is 10%.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'At each time t, we sample an expected reward, $\tilde{Q}(a)$, from the prior
    distribution $\text{Beta}(\alpha_i, \beta_i)$ for every action. The best action
    is selected among samples: $a^{TS}_t = \arg\max_{a \in \mathcal{A}} \tilde{Q}(a)$.
    After the true reward is observed, we can update the Beta distribution accordingly,
    which is essentially doing Bayesian inference to compute the posterior with the
    known prior and the likelihood of getting the sampled data.'
  prefs: []
  type: TYPE_NORMAL
- en: $$ \begin{aligned} \alpha_i & \leftarrow \alpha_i + r_t \mathbb{1}[a^{TS}_t
    = a_i] \\ \beta_i & \leftarrow \beta_i + (1-r_t) \mathbb{1}[a^{TS}_t = a_i] \end{aligned}
    $$
  prefs: []
  type: TYPE_NORMAL
- en: Thompson sampling implements the idea of [probability matching](https://en.wikipedia.org/wiki/Probability_matching).
    Because its reward estimations $\tilde{Q}$ are sampled from posterior distributions,
    each of these probabilities is equivalent to the probability that the corresponding
    action is optimal, conditioned on observed history.
  prefs: []
  type: TYPE_NORMAL
- en: However, for many practical and complex problems, it can be computationally
    intractable to estimate the posterior distributions with observed true rewards
    using Bayesian inference. Thompson sampling still can work out if we are able
    to approximate the posterior distributions using methods like Gibbs sampling,
    Laplace approximate, and the bootstraps. This [tutorial](https://arxiv.org/pdf/1707.02038.pdf)
    presents a comprehensive review; strongly recommend it if you want to learn more
    about Thompson sampling.
  prefs: []
  type: TYPE_NORMAL
- en: Case Study
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I implemented the above algorithms in [lilianweng/multi-armed-bandit](https://github.com/lilianweng/multi-armed-bandit).
    A [BernoulliBandit](https://github.com/lilianweng/multi-armed-bandit/blob/master/bandits.py#L13)
    object can be constructed with a list of random or predefined reward probabilities.
    The bandit algorithms are implemented as subclasses of [Solver](https://github.com/lilianweng/multi-armed-bandit/blob/master/solvers.py#L9),
    taking a Bandit object as the target problem. The cumulative regrets are tracked
    in time.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b2fa539ac397bfaf719de72ee6633b63.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig. 4\. The result of a small experiment on solving a Bernoulli bandit with
    K = 10 slot machines with reward probabilities, {0.0, 0.1, 0.2, ..., 0.9}. Each
    solver runs 10000 steps.
  prefs: []
  type: TYPE_NORMAL
- en: (Left) The plot of time step vs the cumulative regrets. (Middle) The plot of
    true reward probability vs estimated probability. (Right) The fraction of each
    action is picked during the 10000-step run.*
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We need exploration because information is valuable. In terms of the exploration
    strategies, we can do no exploration at all, focusing on the short-term returns.
    Or we occasionally explore at random. Or even further, we explore and we are picky
    about which options to explore — actions with higher uncertainty are favored because
    they can provide higher information gain.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0802d638db339ffb0b33222591a0a94e.png)'
  prefs: []
  type: TYPE_IMG
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 'Cited as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] CS229 Supplemental Lecture notes: [Hoeffding’s inequality](http://cs229.stanford.edu/extra-notes/hoeffding.pdf).'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] RL Course by David Silver - Lecture 9: [Exploration and Exploitation](https://youtu.be/sGuiWX07sKw)'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] Olivier Chapelle and Lihong Li. [“An empirical evaluation of thompson sampling.”](http://papers.nips.cc/paper/4321-an-empirical-evaluation-of-thompson-sampling.pdf)
    NIPS. 2011.'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] Russo, Daniel, et al. [“A Tutorial on Thompson Sampling.”](https://arxiv.org/pdf/1707.02038.pdf)
    arXiv:1707.02038 (2017).'
  prefs: []
  type: TYPE_NORMAL
