- en: The Multi-Armed Bandit Problem and Its Solutions
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**多臂赌博问题及其解决方案**'
- en: 原文：[https://lilianweng.github.io/posts/2018-01-23-multi-armed-bandit/](https://lilianweng.github.io/posts/2018-01-23-multi-armed-bandit/)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://lilianweng.github.io/posts/2018-01-23-multi-armed-bandit/](https://lilianweng.github.io/posts/2018-01-23-multi-armed-bandit/)
- en: The algorithms are implemented for Bernoulli bandit in [lilianweng/multi-armed-bandit](http://github.com/lilianweng/multi-armed-bandit).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法在[lilianweng/multi-armed-bandit](http://github.com/lilianweng/multi-armed-bandit)中实现。
- en: Exploitation vs Exploration
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开发与勘探
- en: The exploration vs exploitation dilemma exists in many aspects of our life.
    Say, your favorite restaurant is right around the corner. If you go there every
    day, you would be confident of what you will get, but miss the chances of discovering
    an even better option. If you try new places all the time, very likely you are
    gonna have to eat unpleasant food from time to time. Similarly, online advisors
    try to balance between the known most attractive ads and the new ads that might
    be even more successful.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 探索与开发困境存在于我们生活的许多方面。比如，你最喜欢的餐厅就在附近。如果你每天去那里，你会对你会得到什么很有信心，但会错过发现更好选择的机会。如果你一直尝试新的地方，很可能你会不时地吃到不好吃的食物。同样，在线广告商努力在已知最具吸引力的广告和可能更成功的新广告之间取得平衡。
- en: '![](../Images/4346c76dcfd6200509e34603c21987d5.png)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4346c76dcfd6200509e34603c21987d5.png)'
- en: 'Fig. 1\. A real-life example of the exploration vs exploitation dilemma: where
    to eat? (Image source: UC Berkeley AI course [slide](http://ai.berkeley.edu/lecture_slides.html),
    [lecture 11](http://ai.berkeley.edu/slides/Lecture%2011%20--%20Reinforcement%20Learning%20II/SP14%20CS188%20Lecture%2011%20--%20Reinforcement%20Learning%20II.pptx).)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 图1\. 探索与开发困境的现实生活例子：去哪里吃饭？（图片来源：加州大学伯克利分校人工智能课程[幻灯片](http://ai.berkeley.edu/lecture_slides.html)，[第11讲](http://ai.berkeley.edu/slides/Lecture%2011%20--%20Reinforcement%20Learning%20II/SP14%20CS188%20Lecture%2011%20--%20Reinforcement%20Learning%20II.pptx)。）
- en: 'If we have learned all the information about the environment, we are able to
    find the best strategy by even just simulating brute-force, let alone many other
    smart approaches. The dilemma comes from the *incomplete* information: we need
    to gather enough information to make best overall decisions while keeping the
    risk under control. With exploitation, we take advantage of the best option we
    know. With exploration, we take some risk to collect information about unknown
    options. The best long-term strategy may involve short-term sacrifices. For example,
    one exploration trial could be a total failure, but it warns us of not taking
    that action too often in the future.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们已经了解了环境的所有信息，甚至只需模拟蛮力，就能找到最佳策略，更不用说许多其他聪明的方法了。困境来自于*不完整*的信息：我们需要收集足够的信息以做出最佳的整体决策，同时控制风险。通过开发，我们利用我们所知道的最佳选项。通过勘探，我们冒一些风险来收集有关未知选项的信息。最佳的长期策略可能涉及短期的牺牲。例如，一个勘探试验可能完全失败，但它警告我们不要在将来经常采取那种行动。
- en: What is Multi-Armed Bandit?
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是多臂赌博？
- en: 'The [multi-armed bandit](https://en.wikipedia.org/wiki/Multi-armed_bandit)
    problem is a classic problem that well demonstrates the exploration vs exploitation
    dilemma. Imagine you are in a casino facing multiple slot machines and each is
    configured with an unknown probability of how likely you can get a reward at one
    play. The question is: *What is the best strategy to achieve highest long-term
    rewards?*'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[多臂赌博](https://en.wikipedia.org/wiki/Multi-armed_bandit)问题是一个经典问题，很好地展示了勘探与开发之间的困境。想象一下，你在赌场面对多台老虎机，每台机器都配置有一个未知的概率，表示你在一次游戏中获得奖励的可能性有多大。问题是：*如何制定最佳策略以获得最高的长期回报？*'
- en: In this post, we will only discuss the setting of having an infinite number
    of trials. The restriction on a finite number of trials introduces a new type
    of exploration problem. For instance, if the number of trials is smaller than
    the number of slot machines, we cannot even try every machine to estimate the
    reward probability (!) and hence we have to behave smartly w.r.t. a limited set
    of knowledge and resources (i.e. time).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将仅讨论具有无限次试验的设置。对有限次试验的限制引入了一种新类型的勘探问题。例如，如果试验次数小于老虎机的数量，我们甚至无法尝试每台机器以估计奖励概率(!)，因此我们必须在有限的知识和资源（即时间）集合方面表现聪明。
- en: '![](../Images/3f371ed9866c1a75effe0fe91dbc5fc1.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3f371ed9866c1a75effe0fe91dbc5fc1.png)'
- en: Fig. 2\. An illustration of how a Bernoulli multi-armed bandit works. The reward
    probabilities are **unknown** to the player.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图2\. 展示了伯努利多臂赌博的工作原理。玩家对奖励概率**未知**。
- en: A naive approach can be that you continue to playing with one machine for many
    many rounds so as to eventually estimate the “true” reward probability according
    to the [law of large numbers](https://en.wikipedia.org/wiki/Law_of_large_numbers).
    However, this is quite wasteful and surely does not guarantee the best long-term
    reward.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 一个天真的方法可能是你继续与一个老虎机玩很多轮，以便最终根据[大数定律](https://en.wikipedia.org/wiki/Law_of_large_numbers)估计“真实”奖励概率。然而，这是相当浪费的，肯定不能保证最佳的长期奖励。
- en: Definition
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义
- en: Now let’s give it a scientific definition.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们给它一个科学的定义。
- en: 'A Bernoulli multi-armed bandit can be described as a tuple of $\langle \mathcal{A},
    \mathcal{R} \rangle$, where:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 一个伯努利多臂老虎机可以描述为 $\langle \mathcal{A}, \mathcal{R} \rangle$ 的元组，其中：
- en: We have $K$ machines with reward probabilities, $\{ \theta_1, \dots, \theta_K
    \}$.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们有 $K$ 台机器，具有奖励概率 $\{ \theta_1, \dots, \theta_K \}$。
- en: At each time step t, we take an action a on one slot machine and receive a reward
    r.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在每个时间步骤 t，我们在一台老虎机上采取一个动作 a 并获得一个奖励 r。
- en: $\mathcal{A}$ is a set of actions, each referring to the interaction with one
    slot machine. The value of action a is the expected reward, $Q(a) = \mathbb{E}
    [r \vert a] = \theta$. If action $a_t$ at the time step t is on the i-th machine,
    then $Q(a_t) = \theta_i$.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\mathcal{A}$ 是一组动作，每个动作对应与一个老虎机的交互。动作 a 的值是预期奖励，$Q(a) = \mathbb{E} [r \vert
    a] = \theta$。如果时间步骤 t 的动作 $a_t$ 在第 i 台机器上，那么 $Q(a_t) = \theta_i$。
- en: $\mathcal{R}$ is a reward function. In the case of Bernoulli bandit, we observe
    a reward r in a *stochastic* fashion. At the time step t, $r_t = \mathcal{R}(a_t)$
    may return reward 1 with a probability $Q(a_t)$ or 0 otherwise.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\mathcal{R}$ 是一个奖励函数。在伯努利老虎机的情况下，我们以*随机*的方式观察到奖励 r。在时间步骤 t，$r_t = \mathcal{R}(a_t)$
    可能以概率 $Q(a_t)$ 返回奖励 1，否则返回 0。
- en: It is a simplified version of [Markov decision process](https://en.wikipedia.org/wiki/Markov_decision_process),
    as there is no state $\mathcal{S}$.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这是[马尔可夫决策过程](https://en.wikipedia.org/wiki/Markov_decision_process)的简化版本，因为没有状态
    $\mathcal{S}$。
- en: The goal is to maximize the cumulative reward $\sum_{t=1}^T r_t$. If we know
    the optimal action with the best reward, then the goal is same as to minimize
    the potential [regret](https://en.wikipedia.org/wiki/Regret_(decision_theory))
    or loss by not picking the optimal action.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是最大化累积奖励 $\sum_{t=1}^T r_t$。如果我们知道具有最佳奖励的最佳动作，那么目标就是最小化通过不选择最佳动作而产生的潜在[后悔](https://en.wikipedia.org/wiki/Regret_(decision_theory))或损失。
- en: 'The optimal reward probability $\theta^{*}$ of the optimal action $a^{*}$ is:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳奖励概率 $\theta^{*}$ 的最佳动作 $a^{*}$ 是：
- en: $$ \theta^{*}=Q(a^{*})=\max_{a \in \mathcal{A}} Q(a) = \max_{1 \leq i \leq K}
    \theta_i $$
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \theta^{*}=Q(a^{*})=\max_{a \in \mathcal{A}} Q(a) = \max_{1 \leq i \leq K}
    \theta_i $$
- en: 'Our loss function is the total regret we might have by not selecting the optimal
    action up to the time step T:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的损失函数是到时间步骤 T 为止由于未选择最佳动作可能导致的总后悔：
- en: $$ \mathcal{L}_T = \mathbb{E} \Big[ \sum_{t=1}^T \big( \theta^{*} - Q(a_t) \big)
    \Big] $$
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \mathcal{L}_T = \mathbb{E} \Big[ \sum_{t=1}^T \big( \theta^{*} - Q(a_t) \big)
    \Big] $$
- en: Bandit Strategies
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 老虎机策略
- en: Based on how we do exploration, there several ways to solve the multi-armed
    bandit.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们进行探索的方式，有几种解决多臂老虎机问题的方法。
- en: 'No exploration: the most naive approach and a bad one.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无探索：最天真的方法和一个糟糕的方法。
- en: Exploration at random
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机探索
- en: Exploration smartly with preference to uncertainty
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 智能地探索，偏好于不确定性
- en: ε-Greedy Algorithm
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ε-Greedy 算法
- en: 'The ε-greedy algorithm takes the best action most of the time, but does random
    exploration occasionally. The action value is estimated according to the past
    experience by averaging the rewards associated with the target action a that we
    have observed so far (up to the current time step t):'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ε-贪心算法大部分时间选择最佳动作，但偶尔进行随机探索。根据过去的经验，通过平均观察到目标动作 a 相关的奖励来估计动作值（直到当前时间步骤 t）：
- en: $$ \hat{Q}_t(a) = \frac{1}{N_t(a)} \sum_{\tau=1}^t r_\tau \mathbb{1}[a_\tau
    = a] $$
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \hat{Q}_t(a) = \frac{1}{N_t(a)} \sum_{\tau=1}^t r_\tau \mathbb{1}[a_\tau
    = a] $$
- en: where $\mathbb{1}$ is a binary indicator function and $N_t(a)$ is how many times
    the action a has been selected so far, $N_t(a) = \sum_{\tau=1}^t \mathbb{1}[a_\tau
    = a]$.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbb{1}$ 是一个二元指示函数，$N_t(a)$ 是到目前为止选择动作 a 的次数，$N_t(a) = \sum_{\tau=1}^t
    \mathbb{1}[a_\tau = a]$。
- en: 'According to the ε-greedy algorithm, with a small probability $\epsilon$ we
    take a random action, but otherwise (which should be the most of the time, probability
    1-$\epsilon$) we pick the best action that we have learnt so far: $\hat{a}^{*}_t
    = \arg\max_{a \in \mathcal{A}} \hat{Q}_t(a)$.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 根据ε-贪婪算法，以小概率$\epsilon$我们会采取随机动作，但否则（大部分时间，概率为1-$\epsilon$）我们选择到目前为止学到的最佳动作：$\hat{a}^{*}_t
    = \arg\max_{a \in \mathcal{A}} \hat{Q}_t(a)$。
- en: Check my toy implementation [here](https://github.com/lilianweng/multi-armed-bandit/blob/master/solvers.py#L45).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 查看我的玩具实现[这里](https://github.com/lilianweng/multi-armed-bandit/blob/master/solvers.py#L45)。
- en: Upper Confidence Bounds
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 上限置信度
- en: Random exploration gives us an opportunity to try out options that we have not
    known much about. However, due to the randomness, it is possible we end up exploring
    a bad action which we have confirmed in the past (bad luck!). To avoid such inefficient
    exploration, one approach is to decrease the parameter ε in time and the other
    is to be optimistic about options with *high uncertainty* and thus to prefer actions
    for which we haven’t had a confident value estimation yet. Or in other words,
    we favor exploration of actions with a strong potential to have a optimal value.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 随机探索给了我们一个机会尝试我们不太了解的选项。然而，由于随机性，我们可能最终会探索到一个我们过去确认过的不好的动作（倒霉！）。为了避免这种低效的探索，一种方法是随着时间减少参数ε，另一种方法是对*高不确定性*的选项持乐观态度，因此更倾向于那些我们尚未对其价值进行自信估计的动作。换句话说，我们更倾向于探索具有最优值潜力的动作。
- en: The Upper Confidence Bounds (UCB) algorithm measures this potential by an upper
    confidence bound of the reward value, $\hat{U}_t(a)$, so that the true value is
    below with bound $Q(a) \leq \hat{Q}_t(a) + \hat{U}_t(a)$ with high probability.
    The upper bound $\hat{U}_t(a)$ is a function of $N_t(a)$; a larger number of trials
    $N_t(a)$ should give us a smaller bound $\hat{U}_t(a)$.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 上限置信度（UCB）算法通过奖励值的上限置信度$\hat{U}_t(a)$来衡量这种潜力，以便真实值在高概率下低于边界$Q(a) \leq \hat{Q}_t(a)
    + \hat{U}_t(a)$。上限$\hat{U}_t(a)$是$N_t(a)$的函数；更多的试验次数$N_t(a)$应该给我们一个更小的边界$\hat{U}_t(a)$。
- en: 'In UCB algorithm, we always select the greediest action to maximize the upper
    confidence bound:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在UCB算法中，我们总是选择最贪婪的动作来最大化上限置信度：
- en: $$ a^{UCB}_t = argmax_{a \in \mathcal{A}} \hat{Q}_t(a) + \hat{U}_t(a) $$
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: $$ a^{UCB}_t = argmax_{a \in \mathcal{A}} \hat{Q}_t(a) + \hat{U}_t(a) $$
- en: Now, the question is *how to estimate the upper confidence bound*.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，问题是*如何估计上限置信度*。
- en: Hoeffding’s Inequality
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Hoeffding’s Inequality
- en: If we do not want to assign any prior knowledge on how the distribution looks
    like, we can get help from [“Hoeffding’s Inequality”](http://cs229.stanford.edu/extra-notes/hoeffding.pdf)
    — a theorem applicable to any bounded distribution.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们不想对分布的外观做任何先验知识，我们可以从[“Hoeffding’s Inequality”](http://cs229.stanford.edu/extra-notes/hoeffding.pdf)获得帮助——这是适用于任何有界分布的定理。
- en: 'Let $X_1, \dots, X_t$ be i.i.d. (independent and identically distributed) random
    variables and they are all bounded by the interval [0, 1]. The sample mean is
    $\overline{X}_t = \frac{1}{t}\sum_{\tau=1}^t X_\tau$. Then for u > 0, we have:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 让$X_1, \dots, X_t$为独立同分布的随机变量，它们都被区间[0, 1]所限制。样本均值为$\overline{X}_t = \frac{1}{t}\sum_{\tau=1}^t
    X_\tau$。那么对于u > 0，我们有：
- en: $$ \mathbb{P} [ \mathbb{E}[X] > \overline{X}_t + u] \leq e^{-2tu^2} $$
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \mathbb{P} [ \mathbb{E}[X] > \overline{X}_t + u] \leq e^{-2tu^2} $$
- en: 'Given one target action a, let us consider:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个目标动作a，让我们考虑：
- en: $r_t(a)$ as the random variables,
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $r_t(a)$作为随机变量，
- en: $Q(a)$ as the true mean,
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $Q(a)$作为真实均值，
- en: $\hat{Q}_t(a)$ as the sample mean,
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\hat{Q}_t(a)$作为样本均值，
- en: And $u$ as the upper confidence bound, $u = U_t(a)$
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以及$u$作为上限置信度，$u = U_t(a)$
- en: Then we have,
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们有，
- en: $$ \mathbb{P} [ Q(a) > \hat{Q}_t(a) + U_t(a)] \leq e^{-2t{U_t(a)}^2} $$
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \mathbb{P} [ Q(a) > \hat{Q}_t(a) + U_t(a)] \leq e^{-2t{U_t(a)}^2} $$
- en: 'We want to pick a bound so that with high chances the true mean is blow the
    sample mean + the upper confidence bound. Thus $e^{-2t U_t(a)^2}$ should be a
    small probability. Let’s say we are ok with a tiny threshold p:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想选择一个边界，以便在很大概率下真实均值低于样本均值+上限置信度。因此$e^{-2t U_t(a)^2}$应该是一个小概率。假设我们对一个微小阈值p感到满意：
- en: $$ e^{-2t U_t(a)^2} = p \text{ Thus, } U_t(a) = \sqrt{\frac{-\log p}{2 N_t(a)}}
    $$
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: $$ e^{-2t U_t(a)^2} = p \text{ 因此，} U_t(a) = \sqrt{\frac{-\log p}{2 N_t(a)}}
    $$
- en: UCB1
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: UCB1
- en: 'One heuristic is to reduce the threshold p in time, as we want to make more
    confident bound estimation with more rewards observed. Set $p=t^{-4}$ we get **UCB1**
    algorithm:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 一个启发式方法是随着时间减少阈值p，因为我们希望随着观察到更多奖励而做出更有信心的边界估计。设定$p=t^{-4}$我们得到**UCB1**算法：
- en: $$ U_t(a) = \sqrt{\frac{2 \log t}{N_t(a)}} \text{ and } a^{UCB1}_t = \arg\max_{a
    \in \mathcal{A}} Q(a) + \sqrt{\frac{2 \log t}{N_t(a)}} $$
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: $$ U_t(a) = \sqrt{\frac{2 \log t}{N_t(a)}} \text{ and } a^{UCB1}_t = \arg\max_{a
    \in \mathcal{A}} Q(a) + \sqrt{\frac{2 \log t}{N_t(a)}} $$
- en: Bayesian UCB
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 贝叶斯 UCB
- en: In UCB or UCB1 algorithm, we do not assume any prior on the reward distribution
    and therefore we have to rely on the Hoeffding’s Inequality for a very generalize
    estimation. If we are able to know the distribution upfront, we would be able
    to make better bound estimation.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在 UCB 或 UCB1 算法中，我们不假设奖励分布的任何先验，因此我们必须依赖 Hoeffding 不等式进行非常一般化的估计。如果我们能事先了解分布，我们将能够做出更好的边界估计。
- en: For example, if we expect the mean reward of every slot machine to be Gaussian
    as in Fig 2, we can set the upper bound as 95% confidence interval by setting
    $\hat{U}_t(a)$ to be twice the standard deviation.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们期望每台老虎机的平均奖励是如图 2 中的高斯分布，我们可以通过将 $\hat{U}_t(a)$ 设置为两倍标准差来设置上限为 95% 置信区间。
- en: '![](../Images/9b40be6abd4d5c5f5f186b8d3c5e6bf0.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9b40be6abd4d5c5f5f186b8d3c5e6bf0.png)'
- en: 'Fig. 3\. When the expected reward has a Gaussian distribution. $\sigma(a\_i)$
    is the standard deviation and $c\sigma(a\_i)$ is the upper confidence bound. The
    constant $c$ is a adjustable hyperparameter. (Image source: [UCL RL course lecture
    9''s slides](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/XX.pdf))'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3\. 当期望奖励呈高斯分布时。$\sigma(a\_i)$ 是标准差，$c\sigma(a\_i)$ 是上限置信界。常数 $c$ 是可调超参数。（图片来源：[UCL
    强化学习课程第 9 讲幻灯片](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/XX.pdf)）
- en: Check my toy implementation of [UCB1](https://github.com/lilianweng/multi-armed-bandit/blob/master/solvers.py#L76)
    and [Bayesian UCB](https://github.com/lilianweng/multi-armed-bandit/blob/master/solvers.py#L99)
    with Beta prior on θ.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 查看我对[UCB1](https://github.com/lilianweng/multi-armed-bandit/blob/master/solvers.py#L76)和[Bayesian
    UCB](https://github.com/lilianweng/multi-armed-bandit/blob/master/solvers.py#L99)的玩具实现，其中
    θ 采用 Beta 先验。
- en: Thompson Sampling
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 汤普森采样
- en: Thompson sampling has a simple idea but it works great for solving the multi-armed
    bandit problem.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 汤普森采样有一个简单的想法，但对解决多臂老虎机问题非常有效。
- en: '![](../Images/3eedd17dfeb10d3f934660fb1bc589d9.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3eedd17dfeb10d3f934660fb1bc589d9.png)'
- en: Fig. 4\. Oops, I guess not this Thompson? (Credit goes to [Ben Taborsky](https://www.linkedin.com/in/benjamin-taborsky);
    he has a full theorem of how Thompson invented while pondering over who to pass
    the ball. Yes I stole his joke.)
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4\. 哎呀，我猜这不是那个汤普森吧？（感谢[Ben Taborsky](https://www.linkedin.com/in/benjamin-taborsky)；他有一个完整的定理，说明了汤普森在思考该把球传给谁时是如何发明的。是的，我偷了他的笑话。）
- en: 'At each time step, we want to select action a according to the probability
    that a is **optimal**:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个时间步，我们希望根据 a 是**最优**的概率选择动作：
- en: $$ \begin{aligned} \pi(a \; \vert \; h_t) &= \mathbb{P} [ Q(a) > Q(a'), \forall
    a' \neq a \; \vert \; h_t] \\ &= \mathbb{E}_{\mathcal{R} \vert h_t} [ \mathbb{1}(a
    = \arg\max_{a \in \mathcal{A}} Q(a)) ] \end{aligned} $$
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \begin{aligned} \pi(a \; \vert \; h_t) &= \mathbb{P} [ Q(a) > Q(a'), \forall
    a' \neq a \; \vert \; h_t] \\ &= \mathbb{E}_{\mathcal{R} \vert h_t} [ \mathbb{1}(a
    = \arg\max_{a \in \mathcal{A}} Q(a)) ] \end{aligned} $$
- en: where $\pi(a ; \vert ; h_t)$ is the probability of taking action a given the
    history $h_t$.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\pi(a ; \vert ; h_t)$ 是在给定历史 $h_t$ 的情况下采取行动 a 的概率。
- en: For the Bernoulli bandit, it is natural to assume that $Q(a)$ follows a [Beta](https://en.wikipedia.org/wiki/Beta_distribution)
    distribution, as $Q(a)$ is essentially the success probability θ in [Bernoulli](https://en.wikipedia.org/wiki/Bernoulli_distribution)
    distribution. The value of $\text{Beta}(\alpha, \beta)$ is within the interval
    [0, 1]; α and β correspond to the counts when we **succeeded** or **failed** to
    get a reward respectively.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 对于伯努利老虎机，自然地假设 $Q(a)$ 服从[Beta](https://en.wikipedia.org/wiki/Beta_distribution)分布，因为
    $Q(a)$ 本质上是伯努利分布中的成功概率 θ。$\text{Beta}(\alpha, \beta)$ 的值在区间 [0, 1] 内；α 和 β 分别对应于我们**成功**或**失败**获得奖励时的计数。
- en: First, let us initialize the Beta parameters α and β based on some prior knowledge
    or belief for every action. For example,
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们根据某些先验知识或信念为每个动作初始化 Beta 参数 α 和 β。例如，
- en: α = 1 and β = 1; we expect the reward probability to be 50% but we are not very
    confident.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: α = 1，β = 1；我们期望奖励概率为 50%，但我们并不是很有信心。
- en: α = 1000 and β = 9000; we strongly believe that the reward probability is 10%.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: α = 1000，β = 9000；我们坚信奖励概率为 10%。
- en: 'At each time t, we sample an expected reward, $\tilde{Q}(a)$, from the prior
    distribution $\text{Beta}(\alpha_i, \beta_i)$ for every action. The best action
    is selected among samples: $a^{TS}_t = \arg\max_{a \in \mathcal{A}} \tilde{Q}(a)$.
    After the true reward is observed, we can update the Beta distribution accordingly,
    which is essentially doing Bayesian inference to compute the posterior with the
    known prior and the likelihood of getting the sampled data.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个时间点t，我们从先验分布$\text{Beta}(\alpha_i, \beta_i)$中为每个动作采样预期奖励$\tilde{Q}(a)$。在样本中选择最佳动作：$a^{TS}_t
    = \arg\max_{a \in \mathcal{A}} \tilde{Q}(a)$。观察到真实奖励后，我们可以相应地更新Beta分布，这本质上是利用已知先验和获得的样本数据的似然进行贝叶斯推断来计算后验。
- en: $$ \begin{aligned} \alpha_i & \leftarrow \alpha_i + r_t \mathbb{1}[a^{TS}_t
    = a_i] \\ \beta_i & \leftarrow \beta_i + (1-r_t) \mathbb{1}[a^{TS}_t = a_i] \end{aligned}
    $$
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \begin{aligned} \alpha_i & \leftarrow \alpha_i + r_t \mathbb{1}[a^{TS}_t
    = a_i] \\ \beta_i & \leftarrow \beta_i + (1-r_t) \mathbb{1}[a^{TS}_t = a_i] \end{aligned}
    $$
- en: Thompson sampling implements the idea of [probability matching](https://en.wikipedia.org/wiki/Probability_matching).
    Because its reward estimations $\tilde{Q}$ are sampled from posterior distributions,
    each of these probabilities is equivalent to the probability that the corresponding
    action is optimal, conditioned on observed history.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '汤普森抽样实现了[概率匹配](https://en.wikipedia.org/wiki/Probability_matching)的概念。因为其奖励估计$\tilde{Q}$是从后验分布中采样的，每个这些概率等同于在观察历史条件下相应动作是最优的概率。 '
- en: However, for many practical and complex problems, it can be computationally
    intractable to estimate the posterior distributions with observed true rewards
    using Bayesian inference. Thompson sampling still can work out if we are able
    to approximate the posterior distributions using methods like Gibbs sampling,
    Laplace approximate, and the bootstraps. This [tutorial](https://arxiv.org/pdf/1707.02038.pdf)
    presents a comprehensive review; strongly recommend it if you want to learn more
    about Thompson sampling.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，对于许多实际和复杂的问题，使用贝叶斯推断估计观察到的真实奖励的后验分布可能在计算上是棘手的。如果我们能够使用吉布斯采样、拉普拉斯近似和自助法等方法来近似后验分布，汤普森抽样仍然可以奏效。如果您想更多了解汤普森抽样，这个[tutorial](https://arxiv.org/pdf/1707.02038.pdf)提供了全面的评论；强烈推荐阅读。
- en: Case Study
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 案例研究
- en: I implemented the above algorithms in [lilianweng/multi-armed-bandit](https://github.com/lilianweng/multi-armed-bandit).
    A [BernoulliBandit](https://github.com/lilianweng/multi-armed-bandit/blob/master/bandits.py#L13)
    object can be constructed with a list of random or predefined reward probabilities.
    The bandit algorithms are implemented as subclasses of [Solver](https://github.com/lilianweng/multi-armed-bandit/blob/master/solvers.py#L9),
    taking a Bandit object as the target problem. The cumulative regrets are tracked
    in time.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我在[lilianweng/multi-armed-bandit](https://github.com/lilianweng/multi-armed-bandit)中实现了上述算法。可以使用一组随机或预定义的奖励概率构建一个[BernoulliBandit](https://github.com/lilianweng/multi-armed-bandit/blob/master/bandits.py#L13)对象。这些老虎机算法被实现为[Solver](https://github.com/lilianweng/multi-armed-bandit/blob/master/solvers.py#L9)的子类，以Bandit对象作为目标问题。累积遗憾在时间上被跟踪。
- en: '![](../Images/b2fa539ac397bfaf719de72ee6633b63.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b2fa539ac397bfaf719de72ee6633b63.png)'
- en: Fig. 4\. The result of a small experiment on solving a Bernoulli bandit with
    K = 10 slot machines with reward probabilities, {0.0, 0.1, 0.2, ..., 0.9}. Each
    solver runs 10000 steps.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图4\. 对解决具有奖励概率{0.0, 0.1, 0.2, ..., 0.9}的K = 10个老虎机的小实验结果。每个解算器运行10000步。
- en: (Left) The plot of time step vs the cumulative regrets. (Middle) The plot of
    true reward probability vs estimated probability. (Right) The fraction of each
    action is picked during the 10000-step run.*
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: （左）时间步长与累积遗憾的图。 （中）真实奖励概率与估计概率的图。 （右）在10000步运行中每个动作被选择的比例。
- en: Summary
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: We need exploration because information is valuable. In terms of the exploration
    strategies, we can do no exploration at all, focusing on the short-term returns.
    Or we occasionally explore at random. Or even further, we explore and we are picky
    about which options to explore — actions with higher uncertainty are favored because
    they can provide higher information gain.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要探索，因为信息是宝贵的。在探索策略方面，我们可以完全不进行探索，专注于短期回报。或者我们偶尔随机探索。甚至更进一步，我们探索并且挑剔要探索哪些选项——因为具有更高不确定性的动作更受青睐，因为它们可以提供更高的信息增益。
- en: '![](../Images/0802d638db339ffb0b33222591a0a94e.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0802d638db339ffb0b33222591a0a94e.png)'
- en: '* * *'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 'Cited as:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 引用为：
- en: '[PRE0]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: References
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] CS229 Supplemental Lecture notes: [Hoeffding’s inequality](http://cs229.stanford.edu/extra-notes/hoeffding.pdf).'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] CS229 附加讲座笔记: [Hoeffding’s 不等式](http://cs229.stanford.edu/extra-notes/hoeffding.pdf).'
- en: '[2] RL Course by David Silver - Lecture 9: [Exploration and Exploitation](https://youtu.be/sGuiWX07sKw)'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] David Silver 的强化学习课程 - 第9讲: [探索与利用](https://youtu.be/sGuiWX07sKw)'
- en: '[3] Olivier Chapelle and Lihong Li. [“An empirical evaluation of thompson sampling.”](http://papers.nips.cc/paper/4321-an-empirical-evaluation-of-thompson-sampling.pdf)
    NIPS. 2011.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Olivier Chapelle 和 Lihong Li. [“Thompson Sampling 的实证评估.”](http://papers.nips.cc/paper/4321-an-empirical-evaluation-of-thompson-sampling.pdf)
    NIPS. 2011.'
- en: '[4] Russo, Daniel, et al. [“A Tutorial on Thompson Sampling.”](https://arxiv.org/pdf/1707.02038.pdf)
    arXiv:1707.02038 (2017).'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Russo, Daniel, 等人. [“Thompson Sampling 教程.”](https://arxiv.org/pdf/1707.02038.pdf)
    arXiv:1707.02038 (2017).'
