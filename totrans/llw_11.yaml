- en: 'Learning with not Enough Data Part 1: Semi-Supervised Learning'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习不足数据第1部分：半监督学习
- en: 原文：[https://lilianweng.github.io/posts/2021-12-05-semi-supervised/](https://lilianweng.github.io/posts/2021-12-05-semi-supervised/)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://lilianweng.github.io/posts/2021-12-05-semi-supervised/](https://lilianweng.github.io/posts/2021-12-05-semi-supervised/)
- en: When facing a limited amount of labeled data for supervised learning tasks,
    four approaches are commonly discussed.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 面对有限数量的有标签数据进行监督学习任务时，通常讨论四种方法。
- en: '**Pre-training + fine-tuning**: Pre-train a powerful task-agnostic model on
    a large unsupervised data corpus, e.g. [pre-training LMs](https://lilianweng.github.io/posts/2019-01-31-lm/)
    on free text, or pre-training vision models on unlabelled images via [self-supervised
    learning](https://lilianweng.github.io/posts/2019-11-10-self-supervised/), and
    then fine-tune it on the downstream task with a small set of labeled samples.'
  id: totrans-3
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**预训练 + 微调**：在大型无监督数据语料库上预训练一个强大的任务无关模型，例如在自由文本上[预训练语言模型](https://lilianweng.github.io/posts/2019-01-31-lm/)，或者通过[自监督学习](https://lilianweng.github.io/posts/2019-11-10-self-supervised/)在未标记图像上预训练视觉模型，然后用少量标记样本在下游任务上进行微调。'
- en: '**Semi-supervised learning**: Learn from the labelled and unlabeled samples
    together. A lot of research has happened on vision tasks within this approach.'
  id: totrans-4
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**半监督学习**：从有标签和无标签样本一起学习。在这种方法中，对视觉任务进行了大量研究。'
- en: '**Active learning**: Labeling is expensive, but we still want to collect more
    given a cost budget. Active learning learns to select most valuable unlabeled
    samples to be collected next and helps us act smartly with a limited budget.'
  id: totrans-5
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**主动学习**：标记是昂贵的，但我们仍然希望在给定成本预算的情况下收集更多数据。主动学习学习选择最有价值的未标记样本以便下一步收集，并帮助我们在有限的预算下聪明地行动。'
- en: '**Pre-training + dataset auto-generation**: Given a capable pre-trained model,
    we can utilize it to auto-generate a lot more labeled samples. This has been especially
    popular within the language domain driven by the success of few-shot learning.'
  id: totrans-6
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**预训练 + 数据集自动生成**：给定一个能力强大的预训练模型，我们可以利用它自动生成更多有标签样本。这在语言领域特别受欢迎，受到少样本学习成功的推动。'
- en: I plan to write a series of posts on the topic of “Learning with not enough
    data”. Part 1 is on *Semi-Supervised Learning*.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我计划写一系列关于“学习不足数据”的主题的文章。第一部分是关于*半监督学习*。
- en: What is semi-supervised learning?
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是半监督学习？
- en: Semi-supervised learning uses both labeled and unlabeled data to train a model.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 半监督学习使用有标签和无标签数据来训练模型。
- en: Interestingly most existing literature on semi-supervised learning focuses on
    vision tasks. And instead pre-training + fine-tuning is a more common paradigm
    for language tasks.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，大多数现有的关于半监督学习的文献都集中在视觉任务上。而对于语言任务，预训练 + 微调是一种更常见的范式。
- en: 'All the methods introduced in this post have a loss combining two parts: $\mathcal{L}
    = \mathcal{L}_s + \mu(t) \mathcal{L}_u$. The supervised loss $\mathcal{L}_s$ is
    easy to get given all the labeled examples. We will focus on how the unsupervised
    loss $\mathcal{L}_u$ is designed. A common choice of the weighting term $\mu(t)$
    is a ramp function increasing the importance of $\mathcal{L}_u$ in time, where
    $t$ is the training step.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本文介绍的所有方法都有一个结合两部分的损失：$\mathcal{L} = \mathcal{L}_s + \mu(t) \mathcal{L}_u$。监督损失$\mathcal{L}_s$很容易得到，因为有所有标记样本。我们将重点关注无监督损失$\mathcal{L}_u$的设计。一个常见的权重项$\mu(t)$的选择是一个斜坡函数，随着时间增加$\mathcal{L}_u$的重要性，其中$t$是训练步骤。
- en: '*Disclaimer*: The post is not gonna cover semi-supervised methods with focus
    on model architecture modification. Check [this survey](https://arxiv.org/abs/2006.05278)
    for how to use generative models and graph-based methods in semi-supervised learning.'
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*免责声明*：本文不会涵盖重点放在模型架构修改上的半监督方法。查看[这份调查报告](https://arxiv.org/abs/2006.05278)了解如何在半监督学习中使用生成模型和基于图的方法。'
- en: Notations
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 符号
- en: '| Symbol | Meaning |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '| 符号 | 含义 |'
- en: '| --- | --- |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| $L$ | Number of unique labels. |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| $L$ | 唯一标签的数量。 |'
- en: '| $(\mathbf{x}^l, y) \sim \mathcal{X}, y \in \{0, 1\}^L$ | Labeled dataset.
    $y$ is a one-hot representation of the true label. |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| $(\mathbf{x}^l, y) \sim \mathcal{X}, y \in \{0, 1\}^L$ | 有标签数据集。$y$是真实标签的独热表示。
    |'
- en: '| $\mathbf{u} \sim \mathcal{U}$ | Unlabeled dataset. |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| $\mathbf{u} \sim \mathcal{U}$ | 未标记数据集。 |'
- en: '| $\mathcal{D} = \mathcal{X} \cup \mathcal{U}$ | The entire dataset, including
    both labeled and unlabeled examples. |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| $\mathcal{D} = \mathcal{X} \cup \mathcal{U}$ | 整个数据集，包括有标签和无标签样本。 |'
- en: '| $\mathbf{x}$ | Any sample which can be either labeled or unlabeled. |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| $\mathbf{x}$ | 任何样本，可以是有标签或无标签的。 |'
- en: '| $\bar{\mathbf{x}}$ | $\mathbf{x}$ with augmentation applied. |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| $\bar{\mathbf{x}}$ | 应用增强后的$\mathbf{x}$。 |'
- en: '| $\mathbf{x}_i$ | The $i$-th sample. |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| $\mathbf{x}_i$ | 第 $i$ 个样本。'
- en: '| $\mathcal{L}$, $\mathcal{L}_s$, $\mathcal{L}_u$ | Loss, supervised loss,
    and unsupervised loss. |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| $\mathcal{L}$, $\mathcal{L}_s$, $\mathcal{L}_u$ | 损失、监督损失和无监督损失。'
- en: '| $\mu(t)$ | The unsupervised loss weight, increasing in time. |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| $\mu(t)$ | 无监督损失权重，随时间增加。'
- en: '| $p(y \vert \mathbf{x}), p_\theta(y \vert \mathbf{x})$ | The conditional probability
    over the label set given the input. |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| $p(y \vert \mathbf{x}), p_\theta(y \vert \mathbf{x})$ | 给定输入的标签集的条件概率。'
- en: '| $f_\theta(.)$ | The implemented neural network with weights $\theta$, the
    model that we want to train. |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| $f_\theta(.)$ | 具有权重 $\theta$ 的实现的神经网络，我们要训练的模型。'
- en: '| $\mathbf{z} = f_\theta(\mathbf{x})$ | A vector of logits output by $f$. |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| $\mathbf{z} = f_\theta(\mathbf{x})$ | $f$ 输出的 logits 向量。'
- en: '| $\hat{y} = \text{softmax}(\mathbf{z})$ | The predicted label distribution.
    |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| $\hat{y} = \text{softmax}(\mathbf{z})$ | 预测的标签分布。'
- en: '| $D[.,.]$ | A distance function between two distributions, such as MSE, cross
    entropy, KL divergence, etc. |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| $D[.,.]$ | 两个分布之间的距离函数，例如 MSE、交叉熵、KL 散度等。'
- en: '| $\beta$ | EMA weighting hyperparameter for [teacher](#mean-teachers) model
    weights. |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| $\beta$ | [教师](#mean-teachers) 模型权重的 EMA 加权超参数。'
- en: '| $\alpha, \lambda$ | Parameters for MixUp, $\lambda \sim \text{Beta}(\alpha,
    \alpha)$. |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| $\alpha, \lambda$ | MixUp 的参数，$\lambda \sim \text{Beta}(\alpha, \alpha)$。'
- en: '| $T$ | Temperature for sharpening the predicted distribution. |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| $T$ | 用于加强预测分布的温度。'
- en: '| $\tau$ | A confidence threshold for selecting the qualified prediction. |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| $\tau$ | 用于选择合格预测的置信阈值。'
- en: Hypotheses
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 假设
- en: Several hypotheses have been discussed in literature to support certain design
    decisions in semi-supervised learning methods.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 文献中讨论了几个假设，以支持半监督学习方法中的某些设计决策。
- en: 'H1: **Smoothness Assumptions**: If two data samples are close in a high-density
    region of the feature space, their labels should be the same or very similar.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'H1: **平滑性假设**：如果两个数据样本在特征空间的高密度区域中接近，它们的标签应该相同或非常相似。'
- en: 'H2: **Cluster Assumptions**: The feature space has both dense regions and sparse
    regions. Densely grouped data points naturally form a cluster. Samples in the
    same cluster are expected to have the same label. This is a small extension of
    H1.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'H2: **聚类假设**：特征空间既有密集区域又有稀疏区域。密集分组的数据点自然形成一个聚类。同一聚类中的样本预期具有相同的标签。这是 H1 的一个小扩展。'
- en: 'H3: **Low-density Separation Assumptions**: The decision boundary between classes
    tends to be located in the sparse, low density regions, because otherwise the
    decision boundary would cut a high-density cluster into two classes, corresponding
    to two clusters, which invalidates H1 and H2.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'H3: **低密度分离假设**：类之间的决策边界倾向于位于稀疏的低密度区域，否则决策边界将把高密度聚类切割成两个类，对应于两个聚类，这将使 H1 和
    H2 无效。'
- en: 'H4: **Manifold Assumptions**: The high-dimensional data tends to locate on
    a low-dimensional manifold. Even though real-world data might be observed in very
    high dimensions (e.g. such as images of real-world objects/scenes), they actually
    can be captured by a lower dimensional manifold where certain attributes are captured
    and similar points are grouped closely (e.g. images of real-world objects/scenes
    are not drawn from a uniform distribution over all pixel combinations). This enables
    us to learn a more efficient representation for us to discover and measure similarity
    between unlabeled data points. This is also the foundation for representation
    learning. [see [a helpful link](https://stats.stackexchange.com/questions/66939/what-is-the-manifold-assumption-in-semi-supervised-learning)].'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'H4: **流形假设**：高维数据倾向于位于低维流形上。即使真实世界的数据可能在非常高的维度中观察到（例如真实世界物体/场景的图像），它们实际上可以被捕捉在一个较低维度的流形上，其中某些属性被捕捉，相似的点被紧密分组（例如真实世界物体/场景的图像并不是从所有像素组合的均匀分布中绘制的）。这使我们能够学习一个更有效的表示，以便发现和衡量未标记数据点之间的相似性。这也是表示学习的基础。[参见
    [一个有用的链接](https://stats.stackexchange.com/questions/66939/what-is-the-manifold-assumption-in-semi-supervised-learning)]。'
- en: Consistency Regularization
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一致性正则化
- en: '**Consistency Regularization**, also known as **Consistency Training**, assumes
    that randomness within the neural network (e.g. with Dropout) or data augmentation
    transformations should not modify model predictions given the same input. Every
    method in this section has a consistency regularization loss as $\mathcal{L}_u$.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**一致性正则化**，也称为**一致性训练**，假设神经网络内部的随机性（例如使用 Dropout）或数据增强转换不应该在给定相同输入时修改模型预测。本节中的每种方法都有一致性正则化损失
    $\mathcal{L}_u$。'
- en: This idea has been adopted in several [self-supervised](https://lilianweng.github.io/posts/2019-11-10-self-supervised/)
    [learning](https://lilianweng.github.io/posts/2021-05-31-contrastive/) methods,
    such as SimCLR, BYOL, SimCSE, etc. Different augmented versions of the same sample
    should result in the same representation. [Cross-view training](https://lilianweng.github.io/posts/2019-01-31-lm/#cross-view-training)
    in language modeling and multi-view learning in self-supervised learning all share
    the same motivation.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这个想法已经被应用在几种[自监督](https://lilianweng.github.io/posts/2019-11-10-self-supervised/)
    [学习](https://lilianweng.github.io/posts/2021-05-31-contrastive/) 方法中，如SimCLR、BYOL、SimCSE等。同一样本的不同增强版本应该产生相同的表示。[交叉视图训练](https://lilianweng.github.io/posts/2019-01-31-lm/#cross-view-training)在语言建模中以及自监督学习中的多视图学习都有相同的动机。
- en: Π-model
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Π-模型
- en: '![](../Images/77318cadcc92c08dc4d64fb52e3dd6a2.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/77318cadcc92c08dc4d64fb52e3dd6a2.png)'
- en: 'Fig. 1\. Overview of the Π-model. Two versions of the same input with different
    stochastic augmentation and dropout masks pass through the network and the outputs
    are expected to be consistent. (Image source: [Laine & Aila (2017)](https://arxiv.org/abs/1610.02242))'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图1\. Π-模型的概览。通过网络传递具有不同随机增强和dropout掩模的相同输入的两个版本，并期望输出保持一致。（图片来源：[Laine & Aila
    (2017)](https://arxiv.org/abs/1610.02242)）
- en: '[Sajjadi et al. (2016)](https://arxiv.org/abs/1606.04586) proposed an unsupervised
    learning loss to minimize the difference between two passes through the network
    with stochastic transformations (e.g. dropout, random max-pooling) for the same
    data point. The label is not explicitly used, so the loss can be applied to unlabeled
    dataset. [Laine & Aila (2017)](https://arxiv.org/abs/1610.02242) later coined
    the name, **Π-Model**, for such a setup.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '[Sajjadi et al. (2016)](https://arxiv.org/abs/1606.04586) 提出了一种无监督学习损失，用于最小化通过网络进行两次随机变换（例如dropout，随机最大池化）的相同数据点之间的差异。标签没有被明确使用，因此该损失可以应用于无标签数据集。[Laine
    & Aila (2017)](https://arxiv.org/abs/1610.02242) 后来为这种设置创造了名字，**Π-模型**。'
- en: $$ \mathcal{L}_u^\Pi = \sum_{\mathbf{x} \in \mathcal{D}} \text{MSE}(f_\theta(\mathbf{x}),
    f'_\theta(\mathbf{x})) $$
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \mathcal{L}_u^\Pi = \sum_{\mathbf{x} \in \mathcal{D}} \text{MSE}(f_\theta(\mathbf{x}),
    f'_\theta(\mathbf{x})) $$
- en: where $f’$ is the same neural network with different stochastic augmentation
    or dropout masks applied. This loss utilizes the entire dataset.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$f'$是应用不同随机增强或dropout掩模的相同神经网络。这种损失利用整个数据集。
- en: Temporal ensembling
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 时间集成
- en: '![](../Images/0a485a0198284ef971cb47e3d92b39f9.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0a485a0198284ef971cb47e3d92b39f9.png)'
- en: 'Fig. 2\. Overview of Temporal Ensembling. The per-sample EMA label prediction
    is the learning target. (Image source: [Laine & Aila (2017)](https://arxiv.org/abs/1610.02242))'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图2\. 时间集成的概览。每个样本的EMA标签预测是学习目标。（图片来源：[Laine & Aila (2017)](https://arxiv.org/abs/1610.02242)）
- en: Π-model requests the network to run two passes per sample, doubling the computation
    cost. To reduce the cost, **Temporal Ensembling** ([Laine & Aila 2017](https://arxiv.org/abs/1610.02242))
    maintains an exponential moving average (EMA) of the model prediction in time
    per training sample $\tilde{\mathbf{z}}_i$ as the learning target, which is only
    evaluated and updated once per epoch. Because the ensemble output $\tilde{\mathbf{z}}_i$
    is initialized to $\mathbf{0}$, it is normalized by $(1-\alpha^t)$ to correct
    this startup bias. Adam optimizer has such [bias correction](https://stats.stackexchange.com/questions/232741/why-is-it-important-to-include-a-bias-correction-term-for-the-adam-optimizer-for)
    terms for the same reason.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: Π-模型要求网络对每个样本运行两次，使计算成本加倍。为了减少成本，**时间集成**（[Laine & Aila 2017](https://arxiv.org/abs/1610.02242)）维护每个训练样本的模型预测的指数移动平均值（EMA）作为学习目标，该值仅在每个时期评估和更新一次。由于集成输出$\tilde{\mathbf{z}}_i$初始化为$\mathbf{0}$，因此通过$(1-\alpha^t)$进行归一化以纠正这种启动偏差。Adam优化器也有这种[偏差校正](https://stats.stackexchange.com/questions/232741/why-is-it-important-to-include-a-bias-correction-term-for-the-adam-optimizer-for)项出于同样的原因。
- en: $$ \tilde{\mathbf{z}}^{(t)}_i = \frac{\alpha \tilde{\mathbf{z}}^{(t-1)}_i +
    (1-\alpha) \mathbf{z}_i}{1-\alpha^t} $$
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \tilde{\mathbf{z}}^{(t)}_i = \frac{\alpha \tilde{\mathbf{z}}^{(t-1)}_i +
    (1-\alpha) \mathbf{z}_i}{1-\alpha^t} $$
- en: where $\tilde{\mathbf{z}}^{(t)}$ is the ensemble prediction at epoch $t$ and
    $\mathbf{z}_i$ is the model prediction in the current round. Note that since $\tilde{\mathbf{z}}^{(0)}
    = \mathbf{0}$, with correction, $\tilde{\mathbf{z}}^{(1)}$ is simply equivalent
    to $\mathbf{z}_i$ at epoch 1.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\tilde{\mathbf{z}}^{(t)}$是第$t$个时期的集成预测，$\mathbf{z}_i$是当前轮次的模型预测。请注意，由于$\tilde{\mathbf{z}}^{(0)}
    = \mathbf{0}$，经过校正，$\tilde{\mathbf{z}}^{(1)}$在第1个时期简单等同于$\mathbf{z}_i$。
- en: Mean teachers
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 严厉的老师
- en: '![](../Images/225a14bb8ab363061255054c7856bd8f.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/225a14bb8ab363061255054c7856bd8f.png)'
- en: 'Fig. 3\. Overview of the Mean Teacher framework. (Image source: [Tarvaninen
    & Valpola, 2017](https://arxiv.org/abs/1703.01780))'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图3. [Mean Teacher框架概述](https://arxiv.org/abs/1703.01780)。（图片来源：Tarvaninen &
    Valpola, 2017）
- en: 'Temporal Ensembling keeps track of an EMA of label predictions for each training
    sample as a learning target. However, this label prediction only changes *every
    epoch*, making the approach clumsy when the training dataset is large. **Mean
    Teacher** ([Tarvaninen & Valpola, 2017](https://arxiv.org/abs/1703.01780)) is
    proposed to overcome the slowness of target update by tracking the moving average
    of model weights instead of model outputs. Let’s call the original model with
    weights $\theta$ as the *student* model and the model with moving averaged weights
    $\theta’$ across consecutive student models as the *mean teacher*: $\theta’ \gets
    \beta \theta’ + (1-\beta)\theta$'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 时间集成跟踪每个训练样本的标签预测的EMA作为学习目标。然而，这种标签预测仅在*每个时代*更改，使得当训练数据集很大时，这种方法变得笨拙。**Mean
    Teacher**（[Tarvaninen & Valpola, 2017](https://arxiv.org/abs/1703.01780)）被提出来克服目标更新速度慢的问题，通过跟踪模型权重的移动平均而不是模型输出。让我们将具有权重$\theta$的原始模型称为*学生*模型，将连续学生模型之间的移动平均权重$\theta'$的模型称为*平均老师*：$\theta'
    \gets \beta \theta' + (1-\beta)\theta$
- en: The consistency regularization loss is the distance between predictions by the
    student and teacher and the student-teacher gap should be minimized. The mean
    teacher is expected to provide more accurate predictions than the student. It
    got confirmed in the empirical experiments, as shown in Fig. 4.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 一致性正则化损失是学生和老师的预测之间的距离，学生-老师间隙应该被最小化。平均老师预计会提供比学生更准确的预测。这在经验实验中得到了确认，如图4所示。
- en: '![](../Images/85c99bdf5aac786daeadc1adbcd16ec3.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/85c99bdf5aac786daeadc1adbcd16ec3.png)'
- en: 'Fig. 4\. Classification error on SVHN of Mean Teacher and the Π Model. The
    mean teacher (in orange) has better performance than the student model (in blue).
    (Image source: [Tarvaninen & Valpola, 2017](https://arxiv.org/abs/1703.01780))'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图4. Mean Teacher和Π模型在SVHN上的分类错误。平均老师（橙色）比学生模型（蓝色）表现更好。（图片来源：Tarvaninen & Valpola,
    2017）
- en: According to their ablation studies,
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 根据他们的消融研究，
- en: Input augmentation (e.g. random flips of input images, Gaussian noise) or student
    model dropout is necessary for good performance. Dropout is not needed on the
    teacher model.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入增强（例如输入图像的随机翻转，高斯噪声）或学生模型的辍学对于良好的性能是必要的。老师模型不需要辍学。
- en: The performance is sensitive to the EMA decay hyperparameter $\beta$. A good
    strategy is to use a small $\beta=0.99$ during the ramp up stage and a larger
    $\beta=0.999$ in the later stage when the student model improvement slows down.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 性能对EMA衰减超参数$\beta$敏感。一个好的策略是在上升阶段使用小的$\beta=0.99$，当学生模型改进减缓时，在后期阶段使用较大的$\beta=0.999$。
- en: They found that MSE as the consistency cost function performs better than other
    cost functions like KL divergence.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 他们发现作为一致性成本函数的MSE比其他成本函数如KL散度表现更好。
- en: Noisy samples as learning targets
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 以嘈杂样本作为学习目标
- en: Several recent consistency training methods learn to minimize prediction difference
    between the original unlabeled sample and its corresponding augmented version.
    It is quite similar to the Π-model but the consistency regularization loss is
    *only* applied to the unlabeled data.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 几种最近的一致性训练方法学习最小化原始未标记样本与其对应增强版本之间的预测差异。这与Π模型非常相似，但一致性正则化损失*仅*应用于未标记数据。
- en: '![](../Images/9f92d5d3f3abc166ab038086168483c7.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9f92d5d3f3abc166ab038086168483c7.png)'
- en: Fig. 5\. Consistency training with noisy samples.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图5. 使用嘈杂样本进行一致性训练。
- en: Adversarial Training ([Goodfellow et al. 2014](https://arxiv.org/abs/1412.6572))
    applies adversarial noise onto the input and trains the model to be robust to
    such adversarial attack. The setup works in supervised learning,
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗训练（[Goodfellow等人，2014](https://arxiv.org/abs/1412.6572)）将对抗性噪声施加到输入上，并训练模型对此类对抗性攻击具有鲁棒性。该设置适用于监督学习，
- en: $$ \begin{aligned} \mathcal{L}_\text{adv}(\mathbf{x}^l, \theta) &= D[q(y\mid
    \mathbf{x}^l), p_\theta(y\mid \mathbf{x}^l + r_\text{adv})] \\ r_\text{adv} &=
    {\arg\max}_{r; \|r\| \leq \epsilon} D[q(y\mid \mathbf{x}^l), p_\theta(y\mid \mathbf{x}^l
    + r_\text{adv})] \\ r_\text{adv} &\approx \epsilon \frac{g}{\|g\|_2} \approx \epsilon\text{sign}(g)\quad\text{where
    }g = \nabla_{r} D[y, p_\theta(y\mid \mathbf{x}^l + r)] \end{aligned} $$
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \begin{aligned} \mathcal{L}_\text{adv}(\mathbf{x}^l, \theta) &= D[q(y\mid
    \mathbf{x}^l), p_\theta(y\mid \mathbf{x}^l + r_\text{adv})] \\ r_\text{adv} &=
    {\arg\max}_{r; \|r\| \leq \epsilon} D[q(y\mid \mathbf{x}^l), p_\theta(y\mid \mathbf{x}^l
    + r_\text{adv})] \\ r_\text{adv} &\approx \epsilon \frac{g}{\|g\|_2} \approx \epsilon\text{sign}(g)\quad\text{where
    }g = \nabla_{r} D[y, p_\theta(y\mid \mathbf{x}^l + r)] \end{aligned} $$
- en: where $q(y \mid \mathbf{x}^l)$ is the true distribution, approximated by one-hot
    encoding of the ground truth label, $y$. $p_\theta(y \mid \mathbf{x}^l)$ is the
    model prediction. $D[.,.]$ is a distance function measuring the divergence between
    two distributions.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $q(y \mid \mathbf{x}^l)$ 是真实分布，通过独热编码地面实际标签 $y$ 近似得到。$p_\theta(y \mid \mathbf{x}^l)$
    是模型预测。$D[.,.]$ 是衡量两个分布之间差异的距离函数。
- en: '**Virtual Adversarial Training** (**VAT**; [Miyato et al. 2018](https://arxiv.org/abs/1704.03976))
    extends the idea to work in semi-supervised learning. Because $q(y \mid \mathbf{x}^l)$
    is unknown, VAT replaces it with the current model prediction for the original
    input with the current weights $\hat{\theta}$. Note that $\hat{\theta}$ is a fixed
    copy of model weights, so there is no gradient update on $\hat{\theta}$.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '**虚拟对抗训练**（**VAT**；[Miyato et al. 2018](https://arxiv.org/abs/1704.03976)）将这一思想扩展到半监督学习中。由于
    $q(y \mid \mathbf{x}^l)$ 是未知的，VAT 用当前模型对原始输入的预测替换它，使用当前权重 $\hat{\theta}$。注意 $\hat{\theta}$
    是模型权重的固定副本，因此 $\hat{\theta}$ 上没有梯度更新。'
- en: $$ \begin{aligned} \mathcal{L}_u^\text{VAT}(\mathbf{x}, \theta) &= D[p_{\hat{\theta}}(y\mid
    \mathbf{x}), p_\theta(y\mid \mathbf{x} + r_\text{vadv})] \\ r_\text{vadv} &= {\arg\max}_{r;
    \|r\| \leq \epsilon} D[p_{\hat{\theta}}(y\mid \mathbf{x}), p_\theta(y\mid \mathbf{x}
    + r)] \end{aligned} $$
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \begin{aligned} \mathcal{L}_u^\text{VAT}(\mathbf{x}, \theta) &= D[p_{\hat{\theta}}(y\mid
    \mathbf{x}), p_\theta(y\mid \mathbf{x} + r_\text{vadv})] \\ r_\text{vadv} &= {\arg\max}_{r;
    \|r\| \leq \epsilon} D[p_{\hat{\theta}}(y\mid \mathbf{x}), p_\theta(y\mid \mathbf{x}
    + r)] \end{aligned} $$
- en: The VAT loss applies to both labeled and unlabeled samples. It is a negative
    smoothness measure of the current model’s prediction manifold at each data point.
    The optimization of such loss motivates the manifold to be smoother.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: VAT 损失适用于有标签和无标签样本。它是当前模型在每个数据点处预测流形的负平滑度量。这种损失的优化促使流形更加平滑。
- en: '**Interpolation Consistency Training** (**ICT**; [Verma et al. 2019](https://arxiv.org/abs/1903.03825))
    enhances the dataset by adding more interpolations of data points and expects
    the model prediction to be consistent with interpolations of the corresponding
    labels. MixUp ([Zheng et al. 2018](https://arxiv.org/abs/1710.09412)) operation
    mixes two images via a simple weighted sum and combines it with label smoothing.
    Following the idea of MixUp, ICT expects the prediction model to produce a label
    on a mixup sample to match the interpolation of predictions of corresponding inputs:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '**插值一致性训练**（**ICT**；[Verma et al. 2019](https://arxiv.org/abs/1903.03825)）通过添加更多数据点的插值来增强数据集，并期望模型预测与相应标签的插值一致。MixUp（[Zheng
    et al. 2018](https://arxiv.org/abs/1710.09412)）操作通过简单的加权和混合两个图像，并结合标签平滑。在 MixUp
    的思想指导下，ICT 期望预测模型在混合样本上产生一个标签，以匹配相应输入的预测插值：'
- en: $$ \begin{aligned} \text{mixup}_\lambda (\mathbf{x}_i, \mathbf{x}_j) &= \lambda
    \mathbf{x}_i + (1-\lambda)\mathbf{x}_j \\ p(\text{mixup}_\lambda (y \mid \mathbf{x}_i,
    \mathbf{x}_j)) &\approx \lambda p(y \mid \mathbf{x}_i) + (1-\lambda) p(y \mid
    \mathbf{x}_j) \end{aligned} $$
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \begin{aligned} \text{mixup}_\lambda (\mathbf{x}_i, \mathbf{x}_j) &= \lambda
    \mathbf{x}_i + (1-\lambda)\mathbf{x}_j \\ p(\text{mixup}_\lambda (y \mid \mathbf{x}_i,
    \mathbf{x}_j)) &\approx \lambda p(y \mid \mathbf{x}_i) + (1-\lambda) p(y \mid
    \mathbf{x}_j) \end{aligned} $$
- en: where $\theta’$ is a moving average of $\theta$, which is a [mean teacher](#mean-teachers).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\theta'$ 是 $\theta$ 的移动平均值，是 [mean teacher](#mean-teachers)。
- en: '![](../Images/cab30fd58d9e2054de596de316b8b10e.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cab30fd58d9e2054de596de316b8b10e.png)'
- en: 'Fig. 6\. Overview of Interpolation Consistency Training. MixUp is applied to
    produce more interpolated samples with interpolated labels as learning targets.
    (Image source: [Verma et al. 2019](https://arxiv.org/abs/1903.03825))'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: Fig. 6\. Interpolation Consistency Training 概述。MixUp 用于生成更多插值样本，插值标签作为学习目标。（图片来源：[Verma
    et al. 2019](https://arxiv.org/abs/1903.03825)）
- en: Because the probability of two randomly selected unlabeled samples belonging
    to different classes is high (e.g. There are 1000 object classes in ImageNet),
    the interpolation by applying a mixup between two random unlabeled samples is
    likely to happen around the decision boundary. According to the low-density separation
    [assumptions](#hypotheses), the decision boundary tends to locate in the low density
    regions.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 因为两个随机选择的未标记样本属于不同类别的概率很高（例如ImageNet中有1000个对象类别），所以通过在两个随机未标记样本之间应用混合可能会发生在决策边界附近。
    根据低密度分离的[假设](#hypotheses)，决策边界倾向于位于低密度区域。
- en: $$ \mathcal{L}^\text{ICT}_{u} = \mathbb{E}_{\mathbf{u}_i, \mathbf{u}_j \sim
    \mathcal{U}} \mathbb{E}_{\lambda \sim \text{Beta}(\alpha, \alpha)} D[p_\theta(y
    \mid \text{mixup}_\lambda (\mathbf{u}_i, \mathbf{u}_j)), \text{mixup}_\lambda(p_{\theta’}(y
    \mid \mathbf{u}_i), p_{\theta'}(y \mid \mathbf{u}_j)] $$
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \mathcal{L}^\text{ICT}_{u} = \mathbb{E}_{\mathbf{u}_i, \mathbf{u}_j \sim
    \mathcal{U}} \mathbb{E}_{\lambda \sim \text{Beta}(\alpha, \alpha)} D[p_\theta(y
    \mid \text{mixup}_\lambda (\mathbf{u}_i, \mathbf{u}_j)), \text{mixup}_\lambda(p_{\theta’}(y
    \mid \mathbf{u}_i), p_{\theta'}(y \mid \mathbf{u}_j)] $$
- en: where $\theta’$ is a moving average of $\theta$.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\theta’$是$\theta$的移动平均值。
- en: Similar to VAT, **Unsupervised Data Augmentation** (**UDA**; [Xie et al. 2020](https://arxiv.org/abs/1904.12848))
    learns to predict the same output for an unlabeled example and the augmented one.
    UDA especially focuses on studying how the *“quality”* of noise can impact the
    semi-supervised learning performance with consistency training. It is crucial
    to use advanced data augmentation methods for producing meaningful and effective
    noisy samples. Good data augmentation should produce valid (i.e. does not change
    the label) and diverse noise, and carry targeted inductive biases.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 与VAT类似，**无监督数据增强** (**UDA**; [Xie et al. 2020](https://arxiv.org/abs/1904.12848))
    学习为未标记的示例和增强后的示例预测相同的输出。 UDA特别关注研究噪声的*“质量”*如何影响一致性训练的半监督学习性能。 使用先进的数据增强方法产生有意义且有效的噪声样本至关重要。
    良好的数据增强应产生有效的（即不更改标签）和多样化的噪声，并携带有针对性的归纳偏差。
- en: For images, UDA adopts RandAugment ([Cubuk et al. 2019](https://arxiv.org/abs/1909.13719))
    which uniformly samples augmentation operations available in [PIL](https://pillow.readthedocs.io/en/stable/),
    no learning or optimization, so it is much cheaper than AutoAugment.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 对于图像，UDA采用了RandAugment ([Cubuk et al. 2019](https://arxiv.org/abs/1909.13719))，它均匀地采样[PIL](https://pillow.readthedocs.io/en/stable/)中可用的增强操作，无需学习或优化，因此比AutoAugment要便宜得多。
- en: '![](../Images/846a154edf9b826d24f603b2394105c2.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/846a154edf9b826d24f603b2394105c2.png)'
- en: 'Fig. 7\. Comparison of various semi-supervised learning methods on CIFAR-10
    classification. Fully supervised Wide-ResNet-28-2 and PyramidNet+ShakeDrop have
    an error rate of **5.4** and **2.7** respectively when trained on 50,000 examples
    without RandAugment. (Image source: [Xie et al. 2020](https://arxiv.org/abs/1904.12848))'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 'Fig. 7\. CIFAR-10分类上各种半监督学习方法的比较。 在没有RandAugment的情况下，完全监督的Wide-ResNet-28-2和PyramidNet+ShakeDrop在训练50,000个示例时的错误率分别为**5.4**和**2.7**。
    (图片来源: [Xie et al. 2020](https://arxiv.org/abs/1904.12848))'
- en: For language, UDA combines back-translation and TF-IDF based word replacement.
    Back-translation preserves the high-level meaning but may not retain certain words,
    while TF-IDF based word replacement drops uninformative words with low TF-IDF
    scores. In the experiments on language tasks, they found UDA to be complementary
    to transfer learning and representation learning; For example, BERT fine-tuned
    (i.e. $\text{BERT}_\text{FINETUNE}$ in Fig. 8.) on in-domain unlabeled data can
    further improve the performance.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 对于语言，UDA结合了反向翻译和基于TF-IDF的词替换。 反向翻译保留了高层次的含义，但可能不保留某些词，而基于TF-IDF的词替换则删除具有低TF-IDF分数的无信息词。
    在语言任务的实验中，他们发现UDA对于迁移学习和表示学习是互补的； 例如，在领域内未标记数据上微调的BERT（即图8中的$\text{BERT}_\text{FINETUNE}$）可以进一步提高性能。
- en: '![](../Images/79d1f8e275f2831f1d00b51c08125376.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/79d1f8e275f2831f1d00b51c08125376.png)'
- en: 'Fig. 8\. Comparison of UDA with different initialization configurations on
    various text classification tasks. (Image source: [Xie et al. 2020](https://arxiv.org/abs/1904.12848))'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 'Fig. 8\. UDA在不同初始化配置下在各种文本分类任务上的比较。 (图片来源: [Xie et al. 2020](https://arxiv.org/abs/1904.12848))'
- en: When calculating $\mathcal{L}_u$, UDA found two training techniques to help
    improve the results.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算$\mathcal{L}_u$时，UDA发现两种训练技巧有助于改善结果。
- en: '*Low confidence masking*: Mask out examples with low prediction confidence
    if lower than a threshold $\tau$.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*低置信度掩码*：如果低于阈值$\tau$，则屏蔽预测置信度低的示例。'
- en: '*Sharpening prediction distribution*: Use a low temperature $T$ in softmax
    to sharpen the predicted probability distribution.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*加强预测分布*：在softmax中使用低温$T$来加强预测的概率分布。'
- en: '*In-domain data filtration*: In order to extract more in-domain data from a
    large out-of-domain dataset, they trained a classifier to predict in-domain labels
    and then retain samples with high confidence predictions as in-domain candidates.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*领域内数据过滤*：为了从大型领域外数据集中提取更多领域内数据，他们训练了一个分类器来预测领域内标签，然后保留具有高置信度预测的样本作为领域内候选样本。'
- en: $$ \begin{aligned} &\mathcal{L}_u^\text{UDA} = \mathbb{1}[\max_{y'} p_{\hat{\theta}}(y'\mid
    \mathbf{x}) > \tau ] \cdot D[p^\text{(sharp)}_{\hat{\theta}}(y \mid \mathbf{x};
    T), p_\theta(y \mid \bar{\mathbf{x}})] \\ &\text{where } p_{\hat{\theta}}^\text{(sharp)}(y
    \mid \mathbf{x}; T) = \frac{\exp(z^{(y)} / T)}{ \sum_{y'} \exp(z^{(y')} / T) }
    \end{aligned} $$
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \begin{aligned} &\mathcal{L}_u^\text{UDA} = \mathbb{1}[\max_{y'} p_{\hat{\theta}}(y'\mid
    \mathbf{x}) > \tau ] \cdot D[p^\text{(sharp)}_{\hat{\theta}}(y \mid \mathbf{x};
    T), p_\theta(y \mid \bar{\mathbf{x}})] \\ &\text{其中 } p_{\hat{\theta}}^\text{(sharp)}(y
    \mid \mathbf{x}; T) = \frac{\exp(z^{(y)} / T)}{ \sum_{y'} \exp(z^{(y')} / T) }
    \end{aligned} $$
- en: where $\hat{\theta}$ is a fixed copy of model weights, same as in VAT, so no
    gradient update, and $\bar{\mathbf{x}}$ is the augmented data point. $\tau$ is
    the prediction confidence threshold and $T$ is the distribution sharpening temperature.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\hat{\theta}$是模型权重的固定副本，与VAT中相同，因此没有梯度更新，$\bar{\mathbf{x}}$是增强的数据点。$\tau$是预测置信度阈值，$T$是分布加热温度。
- en: Pseudo Labeling
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 伪标签
- en: '**Pseudo Labeling** ([Lee 2013](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.664.3543&rep=rep1&type=pdf))
    assigns fake labels to unlabeled samples based on the maximum softmax probabilities
    predicted by the current model and then trains the model on both labeled and unlabeled
    samples simultaneously in a pure supervised setup.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '**伪标签**（[Lee，2013](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.664.3543&rep=rep1&type=pdf)）根据当前模型预测的最大softmax概率为未标记样本分配虚假标签，然后在纯监督设置中同时对标记和未标记样本进行训练。'
- en: Why could pseudo labels work? Pseudo label is in effect equivalent to *Entropy
    Regularization* ([Grandvalet & Bengio 2004](https://papers.nips.cc/paper/2004/hash/96f2b50b5d3613adf9c27049b2a888c7-Abstract.html)),
    which minimizes the conditional entropy of class probabilities for unlabeled data
    to favor low density separation between classes. In other words, the predicted
    class probabilities is in fact a measure of class overlap, minimizing the entropy
    is equivalent to reduced class overlap and thus low density separation.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么伪标签有效？伪标签实际上等效于*熵正则化*（[Grandvalet＆Bengio，2004](https://papers.nips.cc/paper/2004/hash/96f2b50b5d3613adf9c27049b2a888c7-Abstract.html)），它最小化未标记数据的类概率的条件熵，以支持类之间的低密度分离。换句话说，预测的类概率实际上是类重叠的度量，最小化熵等效于减少类重叠，从而实现低密度分离。
- en: '![](../Images/f9dbdc8c3843eb4f3ec7cb8664ca63d9.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f9dbdc8c3843eb4f3ec7cb8664ca63d9.png)'
- en: 'Fig. 9\. t-SNE visualization of outputs on MNIST test set by models training
    (a) without and (b) with pseudo labeling on 60000 unlabeled samples, in addition
    to 600 labeled data. Pseudo labeling leads to better segregation in the learned
    embedding space. (Image source: [Lee 2013](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.664.3543&rep=rep1&type=pdf))'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图9. 通过在60000个未标记样本上进行伪标记的模型训练，在MNIST测试集上的t-SNE可视化输出，（a）没有和（b）使用伪标签。伪标签导致学习嵌入空间中更好的分离。（图片来源：[Lee，2013](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.664.3543&rep=rep1&type=pdf)）
- en: Training with pseudo labeling naturally comes as an iterative process. We refer
    to the model that produces pseudo labels as teacher and the model that learns
    with pseudo labels as student.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 使用伪标签进行训练自然是一个迭代过程。我们将产生伪标签的模型称为教师，学习伪标签的模型称为学生。
- en: Label propagation
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 标签传播
- en: '**Label Propagation** ([Iscen et al. 2019](https://arxiv.org/abs/1904.04717))
    is an idea to construct a similarity graph among samples based on feature embedding.
    Then the pseudo labels are “diffused” from known samples to unlabeled ones where
    the propagation weights are proportional to pairwise similarity scores in the
    graph. Conceptually it is similar to a k-NN classifier and both suffer from the
    problem of not scaling up well with a large dataset.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '**标签传播**（[Iscen等人，2019](https://arxiv.org/abs/1904.04717)）是一种基于特征嵌入构建样本之间相似性图的想法。然后，伪标签从已知样本“扩散”到未标记的样本，其中传播权重与图中成对相似性分数成比例。从概念上讲，它类似于k-NN分类器，两者都存在无法很好地处理大型数据集的问题。'
- en: '![](../Images/f0c911035ec3561c58253f77c3666c18.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f0c911035ec3561c58253f77c3666c18.png)'
- en: 'Fig. 10\. Illustration of how Label Propagation works. (Image source: [Iscen
    et al. 2019](https://arxiv.org/abs/1904.04717))'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图10. 标签传播的工作原理示意图（图片来源：[Iscen等人2019](https://arxiv.org/abs/1904.04717)）
- en: Self-Training
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自训练
- en: '**Self-Training** is not a new concept ([Scudder 1965](https://ieeexplore.ieee.org/document/1053799),
    [Nigram & Ghani CIKM 2000](http://www.kamalnigam.com/papers/cotrain-CIKM00.pdf)).
    It is an iterative algorithm, alternating between the following two steps until
    every unlabeled sample has a label assigned:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '**自训练**不是一个新概念（[斯卡德1965](https://ieeexplore.ieee.org/document/1053799)、[尼格拉姆＆加尼CIKM
    2000](http://www.kamalnigam.com/papers/cotrain-CIKM00.pdf)）。它是一个迭代算法，交替执行以下两个步骤，直到每个未标记样本都有一个标签分配：'
- en: Initially it builds a classifier on labeled data.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最初在有标签数据上构建分类器。
- en: Then it uses this classifier to predict labels for the unlabeled data and converts
    the most confident ones into labeled samples.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后使用这个分类器预测未标记数据的标签，并将最有信心的转换为标记样本。
- en: '[Xie et al. (2020)](https://arxiv.org/abs/1911.04252) applied self-training
    in deep learning and achieved great results. On the ImageNet classification task,
    they first trained an EfficientNet ([Tan & Le 2019](https://arxiv.org/abs/1905.11946))
    model as teacher to generate pseudo labels for 300M unlabeled images and then
    trained a larger EfficientNet as student to learn with both true labeled and pseudo
    labeled images. One critical element in their setup is to have *noise* during
    student model training but have no noise for the teacher to produce pseudo labels.
    Thus their method is called **Noisy Student**. They applied stochastic depth ([Huang
    et al. 2016](https://arxiv.org/abs/1603.09382)), dropout and RandAugment to noise
    the student. Noise is important for the student to perform better than the teacher.
    The added noise has a compound effect to encourage the model’s decision making
    frontier to be smooth, on both labeled and unlabeled data.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '[谢等人（2020）](https://arxiv.org/abs/1911.04252)在深度学习中应用了自训练，并取得了很好的结果。在ImageNet分类任务中，他们首先训练了一个EfficientNet（[谭＆莱2019](https://arxiv.org/abs/1905.11946)）模型作为老师，为3亿个未标记图像生成伪标签，然后训练一个更大的EfficientNet作为学生，使用真实标记和伪标记图像进行学习。在他们的设置中，一个关键元素是在学生模型训练过程中引入*噪音*，但老师产生伪标签时没有噪音。因此他们的方法被称为**嘈杂学生**。他们对学生应用了随机深度（[黄等人2016](https://arxiv.org/abs/1603.09382)）、辍学和RandAugment来为学生引入噪音。噪音对于学生表现优于老师至关重要。添加的噪音具有复合效应，鼓励模型在有标记和无标记数据上的决策边界变得平滑。'
- en: 'A few other important technical configs in noisy student self-training are:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在嘈杂的学生自训练中的一些其他重要技术配置是：
- en: The student model should be sufficiently large (i.e. larger than the teacher)
    to fit more data.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学生模型应该足够大（即比老师大）以适应更多数据。
- en: Noisy student should be paired with data balancing, especially important to
    balance the number of pseudo labeled images in each class.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 嘈杂学生应该与数据平衡配对，特别重要的是平衡每个类别中的伪标记图像数量。
- en: Soft pseudo labels work better than hard ones.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 软伪标签比硬标签效果更好。
- en: Noisy student also improves adversarial robustness against an FGSM (Fast Gradient
    Sign Attack = The attack uses the gradient of the loss w.r.t the input data and
    adjusts the input data to maximize the loss) attack though the model is not optimized
    for adversarial robustness.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管模型并非针对对抗性鲁棒性进行优化，但嘈杂学生也提高了对FGSM（快速梯度符号攻击 = 该攻击使用损失相对于输入数据的梯度，并调整输入数据以最大化损失）攻击的对抗性鲁棒性。
- en: SentAugment, proposed by [Du et al. (2020)](https://arxiv.org/abs/2010.02194),
    aims to solve the problem when there is not enough in-domain unlabeled data for
    self-training in the language domain. It relies on sentence embedding to find
    unlabeled in-domain samples from a large corpus and uses the retrieved sentences
    for self-training.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: SentAugment，由[杜等人（2020）](https://arxiv.org/abs/2010.02194)提出，旨在解决语言领域自训练中没有足够的领域内未标记数据的问题。它依赖于句子嵌入来从大型语料库中找到未标记的领域内样本，并使用检索到的句子进行自训练。
- en: Reducing confirmation bias
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 减少确认偏见
- en: Confirmation bias is a problem with incorrect pseudo labels provided by an imperfect
    teacher model. Overfitting to wrong labels may not give us a better student model.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 确认偏见是由不完美的老师模型提供的错误伪标签的问题。过度拟合错误标签可能不会给我们带来更好的学生模型。
- en: 'To reduce confirmation bias, [Arazo et al. (2019)](https://arxiv.org/abs/1908.02983)
    proposed two techniques. One is to adopt MixUp with soft labels. Given two samples,
    $(\mathbf{x}_i, \mathbf{x}_j)$ and their corresponding true or pseudo labels $(y_i,
    y_j)$, the interpolated label equation can be translated to a cross entropy loss
    with softmax outputs:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减少确认偏见，[Arazo et al. (2019)](https://arxiv.org/abs/1908.02983)提出了两种技术。一种是采用带软标签的MixUp。给定两个样本，$(\mathbf{x}_i,
    \mathbf{x}_j)$及其对应的真实或伪标签$(y_i, y_j)$，插值标签方程可以转化为具有softmax输出的交叉熵损失：
- en: $$ \begin{aligned} &\bar{\mathbf{x}} = \lambda \mathbf{x}_i + (1-\lambda) \mathbf{x}_j
    \\ &\bar{y} = \lambda y_i + (1-\lambda) y_j \Leftrightarrow \mathcal{L} = \lambda
    [y_i^\top \log f_\theta(\bar{\mathbf{x}})] + (1-\lambda) [y_j^\top \log f_\theta(\bar{\mathbf{x}})]
    \end{aligned} $$
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \begin{aligned} &\bar{\mathbf{x}} = \lambda \mathbf{x}_i + (1-\lambda) \mathbf{x}_j
    \\ &\bar{y} = \lambda y_i + (1-\lambda) y_j \Leftrightarrow \mathcal{L} = \lambda
    [y_i^\top \log f_\theta(\bar{\mathbf{x}})] + (1-\lambda) [y_j^\top \log f_\theta(\bar{\mathbf{x}})]
    \end{aligned} $$
- en: Mixup is insufficient if there are too few labeled samples. They further set
    a minimum number of labeled samples in every mini batch by oversampling the labeled
    samples. This works better than upweighting labeled samples, because it leads
    to more frequent updates rather than few updates of larger magnitude which could
    be less stable. Like consistency regularization, data augmentation and dropout
    are also important for pseudo labeling to work well.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 如果标记样本太少，Mixup是不够的。他们通过过采样标记样本在每个小批次中设置了最小数量的标记样本。这比加权标记样本更好，因为它导致更频繁的更新而不是较大幅度的少量更新，这可能不太稳定。像一致性正则化、数据增强和丢弃对于伪标记工作良好也很重要。
- en: '**Meta Pseudo Labels** ([Pham et al. 2021](https://arxiv.org/abs/2003.10580))
    adapts the teacher model constantly with the feedback of how well the student
    performs on the labeled dataset. The teacher and the student are trained in parallel,
    where the teacher learns to generate better pseudo labels and the student learns
    from the pseudo labels.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '**元伪标签**([Pham et al. 2021](https://arxiv.org/abs/2003.10580))不断调整教师模型，根据学生在标记数据集上的表现反馈。教师和学生并行训练，其中教师学习生成更好的伪标签，学生从伪标签中学习。'
- en: Let the teacher and student model weights be $\theta_T$ and $\theta_S$, respectively.
    The student model’s loss on the labeled samples is defined as a function $\theta^\text{PL}_S(.)$
    of $\theta_T$ and we would like to minimize this loss by optimizing the teacher
    model accordingly.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 让教师和学生模型的权重分别为$\theta_T$和$\theta_S$。学生模型在标记样本上的损失被定义为$\theta^\text{PL}_S(.)$的函数，我们希望通过相应地优化教师模型来最小化这个损失。
- en: $$ \begin{aligned} \min_{\theta_T} &\mathcal{L}_s(\theta^\text{PL}_S(\theta_T))
    = \min_{\theta_T} \mathbb{E}_{(\mathbf{x}^l, y) \in \mathcal{X}} \text{CE}[y,
    f_{\theta_S}(\mathbf{x}^l)] \\ \text{where } &\theta^\text{PL}_S(\theta_T) = \arg\min_{\theta_S}
    \mathcal{L}_u (\theta_T, \theta_S) = \arg\min_{\theta_S} \mathbb{E}_{\mathbf{u}
    \sim \mathcal{U}} \text{CE}[(f_{\theta_T}(\mathbf{u}), f_{\theta_S}(\mathbf{u}))]
    \end{aligned} $$
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \begin{aligned} \min_{\theta_T} &\mathcal{L}_s(\theta^\text{PL}_S(\theta_T))
    = \min_{\theta_T} \mathbb{E}_{(\mathbf{x}^l, y) \in \mathcal{X}} \text{CE}[y,
    f_{\theta_S}(\mathbf{x}^l)] \\ \text{其中 } &\theta^\text{PL}_S(\theta_T) = \arg\min_{\theta_S}
    \mathcal{L}_u (\theta_T, \theta_S) = \arg\min_{\theta_S} \mathbb{E}_{\mathbf{u}
    \sim \mathcal{U}} \text{CE}[(f_{\theta_T}(\mathbf{u}), f_{\theta_S}(\mathbf{u}))]
    \end{aligned} $$
- en: However, it is not trivial to optimize the above equation. Borrowing the idea
    of [MAML](https://arxiv.org/abs/1703.03400), it approximates the multi-step $\arg\min_{\theta_S}$
    with the one-step gradient update of $\theta_S$,
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，优化上述方程并不是一件简单的事情。借鉴[MAML](https://arxiv.org/abs/1703.03400)的思想，它用$\theta_S$的一步梯度更新来近似多步$\arg\min_{\theta_S}$，
- en: $$ \begin{aligned} \theta^\text{PL}_S(\theta_T) &\approx \theta_S - \eta_S \cdot
    \nabla_{\theta_S} \mathcal{L}_u(\theta_T, \theta_S) \\ \min_{\theta_T} \mathcal{L}_s
    (\theta^\text{PL}_S(\theta_T)) &\approx \min_{\theta_T} \mathcal{L}_s \big( \theta_S
    - \eta_S \cdot \nabla_{\theta_S} \mathcal{L}_u(\theta_T, \theta_S) \big) \end{aligned}
    $$
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \begin{aligned} \theta^\text{PL}_S(\theta_T) &\approx \theta_S - \eta_S \cdot
    \nabla_{\theta_S} \mathcal{L}_u(\theta_T, \theta_S) \\ \min_{\theta_T} \mathcal{L}_s
    (\theta^\text{PL}_S(\theta_T)) &\approx \min_{\theta_T} \mathcal{L}_s \big( \theta_S
    - \eta_S \cdot \nabla_{\theta_S} \mathcal{L}_u(\theta_T, \theta_S) \big) \end{aligned}
    $$
- en: With soft pseudo labels, the above objective is differentiable. But if using
    hard pseudo labels, it is not differentiable and thus we need to use RL, e.g.
    REINFORCE.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 使用软伪标签，上述目标是可微的。但是如果使用硬伪标签，它是不可微的，因此我们需要使用RL，例如REINFORCE。
- en: 'The optimization procedure is alternative between training two models:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 优化过程在训练两个模型之间交替进行：
- en: '*Student model update*: Given a batch of unlabeled samples $\{ \mathbf{u} \}$,
    we generate pseudo labels by $f_{\theta_T}(\mathbf{u})$ and optimize $\theta_S$
    with one step SGD: $\theta’_S = \color{green}{\theta_S - \eta_S \cdot \nabla_{\theta_S}
    \mathcal{L}_u(\theta_T, \theta_S)}$.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*学生模型更新*：给定一批未标记样本$\{ \mathbf{u} \}$，我们通过$f_{\theta_T}(\mathbf{u})$生成伪标签，并用一步SGD优化$\theta_S$：$\theta’_S
    = \color{green}{\theta_S - \eta_S \cdot \nabla_{\theta_S} \mathcal{L}_u(\theta_T,
    \theta_S)}$。'
- en: '*Teacher model update*: Given a batch of labeled samples $\{(\mathbf{x}^l,
    y)\}$, we reuse the student’s update to optimize $\theta_T$: $\theta’_T = \theta_T
    - \eta_T \cdot \nabla_{\theta_T} \mathcal{L}_s ( \color{green}{\theta_S - \eta_S
    \cdot \nabla_{\theta_S} \mathcal{L}_u(\theta_T, \theta_S)} )$. In addition, the
    UDA objective is applied to the teacher model to incorporate consistency regularization.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*教师模型更新*：给定一批标记样本$\{(\mathbf{x}^l, y)\}$，我们重复使用学生的更新来优化$\theta_T$：$\theta’_T
    = \theta_T - \eta_T \cdot \nabla_{\theta_T} \mathcal{L}_s ( \color{green}{\theta_S
    - \eta_S \cdot \nabla_{\theta_S} \mathcal{L}_u(\theta_T, \theta_S)} )$。此外，UDA目标应用于教师模型以整合一致性正则化。'
- en: '![](../Images/5dc73dd5456030dc37cb2fe0d135f331.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5dc73dd5456030dc37cb2fe0d135f331.png)'
- en: 'Fig. 11\. Comparison of Meta Pseudo Labels with other semi- or self-supervised
    learning methods on image classification tasks. (Image source: [Pham et al. 2021](https://arxiv.org/abs/2003.10580))'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图11。Meta Pseudo Labels与其他图像分类任务上的半监督或自监督学习方法的比较。（图片来源：[Pham et al. 2021](https://arxiv.org/abs/2003.10580)）
- en: Pseudo Labeling with Consistency Regularization
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 具有一致性正则化的伪标记
- en: It is possible to combine the above two approaches together, running semi-supervised
    learning with both pseudo labeling and consistency training.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 可以将上述两种方法结合在一起，同时运行具有伪标记和一致性训练的半监督学习。
- en: MixMatch
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MixMatch
- en: '**MixMatch** ([Berthelot et al. 2019](https://arxiv.org/abs/1905.02249)), as
    a holistic approach to semi-supervised learning, utilizes unlabeled data by merging
    the following techniques:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '**MixMatch**（[Berthelot et al. 2019](https://arxiv.org/abs/1905.02249)）作为半监督学习的一种整体方法，利用未标记数据通过合并以下技术：'
- en: '*Consistency regularization*: Encourage the model to output the same predictions
    on perturbed unlabeled samples.'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*一致性正则化*：鼓励模型在扰动的未标记样本上输出相同的预测。'
- en: '*Entropy minimization*: Encourage the model to output confident predictions
    on unlabeled data.'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*熵最小化*：鼓励模型对未标记数据输出自信的预测。'
- en: '*MixUp* augmentation: Encourage the model to have linear behaviour between
    samples.'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*MixUp*增强：鼓励模型在样本之间具有线性行为。'
- en: Given a batch of labeled data $\mathcal{X}$ and unlabeled data $\mathcal{U}$,
    we create augmented versions of them via $\text{MixMatch}(.)$, $\bar{\mathcal{X}}$
    and $\bar{\mathcal{U}}$, containing augmented samples and guessed labels for unlabeled
    examples.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一批标记数据$\mathcal{X}$和未标记数据$\mathcal{U}$，我们通过$\text{MixMatch}(.)$创建它们的增强版本，$\bar{\mathcal{X}}$和$\bar{\mathcal{U}}$，包含增强样本和对未标记示例的猜测标签。
- en: $$ \begin{aligned} \bar{\mathcal{X}}, \bar{\mathcal{U}} &= \text{MixMatch}(\mathcal{X},
    \mathcal{U}, T, K, \alpha) \\ \mathcal{L}^\text{MM}_s &= \frac{1}{\vert \bar{\mathcal{X}}
    \vert} \sum_{(\bar{\mathbf{x}}^l, y)\in \bar{\mathcal{X}}} D[y, p_\theta(y \mid
    \bar{\mathbf{x}}^l)] \\ \mathcal{L}^\text{MM}_u &= \frac{1}{L\vert \bar{\mathcal{U}}
    \vert} \sum_{(\bar{\mathbf{u}}, \hat{y})\in \bar{\mathcal{U}}} \| \hat{y} - p_\theta(y
    \mid \bar{\mathbf{u}}) \|^2_2 \\ \end{aligned} $$
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \begin{aligned} \bar{\mathcal{X}}, \bar{\mathcal{U}} &= \text{MixMatch}(\mathcal{X},
    \mathcal{U}, T, K, \alpha) \\ \mathcal{L}^\text{MM}_s &= \frac{1}{\vert \bar{\mathcal{X}}
    \vert} \sum_{(\bar{\mathbf{x}}^l, y)\in \bar{\mathcal{X}}} D[y, p_\theta(y \mid
    \bar{\mathbf{x}}^l)] \\ \mathcal{L}^\text{MM}_u &= \frac{1}{L\vert \bar{\mathcal{U}}
    \vert} \sum_{(\bar{\mathbf{u}}, \hat{y})\in \bar{\mathcal{U}}} \| \hat{y} - p_\theta(y
    \mid \bar{\mathbf{u}}) \|^2_2 \\ \end{aligned} $$
- en: where $T$ is the sharpening temperature to reduce the guessed label overlap;
    $K$ is the number of augmentations generated per unlabeled example; $\alpha$ is
    the parameter in MixUp.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$T$是用于减少猜测标签重叠的锐化温度；$K$是每个未标记示例生成的增强数量；$\alpha$是MixUp中的参数。
- en: 'For each $\mathbf{u}$, MixMatch generates $K$ augmentations, $\bar{\mathbf{u}}^{(k)}
    = \text{Augment}(\mathbf{u})$ for $k=1, \dots, K$ and the pseudo label is guessed
    based on the average: $\hat{y} = \frac{1}{K} \sum_{k=1}^K p_\theta(y \mid \bar{\mathbf{u}}^{(k)})$.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个$\mathbf{u}$，MixMatch生成$K$个增强，$\bar{\mathbf{u}}^{(k)} = \text{Augment}(\mathbf{u})$，$k=1,
    \dots, K$，并且基于平均值猜测伪标签：$\hat{y} = \frac{1}{K} \sum_{k=1}^K p_\theta(y \mid \bar{\mathbf{u}}^{(k)})$。
- en: '![](../Images/277f9d3c34c442f8dc806c6f86ca470d.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/277f9d3c34c442f8dc806c6f86ca470d.png)'
- en: 'Fig. 12\. The process of "label guessing" in MixMatch: averaging $K$ augmentations,
    correcting the predicted marginal distribution and finally sharpening the distribution.
    (Image source: [Berthelot et al. 2019](https://arxiv.org/abs/1905.02249))'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图12。在MixMatch中“标签猜测”的过程：对$K$个增强进行平均，校正预测的边际分布，最后锐化分布。（图片来源：[Berthelot等人，2019](https://arxiv.org/abs/1905.02249)）
- en: According to their ablation studies, it is critical to have MixUp especially
    on the unlabeled data. Removing temperature sharpening on the pseudo label distribution
    hurts the performance quite a lot. Average over multiple augmentations for label
    guessing is also necessary.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 根据他们的消融研究，特别是在未标记数据上使用MixUp至关重要。去除伪标签分布上的温度调整会严重影响性能。对于标签猜测，需要对多个增强进行平均。
- en: '**ReMixMatch** ([Berthelot et al. 2020](https://arxiv.org/abs/1911.09785))
    improves MixMatch by introducing two new mechanisms:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '**ReMixMatch**（[Berthelot等人，2020](https://arxiv.org/abs/1911.09785)）通过引入两种新机制改进了MixMatch：'
- en: '![](../Images/66caaf15ddf5fb52aeec638c29e793e7.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/66caaf15ddf5fb52aeec638c29e793e7.png)'
- en: 'Fig. 13\. Illustration of two improvements introduced in ReMixMatch over MixMatch.
    (Image source: [Berthelot et al. 2020](https://arxiv.org/abs/1911.09785))'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 图13。ReMixMatch相对于MixMatch引入的两个改进的示意图。（图片来源：[Berthelot等人，2020](https://arxiv.org/abs/1911.09785)）
- en: '*Distribution alignment.* It encourages the marginal distribution $p(y)$ to
    be close to the marginal distribution of the ground truth labels. Let $p(y)$ be
    the class distribution in the true labels and $\tilde{p}(\hat{y})$ be a running
    average of the predicted class distribution among the unlabeled data. The model
    prediction on an unlabeled sample $p_\theta(y \vert \mathbf{u})$ is normalized
    to be $\text{Normalize}\big( \frac{p_\theta(y \vert \mathbf{u}) p(y)}{\tilde{p}(\hat{y})}
    \big)$ to match the true marginal distribution.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*分布对齐*。它鼓励边际分布$p(y)$接近地面真实标签的边际分布。设$p(y)$为真实标签中的类分布，$\tilde{p}(\hat{y})$为未标记数据中预测的类分布的滑动平均。对于未标记样本上的模型预测$p_\theta(y
    \vert \mathbf{u})$被归一化为$\text{Normalize}\big( \frac{p_\theta(y \vert \mathbf{u})
    p(y)}{\tilde{p}(\hat{y})} \big)$以匹配真实的边际分布。'
- en: Note that entropy minimization is not a useful objective if the marginal distribution
    is not uniform.
  id: totrans-152
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 请注意，如果边际分布不均匀，则熵最小化不是一个有用的目标。
- en: I do feel the assumption that the class distributions on the labeled and unlabeled
    data should match is too strong and not necessarily to be true in the real-world
    setting.
  id: totrans-153
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我确实觉得假设标记数据和未标记数据上的类分布应该匹配的假设太强了，在现实世界的设置中未必成立。
- en: '*Augmentation anchoring*. Given an unlabeled sample, it first generates an
    “anchor” version with weak augmentation and then averages $K$ strongly augmented
    versions using CTAugment (Control Theory Augment). CTAugment only samples augmentations
    that keep the model predictions within the network tolerance.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*增强锚定*。给定一个未标记的样本，首先生成一个带有弱增强的“锚定”版本，然后使用CTAugment（控制理论增强）对$K$个强增强版本进行平均。 CTAugment仅对保持模型预测在网络容差范围内的增强进行采样。'
- en: The ReMixMatch loss is a combination of several terms,
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: ReMixMatch损失是几个项的组合，
- en: a supervised loss with data augmentation and MixUp applied;
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个带有数据增强和MixUp的监督损失；
- en: an unsupervised loss with data augmentation and MixUp applied, using pseudo
    labels as targets;
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个无监督的损失函数，应用数据增强和MixUp，使用伪标签作为目标；
- en: a CE loss on a single heavily-augmented unlabeled image without MixUp;
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在单个经过大量增强的未标记图像上进行的CE损失，没有MixUp；
- en: a [rotation](https://lilianweng.github.io/posts/2019-11-10-self-supervised/#distortion)
    loss as in self-supervised learning.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个[旋转](https://lilianweng.github.io/posts/2019-11-10-self-supervised/#distortion)损失，就像自监督学习中的那样。
- en: DivideMix
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DivideMix
- en: '**DivideMix** ([Junnan Li et al. 2020](https://arxiv.org/abs/2002.07394)) combines
    semi-supervised learning with Learning with noisy labels (LNL). It models the
    per-sample loss distribution via a [GMM](https://scikit-learn.org/stable/modules/mixture.html)
    to dynamically divide the training data into a labeled set with clean examples
    and an unlabeled set with noisy ones. Following the idea in [Arazo et al. 2019](https://arxiv.org/abs/1904.11238),
    they fit a two-component GMM on the per-sample cross entropy loss $\ell_i = y_i^\top
    \log f_\theta(\mathbf{x}_i)$. Clean samples are expected to get lower loss faster
    than noisy samples. The component with smaller mean is the cluster corresponding
    to clean labels and let’s denote it as $c$. If the GMM posterior probability $w_i
    = p_\text{GMM}(c \mid \ell_i)$ (i.e. the probability of the sampling belonging
    to the clean sample set) is larger than the threshold $\tau$, this sample is considered
    as a clean sample and otherwise a noisy one.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '**DivideMix**（[Junnan Li 等人 2020](https://arxiv.org/abs/2002.07394)）将半监督学习与带有噪声标签的学习（LNL）相结合。它通过一个
    [GMM](https://scikit-learn.org/stable/modules/mixture.html) 模型来建模每个样本的损失分布，动态地将训练数据分为一个带有干净示例的标记集和一个带有嘈杂示例的未标记集。遵循
    [Arazo 等人 2019](https://arxiv.org/abs/1904.11238) 的思路，他们在每个样本的交叉熵损失 $\ell_i =
    y_i^\top \log f_\theta(\mathbf{x}_i)$ 上拟合一个两分量的 GMM。干净样本预计比嘈杂样本更快地获得较低的损失。均值较小的分量是对应于干净标签的簇，我们将其表示为
    $c$。如果 GMM 后验概率 $w_i = p_\text{GMM}(c \mid \ell_i)$（即样本属于干净样本集的概率）大于阈值 $\tau$，则将该样本视为干净样本，否则视为嘈杂样本。'
- en: The data clustering step is named *co-divide*. To avoid confirmation bias, DivideMix
    simultaneously trains two diverged networks where each network uses the dataset
    division from the other network; e.g. thinking about how Double Q Learning works.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 数据聚类步骤被命名为 *协同分割*。为了避免确认偏差，DivideMix 同时训练两个分歧网络，其中每个网络使用另一个网络的数据集划分；例如，考虑双 Q
    学习的工作原理。
- en: '![](../Images/51c13acb0f686c2b68a7379168482993.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/51c13acb0f686c2b68a7379168482993.png)'
- en: 'Fig. 14\. DivideMix trains two networks independently to reduce confirmation
    bias. They run co-divide, co-refinement, and co-guessing together. (Image source:
    [Junnan Li et al. 2020](https://arxiv.org/abs/2002.07394))'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14\. DivideMix 独立训练两个网络以减少确认偏差。它们同时运行协同分割、协同细化和协同猜测。（图片来源：[Junnan Li 等人 2020](https://arxiv.org/abs/2002.07394)）
- en: 'Compared to MixMatch, DivideMix has an additional *co-divide* stage for handling
    noisy samples, as well as the following improvements during training:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 与 MixMatch 相比，DivideMix 在训练过程中具有额外的 *协同分割* 阶段来处理嘈杂样本，并且在训练过程中有以下改进：
- en: '*Label co-refinement*: It linearly combines the ground-truth label $y_i$ with
    the network’s prediction $\hat{y}_i$, which is averaged across multiple augmentations
    of $\mathbf{x}_i$, guided by the clean set probability $w_i$ produced by the other
    network.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*标签协同细化*：它线性组合了地面真实标签 $y_i$ 和网络的预测 $\hat{y}_i$，后者在多次对 $\mathbf{x}_i$ 进行增强后取平均，由另一个网络产生的干净集概率
    $w_i$ 引导。'
- en: '*Label co-guessing*: It averages the predictions from two models for unlabelled
    data samples.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*标签协同猜测*：它对未标记数据样本的两个模型的预测进行平均。'
- en: '![](../Images/fc15f7a6c707e90aa87aad4a53695b36.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fc15f7a6c707e90aa87aad4a53695b36.png)'
- en: 'Fig. 15\. The algorithm of DivideMix. (Image source: [Junnan Li et al. 2020](https://arxiv.org/abs/2002.07394))'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15\. DivideMix 算法。（图片来源：[Junnan Li 等人 2020](https://arxiv.org/abs/2002.07394)）
- en: FixMatch
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: FixMatch
- en: '**FixMatch** ([Sohn et al. 2020](https://arxiv.org/abs/2001.07685)) generates
    pseudo labels on unlabeled samples with weak augmentation and only keeps predictions
    with high confidence. Here both weak augmentation and high confidence filtering
    help produce high-quality trustworthy pseudo label targets. Then FixMatch learns
    to predict these pseudo labels given a heavily-augmented sample.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '**FixMatch**（[Sohn 等人 2020](https://arxiv.org/abs/2001.07685)）在未标记样本上生成伪标签，并仅保留高置信度的预测。这里，弱数据增强和高置信度过滤都有助于生成高质量可信赖的伪标签目标。然后
    FixMatch 学习预测这些伪标签，给定一个经过大量增强的样本。'
- en: '![](../Images/9336cc01f4237d5026a944b51a52ba03.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9336cc01f4237d5026a944b51a52ba03.png)'
- en: 'Fig. 16\. Illustration of how FixMatch works. (Image source: [Sohn et al. 2020](https://arxiv.org/abs/2001.07685))'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16\. FixMatch 工作原理示意图。（图片来源：[Sohn 等人 2020](https://arxiv.org/abs/2001.07685)）
- en: $$ \begin{aligned} \mathcal{L}_s &= \frac{1}{B} \sum^B_{b=1} \text{CE}[y_b,
    p_\theta(y \mid \mathcal{A}_\text{weak}(\mathbf{x}_b))] \\ \mathcal{L}_u &= \frac{1}{\mu
    B} \sum_{b=1}^{\mu B} \mathbb{1}[\max(\hat{y}_b) \geq \tau]\;\text{CE}(\hat{y}_b,
    p_\theta(y \mid \mathcal{A}_\text{strong}(\mathbf{u}_b))) \end{aligned} $$
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \begin{aligned} \mathcal{L}_s &= \frac{1}{B} \sum^B_{b=1} \text{CE}[y_b,
    p_\theta(y \mid \mathcal{A}_\text{weak}(\mathbf{x}_b))] \\ \mathcal{L}_u &= \frac{1}{\mu
    B} \sum_{b=1}^{\mu B} \mathbb{1}[\max(\hat{y}_b) \geq \tau]\;\text{CE}(\hat{y}_b,
    p_\theta(y \mid \mathcal{A}_\text{strong}(\mathbf{u}_b))) \end{aligned} $$
- en: where $\hat{y}_b$ is the pseudo label for an unlabeled example; $\mu$ is a hyperparameter
    that determines the relative sizes of $\mathcal{X}$ and $\mathcal{U}$.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\hat{y}_b$ 是无标签样本的伪标签；$\mu$ 是一个超参数，确定了 $\mathcal{X}$ 和 $\mathcal{U}$ 的相对大小。
- en: 'Weak augmentation $\mathcal{A}_\text{weak}(.)$: A standard flip-and-shift augmentation'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 弱数据增强 $\mathcal{A}_\text{weak}(.)$：标准的翻转和平移增强
- en: 'Strong augmentation $\mathcal{A}_\text{strong}(.)$ : AutoAugment, Cutout, RandAugment,
    CTAugment'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强数据增强 $\mathcal{A}_\text{strong}(.)$：AutoAugment、Cutout、RandAugment、CTAugment
- en: '![](../Images/ba8821d556b6228c8baaf5e4712f37c4.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ba8821d556b6228c8baaf5e4712f37c4.png)'
- en: 'Fig. 17\. Performance of FixMatch and several other semi-supervised learning
    methods on image classification tasks. (Image source: [Sohn et al. 2020](https://arxiv.org/abs/2001.07685))'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17\. FixMatch 和其他几种半监督学习方法在图像分类任务上的性能。 (图片来源：[Sohn et al. 2020](https://arxiv.org/abs/2001.07685))
- en: According to the ablation studies of FixMatch,
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 根据 FixMatch 的消融研究，
- en: Sharpening the predicted distribution with a temperature parameter $T$ does
    not have a significant impact when the threshold $\tau$ is used.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用温度参数 $T$ 调整预测分布并不会对使用阈值 $\tau$ 时产生显著影响。
- en: Cutout and CTAugment as part of strong augmentations are necessary for good
    performance.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cutout 和 CTAugment 作为强数据增强的一部分对于良好性能是必要的。
- en: When the weak augmentation for label guessing is replaced with strong augmentation,
    the model diverges early in training. If discarding weak augmentation completely,
    the model overfit the guessed labels.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当用强数据增强替换标签猜测的弱数据增强时，模型在训练早期就会发散。如果完全丢弃弱数据增强，则模型会过拟合猜测的标签。
- en: Using weak instead of strong augmentation for pseudo label prediction leads
    to unstable performance. Strong data augmentation is critical.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用弱数据增强而不是强数据增强进行伪标签预测会导致性能不稳定。强数据增强至关重要。
- en: Combined with Powerful Pre-Training
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结合强大的预训练
- en: It is a common paradigm, especially in language tasks, to first pre-train a
    task-agnostic model on a large unsupervised data corpus via self-supervised learning
    and then fine-tune it on the downstream task with a small labeled dataset. Research
    has shown that we can obtain extra gain if combining semi-supervised learning
    with pretraining.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种常见的范式，特别是在语言任务中，首先在大型无监督数据语料库上通过自监督学习对任务不可知模型进行预训练，然后在小型标记数据集上微调它以适应下游任务。研究表明，如果将半监督学习与预训练相结合，我们可以获得额外的收益。
- en: '[Zoph et al. (2020)](https://arxiv.org/abs/2006.06882) studied to what degree
    [self-training](#self-training) can work better than pre-training. Their experiment
    setup was to use ImageNet for pre-training or self-training to improve COCO. Note
    that when using ImageNet for self-training, it discards labels and only uses ImageNet
    samples as unlabeled data points. [He et al. (2018)](https://arxiv.org/abs/1811.08883)
    has demonstrated that ImageNet classification pre-training does not work well
    if the downstream task is very different, such as object detection.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '[Zoph et al. (2020)](https://arxiv.org/abs/2006.06882) 研究了自我训练比预训练效果更好的程度。他们的实验设置是使用
    ImageNet 进行预训练或自我训练以提高 COCO 的性能。请注意，当使用 ImageNet 进行自我训练时，会丢弃标签，只使用 ImageNet 样本作为无标签数据点。[He
    et al. (2018)](https://arxiv.org/abs/1811.08883) 已经证明，如果下游任务非常不同，例如目标检测，那么 ImageNet
    分类预训练效果不佳。'
- en: '![](../Images/0a1f69168bf1852fcb655bb64d002f9c.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0a1f69168bf1852fcb655bb64d002f9c.png)'
- en: 'Fig. 18\. The effect of (a) data augment (from weak to strong) and (b) the
    labeled dataset size on the object detection performance. In the legend: `Rand
    Init` refers to a model initialized w/ random weights; `ImageNet` is initialized
    with a pre-trained checkpoint at 84.5% top-1 ImageNet accuracy; `ImageNet++` is
    initialized with a checkpoint with a higher accuracy 86.9%. (Image source: [Zoph
    et al. 2020](https://arxiv.org/abs/2006.06882))'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18\. (a) 数据增强（从弱到强）和 (b) 标记数据集大小对目标检测性能的影响。图例中：`Rand Init` 表示使用随机权重初始化的模型；`ImageNet`
    使用预训练检查点初始化，准确率为 84.5%；`ImageNet++` 使用准确率更高的检查点初始化，准确率为 86.9%。 (图片来源：[Zoph et
    al. 2020](https://arxiv.org/abs/2006.06882))
- en: 'Their experiments demonstrated a series of interesting findings:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 他们的实验展示了一系列有趣的发现：
- en: The effectiveness of pre-training diminishes with more labeled samples available
    for the downstream task. Pre-training is helpful in the low-data regimes (20%)
    but neutral or harmful in the high-data regime.
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随着下游任务可用的标记样本数量增加，预训练的有效性会降低。预训练在低数据情况下（20%）有帮助，但在高数据情况下是中性或有害的。
- en: Self-training helps in high data/strong augmentation regimes, even when pre-training
    hurts.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自训练有助于高数据/强数据增强情况，即使预训练有害。
- en: Self-training can bring in additive improvement on top of pre-training, even
    using the same data source.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自训练可以在预训练的基础上带来额外的改进，即使使用相同的数据源。
- en: Self-supervised pre-training (e.g. via SimCLR) hurts the performance in a high
    data regime, similar to how supervised pre-training does.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自监督预训练（例如通过SimCLR）在高数据量情况下会降低性能，类似于监督预训练的情况。
- en: Joint-training supervised and self-supervised objectives help resolve the mismatch
    between the pre-training and downstream tasks. Pre-training, joint-training and
    self-training are all additive.
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 联合训练监督和自监督目标有助于解决预训练和下游任务之间的不匹配。预训练、联合训练和自训练都是增量的。
- en: Noisy labels or un-targeted labeling (i.e. pre-training labels are not aligned
    with downstream task labels) is worse than targeted pseudo labeling.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 嘈杂的标签或非目标标记（即，预训练标签与下游任务标签不对齐）比有针对性的伪标记更糟糕。
- en: Self-training is computationally more expensive than fine-tuning on a pre-trained
    model.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自训练的计算成本比在预训练模型上进行微调更高。
- en: '[Chen et al. (2020)](https://arxiv.org/abs/2006.10029) proposed a three-step
    procedure to merge the benefits of self-supervised pretraining, supervised fine-tuning
    and self-training together:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '[Chen等人 (2020)](https://arxiv.org/abs/2006.10029) 提出了一个三步程序，将自监督预训练、监督微调和自训练的好处合并在一起：'
- en: Unsupervised or self-supervised pretrain a big model.
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 无监督或自监督预训练一个大模型。
- en: Supervised fine-tune it on a few labeled examples. It is important to use a
    big (deep and wide) neural network. *Bigger models yield better performance with
    fewer labeled samples.*
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在少量标记示例上进行监督微调。使用一个大（深且宽）的神经网络是重要的。*更大的模型在使用更少标记样本时表现更好。*
- en: Distillation with unlabeled examples by adopting pseudo labels in self-training.
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过在自训练中采用伪标签对未标记的示例进行蒸馏。
- en: It is possible to distill the knowledge from a large model into a small one
    because the task-specific use does not require extra capacity of the learned representation.
  id: totrans-202
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以将大模型的知识蒸馏到小模型中，因为任务特定的使用不需要学习表示的额外容量。
- en: The distillation loss is formatted as the following, where the teacher network
    is fixed with weights $\hat{\theta}_T$.
  id: totrans-203
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 蒸馏损失的格式如下，其中教师网络的权重$\hat{\theta}_T$是固定的。
- en: $$ \mathcal{L}_\text{distill} = - (1-\alpha) \underbrace{\sum_{(\mathbf{x}^l_i,
    y_i) \in \mathcal{X}} \big[ \log p_{\theta_S}(y_i \mid \mathbf{x}^l_i) \big]}_\text{Supervised
    loss} - \alpha \underbrace{\sum_{\mathbf{u}_i \in \mathcal{U}} \Big[ \sum_{i=1}^L
    p_{\hat{\theta}_T}(y^{(i)} \mid \mathbf{u}_i; T) \log p_{\theta_S}(y^{(i)} \mid
    \mathbf{u}_i; T) \Big]}_\text{Distillation loss using unlabeled data} $$![](../Images/0ad25e14e91d8b9cec2d0b69ea82fb6c.png)
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \mathcal{L}_\text{distill} = - (1-\alpha) \underbrace{\sum_{(\mathbf{x}^l_i,
    y_i) \in \mathcal{X}} \big[ \log p_{\theta_S}(y_i \mid \mathbf{x}^l_i) \big]}_\text{监督损失}
    - \alpha \underbrace{\sum_{\mathbf{u}_i \in \mathcal{U}} \Big[ \sum_{i=1}^L p_{\hat{\theta}_T}(y^{(i)}
    \mid \mathbf{u}_i; T) \log p_{\theta_S}(y^{(i)} \mid \mathbf{u}_i; T) \Big]}_\text{使用未标记数据进行蒸馏损失}
    $$![](../Images/0ad25e14e91d8b9cec2d0b69ea82fb6c.png)
- en: 'Fig. 19\. A semi-supervised learning framework leverages unlabeled data corpus
    by (Left) task-agnostic unsupervised pretraining and (Right) task-specific self-training
    and distillation. (Image source: [Chen et al. 2020](https://arxiv.org/abs/2006.10029))'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '图19\. 半监督学习框架通过（左）任务无关的无监督预训练和（右）任务特定的自训练和蒸馏来利用未标记的数据语料库。 (图片来源: [Chen等人，2020](https://arxiv.org/abs/2006.10029))'
- en: 'They experimented on the ImageNet classification task. The self-supervised
    pre-training uses SimCLRv2, a directly improved version of [SimCLR](https://lilianweng.github.io/posts/2021-05-31-contrastive/#simclr).
    Observations in their empirical studies confirmed several learnings, aligned with
    [Zoph et al. 2020](https://arxiv.org/abs/2006.06882):'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 他们在ImageNet分类任务上进行了实验。自监督预训练使用了SimCLRv2，这是[SimCLR](https://lilianweng.github.io/posts/2021-05-31-contrastive/#simclr)的直接改进版本。在他们的实证研究中观察到的一些发现与[Zoph等人，2020](https://arxiv.org/abs/2006.06882)一致：
- en: Bigger models are more label-efficient;
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更大的模型更具标签效率；
- en: Bigger/deeper project heads in SimCLR improve representation learning;
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SimCLR中更大/更深的项目头可以改进表示学习；
- en: Distillation using unlabeled data improves semi-supervised learning.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用未标记数据进行蒸馏可以改进半监督学习。
- en: '![](../Images/09996939cecbf6bd979092ebcc9dc6af.png)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/09996939cecbf6bd979092ebcc9dc6af.png)'
- en: 'Fig. 20\. Comparison of performance by SimCLRv2 + semi-supervised distillation
    on ImageNet classification. (Image source: [Chen et al. 2020](https://arxiv.org/abs/2006.10029))'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 图20。通过SimCLRv2 +半监督蒸馏在ImageNet分类上的性能比较。 （图片来源：[Chen等人2020](https://arxiv.org/abs/2006.10029)）
- en: '* * *'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '💡 Quick summary of common themes among recent semi-supervised learning methods,
    many aiming to reduce confirmation bias:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 💡 最近半监督学习方法中常见主题的快速摘要，许多旨在减少确认偏见：
- en: Apply valid and diverse noise to samples by advanced data augmentation methods.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过先进的数据增强方法对样本应用有效和多样化的噪声。
- en: When dealing with images, MixUp is an effective augmentation. Mixup could work
    on language too, resulting in a small incremental improvement ([Guo et al. 2019](https://arxiv.org/abs/1905.08941)).
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理图像时，MixUp是一种有效的增强方法。 Mixup也可以用于语言，从而导致小幅改进（[Guo等人2019](https://arxiv.org/abs/1905.08941)）。
- en: Set a threshold and discard pseudo labels with low confidence.
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置阈值并丢弃置信度低的伪标签。
- en: Set a minimum number of labeled samples per mini-batch.
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置每个小批量的标记样本的最小数量。
- en: Sharpen the pseudo label distribution to reduce the class overlap.
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 锐化伪标签分布以减少类别重叠。
- en: Citation
  id: totrans-219
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引用
- en: 'Cited as:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 被引用为：
- en: 'Weng, Lilian. (Dec 2021). Learning with not enough data part 1: semi-supervised
    learning. Lil’Log. https://lilianweng.github.io/posts/2021-12-05-semi-supervised/.'
  id: totrans-221
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Weng，Lilian。 （2021年12月）。数据不足的学习第1部分：半监督学习。 Lil’Log。 https://lilianweng.github.io/posts/2021-12-05-semi-supervised/。
- en: Or
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 或
- en: '[PRE0]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: References
  id: totrans-224
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Ouali, Hudelot & Tami. [“An Overview of Deep Semi-Supervised Learning”](https://arxiv.org/abs/2006.05278)
    arXiv preprint arXiv:2006.05278 (2020).'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Ouali，Hudelot和Tami。[“深度半监督学习概述”](https://arxiv.org/abs/2006.05278) arXiv预印本arXiv:2006.05278（2020）。'
- en: '[2] Sajjadi, Javanmardi & Tasdizen [“Regularization With Stochastic Transformations
    and Perturbations for Deep Semi-Supervised Learning.”](https://arxiv.org/abs/1606.04586)
    arXiv preprint arXiv:1606.04586 (2016).'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Sajjadi，Javanmardi和Tasdizen [“使用随机变换和扰动进行深度半监督学习的正则化。”](https://arxiv.org/abs/1606.04586)
    arXiv预印本arXiv:1606.04586（2016）。'
- en: '[3] Pham et al. [“Meta Pseudo Labels.”](https://arxiv.org/abs/2003.10580) CVPR
    2021.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] 范等人。[“元伪标签。”](https://arxiv.org/abs/2003.10580) CVPR 2021。'
- en: '[4] Laine & Aila. [“Temporal Ensembling for Semi-Supervised Learning”](https://arxiv.org/abs/1610.02242)
    ICLR 2017.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] 莱因和艾拉。[“用于半监督学习的时间集成”](https://arxiv.org/abs/1610.02242) ICLR 2017。'
- en: '[5] Tarvaninen & Valpola. [“Mean teachers are better role models: Weight-averaged
    consistency targets improve semi-supervised deep learning results.”](https://arxiv.org/abs/1703.01780)
    NeuriPS 2017'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Tarvaninen和Valpola。[“平均教师更好：加权一致性目标改进半监督深度学习结果。”](https://arxiv.org/abs/1703.01780)
    NeuriPS 2017'
- en: '[6] Xie et al. [“Unsupervised Data Augmentation for Consistency Training.”](https://arxiv.org/abs/1904.12848)
    NeuriPS 2020.'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] 谢等人。[“一致性训练的无监督数据增强。”](https://arxiv.org/abs/1904.12848) NeuriPS 2020。'
- en: '[7] Miyato et al. [“Virtual Adversarial Training: A Regularization Method for
    Supervised and Semi-Supervised Learning.”](https://arxiv.org/abs/1704.03976) IEEE
    transactions on pattern analysis and machine intelligence 41.8 (2018).'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] 宫本等人。[“虚拟对抗训练：监督和半监督学习的正则化方法。”](https://arxiv.org/abs/1704.03976) IEEE模式分析与机器智能交易41.8（2018）。'
- en: '[8] Verma et al. [“Interpolation consistency training for semi-supervised learning.”](https://arxiv.org/abs/1903.03825)
    IJCAI 2019'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] Verma等人。[“用于半监督学习的插值一致性训练。”](https://arxiv.org/abs/1903.03825) IJCAI 2019'
- en: '[9] Lee. [“Pseudo-label: The simple and efficient semi-supervised learning
    method for deep neural networks.”](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.664.3543&rep=rep1&type=pdf)
    ICML 2013 Workshop: Challenges in Representation Learning.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] 李。[“伪标签：深度神经网络的简单高效半监督学习方法。”](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.664.3543&rep=rep1&type=pdf)
    ICML 2013研讨会：表示学习中的挑战。'
- en: '[10] Iscen et al. [“Label propagation for deep semi-supervised learning.”](https://arxiv.org/abs/1904.04717)
    CVPR 2019.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] Iscen等人。[“用于深度半监督学习的标签传播。”](https://arxiv.org/abs/1904.04717) CVPR 2019。'
- en: '[11] Xie et al. [“Self-training with Noisy Student improves ImageNet classification”](https://arxiv.org/abs/1911.04252)
    CVPR 2020.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] 谢等人。[“自我训练与有噪声学生改进ImageNet分类”](https://arxiv.org/abs/1911.04252) CVPR
    2020。'
- en: '[12] Jingfei Du et al. [“Self-training Improves Pre-training for Natural Language
    Understanding.”](https://arxiv.org/abs/2010.02194) 2020'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '[12] 杜静飞等人 [“自训练改善自然语言理解的预训练。”](https://arxiv.org/abs/2010.02194) 2020'
- en: '[13] Iscen et al. [“Label propagation for deep semi-supervised learning.”](https://arxiv.org/abs/1904.04717)
    CVPR 2019'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '[13] Iscen等人 [“用于深度半监督学习的标签传播。”](https://arxiv.org/abs/1904.04717) CVPR 2019'
- en: '[14] Arazo et al. [“Pseudo-labeling and confirmation bias in deep semi-supervised
    learning.”](https://arxiv.org/abs/1908.02983) IJCNN 2020.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '[14] Arazo等人 [“伪标记和深度半监督学习中的确认偏差。”](https://arxiv.org/abs/1908.02983) IJCNN
    2020.'
- en: '[15] Berthelot et al. [“MixMatch: A holistic approach to semi-supervised learning.”](https://arxiv.org/abs/1905.02249)
    NeuriPS 2019'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '[15] Berthelot等人 [“MixMatch: 一种全面的半监督学习方法。”](https://arxiv.org/abs/1905.02249)
    NeuriPS 2019'
- en: '[16] Berthelot et al. [“ReMixMatch: Semi-supervised learning with distribution
    alignment and augmentation anchoring.”](https://arxiv.org/abs/1911.09785) ICLR
    2020'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '[16] Berthelot等人 [“ReMixMatch: 具有分布对齐和增强锚定的半监督学习。”](https://arxiv.org/abs/1911.09785)
    ICLR 2020'
- en: '[17] Sohn et al. [“FixMatch: Simplifying semi-supervised learning with consistency
    and confidence.”](https://arxiv.org/abs/2001.07685) CVPR 2020'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '[17] Sohn等人 [“FixMatch: 用一致性和信心简化半监督学习。”](https://arxiv.org/abs/2001.07685)
    CVPR 2020'
- en: '[18] Junnan Li et al. [“DivideMix: Learning with Noisy Labels as Semi-supervised
    Learning.”](https://arxiv.org/abs/2002.07394) 2020 [[code](https://github.com/LiJunnan1992/DivideMix)]'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '[18] 李俊楠等人 [“DivideMix: 将有噪声标签的学习作为半监督学习。”](https://arxiv.org/abs/2002.07394)
    2020 [[代码](https://github.com/LiJunnan1992/DivideMix)]'
- en: '[19] Zoph et al. [“Rethinking pre-training and self-training.”](https://arxiv.org/abs/2006.06882)
    2020.'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '[19] Zoph等人 [“重新思考预训练和自训练。”](https://arxiv.org/abs/2006.06882) 2020.'
- en: '[20] Chen et al. [“Big Self-Supervised Models are Strong Semi-Supervised Learners”](https://arxiv.org/abs/2006.10029)
    2020'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '[20] Chen等人 [“大型自监督模型是强大的半监督学习者”](https://arxiv.org/abs/2006.10029) 2020'
