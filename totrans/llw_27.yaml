- en: Generalized Language Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://lilianweng.github.io/posts/2019-01-31-lm/](https://lilianweng.github.io/posts/2019-01-31-lm/)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[Updated on 2019-02-14: add [ULMFiT](#ulmfit) and [GPT-2](#gpt-2).]'
  prefs: []
  type: TYPE_NORMAL
- en: '[Updated on 2020-02-29: add [ALBERT](#albert).]'
  prefs: []
  type: TYPE_NORMAL
- en: '[Updated on 2020-10-25: add [RoBERTa](#roberta).]'
  prefs: []
  type: TYPE_NORMAL
- en: '[Updated on 2020-12-13: add [T5](#t5).]'
  prefs: []
  type: TYPE_NORMAL
- en: '[Updated on 2020-12-30: add [GPT-3](#gpt-3).]'
  prefs: []
  type: TYPE_NORMAL
- en: '[Updated on 2021-11-13: add [XLNet](#xlnet), [BART](#bart) and [ELECTRA](#electra);
    Also updated the [Summary](#summary) section.]'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b295e956fc053ad892d37a2ed1912cf6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 0\. I guess they are Elmo & Bert? (Image source: [here](https://www.youtube.com/watch?v=l5einDQ-Ttc))'
  prefs: []
  type: TYPE_NORMAL
- en: We have seen amazing progress in NLP in 2018\. Large-scale pre-trained language
    modes like [OpenAI GPT](https://blog.openai.com/language-unsupervised/) and [BERT](https://arxiv.org/abs/1810.04805)
    have achieved great performance on a variety of language tasks using generic model
    architectures. The idea is similar to how ImageNet classification pre-training
    helps many vision tasks (*). Even better than vision classification pre-training,
    this simple and powerful approach in NLP does not require labeled data for pre-training,
    allowing us to experiment with increased training scale, up to our very limit.
  prefs: []
  type: TYPE_NORMAL
- en: '*(*) He et al. (2018) [found](https://arxiv.org/abs/1811.08883) that pre-training
    might not be necessary for image segmentation task.*'
  prefs: []
  type: TYPE_NORMAL
- en: In my previous NLP [post on word embedding](https://lilianweng.github.io/posts/2017-10-15-word-embedding/),
    the introduced embeddings are not context-specific — they are learned based on
    word concurrency but not sequential context. So in two sentences, “*I am eating
    an apple*” and “*I have an Apple phone*”, two “apple” words refer to very different
    things but they would still share the same word embedding vector.
  prefs: []
  type: TYPE_NORMAL
- en: Despite this, early adoption of word embeddings in problem-solving is to use
    them as additional features for an existing task-specific model and in a way the
    improvement is bounded.
  prefs: []
  type: TYPE_NORMAL
- en: In this post, we will discuss how various approaches were proposed to make embeddings
    dependent on context, and to make them easier and cheaper to be applied to downstream
    tasks in general form.
  prefs: []
  type: TYPE_NORMAL
- en: CoVe
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**CoVe** ([McCann et al. 2017](https://arxiv.org/abs/1708.00107)), short for
    **Contextual Word Vectors**, is a type of word embeddings learned by an encoder
    in an [attentional seq-to-seq](https://lilianweng.github.io/posts/2018-06-24-attention/#born-for-translation)
    machine translation model. Different from traditional word embeddings introduced
    [here](https://lilianweng.github.io/posts/2017-10-15-word-embedding/), CoVe word
    representations are functions of the entire input sentence.'
  prefs: []
  type: TYPE_NORMAL
- en: NMT Recap
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here the Neural Machine Translation ([NMT](https://github.com/THUNLP-MT/MT-Reading-List))
    model is composed of a standard, two-layer, bidirectional LSTM encoder and an
    attentional two-layer unidirectional LSTM decoder. It is pre-trained on the English-German
    translation task. The encoder learns and optimizes the embedding vectors of English
    words in order to translate them to German. With the intuition that the encoder
    should capture high-level semantic and syntactic meanings before transforming
    words into another language, the encoder output is used to provide contextualized
    word embeddings for various downstream language tasks.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6c8839f955a10f40612e99c1615e8e70.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig. 1\. The NMT base model used in CoVe.
  prefs: []
  type: TYPE_NORMAL
- en: 'A sequence of $n$ words in source language (English): $x = [x_1, \dots, x_n]$.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A sequence of $m$ words in target language (German): $y = [y_1, \dots, y_m]$.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The [GloVe](https://lilianweng.github.io/posts/2017-10-15-word-embedding/#glove-global-vectors)
    vectors of source words: $\text{GloVe}(x)$.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Randomly initialized embedding vectors of target words: $z = [z_1, \dots, z_m]$.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The biLSTM encoder outputs a sequence of hidden states: $h = [h_1, \dots, h_n]
    = \text{biLSTM}(\text{GloVe}(x))$ and $h_t = [\overrightarrow{h}_t; \overleftarrow{h}_t]$
    where the forward LSTM computes $\overrightarrow{h}_t = \text{LSTM}(x_t, \overrightarrow{h}_{t-1})$
    and the backward computation gives us $\overleftarrow{h}_t = \text{LSTM}(x_t,
    \overleftarrow{h}_{t-1})$.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The attentional decoder outputs a distribution over words: $p(y_t \mid H, y_1,
    \dots, y_{t-1})$ where $H$ is a stack of hidden states $\{h\}$ along the time
    dimension:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '$$ \begin{aligned} \text{decoder hidden state: } s_t &= \text{LSTM}([z_{t-1};
    \tilde{h}_{t-1}], s_{t-1}) \\ \text{attention weights: } \alpha_t &= \text{softmax}(H(W_1
    s_t + b_1)) \\ \text{context-adjusted hidden state: } \tilde{h}_t &= \tanh(W_2[H^\top\alpha_t;s_t]
    + b_2) \\ \text{decoder output: } p(y_t\mid H, y_1, \dots, y_{t-1}) &= \text{softmax}(W_\text{out}
    \tilde{h}_t + b_\text{out}) \end{aligned} $$'
  prefs: []
  type: TYPE_NORMAL
- en: Use CoVe in Downstream Tasks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The hidden states of NMT encoder are defined as **context vectors** for other
    language tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: $$ \text{CoVe}(x) = \text{biLSTM}(\text{GloVe}(x)) $$
  prefs: []
  type: TYPE_NORMAL
- en: The paper proposed to use the concatenation of GloVe and CoVe for question-answering
    and classification tasks. GloVe learns from the ratios of global word co-occurrences,
    so it has no sentence context, while CoVe is generated by processing text sequences
    is able to capture the contextual information.
  prefs: []
  type: TYPE_NORMAL
- en: $$ v = [\text{GloVe}(x); \text{CoVe}(x)] $$
  prefs: []
  type: TYPE_NORMAL
- en: Given a downstream task, we first generate the concatenation of GloVe + CoVe
    vectors of input words and then feed them into the task-specific models as additional
    features.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7885bd5c60581360a3b1a0ef989e3f50.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 2\. The CoVe embeddings are generated by an encoder trained for machine
    translation task. The encoder can be plugged into any downstream task-specific
    model. (Image source: [original paper](https://arxiv.org/abs/1708.00107))'
  prefs: []
  type: TYPE_NORMAL
- en: '**Summary**: The limitation of CoVe is obvious: (1) pre-training is bounded
    by available datasets on the supervised translation task; (2) the contribution
    of CoVe to the final performance is constrained by the task-specific model architecture.'
  prefs: []
  type: TYPE_NORMAL
- en: In the following sections, we will see that ELMo overcomes issue (1) by unsupervised
    pre-training and OpenAI GPT & BERT further overcome both problems by unsupervised
    pre-training + using generative model architecture for different downstream tasks.
  prefs: []
  type: TYPE_NORMAL
- en: ELMo
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**ELMo**, short for **Embeddings from Language Model** ([Peters, et al, 2018](https://arxiv.org/abs/1802.05365))
    learns contextualized word representation by pre-training a language model in
    an *unsupervised* way.'
  prefs: []
  type: TYPE_NORMAL
- en: Bidirectional Language Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The bidirectional Language Model (**biLM**) is the foundation for ELMo. While
    the input is a sequence of $n$ tokens, $(x_1, \dots, x_n)$, the language model
    learns to predict the probability of next token given the history.
  prefs: []
  type: TYPE_NORMAL
- en: In the forward pass, the history contains words before the target token,
  prefs: []
  type: TYPE_NORMAL
- en: $$ p(x_1, \dots, x_n) = \prod_{i=1}^n p(x_i \mid x_1, \dots, x_{i-1}) $$
  prefs: []
  type: TYPE_NORMAL
- en: In the backward pass, the history contains words after the target token,
  prefs: []
  type: TYPE_NORMAL
- en: $$ p(x_1, \dots, x_n) = \prod_{i=1}^n p(x_i \mid x_{i+1}, \dots, x_n) $$
  prefs: []
  type: TYPE_NORMAL
- en: The predictions in both directions are modeled by multi-layer LSTMs with hidden
    states $\overrightarrow{\mathbf{h}}_{i,\ell}$ and $\overleftarrow{\mathbf{h}}_{i,\ell}$
    for input token $x_i$ at the layer level $\ell=1,\dots,L$. The final layer’s hidden
    state $\mathbf{h}_{i,L} = [\overrightarrow{\mathbf{h}}_{i,L}; \overleftarrow{\mathbf{h}}_{i,L}]$
    is used to output the probabilities over tokens after softmax normalization. They
    share the embedding layer and the softmax layer, parameterized by $\Theta_e$ and
    $\Theta_s$ respectively.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8894e79aa27e2cd7a4dd5923cf770327.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 3\. The biLSTM base model of ELMo. (Image source: recreated based on the
    figure in ["Neural Networks, Types, and Functional Programming"](http://colah.github.io/posts/2015-09-NN-Types-FP/)
    by Christopher Olah.)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The model is trained to minimize the negative log likelihood (= maximize the
    log likelihood for true words) in both directions:'
  prefs: []
  type: TYPE_NORMAL
- en: $$ \begin{aligned} \mathcal{L} = - \sum_{i=1}^n \Big( \log p(x_i \mid x_1, \dots,
    x_{i-1}; \Theta_e, \overrightarrow{\Theta}_\text{LSTM}, \Theta_s) + \\ \log p(x_i
    \mid x_{i+1}, \dots, x_n; \Theta_e, \overleftarrow{\Theta}_\text{LSTM}, \Theta_s)
    \Big) \end{aligned} $$
  prefs: []
  type: TYPE_NORMAL
- en: ELMo Representations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'On top of a $L$-layer biLM, ELMo stacks all the hidden states across layers
    together by learning a task-specific linear combination. The hidden state representation
    for the token $x_i$ contains $2L+1$ vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: $$ R_i = \{ \mathbf{h}_{i,\ell} \mid \ell = 0, \dots, L \} $$
  prefs: []
  type: TYPE_NORMAL
- en: where $\mathbf{h}_{0, \ell}$ is the embedding layer output and $\mathbf{h}_{i,
    \ell} = [\overrightarrow{\mathbf{h}}_{i,\ell}; \overleftarrow{\mathbf{h}}_{i,\ell}]$.
  prefs: []
  type: TYPE_NORMAL
- en: The weights, $\mathbf{s}^\text{task}$, in the linear combination are learned
    for each end task and normalized by softmax. The scaling factor $\gamma^\text{task}$
    is used to correct the misalignment between the distribution of biLM hidden states
    and the distribution of task specific representations.
  prefs: []
  type: TYPE_NORMAL
- en: $$ v_i = f(R_i; \Theta^\text{task}) = \gamma^\text{task} \sum_{\ell=0}^L s^\text{task}_i
    \mathbf{h}_{i,\ell} $$
  prefs: []
  type: TYPE_NORMAL
- en: 'To evaluate what kind of information is captured by hidden states across different
    layers, ELMo is applied on semantic-intensive and syntax-intensive tasks respectively
    using representations in different layers of biLM:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Semantic task**: The *word sense disambiguation (WSD)* task emphasizes the
    meaning of a word given a context. The biLM top layer is better at this task than
    the first layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Syntax task**: The *[part-of-speech](https://en.wikipedia.org/wiki/Part-of-speech_tagging)
    (POS) tagging* task aims to infer the grammatical role of a word in one sentence.
    A higher accuracy can be achieved by using the biLM first layer than the top layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The comparison study indicates that syntactic information is better represented
    at lower layers while semantic information is captured by higher layers. Because
    different layers tend to carry different type of information, *stacking them together
    helps*.
  prefs: []
  type: TYPE_NORMAL
- en: Use ELMo in Downstream Tasks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Similar to how [CoVe](#use-cove-in-downstream-tasks) can help different downstream
    tasks, ELMo embedding vectors are included in the input or lower levels of task-specific
    models. Moreover, for some tasks (i.e., [SNLI](#nli) and [SQuAD](#qa), but not
    [SRL](#srl)), adding them into the output level helps too.
  prefs: []
  type: TYPE_NORMAL
- en: The improvements brought up by ELMo are largest for tasks with a small supervised
    dataset. With ELMo, we can also achieve similar performance with much less labeled
    data.
  prefs: []
  type: TYPE_NORMAL
- en: '**Summary**: The language model pre-training is unsupervised and theoretically
    the pre-training can be scaled up as much as possible since the unlabeled text
    corpora are abundant. However, it still has the dependency on task-customized
    models and thus the improvement is only incremental, while searching for a good
    model architecture for every task remains non-trivial.'
  prefs: []
  type: TYPE_NORMAL
- en: Cross-View Training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In ELMo the unsupervised pre-training and task-specific learning happen for
    two independent models in two separate training stages. **Cross-View Training**
    (abbr. **CVT**; [Clark et al., 2018](https://arxiv.org/abs/1809.08370)) combines
    them into one unified semi-supervised learning procedure where the representation
    of a biLSTM encoder is improved by both supervised learning with labeled data
    and unsupervised learning with unlabeled data on auxiliary tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Model Architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The model consists of a two-layer bidirectional LSTM encoder and a primary prediction
    module. During training, the model is fed with labeled and unlabeled data batches
    alternatively.
  prefs: []
  type: TYPE_NORMAL
- en: On *labeled examples*, all the model parameters are updated by standard supervised
    learning. The loss is the standard cross entropy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On *unlabeled examples*, the primary prediction module still can produce a “soft”
    target, even though we cannot know exactly how accurate they are. In a couple
    of auxiliary tasks, the predictor only sees and processes a restricted view of
    the input, such as only using encoder hidden state representation in one direction.
    The auxiliary task outputs are expected to match the primary prediction target
    for a full view of input.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this way, the encoder is forced to distill the knowledge of the full context
    into partial representation. At this stage, the biLSTM encoder is backpropagated
    but the primary prediction module is *fixed*. The loss is to minimize the distance
    between auxiliary and primary predictions.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](../Images/8154e60ba2d899fa5219289be73c8ba6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 4\. The overview of semi-supervised language model cross-view training.
    (Image source: [original paper](https://arxiv.org/abs/1809.08370))'
  prefs: []
  type: TYPE_NORMAL
- en: Multi-Task Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When training for multiple tasks simultaneously, CVT adds several extra primary
    prediction models for additional tasks. They all share the same sentence representation
    encoder. During supervised training, once one task is randomly selected, parameters
    in its corresponding predictor and the representation encoder are updated. With
    unlabeled data samples, the encoder is optimized jointly across all the tasks
    by minimizing the differences between auxiliary outputs and primary prediction
    for every task.
  prefs: []
  type: TYPE_NORMAL
- en: 'The multi-task learning encourages better generality of representation and
    in the meantime produces a nice side-product: all-tasks-labeled examples from
    unlabeled data. They are precious data labels considering that cross-task labels
    are useful but fairly rare.'
  prefs: []
  type: TYPE_NORMAL
- en: Use CVT in Downstream Tasks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Theoretically the primary prediction module can take any form, generic or task-specific
    design. The examples presented in the CVT paper include both cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'In sequential tagging tasks (classification for every token) like [NER](#ner)
    or [POS](#pos) tagging, the predictor module contains two fully connected layers
    and a softmax layer on the output to produce a probability distribution over class
    labels. For each token $\mathbf{x}_i$, we take the corresponding hidden states
    in two layers, $\mathbf{h}_1^{(i)}$ and $\mathbf{h}_2^{(i)}$:'
  prefs: []
  type: TYPE_NORMAL
- en: $$ \begin{aligned} p_\theta(y_i \mid \mathbf{x}_i) &= \text{NN}(\mathbf{h}^{(i)})
    \\ &= \text{NN}([\mathbf{h}_1^{(i)}; \mathbf{h}_2^{(i)}]) \\ &= \text{softmax}
    \big( \mathbf{W}\cdot\text{ReLU}(\mathbf{W'}\cdot[\mathbf{h}_1^{(i)}; \mathbf{h}_2^{(i)}])
    + \mathbf{b} \big) \end{aligned} $$
  prefs: []
  type: TYPE_NORMAL
- en: The auxiliary tasks are only fed with forward or backward LSTM state in the
    first layer. Because they only observe partial context, either on the left or
    right, they have to learn like a language model, trying to predict the next token
    given the context. The `fwd` and `bwd` auxiliary tasks only take one direction.
    The `future` and `past` tasks take one step further in forward and backward direction,
    respectively.
  prefs: []
  type: TYPE_NORMAL
- en: $$ \begin{aligned} p_\theta^\text{fwd}(y_i \mid \mathbf{x}_i) &= \text{NN}^\text{fwd}(\overrightarrow{\mathbf{h}}^{(i)})
    \\ p_\theta^\text{bwd}(y_i \mid \mathbf{x}_i) &= \text{NN}^\text{bwd}(\overleftarrow{\mathbf{h}}^{(i)})
    \\ p_\theta^\text{future}(y_i \mid \mathbf{x}_i) &= \text{NN}^\text{future}(\overrightarrow{\mathbf{h}}^{(i-1)})
    \\ p_\theta^\text{past}(y_i \mid \mathbf{x}_i) &= \text{NN}^\text{past}(\overleftarrow{\mathbf{h}}^{(i+1)})
    \end{aligned} $$![](../Images/a53253d738a76634f4155d83123fa9c2.png)
  prefs: []
  type: TYPE_NORMAL
- en: 'Fig. 5\. The sequential tagging task depends on four auxiliary prediction models,
    their inputs only involving hidden states in one direction: forward, backward,
    future and past. (Image source: [original paper](https://arxiv.org/abs/1809.08370))'
  prefs: []
  type: TYPE_NORMAL
- en: Note that if the primary prediction module has dropout, the dropout layer works
    as usual when training with labeled data, but it is not applied when generating
    “soft” target for auxiliary tasks during training with unlabeled data.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the machine translation task, the primary prediction module is replaced
    with a standard unidirectional LSTM decoder with attention. There are two auxiliary
    tasks: (1) apply dropout on the attention weight vector by randomly zeroing out
    some values; (2) predict the future word in the target sequence. The primary prediction
    for auxiliary tasks to match is the best predicted target sequence produced by
    running the fixed primary decoder on the input sequence with [beam search](https://en.wikipedia.org/wiki/Beam_search).'
  prefs: []
  type: TYPE_NORMAL
- en: ULMFiT
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The idea of using generative pretrained LM + task-specific fine-tuning was first
    explored in ULMFiT ([Howard & Ruder, 2018](https://arxiv.org/abs/1801.06146)),
    directly motivated by the success of using ImageNet pre-training for computer
    vision tasks. The base model is [AWD-LSTM](https://arxiv.org/abs/1708.02182).
  prefs: []
  type: TYPE_NORMAL
- en: 'ULMFiT follows three steps to achieve good transfer learning results on downstream
    language classification tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '*General LM pre-training*: on Wikipedia text.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Target task LM fine-tuning*: ULMFiT proposed two training techniques for stabilizing
    the fine-tuning process. See below.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Discriminative fine-tuning** is motivated by the fact that different layers
    of LM capture different types of information (see [discussion](#elmo-representations)
    above). ULMFiT proposed to tune each layer with different learning rates, $\{\eta^1,
    \dots, \eta^\ell, \dots, \eta^L\}$, where $\eta$ is the base learning rate for
    the first layer, $\eta^\ell$ is for the $\ell$-th layer and there are $L$ layers
    in total.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Slanted triangular learning rates (STLR)** refer to a special learning rate
    scheduling that first linearly increases the learning rate and then linearly decays
    it. The increase stage is short so that the model can converge to a parameter
    space suitable for the task fast, while the decay period is long allowing for
    better fine-tuning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Target task classifier fine-tuning*: The pretrained LM is augmented with two
    standard feed-forward layers and a softmax normalization at the end to predict
    a target label distribution.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Concat pooling** extracts max-polling and mean-pooling over the history of
    hidden states and concatenates them with the final hidden state.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Gradual unfreezing** helps to avoid catastrophic forgetting by gradually
    unfreezing the model layers starting from the last one. First the last layer is
    unfrozen and fine-tuned for one epoch. Then the next lower layer is unfrozen.
    This process is repeated until all the layers are tuned.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/32bdf1c8e90e836e9925564a53f7fe7b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 6\. Three training stages of ULMFiT. (Image source: [original paper](https://arxiv.org/abs/1801.06146))'
  prefs: []
  type: TYPE_NORMAL
- en: GPT
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Following the similar idea of ELMo, OpenAI **GPT**, short for **Generative Pre-training
    Transformer** ([Radford et al., 2018](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)),
    expands the unsupervised language model to a much larger scale by training on
    a giant collection of free text corpora. Despite of the similarity, GPT has two
    major differences from ELMo.
  prefs: []
  type: TYPE_NORMAL
- en: 'The model architectures are different: ELMo uses a shallow concatenation of
    independently trained left-to-right and right-to-left multi-layer LSTMs, while
    GPT is a multi-layer transformer decoder.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The use of contextualized embeddings in downstream tasks are different: ELMo
    feeds embeddings into models customized for specific tasks as additional features,
    while GPT fine-tunes the same base model for all end tasks.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Transformer Decoder as Language Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Compared to the [original transformer](https://arxiv.org/abs/1706.03762) architecture,
    the [transformer decoder](https://arxiv.org/abs/1801.10198) model discards the
    encoder part, so there is only one single input sentence rather than two separate
    source and target sequences.
  prefs: []
  type: TYPE_NORMAL
- en: This model applies multiple transformer blocks over the embeddings of input
    sequences. Each block contains a masked *multi-headed self-attention* layer and
    a *pointwise feed-forward* layer. The final output produces a distribution over
    target tokens after softmax normalization.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c56af6fb2bb49ce420474bb91bb09e19.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig. 7\. The transformer decoder model architecture in OpenAI GPT.
  prefs: []
  type: TYPE_NORMAL
- en: 'The loss is the negative log-likelihood, same as [ELMo](#elmo), but without
    backward computation. Let’s say, the context window of the size $k$ is located
    before the target word and the loss would look like:'
  prefs: []
  type: TYPE_NORMAL
- en: $$ \mathcal{L}_\text{LM} = -\sum_{i} \log p(x_i\mid x_{i-k}, \dots, x_{i-1})
    $$
  prefs: []
  type: TYPE_NORMAL
- en: Byte Pair Encoding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Byte Pair Encoding** ([**BPE**](https://arxiv.org/abs/1508.07909)) is used
    to encode the input sequences. BPE was originally proposed as a data compression
    algorithm in 1990s and then was adopted to solve the open-vocabulary issue in
    machine translation, as we can easily run into rare and unknown words when translating
    into a new language. Motivated by the intuition that rare and unknown words can
    often be decomposed into multiple subwords, BPE finds the best word segmentation
    by iteratively and greedily merging frequent pairs of characters.'
  prefs: []
  type: TYPE_NORMAL
- en: Supervised Fine-Tuning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The most substantial upgrade that OpenAI GPT proposed is to get rid of the task-specific
    model and use the pre-trained language model directly!
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take classification as an example. Say, in the labeled dataset, each input
    has $n$ tokens, $\mathbf{x} = (x_1, \dots, x_n)$, and one label $y$. GPT first
    processes the input sequence $\mathbf{x}$ through the pre-trained transformer
    decoder and the last layer output for the last token $x_n$ is $\mathbf{h}_L^{(n)}$.
    Then with only one new trainable weight matrix $\mathbf{W}_y$, it can predict
    a distribution over class labels.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/875401fc21f1e27510b35d8e9743ff71.png)$$ P(y\mid x_1, \dots, x_n)
    = \text{softmax}(\mathbf{h}_L^{(n)}\mathbf{W}_y) $$'
  prefs: []
  type: TYPE_NORMAL
- en: 'The loss is to minimize the negative log-likelihood for true labels. In addition,
    adding the LM loss as an auxiliary loss is found to be beneficial, because:'
  prefs: []
  type: TYPE_NORMAL
- en: (1) it helps accelerate convergence during training and
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (2) it is expected to improve the generalization of the supervised model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $$ \begin{aligned} \mathcal{L}_\text{cls} &= \sum_{(\mathbf{x}, y) \in \mathcal{D}}
    \log P(y\mid x_1, \dots, x_n) = \sum_{(\mathbf{x}, y) \in \mathcal{D}} \log \text{softmax}(\mathbf{h}_L^{(n)}(\mathbf{x})\mathbf{W}_y)
    \\ \mathcal{L}_\text{LM} &= -\sum_{i} \log p(x_i\mid x_{i-k}, \dots, x_{i-1})
    \\ \mathcal{L} &= \mathcal{L}_\text{cls} + \lambda \mathcal{L}_\text{LM} \end{aligned}
    $$
  prefs: []
  type: TYPE_NORMAL
- en: With similar designs, no customized model structure is needed for other end
    tasks (see Fig. 7). If the task input contains multiple sentences, a special delimiter
    token (`$`) is added between each pair of sentences. The embedding for this delimiter
    token is a new parameter we need to learn, but it should be pretty minimal.
  prefs: []
  type: TYPE_NORMAL
- en: For the sentence similarity task, because the ordering does not matter, both
    orderings are included. For the multiple choice task, the context is paired with
    every answer candidate.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8df4c9a97a91221efbc0d9a54dc72654.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 8\. Training objects in slightly modified GPT transformer models for downstream
    tasks. (Image source: [original paper](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf))'
  prefs: []
  type: TYPE_NORMAL
- en: '**Summary**: It is super neat and encouraging to see that such a general framework
    is capable to beat SOTA on most language tasks at that time (June 2018). At the
    first stage, generative pre-training of a language model can absorb as much free
    text as possible. Then at the second stage, the model is fine-tuned on specific
    tasks with a small labeled dataset and a minimal set of new parameters to learn.'
  prefs: []
  type: TYPE_NORMAL
- en: One limitation of GPT is its uni-directional nature — the model is only trained
    to predict the future left-to-right context.
  prefs: []
  type: TYPE_NORMAL
- en: BERT
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**BERT**, short for **Bidirectional Encoder Representations from Transformers**
    ([Devlin, et al., 2019](https://arxiv.org/abs/1810.04805)) is a direct descendant
    to [GPT](#gpt): train a large language model on free text and then fine-tune on
    specific tasks without customized network architectures.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Compared to GPT, the largest difference and improvement of BERT is to make
    training **bi-directional**. The model learns to predict both context on the left
    and right. The paper according to the ablation study claimed that:'
  prefs: []
  type: TYPE_NORMAL
- en: “bidirectional nature of our model is the single most important new contribution”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Pre-training Tasks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The model architecture of BERT is a multi-layer bidirectional Transformer encoder.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/21d172e04ad3af78ccd1b25c91f6e051.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 9\. Recap of Transformer Encoder model architecture. (Image source: [Transformer
    paper](https://arxiv.org/abs/1706.03762))'
  prefs: []
  type: TYPE_NORMAL
- en: To encourage the bi-directional prediction and sentence-level understanding,
    BERT is trained with two tasks instead of the basic language task (that is, to
    predict the next token given context).
  prefs: []
  type: TYPE_NORMAL
- en: '***Task 1: Mask language model (MLM)**'
  prefs: []
  type: TYPE_NORMAL
- en: 'From [Wikipedia](https://en.wikipedia.org/wiki/Cloze_test): “A cloze test (also
    cloze deletion test) is an exercise, test, or assessment consisting of a portion
    of language with certain items, words, or signs removed (cloze text), where the
    participant is asked to replace the missing language item. … The exercise was
    first described by W.L. Taylor in 1953.”'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'It is unsurprising to believe that a representation that learns the context
    around a word rather than just after the word is able to better capture its meaning,
    both syntactically and semantically. BERT encourages the model to do so by training
    on the *“mask language model” task*:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Randomly mask 15% of tokens in each sequence. Because if we only replace masked
    tokens with a special placeholder `[MASK]`, the special token would never be encountered
    during fine-tuning. Hence, BERT employed several heuristic tricks:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: (a) with 80% probability, replace the chosen words with `[MASK]`;
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: (b) with 10% probability, replace with a random word;
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: (c) with 10% probability, keep it the same.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The model only predicts the missing words, but it has no information on which
    words have been replaced or which words should be predicted. The output size is
    only 15% of the input size.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Task 2: Next sentence prediction**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Motivated by the fact that many downstream tasks involve the understanding
    of relationships between sentences (i.e., [QA](#qa), [NLI](#nli)), BERT added
    another auxiliary task on training a *binary classifier* for telling whether one
    sentence is the next sentence of the other:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Sample sentence pairs (A, B) so that:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: (a) 50% of the time, B follows A;
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: (b) 50% of the time, B does not follow A.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The model processes both sentences and output a binary label indicating whether
    B is the next sentence of A.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The training data for both auxiliary tasks above can be trivially generated
    from any monolingual corpus. Hence the scale of training is unbounded. The training
    loss is the sum of the mean masked LM likelihood and mean next sentence prediction
    likelihood.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/04602b91571e9fcfa55944ab22493b96.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 10\. Comparison of BERT, OpenAI GPT and ELMo model architectures. (Image
    source: [original paper](https://arxiv.org/abs/1810.04805))'
  prefs: []
  type: TYPE_NORMAL
- en: Input Embedding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The input embedding is the sum of three parts:'
  prefs: []
  type: TYPE_NORMAL
- en: '*WordPiece tokenization embeddings*: The [WordPiece](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/37842.pdf)
    [model](https://arxiv.org/pdf/1609.08144.pdf) was originally proposed for Japanese
    or Korean segmentation problem. Instead of using naturally split English word,
    they can be further divided into smaller sub-word units so that it is more effective
    to handle rare or unknown words. Please read [linked](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/37842.pdf)
    [papers](https://arxiv.org/pdf/1609.08144.pdf) for the optimal way to split words
    if interested.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Segment embeddings*: If the input contains two sentences, they have sentence
    A embeddings and sentence B embeddings respectively and they are separated by
    a special character `[SEP]`; Only sentence A embeddings are used if the input
    only contains one sentence.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Position embeddings*: Positional embeddings are learned rather than hard-coded.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/a51025c2642a16e94382a84078f418ed.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 11\. BERT input representation. (Image source: [original paper](https://arxiv.org/abs/1810.04805))'
  prefs: []
  type: TYPE_NORMAL
- en: Note that the first token is always forced to be `[CLS]` — a placeholder that
    will be used later for prediction in downstream tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Use BERT in Downstream Tasks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: BERT fine-tuning requires only a few new parameters added, just like OpenAI
    GPT.
  prefs: []
  type: TYPE_NORMAL
- en: For classification tasks, we get the prediction by taking the final hidden state
    of the special first token `[CLS]`, $\mathbf{h}^\text{[CLS]}_L$, and multiplying
    it with a small weight matrix, $\text{softmax}(\mathbf{h}^\text{[CLS]}_L \mathbf{W}_\text{cls})$.
  prefs: []
  type: TYPE_NORMAL
- en: For [QA](#qa) tasks like SQuAD, we need to predict the text span in the given
    paragraph for an given question. BERT predicts two probability distributions of
    every token, being the start and the end of the text span. Only two new small
    matrices, $\mathbf{W}_\text{s}$ and $\mathbf{W}_\text{e}$, are newly learned during
    fine-tuning and $\text{softmax}(\mathbf{h}^\text{(i)}_L \mathbf{W}_\text{s})$
    and $\text{softmax}(\mathbf{h}^\text{(i)}_L \mathbf{W}_\text{e})$ define two probability
    distributions.
  prefs: []
  type: TYPE_NORMAL
- en: Overall the add-on part for end task fine-tuning is very minimal — one or two
    weight matrices to convert the Transform hidden states to an interpretable format.
    Check the paper for implementation details for other cases.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2c687a403bfe93c0c5cacfde08a426ff.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 12\. Training objects in slightly modified BERT models for downstream
    tasks. (Image source: [original paper](https://arxiv.org/abs/1810.04805))'
  prefs: []
  type: TYPE_NORMAL
- en: A summary table compares differences between fine-tuning of [OpenAI GPT](#gpt)
    and BERT.
  prefs: []
  type: TYPE_NORMAL
- en: '| | **OpenAI GPT** | **BERT** | | Special char | `[SEP]` and `[CLS]` are only
    introduced at fine-tuning stage. | `[SEP]` and `[CLS]` and sentence A/B embeddings
    are learned at the pre-training stage. | | Training process | 1M steps, batch
    size 32k words. | 1M steps, batch size 128k words. | | Fine-tuning | lr = 5e-5
    for all fine-tuning tasks. | Use task-specific lr for fine-tuning. |'
  prefs: []
  type: TYPE_TB
- en: ALBERT
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**ALBERT** ([Lan, et al. 2019](https://arxiv.org/abs/1909.11942)), short for
    **A Lite BERT**, is a light-weighted version of [BERT](#BERT) model. An ALBERT
    model can be trained 1.7x faster with 18x fewer parameters, compared to a BERT
    model of similar configuration. ALBERT incorporates three changes as follows:
    the first two help reduce parameters and memory consumption and hence speed up
    the training speed, while the third one proposes a more chanllenging training
    task to replace the next sentence prediction (NSP) objective.'
  prefs: []
  type: TYPE_NORMAL
- en: Factorized Embedding Parameterization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In BERT, the WordPiece tokenization embedding size $E$ is configured to be the
    same as the hidden state size $H$. That is saying, if we want to increase the
    model size (larger $H$), we need to learn a larger tokenization embedding too,
    which is expensive because it depends on the vocabulary size ($V$).
  prefs: []
  type: TYPE_NORMAL
- en: Conceptually, because the tokenization embedding is expected to learn *context-independent*
    representation and the hidden states are *context-dependent*, it makes sense to
    separate the size of the hidden layers from the size of vocabulary embedding.
    Using factorized embedding parameterization, the large vocabulary embedding matrix
    of size $V \times H$ is decomposed into two small matrices of size $V \times E$
    and $E \times H$. Given $H \gt E$ or even $H \gg E$, factorization can result
    in significant parameter reduction.
  prefs: []
  type: TYPE_NORMAL
- en: Cross-layer Parameter Sharing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Parameter sharing across layers can happen in many ways: (a) only share feed-forward
    part; (b) only share attention parameters; or (c) share all the parameters. This
    technique reduces the number of parameters by a ton and does not damage the performance
    too much.'
  prefs: []
  type: TYPE_NORMAL
- en: Sentence-Order Prediction (SOP)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Interestingly, the [next sentence prediction (NSP)](#NSP) task of BERT turned
    out to be too easy. ALBERT instead adopted a sentence-order prediction (SOP) [self-supervised](https://lilianweng.github.io/posts/2019-11-10-self-supervised/)
    loss,
  prefs: []
  type: TYPE_NORMAL
- en: 'Positive sample: two consecutive segments from the same document.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Negative sample: same as above, but the segment order is switched.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the NSP task, the model can make reasonable predictions if it is able to
    detect topics when A and B are from different contexts. In comparison, SOP is
    harder as it requires the model to fully understand the coherence and ordering
    between segments.
  prefs: []
  type: TYPE_NORMAL
- en: GPT-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The [OpenAI](https://blog.openai.com/better-language-models/) [GPT-2](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
    language model is a direct successor to [GPT](#gpt). GPT-2 has 1.5B parameters,
    10x more than the original GPT, and it achieves SOTA results on 7 out of 8 tested
    language modeling datasets in a *zero-shot transfer setting* without any task-specific
    fine-tuning. The pre-training dataset contains 8 million Web pages collected by
    crawling qualified outbound links from [Reddit](https://www.reddit.com/). Large
    improvements by OpenAI GPT-2 are specially noticeable on small datasets and datasets
    used for measuring *long-term dependency*.
  prefs: []
  type: TYPE_NORMAL
- en: Zero-Shot Transfer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The pre-training task for GPT-2 is solely language modeling. All the downstream
    language tasks are framed as predicting conditional probabilities and there is
    no task-specific fine-tuning.
  prefs: []
  type: TYPE_NORMAL
- en: Text generation is straightforward using LM.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Machine translation task, for example, English to Chinese, is induced by conditioning
    LM on pairs of “English sentence = Chinese sentence” and “the target English sentence
    =” at the end.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For example, the conditional probability to predict might look like: `P(? |
    I like green apples. = 我喜欢绿苹果。 A cat meows at him. = 一只猫对他喵。It is raining cats
    and dogs. =")`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: QA task is formatted similar to translation with pairs of questions and answers
    in the context.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summarization task is induced by adding `TL;DR:` after the articles in the context.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: BPE on Byte Sequences
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Same as the original GPT, GPT-2 uses [BPE](#byte-pair-encoding) but on [UTF-8](https://en.wikipedia.org/wiki/UTF-8)
    byte sequences. Each byte can represent 256 different values in 8 bits, while
    UTF-8 can use up to 4 bytes for one character, supporting up to $2^{31}$ characters
    in total. Therefore, with byte sequence representation we only need a vocabulary
    of size 256 and do not need to worry about pre-processing, tokenization, etc.
    Despite of the benefit, current byte-level LMs still have non-negligible performance
    gap with the SOTA word-level LMs.
  prefs: []
  type: TYPE_NORMAL
- en: BPE merges frequently co-occurred byte pairs in a greedy manner. To prevent
    it from generating multiple versions of common words (i.e. `dog.`, `dog!` and
    `dog?` for the word `dog`), GPT-2 prevents BPE from merging characters across
    categories (thus `dog` would not be merged with punctuations like `.`, `!` and
    `?`). This tricks help increase the quality of the final byte segmentation.
  prefs: []
  type: TYPE_NORMAL
- en: Using the byte sequence representation, GPT-2 is able to assign a probability
    to any Unicode string, regardless of any pre-processing steps.
  prefs: []
  type: TYPE_NORMAL
- en: Model Modifications
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Compared to GPT, other than having many more transformer layers and parameters,
    GPT-2 incorporates only a few architecture modifications:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Layer normalization](https://arxiv.org/abs/1607.06450) was moved to the input
    of each sub-block, similar to a residual unit of type [“building block”](https://arxiv.org/abs/1603.05027)
    (differently from the original type [“bottleneck”](https://arxiv.org/abs/1512.03385),
    it has batch normalization applied before weight layers).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An additional layer normalization was added after the final self-attention block.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A modified initialization was constructed as a function of the model depth.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The weights of residual layers were initially scaled by a factor of $1/ \sqrt{N}$
    where N is the number of residual layers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use larger vocabulary size and context size.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RoBERTa
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**RoBERTa** (short for **R**obustly **o**ptimized **BERT** **a**pproach; [Liu,
    et al. 2019](https://arxiv.org/abs/1907.11692)) refers to a new receipt for training
    BERT to achieve better results, as they found that the original BERT model is
    significantly undertrained. The receipt contains the following learnings:'
  prefs: []
  type: TYPE_NORMAL
- en: Train for longer with bigger batch size.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Remove the [next sentence prediction (NSP)](#nsp) task.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use longer sequences in training data format. The paper found that using individual
    sentences as inputs hurts downstream performance. Instead we should use multiple
    sentences sampled contiguously to form longer segments.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Change the masking pattern dynamically. The original BERT applies masking once
    during the data preprocessing stage, resulting in a static mask across training
    epochs. RoBERTa applies masks in 10 different ways across 40 epochs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: RoBERTa also added a new dataset [CommonCrawl News](https://commoncrawl.org/2016/10/news-dataset-available/)
    and further confirmed that pretraining with *more data helps* improve the performance
    on downstream tasks. It was trained with the [BPE on byte sequences](#bpe-on-byte-sequences),
    same as in [GPT-2](#gpt-2). They also found that choices of hyperparameters have
    a big impact on the model performance.
  prefs: []
  type: TYPE_NORMAL
- en: T5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The language model **T5** is short for **“Text-to-Text Transfer Transformer”**
    ([Raffel et al., 2020](https://arxiv.org/abs/1910.10683)). The encoder-decoder
    implementation follows the [original Transformer](https://arxiv.org/abs/1706.03762)
    architecture: tokens → embedding → encoder → decoder → output. T5 adopts the framework
    “Natural Language Decathlon” ([McCann et al., 2018](https://arxiv.org/abs/1806.08730)),
    where many common NLP tasks are translated into question-answering over a context.
    Instead of an explicit QA format, T5 uses short task prefixes to distinguish task
    intentions and separately fine-tunes the model on every individual task. The text-to-text
    framework enables easier transfer learning evaluation with the same model on a
    diverse set of tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b46018486da8da6afc2f688082d8ac7f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 13\. A diagram of T5 task evaluation. The text-to-text framework casts
    every task into a generic form: feeding input text to predict some target text.
    (Image source: [Raffel et al., 2020](https://arxiv.org/abs/1910.10683))'
  prefs: []
  type: TYPE_NORMAL
- en: The model is trained on Web corpus extracted from Apr 2019 with various filters
    applied. The model is fine-tuned for each downstream task separately via “adapter
    layers” (add an extra layer for training) or “gradual unfreezing” (see [ULMFiT](#ulmfit)).
    Both fine-tuning approaches only update partial parameters while keeping the majority
    of the model parameters unchanged. T5-11B achieved SOTA results on many NLP tasks.
  prefs: []
  type: TYPE_NORMAL
- en: As the authors mentioned in the paper “…our goal is not to propose new methods
    but instead to provide a comprehensive perspective on where the field stands”,
    the T5 long paper described a lot of training setup and evaluation processes in
    detail, a good read for people who are interested in training a LM from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: GPT-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**GPT-3** ([Brown et al., 2020](https://arxiv.org/abs/2005.14165)) has the
    same architecture as [GPT-2](#gpt-2) but contains 175B parameters, 10x larger
    than GPT-2 (1.5B). In addition, GPT-3 uses alternating dense and locally banded
    sparse attention patterns, same as in [sparse transformer](https://lilianweng.github.io/posts/2020-04-07-the-transformer-family/#sparse-attention-matrix-factorization-sparse-transformers).
    In order to fit such a huge model across multiple GPUs, GPT-3 is trained with
    partitions along both width and depth dimension. The training data is a filtered
    version of Common Crawl mixed with a few other high-quality curated datasets.
    To avoid the contamination that downstream tasks might appear in the training
    data, the authors attempted to remove all the overlaps with all the studied benchmark
    dataset from the training dataset. Unfortunately the filtering process is not
    perfect due to a bug.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1ffc7843a4c8d03d87aa78cb5ccae80b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 14\. Training datasets for GPT-3\. Note that the occurrence of each dataset
    during training is not proportional to the dataset size. (Table source: [Brown
    et al., 2020](https://arxiv.org/abs/2005.14165))'
  prefs: []
  type: TYPE_NORMAL
- en: For all the downstream evaluation, GPT-3 is tested in the few-shot setting without
    any gradient-based fine-tuning. Here the few-shot examples are provided as part
    of the prompt. GPT-3 achieves strong performance on many NLP datasets, comparable
    with fine-tuned BERT models.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/69548e287ddc9f879f2592db08695b5b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 15\. The evaluation performance increases with the model size and the
    number of examples. (Image source: [Brown et al., 2020](https://papers.nips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html))'
  prefs: []
  type: TYPE_NORMAL
- en: XLNet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The *Autoregressive (AR)* model such as GPT and *autoencoder (AE)* model such
    as BERT are two most common ways for language modeling. However, each has their
    own disadvantages: AR does not learn the bidirectional context, which is needed
    by downstream tasks like reading comprehension and AE assumes masked positions
    are independent given all other unmasked tokens which oversimplifies the long
    context dependency.'
  prefs: []
  type: TYPE_NORMAL
- en: '**XLNet** ([Yang et al. 2019](https://arxiv.org/abs/1906.08237)) generalizes
    the AE method to incorporate the benefits of AR. XLNet proposed the **permutation
    language modeling** objective. For a text sequence, it samples a factorization
    order $\mathbf{z}$ and decomposes the likelihood $p_\theta(\mathbf{x})$ according
    to this factorization order,'
  prefs: []
  type: TYPE_NORMAL
- en: $$ \begin{aligned} \mathcal{L}_\text{XLNet} &= - \mathbb{E}_{\mathbf{z} \sim
    \mathcal{Z}_T} \Big[ \sum_{t=1}^T \log p_\theta (X_{z_t} = x \mid \mathbf{x}_{\mathbf{z}_{
  prefs: []
  type: TYPE_NORMAL
- en: where $\mathcal{Z}_T$ is a set of all possible permutation of length $T$; $z_t$
    and $\mathbf{z}_{<t}$ denote the $t$-th element and the first $t-1$ elements of
    a permutation $\mathbf{z} \in \mathcal{Z}_T$.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the naive representation of the hidden state of the context, $h_\theta
    (\mathbf{x}_{\mathbf{z}_{<t}})$ in red, does not depend on which position the
    model tries to predict, as the permutation breaks the default ordering. Therefore,
    XLNet re-parameterized it to a function of the target position too, $g_\theta
    (\mathbf{x}_{\mathbf{z}_{<t}}, z_t)$ in blue.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, two different requirements on $g_\theta (\mathbf{x}_{\mathbf{z}_{<t}},
    z_t)$ lead to a two-stream self-attention design to accommodate:'
  prefs: []
  type: TYPE_NORMAL
- en: When predicting $x_{z_t}$, it should only encode the position $z_t$ but not
    the content $x_{z_t}$; otherwise it is trivial. This is wrapped into the “query
    representation” $g_{z_t} = g_\theta (\mathbf{x}_{\mathbf{z}_{<t}}, z_t)$ does
    not encode $x_{z_t}$.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When predicting $x_j$ where $j > t$, it should encode the content $x_{z_t}$
    as well to provide the full context. This is the “content representation” $h_{z_t}
    = h_\theta(\mathbf{x}_{\leq t})$.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/892f41fe7fba16cf7d7ff7c461cc925a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 16\. The illustration of two-stream self-attention mechanism in XLNet.
    (Image source: [Yang et al. 2019](https://arxiv.org/abs/1906.08237))'
  prefs: []
  type: TYPE_NORMAL
- en: Conceptually, the two streams of representations are updated as follows,
  prefs: []
  type: TYPE_NORMAL
- en: $$ \begin{aligned} g_{z_t}^{(m)} &\gets \text{Attention}(Q = g^{(m-1)}_{z_t},
    KV=\mathbf{h}^{(m-1)}_{\color{red}{\mathbf{z}_{
  prefs: []
  type: TYPE_NORMAL
- en: Given the difficulty of optimization in permutation language modeling, XLNet
    is set to only predict the last chunk of tokens in a factorization order.
  prefs: []
  type: TYPE_NORMAL
- en: The name in XLNet actually comes from [Transformer-XL](https://lilianweng.github.io/posts/2020-04-07-the-transformer-family/#longer-attention-span-transformer-xl).
    It incorporates the design of Transformer-XL to extend the attention span by reusing
    hidden states from previous segments.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a79a7223aef1053bb1991b0e5912ffa9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 17\. Comparison of model performance of XLNet with a couple other language
    models on GLUE, all single-task, no ensembles. (Image source: [Yang et al. 2019](https://arxiv.org/abs/1906.08237))'
  prefs: []
  type: TYPE_NORMAL
- en: BART
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**BART** ([Lewis et al., 2019](https://arxiv.org/abs/1910.13461)) is a denoising
    autoencoder to recover the original text from a randomly corrupted version. It
    combines **B**idirectional and **A**uto**R**egressive **T**ransformer: precisely,
    jointly training BERT-like bidirectional encoder and GPT-like autoregressive decoder
    together. The loss is simply just to minimize the negative log-likelihood.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3366a233257fc3ad2b3ca2b0a394261c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 18\. A schematic comparison of BART with BERT and GPT. (Image source:
    [Lewis et al., 2019](https://arxiv.org/abs/1910.13461))'
  prefs: []
  type: TYPE_NORMAL
- en: They experimented with a variety of noising transformations, including token
    masking, token deletion, text infilling (i.e. A randomly sampled text span, which
    may contain multiple tokens, is replaced with a `[MASK]` token), sentence permutation,
    documentation rotation (i.e. A document is rotated to begin with a random token.).
    The best noising approach they discovered is text infilling and sentence shuffling.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/13b2e600fbf7511dd2131c7c31662dea.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 19\. Comparison of different language modeling pre-training objectives.
    (Image source: [Lewis et al., 2019](https://arxiv.org/abs/1910.13461))'
  prefs: []
  type: TYPE_NORMAL
- en: 'Learnings from their experiments:'
  prefs: []
  type: TYPE_NORMAL
- en: The performance of pre-training methods varies significantly across downstream
    tasks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Token masking is crucial, as the performance is poor when only sentence permutation
    or documentation rotation is applied.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Left-to-right pre-training improves generation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bidirectional encoders are crucial for SQuAD.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The pre-training objective is not the only important factor. Architectural improvements
    such as relative-position embeddings or segment-level recurrence matter too.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Autoregressive language models perform best on ELI5.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: BART achieves the most consistently strong performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ELECTRA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most current pre-training large language models demand a lot of computation
    resources, raising concerns about their cost and accessibility. **ELECTRA** (“Efficiently
    Learning an Encoder that Classifies Token Replacements Accurately”; [Clark et
    al. 2020](https://arxiv.org/abs/2003.10555)) aims to improve the *pre-training
    efficiency*, which frames the language modeling as a discrimination task instead
    of generation task.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4030428bda894cc7bb7ad335bb189190.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 20\. Illustration of ELECTRA model architecture. (Image source: [Clark
    et al. 2020](https://arxiv.org/abs/2003.10555))'
  prefs: []
  type: TYPE_NORMAL
- en: ELECTRA proposes a new pretraining task, called “Replaced Token Detection” (RTD).
    Let’s randomly sample $k$ positions to be masked. Each selected token in the original
    text is replaced by a plausible alternative predicted by a small language model,
    known as the generator $G$. The discriminator $D$ predicts whether each token
    is original or replaced.
  prefs: []
  type: TYPE_NORMAL
- en: $$ \begin{aligned} \boldsymbol{m} &= [m_1, \dots, m_k] \text{ where } m_i \sim
    \text{unif}\{1, n\}\text{ for } i=1, \dots, k \\ \boldsymbol{x}^\text{masked}
    &= \text{REPLACE}(\boldsymbol{x}, \boldsymbol{m}, \texttt{[MASK]}) \\ \boldsymbol{x}^\text{corrupt}
    &= \text{REPLACE}(\boldsymbol{x}, \boldsymbol{m}, \tilde{\boldsymbol{x}}) \text{
    where } \tilde{x}_t \sim p_G(x_i \mid \boldsymbol{x}^\text{masked}) \text{ for
    } i \in \boldsymbol{m} \\ \end{aligned} $$
  prefs: []
  type: TYPE_NORMAL
- en: The loss for the generator is the negative log-likelihood just as in other language
    models. The loss for the discriminator is the cross-entropy. Note that the generator
    is not adversarially trained to fool the discriminator but simply to optimize
    the NLL, since their experiments show negative results.
  prefs: []
  type: TYPE_NORMAL
- en: $$ \begin{aligned} \mathcal{L}_\text{MLM}(\mathbf{x}, \theta_G) &= \mathbb{E}\Big(\sum_{i
    \in \boldsymbol{m}} -\log p_G (x_i \mid \boldsymbol{x}^\text{masked} )\Big) \\
    \mathcal{L}_\text{Disc}(\mathbf{x}, \theta_D) &= \mathbb{E}\Big( - \mathbb{1}[x^\text{corrupt}_t
    = x_t] \log D(\boldsymbol{x}^\text{corrupt}, t) - \mathbb{1}[x^\text{corrupt}_t
    \neq x_t] \log (1 - \log D(\boldsymbol{x}^\text{corrupt}, t)) \Big) \end{aligned}
    $$
  prefs: []
  type: TYPE_NORMAL
- en: They found it more beneficial to only share the embeddings between generator
    & discriminator while using a small generator (1/4 to 1/2 the discriminator size),
    rather than sharing all the weights (i.e. two models have to be the same size
    then). In addition, joint training of the generator and discriminator works better
    than two-stage training of each alternatively.
  prefs: []
  type: TYPE_NORMAL
- en: After pretraining the generator is discarded and only the ELECTRA discriminator
    is fine-tuned further for downstream tasks. The following table shows ELECTRA’s
    performance on the GLUE dev set.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d275ea694a92c86cd6c82e941e3211dc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 21\. Comparison of ELECTRA with other language models on the GLUE dev
    set. (Image source: [Clark et al. 2020](https://arxiv.org/abs/2003.10555))'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '|  | Base model | Pretraining Tasks |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| CoVe | seq2seq NMT model | supervised learning using translation dataset.
    |'
  prefs: []
  type: TYPE_TB
- en: '| ELMo | two-layer biLSTM | next token prediction |'
  prefs: []
  type: TYPE_TB
- en: '| CVT | two-layer biLSTM | semi-supervised learning using both labeled and
    unlabeled datasets |'
  prefs: []
  type: TYPE_TB
- en: '| ULMFiT | AWD-LSTM | autoregressive pretraining on Wikitext-103 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT | Transformer decoder | next token prediction |'
  prefs: []
  type: TYPE_TB
- en: '| BERT | Transformer encoder | mask language model + next sentence prediction
    |'
  prefs: []
  type: TYPE_TB
- en: '| ALBERT | same as BERT but light-weighted | mask language model + sentence
    order prediction |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-2 | Transformer decoder | next token prediction |'
  prefs: []
  type: TYPE_TB
- en: '| RoBERTa | same as BERT | mask language model (dynamic masking) |'
  prefs: []
  type: TYPE_TB
- en: '| T5 | Transformer encoder + decoder | pre-trained on a multi-task mixture
    of unsupervised and supervised tasks and for which each task is converted into
    a text-to-text format. |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-3 | Transformer decoder | next token prediction |'
  prefs: []
  type: TYPE_TB
- en: '| XLNet | same as BERT | permutation language modeling |'
  prefs: []
  type: TYPE_TB
- en: '| BART | BERT encoder + GPT decoder | reconstruct text from a noised version
    |'
  prefs: []
  type: TYPE_TB
- en: '| ELECTRA | same as BERT | replace token detection |'
  prefs: []
  type: TYPE_TB
- en: 'Metric: Perplexity'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Perplexity is often used as an intrinsic evaluation metric for gauging how well
    a language model can capture the real word distribution conditioned on the context.
  prefs: []
  type: TYPE_NORMAL
- en: 'A [perplexity](https://en.wikipedia.org/wiki/Perplexity) of a discrete proability
    distribution $p$ is defined as the exponentiation of the entropy:'
  prefs: []
  type: TYPE_NORMAL
- en: $$ 2^{H(p)} = 2^{-\sum_x p(x) \log_2 p(x)} $$
  prefs: []
  type: TYPE_NORMAL
- en: 'Given a sentence with $N$ words, $s = (w_1, \dots, w_N)$, the entropy looks
    as follows, simply assuming that each word has the same frequency, $\frac{1}{N}$:'
  prefs: []
  type: TYPE_NORMAL
- en: $$ H(s) = -\sum_{i=1}^N P(w_i) \log_2 p(w_i) = -\sum_{i=1}^N \frac{1}{N} \log_2
    p(w_i) $$
  prefs: []
  type: TYPE_NORMAL
- en: 'The perplexity for the sentence becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: $$ \begin{aligned} 2^{H(s)} &= 2^{-\frac{1}{N} \sum_{i=1}^N \log_2 p(w_i)} =
    (2^{\sum_{i=1}^N \log_2 p(w_i)})^{-\frac{1}{N}} = (p(w_1) \dots p(w_N))^{-\frac{1}{N}}
    \end{aligned} $$
  prefs: []
  type: TYPE_NORMAL
- en: A good language model should predict high word probabilities. Therefore, the
    smaller perplexity the better.
  prefs: []
  type: TYPE_NORMAL
- en: Common Tasks and Datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Question-Answering**'
  prefs: []
  type: TYPE_NORMAL
- en: '[SQuAD](https://rajpurkar.github.io/SQuAD-explorer/) (Stanford Question Answering
    Dataset): A reading comprehension dataset, consisting of questions posed on a
    set of Wikipedia articles, where the answer to every question is a span of text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[RACE](http://www.qizhexie.com/data/RACE_leaderboard) (ReAding Comprehension
    from Examinations): A large-scale reading comprehension dataset with more than
    28,000 passages and nearly 100,000 questions. The dataset is collected from English
    examinations in China, which are designed for middle school and high school students.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See [more QA datasets in a later post](https://lilianweng.github.io/posts/2020-10-29-odqa/#appendix-qa-datasets).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Commonsense Reasoning**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Story Cloze Test](http://cs.rochester.edu/nlp/rocstories/): A commonsense
    reasoning framework for evaluating story understanding and generation. The test
    requires a system to choose the correct ending to multi-sentence stories from
    two options.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[SWAG](https://rowanzellers.com/swag/) (Situations With Adversarial Generations):
    multiple choices; contains 113k sentence-pair completion examples that evaluate
    grounded common-sense inference'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Natural Language Inference (NLI)**: also known as **Text Entailment**, an
    exercise to discern in logic whether one sentence can be inferred from another.'
  prefs: []
  type: TYPE_NORMAL
- en: '[RTE](https://aclweb.org/aclwiki/Textual_Entailment_Resource_Pool) (Recognizing
    Textual Entailment): A set of datasets initiated by text entailment challenges.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[SNLI](https://nlp.stanford.edu/projects/snli/) (Stanford Natural Language
    Inference): A collection of 570k human-written English sentence pairs manually
    labeled for balanced classification with the labels `entailment`, `contradiction`,
    and `neutral`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[MNLI](https://www.nyu.edu/projects/bowman/multinli/) (Multi-Genre NLI): Similar
    to SNLI, but with a more diverse variety of text styles and topics, collected
    from transcribed speech, popular fiction, and government reports.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[QNLI](https://gluebenchmark.com/tasks) (Question NLI): Converted from SQuAD
    dataset to be a binary classification task over pairs of (question, sentence).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[SciTail](http://data.allenai.org/scitail/): An entailment dataset created
    from multiple-choice science exams and web sentences.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Named Entity Recognition (NER)**: labels sequences of words in a text which
    are the names of things, such as person and company names, or gene and protein
    names'
  prefs: []
  type: TYPE_NORMAL
- en: '[CoNLL 2003 NER task](https://www.clips.uantwerpen.be/conll2003/): consists
    of newswire from the Reuters, concentrating on four types of named entities: persons,
    locations, organizations and names of miscellaneous entities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[OntoNotes 5.0](https://catalog.ldc.upenn.edu/LDC2013T19): This corpus contains
    text in English, Arabic and Chinese, tagged with four different entity types (PER,
    LOC, ORG, MISC).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Reuters Corpus](https://trec.nist.gov/data/reuters/reuters.html): A large
    collection of Reuters News stories.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fine-Grained NER (FGN)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sentiment Analysis**'
  prefs: []
  type: TYPE_NORMAL
- en: '[SST](https://nlp.stanford.edu/sentiment/index.html) (Stanford Sentiment Treebank)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[IMDb](http://ai.stanford.edu/~amaas/data/sentiment/): A large dataset of movie
    reviews with binary sentiment classification labels.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Semantic Role Labeling (SRL)**: models the predicate-argument structure of
    a sentence, and is often described as answering “Who did what to whom”.'
  prefs: []
  type: TYPE_NORMAL
- en: '[CoNLL-2004 & CoNLL-2005](http://www.lsi.upc.edu/~srlconll/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sentence similarity**: also known as *paraphrase detection*'
  prefs: []
  type: TYPE_NORMAL
- en: '[MRPC](https://www.microsoft.com/en-us/download/details.aspx?id=52398) (MicRosoft
    Paraphrase Corpus): It contains pairs of sentences extracted from news sources
    on the web, with annotations indicating whether each pair is semantically equivalent.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[QQP](https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs) (Quora
    Question Pairs) STS Benchmark: Semantic Textual Similarity'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sentence Acceptability**: a task to annotate sentences for grammatical acceptability.'
  prefs: []
  type: TYPE_NORMAL
- en: '[CoLA](https://nyu-mll.github.io/CoLA/) (Corpus of Linguistic Acceptability):
    a binary single-sentence classification task.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Text Chunking**: To divide a text in syntactically correlated parts of words.'
  prefs: []
  type: TYPE_NORMAL
- en: '[CoNLL-2000](https://www.clips.uantwerpen.be/conll2000/chunking/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Part-of-Speech (POS) Tagging**: tag parts of speech to each token, such as
    noun, verb, adjective, etc. the Wall Street Journal portion of the Penn Treebank
    (Marcus et al., 1993).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Machine Translation**: See [Standard NLP](https://nlp.stanford.edu/projects/nmt/)
    page.'
  prefs: []
  type: TYPE_NORMAL
- en: WMT 2015 English-Czech data (Large)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: WMT 2014 English-German data (Medium)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: IWSLT 2015 English-Vietnamese data (Small)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Coreference Resolution**: cluster mentions in text that refer to the same
    underlying real world entities.'
  prefs: []
  type: TYPE_NORMAL
- en: '[CoNLL-2012](http://conll.cemantix.org/2012/data.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Long-range Dependency**'
  prefs: []
  type: TYPE_NORMAL
- en: '[LAMBADA](http://clic.cimec.unitn.it/lambada/) (LAnguage Modeling Broadened
    to Account for Discourse Aspects): A collection of narrative passages extracted
    from the BookCorpus and the task is to predict the last word, which require at
    least 50 tokens of context for a human to successfully predict.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Children’s Book Test](https://research.fb.com/downloads/babi/): is built from
    books that are freely available in [Project Gutenberg](https://www.gutenberg.org/).
    The task is to predict the missing word among 10 candidates.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multi-task benchmark**'
  prefs: []
  type: TYPE_NORMAL
- en: 'GLUE multi-task benchmark: [https://gluebenchmark.com](https://gluebenchmark.com/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'decaNLP benmark: [https://decanlp.com](https://decanlp.com/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unsupervised pretraining dataset**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Books corpus](https://googlebooks.byu.edu/): The corpus contains “over 7,000
    unique unpublished books from a variety of genres including Adventure, Fantasy,
    and Romance.”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[1B Word Language Model Benchmark](http://www.statmt.org/lm-benchmark/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[English Wikipedia](https://en.wikipedia.org/wiki/Wikipedia:Database_download#English-language_Wikipedia):
    ~2500M words'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 'Cited as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Reference
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] Bryan McCann, et al. [“Learned in translation: Contextualized word vectors.”](https://arxiv.org/abs/1708.00107)
    NIPS. 2017.'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] Kevin Clark et al. [“Semi-Supervised Sequence Modeling with Cross-View
    Training.”](https://arxiv.org/abs/1809.08370) EMNLP 2018.'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] Matthew E. Peters, et al. [“Deep contextualized word representations.”](https://arxiv.org/abs/1802.05365)
    NAACL-HLT 2017.'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] OpenAI Blog [“Improving Language Understanding with Unsupervised Learning”](https://blog.openai.com/language-unsupervised/),
    June 11, 2018.'
  prefs: []
  type: TYPE_NORMAL
- en: '[5] OpenAI Blog [“Better Language Models and Their Implications.”](https://blog.openai.com/better-language-models/)
    Feb 14, 2019.'
  prefs: []
  type: TYPE_NORMAL
- en: '[6] Jeremy Howard and Sebastian Ruder. [“Universal language model fine-tuning
    for text classification.”](https://arxiv.org/abs/1801.06146) ACL 2018.'
  prefs: []
  type: TYPE_NORMAL
- en: '[7] Alec Radford et al. [“Improving Language Understanding by Generative Pre-Training”](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf).
    OpenAI Blog, June 11, 2018.'
  prefs: []
  type: TYPE_NORMAL
- en: '[8] Jacob Devlin, et al. [“BERT: Pre-training of deep bidirectional transformers
    for language understanding.”](https://arxiv.org/abs/1810.04805) arXiv:1810.04805
    (2018).'
  prefs: []
  type: TYPE_NORMAL
- en: '[9] Mike Schuster, and Kaisuke Nakajima. [“Japanese and Korean voice search.”](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/37842.pdf)
    ICASSP. 2012.'
  prefs: []
  type: TYPE_NORMAL
- en: '[10] Google’s Neural Machine Translation System: Bridging the Gap between Human
    and Machine Translation'
  prefs: []
  type: TYPE_NORMAL
- en: '[11] Ashish Vaswani, et al. [“Attention is all you need.”](https://arxiv.org/abs/1706.03762)
    NIPS 2017.'
  prefs: []
  type: TYPE_NORMAL
- en: '[12] Peter J. Liu, et al. [“Generating wikipedia by summarizing long sequences.”](https://arxiv.org/abs/1801.10198)
    ICLR 2018.'
  prefs: []
  type: TYPE_NORMAL
- en: '[13] Sebastian Ruder. [“10 Exciting Ideas of 2018 in NLP”](http://ruder.io/10-exciting-ideas-of-2018-in-nlp/)
    Dec 2018.'
  prefs: []
  type: TYPE_NORMAL
- en: '[14] Alec Radford, et al. [“Language Models are Unsupervised Multitask Learners.”](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf).
    2019.'
  prefs: []
  type: TYPE_NORMAL
- en: '[15] Rico Sennrich, et al. [“Neural machine translation of rare words with
    subword units.”](https://arxiv.org/abs/1508.07909) arXiv preprint arXiv:1508.07909\.
    2015.'
  prefs: []
  type: TYPE_NORMAL
- en: '[16] Zhenzhong Lan, et al. [“ALBERT: A Lite BERT for Self-supervised Learning
    of Language Representations.”](https://arxiv.org/abs/1909.11942) arXiv Preprint
    arXiv:1909.11942 (2019).'
  prefs: []
  type: TYPE_NORMAL
- en: '[17] Yinhan Liu, et al. [“RoBERTa: A Robustly Optimized BERT Pretraining Approach.”](https://arxiv.org/abs/1907.11692)
    arXiv Preprint arXiv:1907.11692 (2019).'
  prefs: []
  type: TYPE_NORMAL
- en: '[18] Tom B Brown, et al. [“Language Models are Few-Shot Learners”](https://arxiv.org/abs/2005.14165)
    NeuriPS 2020.'
  prefs: []
  type: TYPE_NORMAL
- en: '[19] Zhilin Yang et al. [“XLNet: Generalized Autoregressive Pretraining for
    Language Understanding.”](https://arxiv.org/abs/1906.08237) NeuriPS 2019.'
  prefs: []
  type: TYPE_NORMAL
- en: '[20] Mike Lewis et al. [“BART: Denoising Sequence-to-Sequence Pre-training
    for Natural Language Generation, Translation, and Comprehension.”](https://arxiv.org/abs/1910.13461)
    ACL 2020.'
  prefs: []
  type: TYPE_NORMAL
- en: '[21] Kevin Clark et al. [“ELECTRA: Pre-training Text Encoders as Discriminators
    Rather Than Generators.”](https://arxiv.org/abs/2003.10555) ICLR 2020.'
  prefs: []
  type: TYPE_NORMAL
- en: '[22] Colin Raffel, et al. [“Exploring the Limits of Transfer Learning with
    a Unified Text-to-Text Transformer”](https://arxiv.org/abs/1910.10683) JMLR 2020.'
  prefs: []
  type: TYPE_NORMAL
- en: '[architecture](https://lilianweng.github.io/tags/architecture/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[nlp](https://lilianweng.github.io/tags/nlp/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[long-read](https://lilianweng.github.io/tags/long-read/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[transformer](https://lilianweng.github.io/tags/transformer/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[attention](https://lilianweng.github.io/tags/attention/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[language-model](https://lilianweng.github.io/tags/language-model/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[«'
  prefs: []
  type: TYPE_NORMAL
- en: Are Deep Neural Networks Dramatically Overfitted?](https://lilianweng.github.io/posts/2019-03-14-overfit/)
    [»
  prefs: []
  type: TYPE_NORMAL
- en: 'Object Detection Part 4: Fast Detection Models](https://lilianweng.github.io/posts/2018-12-27-object-recognition-part-4/)[](https://twitter.com/intent/tweet/?text=Generalized%20Language%20Models&url=https%3a%2f%2flilianweng.github.io%2fposts%2f2019-01-31-lm%2f&hashtags=architecture%2cnlp%2clong-read%2ctransformer%2cattention%2clanguage-model)[](https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2flilianweng.github.io%2fposts%2f2019-01-31-lm%2f&title=Generalized%20Language%20Models&summary=Generalized%20Language%20Models&source=https%3a%2f%2flilianweng.github.io%2fposts%2f2019-01-31-lm%2f)[](https://reddit.com/submit?url=https%3a%2f%2flilianweng.github.io%2fposts%2f2019-01-31-lm%2f&title=Generalized%20Language%20Models)[](https://facebook.com/sharer/sharer.php?u=https%3a%2f%2flilianweng.github.io%2fposts%2f2019-01-31-lm%2f)[](https://api.whatsapp.com/send?text=Generalized%20Language%20Models%20-%20https%3a%2f%2flilianweng.github.io%2fposts%2f2019-01-31-lm%2f)[](https://telegram.me/share/url?text=Generalized%20Language%20Models&url=https%3a%2f%2flilianweng.github.io%2fposts%2f2019-01-31-lm%2f)©
    2024 [Lil''Log](https://lilianweng.github.io/) Powered by [Hugo](https://gohugo.io/)
    & [PaperMod](https://git.io/hugopapermod)[](#top "Go to Top (Alt + G)")'
  prefs: []
  type: TYPE_NORMAL
