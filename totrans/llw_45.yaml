- en: 'Predict Stock Prices Using RNN: Part 1'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://lilianweng.github.io/posts/2017-07-08-stock-rnn-part-1/](https://lilianweng.github.io/posts/2017-07-08-stock-rnn-part-1/)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This is a tutorial for how to build a recurrent neural network using Tensorflow
    to predict stock market prices. The full working code is available in [github.com/lilianweng/stock-rnn](https://github.com/lilianweng/stock-rnn).
    If you don’t know what is recurrent neural network or LSTM cell, feel free to
    check [my previous post](https://lilianweng.github.io/posts/2017-06-21-overview/#recurrent-neural-network).
  prefs: []
  type: TYPE_NORMAL
- en: '*One thing I would like to emphasize that because my motivation for writing
    this post is more on demonstrating how to build and train an RNN model in Tensorflow
    and less on solve the stock prediction problem, I didn’t try hard on improving
    the prediction outcomes. You are more than welcome to take my [code](https://github.com/lilianweng/stock-rnn)
    as a reference point and add more stock prediction related ideas to improve it.
    Enjoy!*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Overview of Existing Tutorials
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are many tutorials on the Internet, like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[A noob’s guide to implementing RNN-LSTM using Tensorflow](http://monik.in/a-noobs-guide-to-implementing-rnn-lstm-using-tensorflow/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[TensorFlow RNN Tutorial](https://svds.com/tensorflow-rnn-tutorial/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[LSTM by Example using Tensorflow](https://medium.com/towards-data-science/lstm-by-example-using-tensorflow-feb0c1968537)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[How to build a Recurrent Neural Network in TensorFlow](https://medium.com/@erikhallstrm/hello-world-rnn-83cd7105b767)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[RNNs in Tensorflow, a Practical Guide and Undocumented Features](http://www.wildml.com/2016/08/rnns-in-tensorflow-a-practical-guide-and-undocumented-features/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Sequence prediction using recurrent neural networks(LSTM) with TensorFlow](http://mourafiq.com/2016/05/15/predicting-sequences-using-rnn-in-tensorflow.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Anyone Can Learn To Code an LSTM-RNN in Python](https://iamtrask.github.io/2015/11/15/anyone-can-code-lstm/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[How to do time series prediction using RNNs, TensorFlow and Cloud ML Engine](https://medium.com/google-cloud/how-to-do-time-series-prediction-using-rnns-and-tensorflow-and-cloud-ml-engine-2ad2eeb189e8)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Despite all these existing tutorials, I still want to write a new one mainly
    for three reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: Early tutorials cannot cope with the new version any more, as Tensorflow is
    still under development and changes on API interfaces are being made fast.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Many tutorials use synthetic data in the examples. Well, I would like to play
    with the real world data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Some tutorials assume that you have known something about Tensorflow API beforehand,
    which makes the reading a bit difficult.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After reading a bunch of examples, I would like to suggest taking the [official
    example](https://github.com/tensorflow/models/tree/master/tutorials/rnn/ptb) on
    Penn Tree Bank (PTB) dataset as your starting point. The PTB example showcases
    a RNN model in a pretty and modular design pattern, but it might prevent you from
    easily understanding the model structure. Hence, here I will build up the graph
    in a very straightforward manner.
  prefs: []
  type: TYPE_NORMAL
- en: The Goal
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I will explain how to build an RNN model with LSTM cells to predict the prices
    of S&P500 index. The dataset can be downloaded from [Yahoo! Finance ^GSPC](https://finance.yahoo.com/quote/%5EGSPC/history?p=%5EGSPC).
    In the following example, I used S&P 500 data from Jan 3, 1950 (the maximum date
    that Yahoo! Finance is able to trace back to) to Jun 23, 2017\. The dataset provides
    several price points per day. For simplicity, we will only use the daily **close
    prices** for prediction. Meanwhile, I will demonstrate how to use [TensorBoard](https://www.tensorflow.org/get_started/summaries_and_tensorboard)
    for easily debugging and model tracking.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a quick recap: the recurrent neural network (RNN) is a type of artificial
    neural network with self-loop in its hidden layer(s), which enables RNN to use
    the previous state of the hidden neuron(s) to learn the current state given the
    new input. RNN is good at processing sequential data. Long short-term memory (LSTM)
    cell is a specially designed working unit that helps RNN better memorize the long-term
    context.'
  prefs: []
  type: TYPE_NORMAL
- en: For more information in depth, please read [my previous post](https://lilianweng.github.io/posts/2017-06-21-overview/#recurrent-neural-network)
    or [this awesome post](http://colah.github.io/posts/2015-08-Understanding-LSTMs/).
  prefs: []
  type: TYPE_NORMAL
- en: Data Preparation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The stock prices is a time series of length $N$, defined as $p_0, p_1, \dots,
    p_{N-1}$ in which $p_i$ is the close price on day $i$, $0 \le i < N$. Imagine
    that we have a sliding window of a fixed size $w$ (later, we refer to this as
    `input_size`) and every time we move the window to the right by size $w$, so that
    there is no overlap between data in all the sliding windows.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/56ecd91f38f2fe1c3936c0452ee1067d.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig. 1\. The S&P 500 prices in time. We use content in one sliding windows to
    make prediction for the next, while there is no overlap between two consecutive
    windows.
  prefs: []
  type: TYPE_NORMAL
- en: 'The RNN model we are about to build has LSTM cells as basic hidden units. We
    use values from the very beginning in the first sliding window $W_0$ to the window
    $W_t$ at time $t$:'
  prefs: []
  type: TYPE_NORMAL
- en: $$ \begin{aligned} W_0 &= (p_0, p_1, \dots, p_{w-1}) \\ W_1 &= (p_w, p_{w+1},
    \dots, p_{2w-1}) \\ \dots \\ W_t &= (p_{tw}, p_{tw+1}, \dots, p_{(t+1)w-1}) \end{aligned}
    $$
  prefs: []
  type: TYPE_NORMAL
- en: 'to predict the prices in the following window $w_{t+1}$:'
  prefs: []
  type: TYPE_NORMAL
- en: $$ W_{t+1} = (p_{(t+1)w}, p_{(t+1)w+1}, \dots, p_{(t+2)w-1}) $$
  prefs: []
  type: TYPE_NORMAL
- en: Essentially we try to learn an approximation function, $f(W_0, W_1, \dots, W_t)
    \approx W_{t+1}$.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2b88867f0dfc3dd86d13952d601ea872.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig. 2 The unrolled version of RNN.
  prefs: []
  type: TYPE_NORMAL
- en: Considering how [back propagation through time (BPTT)](https://en.wikipedia.org/wiki/Backpropagation_through_time)
    works, we usually train RNN in a “unrolled” version so that we don’t have to do
    propagation computation too far back and save the training complication.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the explanation on `num_steps` from [Tensorflow’s tutorial](tensorflow.org/tutorials/recurrent):'
  prefs: []
  type: TYPE_NORMAL
- en: By design, the output of a recurrent neural network (RNN) depends on arbitrarily
    distant inputs. Unfortunately, this makes backpropagation computation difficult.
    In order to make the learning process tractable, it is common practice to create
    an “unrolled” version of the network, which contains a fixed number (`num_steps`)
    of LSTM inputs and outputs. The model is then trained on this finite approximation
    of the RNN. This can be implemented by feeding inputs of length `num_steps` at
    a time and performing a backward pass after each such input block.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The sequence of prices are first split into non-overlapped small windows. Each
    contains `input_size` numbers and each is considered as one independent input
    element. Then any `num_steps` consecutive input elements are grouped into one
    training input, forming an **“un-rolled”** version of RNN for training on Tensorfow.
    The corresponding label is the input element right after them.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, if `input_size=3` and `num_steps=2`, my first few training examples
    would look like:'
  prefs: []
  type: TYPE_NORMAL
- en: $$ \begin{aligned} \text{Input}_1 &= [[p_0, p_1, p_2], [p_3, p_4, p_5]]\quad\text{Label}_1
    = [p_6, p_7, p_8] \\ \text{Input}_2 &= [[p_3, p_4, p_5], [p_6, p_7, p_8]]\quad\text{Label}_2
    = [p_9, p_{10}, p_{11}] \\ \text{Input}_3 &= [[p_6, p_7, p_8], [p_9, p_{10}, p_{11}]]\quad\text{Label}_3
    = [p_{12}, p_{13}, p_{14}] \end{aligned} $$
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the key part for formatting the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The complete code of data formatting is [here](https://github.com/lilianweng/stock-rnn/blob/master/data_wrapper.py).
  prefs: []
  type: TYPE_NORMAL
- en: Train / Test Split
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since we always want to predict the future, we take the **latest 10%** of data
    as the test data.
  prefs: []
  type: TYPE_NORMAL
- en: Normalization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The S&P 500 index increases in time, bringing about the problem that most values
    in the test set are out of the scale of the train set and thus the model has to
    *predict some numbers it has never seen before*. Sadly and unsurprisingly, it
    does a tragic job. See Fig. 3.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f2fa89e29bd6f8edc3932923b5285bb8.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig. 3 A very sad example when the RNN model have to predict numbers out of
    the scale of the training data.
  prefs: []
  type: TYPE_NORMAL
- en: 'To solve the out-of-scale issue, I normalize the prices in each sliding window.
    The task becomes predicting the relative change rates instead of the absolute
    values. In a normalized sliding window $W’_t$ at time $t$, all the values are
    divided by the last unknown price—the last price in $W_{t-1}$:'
  prefs: []
  type: TYPE_NORMAL
- en: $$ W’_t = (\frac{p_{tw}}{p_{tw-1}}, \frac{p_{tw+1}}{p_{tw-1}}, \dots, \frac{p_{(t+1)w-1}}{p_{tw-1}})
    $$
  prefs: []
  type: TYPE_NORMAL
- en: Here is a data archive [stock-data-lilianweng.tar.gz](https://drive.google.com/open?id=1QKVkiwgCNJsdQMEsfoi6KpqoPgc4O6DD)
    of S & P 500 stock prices I crawled up to Jul, 2017\. Feel free to play with it
    :)
  prefs: []
  type: TYPE_NORMAL
- en: Model Construction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Definitions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`lstm_size`: number of units in one LSTM layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_layers`: number of stacked LSTM layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`keep_prob`: percentage of cell units to keep in the [dropout](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf)
    operation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`init_learning_rate`: the learning rate to start with.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`learning_rate_decay`: decay ratio in later training epochs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`init_epoch`: number of epochs using the constant `init_learning_rate`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_epoch`: total number of epochs in training'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`input_size`: size of the sliding window / one training data point'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch_size`: number of data points to use in one mini-batch.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The LSTM model has `num_layers` stacked LSTM layer(s) and each layer contains
    `lstm_size` number of LSTM cells. Then a [dropout](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf)
    mask with keep probability `keep_prob` is applied to the output of every LSTM
    cell. The goal of dropout is to remove the potential strong dependency on one
    dimension so as to prevent overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: The training requires `max_epoch` epochs in total; an [epoch](http://www.fon.hum.uva.nl/praat/manual/epoch.html)
    is a single full pass of all the training data points. In one epoch, the training
    data points are split into mini-batches of size `batch_size`. We send one mini-batch
    to the model for one BPTT learning. The learning rate is set to `init_learning_rate`
    during the first `init_epoch` epochs and then decay by $\times$ `learning_rate_decay`
    during every succeeding epoch.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Define Graph
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A [`tf.Graph`](https://www.tensorflow.org/api_docs/python/tf/Graph) is not attached
    to any real data. It defines the flow of how to process the data and how to run
    the computation. Later, this graph can be fed with data within a [`tf.session`](https://www.tensorflow.org/api_docs/python/tf/Session)
    and at this moment the computation happens for real.
  prefs: []
  type: TYPE_NORMAL
- en: '**— Let’s start going through some code —**'
  prefs: []
  type: TYPE_NORMAL
- en: (1) Initialize a new graph first.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: (2) How the graph works should be defined within its scope.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: (3) Define the data required for computation. Here we need three input variables,
    all defined as [`tf.placeholder`](https://www.tensorflow.org/versions/master/api_docs/python/tf/placeholder)
    because we don’t know what they are at the graph construction stage.
  prefs: []
  type: TYPE_NORMAL
- en: '`inputs`: the training data *X*, a tensor of shape (# data examples, `num_steps`,
    `input_size`); the number of data examples is unknown, so it is `None`. In our
    case, it would be `batch_size` in training session. Check the [input format example](#input_format_example)
    if confused.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`targets`: the training label *y*, a tensor of shape (# data examples, `input_size`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`learning_rate`: a simple float.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: (4) This function returns one [LSTMCell](https://www.tensorflow.org/versions/r1.0/api_docs/python/tf/contrib/rnn/LSTMCell)
    with or without dropout operation.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: (5) Let’s stack the cells into multiple layers if needed. `MultiRNNCell` helps
    connect sequentially multiple simple cells to compose one cell.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: (6) [`tf.nn.dynamic_rnn`](https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn)
    constructs a recurrent neural network specified by `cell` (RNNCell). It returns
    a pair of (model outpus, state), where the outputs `val` is of size (`batch_size`,
    `num_steps`, `lstm_size`) by default. The state refers to the current state of
    the LSTM cell, not consumed here.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: (7) [`tf.transpose`](https://www.tensorflow.org/api_docs/python/tf/transpose)
    converts the outputs from the dimension (`batch_size`, `num_steps`, `lstm_size`)
    to (`num_steps`, `batch_size`, `lstm_size`). Then the last output is picked.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: (8) Define weights and biases between the hidden and output layers.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: (9) We use mean square error as the loss metric and [the RMSPropOptimizer algorithm](http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf)
    for gradient descent optimization.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Start Training Session
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: (1) To start training the graph with real data, we need to start a [`tf.session`](https://www.tensorflow.org/api_docs/python/tf/Session)
    first.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: (2) Initialize the variables as defined.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: (0) The learning rates for training epochs should have been precomputed beforehand.
    The index refers to the epoch index.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: (3) Each loop below completes one epoch training.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: (4) Don’t forget to save your trained model at the end.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The complete code is available [here](https://github.com/lilianweng/stock-rnn/blob/master/build_graph.py).
  prefs: []
  type: TYPE_NORMAL
- en: Use TensorBoard
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Building the graph without visualization is like drawing in the dark, very obscure
    and error-prone. [Tensorboard](https://github.com/tensorflow/tensorboard) provides
    easy visualization of the graph structure and the learning process. Check out
    this [hand-on tutorial](https://youtu.be/eBbEDRsCmv4), only 20 min, but it is
    very practical and showcases several live demos.
  prefs: []
  type: TYPE_NORMAL
- en: '**Brief Summary**'
  prefs: []
  type: TYPE_NORMAL
- en: Use `with [tf.name_scope](https://www.tensorflow.org/api_docs/python/tf/name_scope)("your_awesome_module_name"):`
    to wrap elements working on the similar goal together.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Many `tf.*` methods accepts `name=` argument. Assigning a customized name can
    make your life much easier when reading the graph.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Methods like [`tf.summary.scalar`](https://www.tensorflow.org/api_docs/python/tf/summary/scalar)
    and [`tf.summary.histogram`](https://www.tensorflow.org/api_docs/python/tf/summary/histogram)
    help track the values of variables in the graph during iterations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the training session, define a log file using [`tf.summary.FileWriter`](https://www.tensorflow.org/api_docs/python/tf/summary/FileWriter).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Later, write the training progress and summary results into the file.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/411595a9adf0df339315eaa6562ec414.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig. 4a The RNN graph built by the example code. The "train" module has been
    "removed from the main graph", as it is not a real part of the model during the
    prediction time.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/97b488cce9096211de3ac82005dcf119.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig. 4b Click the "output_layer" module to expand it and check the structure
    in details.
  prefs: []
  type: TYPE_NORMAL
- en: The full working code is available in [github.com/lilianweng/stock-rnn](https://github.com/lilianweng/stock-rnn).
  prefs: []
  type: TYPE_NORMAL
- en: Results
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I used the following configuration in the experiment.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: (Thanks to Yury for cathcing a bug that I had in the price normalization. Instead
    of using the last price of the previous time window, I ended up with using the
    last price in the same window. The following plots have been corrected.)
  prefs: []
  type: TYPE_NORMAL
- en: Overall predicting the stock prices is not an easy task. Especially after normalization,
    the price trends look very noisy.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/146df55d32c5be9691487675748b7822.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig. 5a Predictoin results for the last 200 days in test data. Model is trained
    with input_size=1 and lstm_size=32.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/612756510988f4965ec39745e32ad9f4.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig. 5b Predictoin results for the last 200 days in test data. Model is trained
    with input_size=1 and lstm_size=128.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fc015f52515ba338298c6ce1bf8c7521.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig. 5c Predictoin results for the last 200 days in test data. Model is trained
    with input_size=5, lstm_size=128 and max_epoch=75 (instead of 50).
  prefs: []
  type: TYPE_NORMAL
- en: The example code in this tutorial is available in [github.com/lilianweng/stock-rnn:scripts](https://github.com/lilianweng/stock-rnn/tree/master/scripts).
  prefs: []
  type: TYPE_NORMAL
- en: '(Updated on Sep 14, 2017) The model code has been updated to be wrapped into
    a class: [LstmRNN](https://github.com/lilianweng/stock-rnn/blob/master/model_rnn.py).
    The model training can be triggered by [main.py](https://github.com/lilianweng/stock-rnn/blob/master/main.py),
    such as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
