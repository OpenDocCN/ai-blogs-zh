- en: 'Predict Stock Prices Using RNN: Part 2'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用RNN预测股票价格：第2部分
- en: 原文：[https://lilianweng.github.io/posts/2017-07-22-stock-rnn-part-2/](https://lilianweng.github.io/posts/2017-07-22-stock-rnn-part-2/)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://lilianweng.github.io/posts/2017-07-22-stock-rnn-part-2/](https://lilianweng.github.io/posts/2017-07-22-stock-rnn-part-2/)
- en: In the Part 2 tutorial, I would like to continue the topic on stock price prediction
    and to endow the recurrent neural network that I have built in [Part 1](https://lilianweng.github.io/posts/2017-07-08-stock-rnn-part-1/)
    with the capability of responding to multiple stocks. In order to distinguish
    the patterns associated with different price sequences, I use the stock symbol
    embedding vectors as part of the input.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在第2部分教程中，我想继续讨论股价预测的主题，并赋予我在[第1部分](https://lilianweng.github.io/posts/2017-07-08-stock-rnn-part-1/)中构建的递归神经网络具有响应多只股票的能力。为了区分与不同价格序列相关的模式，我使用股票符号嵌入向量作为输入的一部分。
- en: '* * *'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Dataset
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据集
- en: During the search, I found [this library](https://github.com/lukaszbanasiak/yahoo-finance)
    for querying Yahoo! Finance API. It would be very useful if Yahoo hasn’t shut
    down the historical data fetch API. You may find it useful for querying other
    information though. Here I pick the Google Finance link, among [a couple of free
    data sources](https://www.quantshare.com/sa-43-10-ways-to-download-historical-stock-quotes-data-for-free)
    for downloading historical stock prices.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在搜索过程中，我发现了用于查询Yahoo! Finance API的[这个库](https://github.com/lukaszbanasiak/yahoo-finance)。如果Yahoo没有关闭历史数据获取API，这将非常有用。尽管您可能会发现它对查询其他信息也很有用。在这里，我选择了Google
    Finance链接，其中包括[几个免费数据源](https://www.quantshare.com/sa-43-10-ways-to-download-historical-stock-quotes-data-for-free)用于下载历史股价。
- en: 'The data fetch code can be written as simple as:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 数据获取代码可以写得如下所示：
- en: '[PRE0]'
  id: totrans-7
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: When fetching the content, remember to add try-catch wrapper in case the link
    fails or the provided stock symbol is not valid.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在获取内容时，请记得添加try-catch包装器，以防链接失败或提供的股票符号无效。
- en: '[PRE1]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The full working data fetcher code is available [here](https://github.com/lilianweng/stock-rnn/blob/master/data_fetcher.py).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的数据获取器代码可以在[这里](https://github.com/lilianweng/stock-rnn/blob/master/data_fetcher.py)找到。
- en: Model Construction
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型构建
- en: 'The model is expected to learn the price sequences of different stocks in time.
    Due to the different underlying patterns, I would like to tell the model which
    stock it is dealing with explicitly. [Embedding](https://en.wikipedia.org/wiki/Embedding)
    is more favored than one-hot encoding, because:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型预计会学习不同股票的价格序列。由于不同的基础模式，我想明确告诉模型它正在处理哪只股票。[嵌入](https://en.wikipedia.org/wiki/Embedding)比独热编码更受青睐，因为：
- en: Given that the train set includes $N$ stocks, the one-hot encoding would introduce
    $N$ (or $N-1$) additional sparse feature dimensions. Once each stock symbol is
    mapped onto a much smaller embedding vector of length $k$, $k \ll N$, we end up
    with a much more compressed representation and smaller dataset to take care of.
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 鉴于训练集包括$N$只股票，独热编码将引入$N$（或$N-1$）个额外的稀疏特征维度。一旦每个股票符号被映射到长度为$k$的更小的嵌入向量，$k \ll
    N$，我们最终得到一个更压缩的表示和更小的数据集来处理。
- en: Since embedding vectors are variables to learn. Similar stocks could be associated
    with similar embeddings and help the prediction of each others, such as “GOOG”
    and “GOOGL” which you will see in Fig. 5\. later.
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于嵌入向量是要学习的变量。相似的股票可能与相似的嵌入相关联，并有助于彼此的预测，比如“GOOG”和“GOOGL”，您将在图5中看到。
- en: In the recurrent neural network, at one time step $t$, the input vector contains
    `input_size` (labelled as $w$) daily price values of $i$-th stock, $(p_{i, tw},
    p_{i, tw+1}, \dots, p_{i, (t+1)w-1})$. The stock symbol is uniquely mapped to
    a vector of length `embedding_size` (labelled as $k$), $(e_{i,0}, e_{i,1}, \dots,
    e_{i,k})$. As illustrated in Fig. 1., the price vector is concatenated with the
    embedding vector and then fed into the LSTM cell.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在递归神经网络中，在时间步$t$，输入向量包含第$i$只股票的`input_size`（标记为$w$）个每日价格值，$(p_{i, tw}, p_{i,
    tw+1}, \dots, p_{i, (t+1)w-1})$。股票符号被唯一映射到长度为`embedding_size`的向量（标记为$k$），$(e_{i,0},
    e_{i,1}, \dots, e_{i,k})$。如图1所示，价格向量与嵌入向量连接，然后输入LSTM单元。
- en: Another alternative is to concatenate the embedding vectors with the last state
    of the LSTM cell and learn new weights $W$ and bias $b$ in the output layer. However,
    in this way, the LSTM cell cannot tell apart prices of one stock from another
    and its power would be largely restrained. Thus I decided to go with the former
    approach.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种选择是将嵌入向量与LSTM单元的最后状态连接起来，并在输出层学习新的权重$W$和偏置$b$。然而，通过这种方式，LSTM单元无法区分一个股票的价格与另一个的价格，其能力将受到很大限制。因此，我决定采用前一种方法。
- en: '![](../Images/6d1de64e5e6a99d70d65371a51754388.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6d1de64e5e6a99d70d65371a51754388.png)'
- en: Fig. 1\. The architecture of the stock price prediction RNN model with stock
    symbol embeddings.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图1。具有股票符号嵌入的股价预测RNN模型的架构。
- en: 'Two new configuration settings are added into `RNNConfig`:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '`RNNConfig`中添加了两个新的配置设置：'
- en: '`embedding_size` controls the size of each embedding vector;'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`embedding_size`控制每个嵌入向量的大小；'
- en: '`stock_count` refers to the number of unique stocks in the dataset.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stock_count`指的是数据集中唯一股票的数量。'
- en: Together they define the size of the embedding matrix, for which the model has
    to learn `embedding_size` $\times$ `stock_count` additional variables compared
    to the model in [Part 1](https://lilianweng.github.io/posts/2017-07-08-stock-rnn-part-1/).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 它们共同定义了嵌入矩阵的大小，模型需要学习`embedding_size` $\times$ `stock_count`个额外变量，与[第1部分](https://lilianweng.github.io/posts/2017-07-08-stock-rnn-part-1/)的模型相比。
- en: '[PRE2]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Define the Graph
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义图形
- en: '**— Let’s start going through some code —**'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**— 让我们开始阅读一些代码 —**'
- en: '(1) As demonstrated in tutorial [Part 1: Define the Graph](https://lilianweng.github.io/posts/2017-07-08-stock-rnn-part-1/#define-graph),
    let us define a `tf.Graph()` named `lstm_graph` and a set of tensors to hold input
    data, `inputs`, `targets`, and `learning_rate` in the same way. One more placeholder
    to define is a list of stock symbols associated with the input prices. Stock symbols
    have been mapped to unique integers beforehand with [label encoding](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html).'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: (1) 如教程[第1部分：定义图形](https://lilianweng.github.io/posts/2017-07-08-stock-rnn-part-1/#define-graph)所示，让我们定义一个名为`lstm_graph`的`tf.Graph()`，以及一组张量来保存输入数据`inputs`、`targets`和`learning_rate`。还需要定义一个占位符，用于存储与输入价格相关联的股票符号列表。股票符号事先已经通过[label
    encoding](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)映射为唯一整数。
- en: '[PRE3]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: (2) Then we need to set up an embedding matrix to play as a lookup table, containing
    the embedding vectors of all the stocks. The matrix is initialized with random
    numbers in the interval [-1, 1] and gets updated during training.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: (2) 然后我们需要设置一个嵌入矩阵作为查找表，其中包含所有股票的嵌入向量。该矩阵用随机数在区间[-1, 1]内初始化，并在训练过程中进行更新。
- en: '[PRE4]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: (3) Repeat the stock labels `num_steps` times to match the unfolded version
    of RNN and the shape of `inputs` tensor during training. The transformation operation
    [tf.tile](https://www.tensorflow.org/api_docs/python/tf/tile) receives a base
    tensor and creates a new tensor by replicating its certain dimensions multiples
    times; precisely the $i$-th dimension of the input tensor gets multiplied by `multiples[i]`
    times. For example, if the `stock_labels` is `[[0], [0], [2], [1]]` tiling it
    by `[1, 5]` produces `[[0 0 0 0 0], [0 0 0 0 0], [2 2 2 2 2], [1 1 1 1 1]]`.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: (3) 重复股票标签`num_steps`次，以匹配RNN的展开版本和训练期间`inputs`张量的形状。转换操作[tf.tile](https://www.tensorflow.org/api_docs/python/tf/tile)接收一个基本张量，并通过多次复制其特定维度来创建一个新张量；准确地说，输入张量的第$i$维将被`multiples[i]`倍。例如，如果`stock_labels`是`[[0],
    [0], [2], [1]]`，通过`[1, 5]`进行平铺将产生`[[0 0 0 0 0], [0 0 0 0 0], [2 2 2 2 2], [1 1
    1 1 1]]`。
- en: '[PRE5]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: (4) Then we map the symbols to embedding vectors according to the lookup table
    `embedding_matrix`.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: (4) 然后根据查找表`embedding_matrix`将符号映射为嵌入向量。
- en: '[PRE6]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: (5) Finally, combine the price values with the embedding vectors. The operation
    [tf.concat](https://www.tensorflow.org/api_docs/python/tf/concat) concatenates
    a list of tensors along the dimension `axis`. In our case, we want to keep the
    batch size and the number of steps unchanged, but only extend the input vector
    of length `input_size` to include embedding features.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: (5) 最后，将价格值与嵌入向量组合起来。操作[tf.concat](https://www.tensorflow.org/api_docs/python/tf/concat)沿着维度`axis`连接张量列表。在我们的情况下，我们希望保持批量大小和步数数量不变，只是扩展长度为`input_size`的输入向量以包含嵌入特征。
- en: '[PRE7]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The rest of code runs the dynamic RNN, extracts the last state of the LSTM
    cell, and handles weights and bias in the output layer. See [Part 1: Define the
    Graph](https://lilianweng.github.io/posts/2017-07-08-stock-rnn-part-1/#define-graph)
    for the details.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 代码的其余部分运行动态RNN，提取LSTM单元的最后状态，并处理输出层中的权重和偏差。有关详细信息，请参阅[第1部分：定义图表](https://lilianweng.github.io/posts/2017-07-08-stock-rnn-part-1/#define-graph)。
- en: Training Session
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练会话
- en: 'Please read [Part 1: Start Training Session](https://lilianweng.github.io/posts/2017-07-08-stock-rnn-part-1/#start-training-session)
    if you haven’t for how to run a training session in Tensorflow.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您还没有阅读过如何在Tensorflow中运行训练会话，请阅读[第1部分：开始训练会话](https://lilianweng.github.io/posts/2017-07-08-stock-rnn-part-1/#start-training-session)。
- en: Before feeding the data into the graph, the stock symbols should be transformed
    to unique integers with [label encoding](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在将数据输入图表之前，股票符号应通过[label encoding](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)转换为唯一整数。
- en: '[PRE8]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The train/test split ratio remains same, 90% for training and 10% for testing,
    for every individual stock.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 训练/测试拆分比例保持不变，每支个股的训练占90%，测试占10%。
- en: Visualize the Graph
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可视化图表
- en: After the graph is defined in code, let us check the visualization in Tensorboard
    to make sure that components are constructed correctly. Essentially it looks very
    much like our architecture illustration in Fig. 1.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码中定义图表后，让我们在Tensorboard中检查可视化，以确保组件构建正确。基本上，它看起来非常像我们在图1中的架构图。
- en: '![](../Images/a46457f2b58949545bc8c01707cf924e.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a46457f2b58949545bc8c01707cf924e.png)'
- en: Fig. 2\. Tensorboard visualization of the graph defined above. Two modules,
    "train" and "save", have been removed from the main graph.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图2. 上述定义的图表的Tensorboard可视化。主图表中已删除两个模块，“train”和“save”。
- en: Other than presenting the graph structure or tracking the variables in time,
    Tensorboard also supports [**embeddings visualization**](https://www.tensorflow.org/get_started/embedding_viz).
    In order to communicate the embedding values to Tensorboard, we need to add proper
    tracking in the training logs.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 除了展示图表结构或跟踪变量随时间的变化外，Tensorboard还支持[**嵌入可视化**](https://www.tensorflow.org/get_started/embedding_viz)。为了将嵌入值传达给Tensorboard，我们需要在训练日志中添加适当的跟踪。
- en: (0) In my embedding visualization, I want to color each stock with its industry
    sector. This metadata should stored in a csv file. The file has two columns, the
    stock symbol and the industry sector. It does not matter whether the csv file
    has header, but the order of the listed stocks must be consistent with `label_encoder.classes_`.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: (0) 在我的嵌入可视化中，我希望用行业部门对每支股票进行着色。此元数据应存储在csv文件中。该文件有两列，股票符号和行业部门。csv文件是否有标题并不重要，但所列股票的顺序必须与`label_encoder.classes_`一致。
- en: '[PRE9]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: (1) Set up the summary writer first within the training `tf.Session`.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: (1) 首先在训练`tf.Session`中设置摘要写入器。
- en: '[PRE10]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: (2) Add the tensor `embedding_matrix` defined in our graph `lstm_graph` into
    the projector config variable and attach the metadata csv file.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: (2) 将我们图表`lstm_graph`中定义的张量`embedding_matrix`添加到投影仪配置变量中，并附加元数据csv文件。
- en: '[PRE11]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: (3) This line creates a file `projector_config.pbtxt` in the folder `your_log_file_folder`.
    TensorBoard will read this file during startup.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: (3) 此行在文件夹`your_log_file_folder`中创建一个文件`projector_config.pbtxt`。TensorBoard将在启动时读取此文件。
- en: '[PRE12]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Results
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结果
- en: The model is trained with top 50 stocks with largest market values in the S&P
    500 index.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型是使用标普500指数中市值最大的前50支股票进行训练的。
- en: (Run the following command within [github.com/lilianweng/stock-rnn](https://github.com/lilianweng/stock-rnn))
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: （在[github.com/lilianweng/stock-rnn](https://github.com/lilianweng/stock-rnn)内运行以下命令）
- en: '[PRE13]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'And the following configuration is used:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 以下配置被使用：
- en: '[PRE14]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Price Prediction
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 价格预测
- en: As a brief overview of the prediction quality, Fig. 3 plots the predictions
    for test data of “KO”, “AAPL”, “GOOG” and “NFLX”. The overall trends matched up
    between the true values and the predictions. Considering how the prediction task
    is designed, the model relies on all the historical data points to predict only
    next 5 (`input_size`) days. With a small `input_size`, the model does not need
    to worry about the long-term growth curve. Once we increase `input_size`, the
    prediction would be much harder.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 简要概述预测质量，图3绘制了“KO”、“AAPL”、“GOOG”和“NFLX”测试数据的预测。真实值和预测之间的总体趋势相匹配。考虑到预测任务的设计方式，模型依赖于所有历史数据点来预测接下来的5（`input_size`）天。使用较小的`input_size`，模型不需要担心长期增长曲线。一旦我们增加`input_size`，预测将变得更加困难。
- en: '![](../Images/62e8a35ab732e4a0d51a23244d964aa8.png) ![](../Images/9834d8cc805ccbf42becaa308d4724e8.png)
    ![](../Images/414df6d22c0e32bd3ec0d400f1087abb.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/62e8a35ab732e4a0d51a23244d964aa8.png) ![](../Images/9834d8cc805ccbf42becaa308d4724e8.png)
    ![](../Images/414df6d22c0e32bd3ec0d400f1087abb.png)'
- en: 'Fig. 3\. True and predicted stock prices of AAPL, MSFT and GOOG in the test
    set. The prices are normalized across consecutive prediction sliding windows (See
    [Part 1: Normalization](https://lilianweng.github.io/posts/2017-07-08-stock-rnn-part-1/#normalization).
    The y-axis values get multiplied by 5 for a better comparison between true and
    predicted trends.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图3. AAPL、MSFT 和 GOOG 在测试集中的真实和预测股价。价格在连续预测滑动窗口中进行了归一化处理（参见[第1部分：归一化](https://lilianweng.github.io/posts/2017-07-08-stock-rnn-part-1/#normalization)）。为了更好地比较真实和预测的趋势，y
    轴值乘以5。
- en: Embedding Visualization
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 嵌入可视化
- en: One common technique to visualize the clusters in embedding space is [t-SNE](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding)
    ([Maaten and Hinton, 2008](http://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf)),
    which is well supported in Tensorboard. t-SNE, short for “t-Distributed Stochastic
    Neighbor Embedding, is a variation of Stochastic Neighbor Embedding ([Hinton and
    Roweis, 2002](http://www.cs.toronto.edu/~fritz/absps/sne.pdf)), but with a modified
    cost function that is easier to optimize.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 一种常见的可视化嵌入空间中聚类的技术是[t-SNE](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding)（[Maaten
    和 Hinton, 2008](http://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf)），在
    Tensorboard 中得到很好的支持。t-SNE，即“t-分布随机邻域嵌入”，是随机邻域嵌入的一种变体（[Hinton 和 Roweis, 2002](http://www.cs.toronto.edu/~fritz/absps/sne.pdf)），但具有修改后的成本函数，更容易优化。
- en: Similar to SNE, t-SNE first converts the high-dimensional Euclidean distances
    between data points into conditional probabilities that represent similarities.
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与 SNE 类似，t-SNE 首先将数据点之间的高维欧氏距离转换为表示相似性的条件概率。
- en: t-SNE defines a similar probability distribution over the data points in the
    low-dimensional space, and it minimizes the [Kullback–Leibler divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)
    between the two distributions with respect to the locations of the points on the
    map.
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: t-SNE 在低维空间中定义了与数据点类似的概率分布，并最小化了两个分布之间的[Kullback–Leibler 散度](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)，以便确定地图上点的位置。
- en: Check [this post](http://distill.pub/2016/misread-tsne/) for how to adjust the
    parameters, Perplexity and learning rate (epsilon), in t-SNE visualization.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 查看[此文章](http://distill.pub/2016/misread-tsne/)了解如何调整 t-SNE 可视化中的参数，困惑度和学习率（epsilon）。
- en: '![](../Images/f298f014ca41e996197be3e90d4a830d.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f298f014ca41e996197be3e90d4a830d.png)'
- en: Fig. 4\. Visualization of the stock embeddings using t-SNE. Each label is colored
    based on the stock industry sector. We have 5 clusters. Interstingly, GOOG, GOOGL
    and FB belong to the same cluster, while AMZN and AAPL stay in another.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图4. 使用 t-SNE 可视化股票嵌入。每个标签根据股票行业部门进行着色。我们有5个聚类。有趣的是，GOOG、GOOGL 和 FB 属于同一聚类，而
    AMZN 和 AAPL 属于另一个聚类。
- en: In the embedding space, we can measure the similarity between two stocks by
    examining the similarity between their embedding vectors. For example, GOOG is
    mostly similar to GOOGL in the learned embeddings (See Fig. 5).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在嵌入空间中，我们可以通过检查它们的嵌入向量之间的相似性来衡量两只股票之间的相似性。例如，在学习的嵌入中，GOOG 与 GOOGL 最相似（见图5）。
- en: '![](../Images/f127e90bca6123cf8ce90291c38559ba.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f127e90bca6123cf8ce90291c38559ba.png)'
- en: Fig. 5\. "GOOG" is clicked in the embedding visualization graph and top 20 similar
    neighbors are highlighted with colors from dark to light as the similarity decreases.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图5. 在嵌入可视化图中点击“GOOG”，并用从深到浅的颜色突出显示前20个相似的邻居，随着相似性的降低颜色逐渐变浅。
- en: Known Problems
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 已知问题
- en: The prediction values get diminished and flatten quite a lot as the training
    goes. That’s why I multiplied the absolute values by a constant to make the trend
    is more visible in Fig. 3., as I’m more curious about whether the prediction on
    the up-or-down direction right. However, there must be a reason for the diminishing
    prediction value problem. Potentially rather than using simple MSE as the loss,
    we can adopt another form of loss function to penalize more when the direction
    is predicted wrong.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随着训练的进行，预测值会减少并趋于平缓。这就是为什么我在图3中将绝对值乘以一个常数，以便更清晰地看到趋势，因为我更关心预测的涨跌方向是否正确。然而，预测值减少的问题肯定有原因。可能不是使用简单的均方误差作为损失，我们可以采用另一种形式的损失函数，在预测方向错误时进行更多的惩罚。
- en: The loss function decreases fast at the beginning, but it suffers from occasional
    value explosion (a sudden peak happens and then goes back immediately). I suspect
    it is related to the form of loss function too. A updated and smarter loss function
    might be able to resolve the issue.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 损失函数一开始下降得很快，但偶尔会出现数值爆炸（突然出现峰值然后立即回落）。我怀疑这也与损失函数的形式有关。一个更新和更智能的损失函数可能能够解决这个问题。
- en: The full code in this tutorial is available in [github.com/lilianweng/stock-rnn](https://github.com/lilianweng/stock-rnn).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程中的完整代码可在[github.com/lilianweng/stock-rnn](https://github.com/lilianweng/stock-rnn)找到。
