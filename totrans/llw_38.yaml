- en: 'Object Detection for Dummies Part 2: CNN, DPM and Overfeat'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://lilianweng.github.io/posts/2017-12-15-object-recognition-part-2/](https://lilianweng.github.io/posts/2017-12-15-object-recognition-part-2/)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[Part 1](https://lilianweng.github.io/posts/2017-10-29-object-recognition-part-1/)
    of the “Object Detection for Dummies” series introduced: (1) the concept of image
    gradient vector and how HOG algorithm summarizes the information across all the
    gradient vectors in one image; (2) how the image segmentation algorithm works
    to detect regions that potentially contain objects; (3) how the Selective Search
    algorithm refines the outcomes of image segmentation for better region proposal.'
  prefs: []
  type: TYPE_NORMAL
- en: In Part 2, we are about to find out more on the classic convolution neural network
    architectures for image classification. They lay the ***foundation*** for further
    progress on the deep learning models for object detection. Go check [Part 3](https://lilianweng.github.io/posts/2017-12-31-object-recognition-part-3/)
    if you want to learn more on R-CNN and related models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Links to all the posts in the series: [[Part 1](https://lilianweng.github.io/posts/2017-10-29-object-recognition-part-1/)]
    [[Part 2](https://lilianweng.github.io/posts/2017-12-15-object-recognition-part-2/)]
    [[Part 3](https://lilianweng.github.io/posts/2017-12-31-object-recognition-part-3/)]
    [[Part 4](https://lilianweng.github.io/posts/2018-12-27-object-recognition-part-4/)].'
  prefs: []
  type: TYPE_NORMAL
- en: CNN for Image Classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: CNN, short for “**Convolutional Neural Network**”, is the go-to solution for
    computer vision problems in the deep learning world. It was, to some extent, [inspired](https://lilianweng.github.io/posts/2017-06-21-overview/#convolutional-neural-network)
    by how human visual cortex system works.
  prefs: []
  type: TYPE_NORMAL
- en: Convolution Operation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I strongly recommend this [guide](https://arxiv.org/pdf/1603.07285.pdf) to convolution
    arithmetic, which provides a clean and solid explanation with tons of visualizations
    and examples. Here let’s focus on two-dimensional convolution as we are working
    with images in this post.
  prefs: []
  type: TYPE_NORMAL
- en: In short, convolution operation slides a predefined [kernel](https://en.wikipedia.org/wiki/Kernel_(image_processing))
    (also called “filter”) on top of the input feature map (matrix of image pixels),
    multiplying and adding the values of the kernel and partial input features to
    generate the output. The values form an output matrix, as usually, the kernel
    is much smaller than the input image.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/01d4c042e809ce81d1ae6b9bd8f99952.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 1\. An illustration of applying a kernel on the input feature map to generate
    the output. (Image source: [River Trail documentation](http://intellabs.github.io/RiverTrail/tutorial/))'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2 showcases two real examples of how to convolve a 3x3 kernel over a
    5x5 2D matrix of numeric values to generate a 3x3 matrix. By controlling the padding
    size and the stride length, we can generate an output matrix of a certain size.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6208a1261b1d80318eb8bf0e780c36f8.png) ![](../Images/5b1ed46db91a7c5e83ce80098ed32534.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 2\. Two examples of 2D convolution operation: (top) no padding and 1x1
    strides; (bottom) 1x1 border zeros padding and 2x2 strides. (Image source: [deeplearning.net](http://deeplearning.net/software/theano_versions/dev/tutorial/conv_arithmetic.html))'
  prefs: []
  type: TYPE_NORMAL
- en: AlexNet (Krizhevsky et al, 2012)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 5 convolution [+ optional max pooling] layers + 2 MLP layers + 1 LR layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use data augmentation techniques to expand the training dataset, such as image
    translations, horizontal reflections, and patch extractions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/de2692e9c6d28e2c4f8489b4c22cf078.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 3\. The architecture of AlexNet. (Image source: [link](http://vision03.csail.mit.edu/cnn_art/index.html))'
  prefs: []
  type: TYPE_NORMAL
- en: VGG (Simonyan and Zisserman, 2014)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The network is considered as “very deep” at its time; 19 layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The architecture is extremely simplified with only 3x3 convolutional layers
    and 2x2 pooling layers. The stacking of small filters simulates a larger filter
    with fewer parameters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ResNet (He et al., 2015)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The network is indeed very deep; 152 layers of simple architecture.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Residual Block**: Some input of a certain layer can be passed to the component
    two layers later. Residual blocks are essential for keeping a deep network trainable
    and eventually work. Without residual blocks, the training loss of a plain network
    does not monotonically decrease as the number of layers increases due to [vanishing
    and exploding gradients](http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/98029e385bce47c73cb2f72229d7d384.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 4\. An illustration of the residual block of ResNet. In some way, we can
    say the design of residual blocks is inspired by V4 getting input directly from
    V1 in the human visual cortex system. (left image source: [Wang et al., 2017](https://arxiv.org/pdf/1312.6229.pdf))'
  prefs: []
  type: TYPE_NORMAL
- en: 'Evaluation Metrics: mAP'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A common evaluation metric used in many object recognition and detection tasks
    is “**mAP**”, short for “**mean average precision**”. It is a number from 0 to
    100; higher value is better.
  prefs: []
  type: TYPE_NORMAL
- en: Combine all detections from all test images to draw a precision-recall curve
    (PR curve) for each class; The “average precision” (AP) is the area under the
    PR curve.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Given that target objects are in different classes, we first compute AP separately
    for each class, and then average over classes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A detection is a true positive if it has **“intersection over union” (IoU)**
    with a ground-truth box greater than some threshold (usually 0.5; if so, the metric
    is “[mAP@0.5](mailto:mAP@0.5)”)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deformable Parts Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Deformable Parts Model (DPM) ([Felzenszwalb et al., 2010](http://people.cs.uchicago.edu/~pff/papers/lsvm-pami.pdf))
    recognizes objects with a mixture graphical model (Markov random fields) of deformable
    parts. The model consists of three major components:'
  prefs: []
  type: TYPE_NORMAL
- en: A coarse ***root filter*** defines a detection window that approximately covers
    an entire object. A filter specifies weights for a region feature vector.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Multiple ***part filters*** that cover smaller parts of the object. Parts filters
    are learned at twice resolution of the root filter.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A ***spatial model*** for scoring the locations of part filters relative to
    the root.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/0df4397cf7ee2b4ec3115e158d699c0c.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig. 5\. The DPM model contains (a) a root filter, (b) multiple part filters
    at twice the resolution, and (c) a model for scoring the location and deformation
    of parts.
  prefs: []
  type: TYPE_NORMAL
- en: 'The quality of detecting an object is measured by the score of filters minus
    the deformation costs. The matching score $f$, in laymen’s terms, is:'
  prefs: []
  type: TYPE_NORMAL
- en: $$ f(\text{model}, x) = f(\beta_\text{root}, x) + \sum_{\beta_\text{part} \in
    \text{part filters}} \max_y [f(\beta_\text{part}, y) - \text{cost}(\beta_\text{part},
    x, y)] $$
  prefs: []
  type: TYPE_NORMAL
- en: in which,
  prefs: []
  type: TYPE_NORMAL
- en: $x$ is an image with a specified position and scale;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $y$ is a sub region of $x$.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\beta_\text{root}$ is the root filter.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\beta_\text{part}$ is one part filter.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: cost() measures the penalty of the part deviating from its ideal location relative
    to the root.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The basic score model is the dot product between the filter $\beta$ and the
    region feature vector $\Phi(x)$: $f(\beta, x) = \beta \cdot \Phi(x)$. The feature
    set $\Phi(x)$ can be defined by HOG or other similar algorithms.'
  prefs: []
  type: TYPE_NORMAL
- en: A root location with high score detects a region with high chances to contain
    an object, while the locations of the parts with high scores confirm a recognized
    object hypothesis. The paper adopted latent SVM to model the classifier.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a65a1e2362b06aef57b0f99d366a0115.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 6\. The matching process by DPM. (Image source: [Felzenszwalb et al.,
    2010](http://people.cs.uchicago.edu/~pff/papers/lsvm-pami.pdf))'
  prefs: []
  type: TYPE_NORMAL
- en: The author later claimed that DPM and CNN models are not two distinct approaches
    to object recognition. Instead, a DPM model can be formulated as a CNN by unrolling
    the DPM inference algorithm and mapping each step to an equivalent CNN layer.
    (Check the details in [Girshick et al., 2015](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Girshick_Deformable_Part_Models_2015_CVPR_paper.pdf)!)
  prefs: []
  type: TYPE_NORMAL
- en: Overfeat
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Overfeat [[paper](https://pdfs.semanticscholar.org/f2c2/fbc35d0541571f54790851de9fcd1adde085.pdf)][[code](https://github.com/sermanet/OverFeat)]
    is a pioneer model of integrating the object detection, localization and classification
    tasks all into one convolutional neural network. The main idea is to (i) do image
    classification at different locations on regions of multiple scales of the image
    in a sliding window fashion, and (ii) predict the bounding box locations with
    a regressor trained on top of the same convolution layers.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Overfeat model architecture is very similar to [AlexNet](#alexnet-krizhevsky-et-al-2012).
    It is trained as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/152d3b3b7e960f44f42829738d0b796b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 7\. The training stages of the Overfeat model. (Image source: [link](http://vision.stanford.edu/teaching/cs231b_spring1415/slides/overfeat_eric.pdf))'
  prefs: []
  type: TYPE_NORMAL
- en: Train a CNN model (similar to AlexNet) on the image classification task.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, we replace the top classifier layers by a regression network and train
    it to predict object bounding boxes at each spatial location and scale. The regressor
    is class-specific, each generated for one image class.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Input: Images with classification and bounding box.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Output: $(x_\text{left}, x_\text{right}, y_\text{top}, y_\text{bottom})$, 4
    values in total, representing the coordinates of the bounding box edges.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Loss: The regressor is trained to minimize $l2$ norm between generated bounding
    box and the ground truth for each training example.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: At the detection time,
  prefs: []
  type: TYPE_NORMAL
- en: Perform classification at each location using the pretrained CNN model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Predict object bounding boxes on all classified regions generated by the classifier.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Merge bounding boxes with sufficient overlap from localization and sufficient
    confidence of being the same object from the classifier.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 'Cited as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Reference
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] Vincent Dumoulin and Francesco Visin. [“A guide to convolution arithmetic
    for deep learning.”](https://arxiv.org/pdf/1603.07285.pdf) arXiv preprint arXiv:1603.07285
    (2016).'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] Haohan Wang, Bhiksha Raj, and Eric P. Xing. [“On the Origin of Deep Learning.”](https://arxiv.org/pdf/1702.07800.pdf)
    arXiv preprint arXiv:1702.07800 (2017).'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] Pedro F. Felzenszwalb, Ross B. Girshick, David McAllester, and Deva Ramanan.
    [“Object detection with discriminatively trained part-based models.”](http://people.cs.uchicago.edu/~pff/papers/lsvm-pami.pdf)
    IEEE transactions on pattern analysis and machine intelligence 32, no. 9 (2010):
    1627-1645.'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] Ross B. Girshick, Forrest Iandola, Trevor Darrell, and Jitendra Malik.
    [“Deformable part models are convolutional neural networks.”](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Girshick_Deformable_Part_Models_2015_CVPR_paper.pdf)
    In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), pp. 437-446\.
    2015.'
  prefs: []
  type: TYPE_NORMAL
- en: '[5] Sermanet, Pierre, David Eigen, Xiang Zhang, Michaël Mathieu, Rob Fergus,
    and Yann LeCun. [“OverFeat: Integrated Recognition, Localization and Detection
    using Convolutional Networks”](https://pdfs.semanticscholar.org/f2c2/fbc35d0541571f54790851de9fcd1adde085.pdf)
    arXiv preprint arXiv:1312.6229 (2013).'
  prefs: []
  type: TYPE_NORMAL
