- en: Implementing Deep Reinforcement Learning Models with Tensorflow + OpenAI Gym
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://lilianweng.github.io/posts/2018-05-05-drl-implementation/](https://lilianweng.github.io/posts/2018-05-05-drl-implementation/)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The full implementation is available in [lilianweng/deep-reinforcement-learning-gym](https://github.com/lilianweng/deep-reinforcement-learning-gym)
  prefs: []
  type: TYPE_NORMAL
- en: In the previous two posts, I have introduced the algorithms of many deep reinforcement
    learning models. Now it is the time to get our hands dirty and practice how to
    implement the models in the wild. The implementation is gonna be built in Tensorflow
    and OpenAI [gym](https://github.com/openai/gym) environment. The full version
    of the code in this tutorial is available in [[lilian/deep-reinforcement-learning-gym]](https://github.com/lilianweng/deep-reinforcement-learning-gym).
  prefs: []
  type: TYPE_NORMAL
- en: Environment Setup
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Make sure you have [Homebrew](https://docs.brew.sh/Installation) installed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: I would suggest starting a virtualenv for your development. It makes life so
    much easier when you have multiple projects with conflicting requirements; i.e.
    one works in Python 2.7 while the other is only compatible with Python 3.5+.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '*[*] For every new installation below, please make sure you are in the virtualenv.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Install OpenAI gym according to the [instruction](https://github.com/openai/gym#installation).
    For a minimal installation, run:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: If you are interested in playing with Atari games or other advanced packages,
    please continue to get a couple of system packages installed.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: For Atari, go to the gym directory and pip install it. This [post](http://alvinwan.com/installing-arcade-learning-environment-with-python3-on-macosx/)
    is pretty helpful if you have troubles with ALE (arcade learning environment)
    installation.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Finally clone the “playground” code and install the requirements.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Gym Environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The [OpenAI Gym](https://gym.openai.com/) toolkit provides a set of physical
    simulation environments, games, and robot simulators that we can play with and
    design reinforcement learning agents for. An environment object can be initialized
    by `gym.make("{environment name}"`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/8545d08d5954add8262c2ec6e9fe0f1a.png)'
  prefs: []
  type: TYPE_IMG
- en: The formats of action and observation of an environment are defined by `env.action_space`
    and `env.observation_space`, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'Types of gym [spaces](https://gym.openai.com/docs/#spaces):'
  prefs: []
  type: TYPE_NORMAL
- en: '`gym.spaces.Discrete(n)`: discrete values from 0 to n-1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`gym.spaces.Box`: a multi-dimensional vector of numeric values, the upper and
    lower bounds of each dimension are defined by `Box.low` and `Box.high`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We interact with the env through two major api calls:'
  prefs: []
  type: TYPE_NORMAL
- en: '**`ob = env.reset()`**'
  prefs: []
  type: TYPE_NORMAL
- en: Resets the env to the original setting.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns the initial observation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**`ob_next, reward, done, info = env.step(action)`**'
  prefs: []
  type: TYPE_NORMAL
- en: Applies one action in the env which should be compatible with `env.action_space`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gets back the new observation `ob_next` (env.observation_space), a reward (float),
    a `done` flag (bool), and other meta information (dict). If `done=True`, the episode
    is complete and we should reset the env to restart. Read more [here](https://gym.openai.com/docs/#observations).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Naive Q-Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Q-learning](https://lilianweng.github.io/posts/2018-02-19-rl-overview/#q-learning-off-policy-td-control)
    (Watkins & Dayan, 1992) learns the action value (“Q-value”) and update it according
    to the [Bellman equation](https://lilianweng.github.io/posts/2018-02-19-rl-overview/#bellman-equations).
    The key point is while estimating what is the next action, it does not follow
    the current policy but rather adopt the best Q value (the part in red) independently.'
  prefs: []
  type: TYPE_NORMAL
- en: $$ Q(s, a) \leftarrow (1 - \alpha) Q(s, a) + \alpha (r + \gamma \color{red}{\max_{a'
    \in \mathcal{A}} Q(s', a')}) $$
  prefs: []
  type: TYPE_NORMAL
- en: In a naive implementation, the Q value for all (s, a) pairs can be simply tracked
    in a dict. No complicated machine learning model is involved yet.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Most gym environments have a multi-dimensional continuous observation space
    (`gym.spaces.Box`). To make sure our Q dictionary will not explode by trying to
    memorize an infinite number of keys, we apply a wrapper to discretize the observation.
    The concept of [wrappers](https://github.com/openai/gym/tree/master/gym/wrappers)
    is very powerful, with which we are capable to customize observation, action,
    step function, etc. of an env. No matter how many wrappers are applied, `env.unwrapped`
    always gives back the internal original environment object.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Let’s plug in the interaction with a gym env and update the Q function every
    time a new transition is generated. When picking the action, we use ε-greedy to
    force exploration.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Often we start with a high `epsilon` and gradually decrease it during the training,
    known as “epsilon annealing”. The full code of `QLearningPolicy` is available
    [here](https://github.com/lilianweng/deep-reinforcement-learning-gym/blob/master/playground/policies/qlearning.py).
  prefs: []
  type: TYPE_NORMAL
- en: Deep Q-Network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Deep Q-network](https://lilianweng.github.io/posts/2018-02-19-rl-overview/#deep-q-network)
    is a seminal piece of work to make the training of Q-learning more stable and
    more data-efficient, when the Q value is approximated with a nonlinear function.
    Two key ingredients are experience replay and a separately updated target network.'
  prefs: []
  type: TYPE_NORMAL
- en: The main loss function looks like the following,
  prefs: []
  type: TYPE_NORMAL
- en: $$ \begin{aligned} & Y(s, a, r, s') = r + \gamma \max_{a'} Q_{\theta^{-}}(s',
    a') \\ & \mathcal{L}(\theta) = \mathbb{E}_{(s, a, r, s') \sim U(D)} \Big[ \big(
    Y(s, a, r, s') - Q_\theta(s, a) \big)^2 \Big] \end{aligned} $$
  prefs: []
  type: TYPE_NORMAL
- en: The Q network can be a multi-layer dense neural network, a convolutional network,
    or a recurrent network, depending on the problem. In the [full implementation](https://github.com/lilianweng/deep-reinforcement-learning-gym/blob/master/playground/policies/dqn.py)
    of the DQN policy, it is determined by the `model_type` parameter, one of (“dense”,
    “conv”, “lstm”).
  prefs: []
  type: TYPE_NORMAL
- en: In the following example, I’m using a 2-layer densely connected neural network
    to learn Q values for the cart pole balancing problem.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We have a helper function for creating the networks below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The Q-network and the target network are updated with a batch of transitions
    (state, action, reward, state_next, done_flag). The input tensors are:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: We have two networks of the same structure. Both have the same network architectures
    with the state observation as the inputs and Q values over all the actions as
    the outputs.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The target network “Q_target” takes the `states_next` tensor as the input, because
    we use its prediction to select the optimal next state in the Bellman equation.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Note that [tf.stop_gradient()](https://www.tensorflow.org/api_docs/python/tf/stop_gradient)
    on the target y, because the target network should stay fixed during the loss-minimizing
    gradient update.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bdb8c0c23dc95dbf2633b4056b07ca45.png)'
  prefs: []
  type: TYPE_IMG
- en: The target network is updated by copying the primary Q network parameters over
    every `C` number of steps (“hard update”) or polyak averaging towards the primary
    network (“soft update”)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Double Q-Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If we look into the standard form of the Q value target, $Y(s, a) = r + \gamma
    \max_{a’ \in \mathcal{A}} Q_\theta (s’, a’)$, it is easy to notice that we use
    $Q_\theta$ to select the best next action at state s’ and then apply the action
    value predicted by the same $Q_\theta$. This two-step reinforcing procedure could
    potentially lead to overestimation of an (already) overestimated value, further
    leading to training instability. The solution proposed by double Q-learning ([Hasselt,
    2010](http://papers.nips.cc/paper/3964-double-q-learning.pdf)) is to decouple
    the action selection and action value estimation by using two Q networks, $Q_1$
    and $Q_2$: when $Q_1$ is being updated, $Q_2$ decides the best next action, and
    vice versa.'
  prefs: []
  type: TYPE_NORMAL
- en: $$ Y_1(s, a, r, s') = r + \gamma Q_1 (s', \arg\max_{a' \in \mathcal{A}}Q_2(s',
    a'))\\ Y_2(s, a, r, s') = r + \gamma Q_2 (s', \arg\max_{a' \in \mathcal{A}}Q_1(s',
    a')) $$
  prefs: []
  type: TYPE_NORMAL
- en: 'To incorporate double Q-learning into DQN, the minimum modification ([Hasselt,
    Guez, & Silver, 2016](https://arxiv.org/pdf/1509.06461.pdf)) is to use the primary
    Q network to select the action while the action value is estimated by the target
    network:'
  prefs: []
  type: TYPE_NORMAL
- en: $$ Y(s, a, r, s') = r + \gamma Q_{\theta^{-}}(s', \arg\max_{a' \in \mathcal{A}}
    Q_\theta(s', a')) $$
  prefs: []
  type: TYPE_NORMAL
- en: In the code, we add a new tensor for getting the action selected by the primary
    Q network as the input and a tensor operation for selecting this action.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The prediction target y in the loss function becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Here I used [tf.gather()](https://www.tensorflow.org/api_docs/python/tf/gather)
    to select the action values of interests.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b4ee4f320324f85b5d952e2c8a98faef.png)'
  prefs: []
  type: TYPE_IMG
- en: '(Image source: tf.gather() docs)'
  prefs: []
  type: TYPE_NORMAL
- en: During the episode rollout, we compute the `actions_next` by feeding the next
    states’ data into the `actions_selected_by_q` operation.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Dueling Q-Network
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The dueling Q-network ([Wang et al., 2016](https://arxiv.org/pdf/1511.06581.pdf))
    is equipped with an enhanced network architecture: the output layer branches out
    into two heads, one for predicting state value, V, and the other for [advantage](https://lilianweng.github.io/posts/2018-02-19-rl-overview/#value-function),
    A. The Q-value is then reconstructed, $Q(s, a) = V(s) + A(s, a)$.'
  prefs: []
  type: TYPE_NORMAL
- en: $$ \begin{aligned} A(s, a) &= Q(s, a) - V(s)\\ V(s) &= \sum_a Q(s, a) \pi(a
    \vert s) = \sum_a (V(s) + A(s, a)) \pi(a \vert s) = V(s) + \sum_a A(s, a)\pi(a
    \vert s)\\ \text{Thus, }& \sum_a A(s, a)\pi(a \vert s) = 0 \end{aligned} $$
  prefs: []
  type: TYPE_NORMAL
- en: To make sure the estimated advantage values sum up to zero, $\sum_a A(s, a)\pi(a
    \vert s) = 0$, we deduct the mean value from the prediction.
  prefs: []
  type: TYPE_NORMAL
- en: $$ Q(s, a) = V(s) + (A(s, a) - \frac{1}{|\mathcal{A}|} \sum_a A(s, a)) $$
  prefs: []
  type: TYPE_NORMAL
- en: 'The code change is straightforward:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/41b8d2b6e957c50908d0772aa30b5a6d.png)'
  prefs: []
  type: TYPE_IMG
- en: '(Image source: [Wang et al., 2016](https://arxiv.org/pdf/1511.06581.pdf))'
  prefs: []
  type: TYPE_NORMAL
- en: Check the [code](https://github.com/lilianweng/deep-reinforcement-learning-gym/blob/master/playground/policies/dqn.py)
    for the complete flow.
  prefs: []
  type: TYPE_NORMAL
- en: Monte-Carlo Policy Gradient
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I reviewed a number of popular policy gradient methods in my [last post](https://lilianweng.github.io/posts/2018-04-08-policy-gradient/).
    Monte-Carlo policy gradient, also known as [REINFORCE](https://lilianweng.github.io/posts/2018-04-08-policy-gradient/#reinforce),
    is a classic on-policy method that learns the policy model explicitly. It uses
    the return estimated from a full on-policy trajectory and updates the policy parameters
    with policy gradient.
  prefs: []
  type: TYPE_NORMAL
- en: The returns are computed during rollouts and then fed into the Tensorflow graph
    as inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The policy network is contructed. We update the policy parameters by minimizing
    the loss function, $\mathcal{L} = - (G_t - V(s)) \log \pi(a \vert s)$. [tf.nn.sparse_softmax_cross_entropy_with_logits()](https://www.tensorflow.org/api_docs/python/tf/nn/sparse_softmax_cross_entropy_with_logits)
    asks for the raw logits as inputs, rather then the probabilities after softmax,
    and that’s why we do not have a softmax layer on top of the policy network.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'During the episode rollout, the return is calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The full implementation of REINFORCE is [here](https://github.com/lilianweng/deep-reinforcement-learning-gym/blob/master/playground/policies/reinforce.py).
  prefs: []
  type: TYPE_NORMAL
- en: Actor-Critic
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The [actor-critic](https://lilianweng.github.io/posts/2018-04-08-policy-gradient/#actor-critic)
    algorithm learns two models at the same time, the actor for learning the best
    policy and the critic for estimating the state value.
  prefs: []
  type: TYPE_NORMAL
- en: Initialize the actor network, $\pi(a \vert s)$ and the critic, $V(s)$
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Collect a new transition (s, a, r, s’): Sample the action $a \sim \pi(a \vert
    s)$ for the current state s, and get the reward r and the next state s''.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the TD target during episode rollout, $G_t = r + \gamma V(s’)$ and TD
    error, $\delta_t = r + \gamma V(s’) - V(s)$.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Update the critic network by minimizing the critic loss: $L_c = (V(s) - G_t)$.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Update the actor network by minimizing the actor loss: $L_a = - \delta_t \log
    \pi(a \vert s)$.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set s’ = s and repeat step 2.-5.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Overall the implementation looks pretty similar to REINFORCE with an extra critic
    network. The full implementation is here.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The tensorboard graph is always helpful: ![](../Images/04ae9b4b6a4f875b577f7014314c8962.png)'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] [Tensorflow API Docs](https://www.tensorflow.org/api_docs/)'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] Christopher JCH Watkins, and Peter Dayan. [“Q-learning.”](https://link.springer.com/content/pdf/10.1007/BF00992698.pdf)
    Machine learning 8.3-4 (1992): 279-292.'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] Hado Van Hasselt, Arthur Guez, and David Silver. [“Deep Reinforcement Learning
    with Double Q-Learning.”](https://arxiv.org/pdf/1509.06461.pdf) AAAI. Vol. 16\.
    2016.'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] Hado van Hasselt. [“Double Q-learning.”](http://papers.nips.cc/paper/3964-double-q-learning.pdf)
    NIPS, 23:2613–2621, 2010.'
  prefs: []
  type: TYPE_NORMAL
- en: '[5] Ziyu Wang, et al. [Dueling network architectures for deep reinforcement
    learning.](https://arxiv.org/pdf/1511.06581.pdf) ICML. 2016.'
  prefs: []
  type: TYPE_NORMAL
