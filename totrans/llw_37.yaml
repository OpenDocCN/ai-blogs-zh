- en: 'Object Detection for Dummies Part 3: R-CNN Family'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'Object Detection for Dummies Part 3: R-CNN Family'
- en: 原文：[https://lilianweng.github.io/posts/2017-12-31-object-recognition-part-3/](https://lilianweng.github.io/posts/2017-12-31-object-recognition-part-3/)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://lilianweng.github.io/posts/2017-12-31-object-recognition-part-3/](https://lilianweng.github.io/posts/2017-12-31-object-recognition-part-3/)
- en: '[Updated on 2018-12-20: Remove YOLO here. Part 4 will cover multiple fast object
    detection algorithms, including YOLO.]'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[更新于2018-12-20：在这里移除YOLO。第4部分将涵盖多种快速目标检测算法，包括YOLO。]'
- en: '[Updated on 2018-12-27: Add [bbox regression](#bounding-box-regression) and
    [tricks](#common-tricks) sections for R-CNN.]'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[更新于2018-12-27：为R-CNN添加[bbox回归](#bounding-box-regression)和[技巧](#common-tricks)部分。]'
- en: In the series of “Object Detection for Dummies”, we started with basic concepts
    in image processing, such as gradient vectors and HOG, in [Part 1](https://lilianweng.github.io/posts/2017-10-29-object-recognition-part-1/).
    Then we introduced classic convolutional neural network architecture designs for
    classification and pioneer models for object recognition, Overfeat and DPM, in
    [Part 2](https://lilianweng.github.io/posts/2017-12-15-object-recognition-part-2/).
    In the third post of this series, we are about to review a set of models in the
    R-CNN (“Region-based CNN”) family.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在“Object Detection for Dummies”系列中，我们从图像处理的基本概念开始，比如梯度向量和HOG，在[第1部分](https://lilianweng.github.io/posts/2017-10-29-object-recognition-part-1/)。然后我们介绍了经典的用于分类的卷积神经网络架构设计和用于目标识别的先驱模型，Overfeat和DPM，在[第2部分](https://lilianweng.github.io/posts/2017-12-15-object-recognition-part-2/)。在这个系列的第三篇文章中，我们将回顾R-CNN（“基于区域的CNN”）系列中的一组模型。
- en: 'Links to all the posts in the series: [[Part 1](https://lilianweng.github.io/posts/2017-10-29-object-recognition-part-1/)]
    [[Part 2](https://lilianweng.github.io/posts/2017-12-15-object-recognition-part-2/)]
    [[Part 3](https://lilianweng.github.io/posts/2017-12-31-object-recognition-part-3/)]
    [[Part 4](https://lilianweng.github.io/posts/2018-12-27-object-recognition-part-4/)].'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 系列中所有文章的链接：[[第1部分](https://lilianweng.github.io/posts/2017-10-29-object-recognition-part-1/)]
    [[第2部分](https://lilianweng.github.io/posts/2017-12-15-object-recognition-part-2/)]
    [[第3部分](https://lilianweng.github.io/posts/2017-12-31-object-recognition-part-3/)]
    [[第4部分](https://lilianweng.github.io/posts/2018-12-27-object-recognition-part-4/)]。
- en: Here is a list of papers covered in this post ;)
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是本文涵盖的论文列表 ;)
- en: '| **Model** | **Goal** | **Resources** |'
  id: totrans-7
  prefs: []
  type: TYPE_TB
  zh: '| **模型** | **目标** | **资源** |'
- en: '| --- | --- | --- |'
  id: totrans-8
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| R-CNN | Object recognition | [[paper](https://arxiv.org/abs/1311.2524)][[code](https://github.com/rbgirshick/rcnn)]
    |'
  id: totrans-9
  prefs: []
  type: TYPE_TB
  zh: '| R-CNN | 目标识别 | [[paper](https://arxiv.org/abs/1311.2524)][[code](https://github.com/rbgirshick/rcnn)]
    |'
- en: '| Fast R-CNN | Object recognition | [[paper](https://arxiv.org/abs/1504.08083)][[code](https://github.com/rbgirshick/fast-rcnn)]
    |'
  id: totrans-10
  prefs: []
  type: TYPE_TB
  zh: '| Fast R-CNN | 目标识别 | [[paper](https://arxiv.org/abs/1504.08083)][[code](https://github.com/rbgirshick/fast-rcnn)]
    |'
- en: '| Faster R-CNN | Object recognition | [[paper](https://arxiv.org/abs/1506.01497)][[code](https://github.com/rbgirshick/py-faster-rcnn)]
    |'
  id: totrans-11
  prefs: []
  type: TYPE_TB
  zh: '| Faster R-CNN | 目标识别 | [[paper](https://arxiv.org/abs/1506.01497)][[code](https://github.com/rbgirshick/py-faster-rcnn)]
    |'
- en: '| Mask R-CNN | Image segmentation | [[paper](https://arxiv.org/abs/1703.06870)][[code](https://github.com/CharlesShang/FastMaskRCNN)]
    |'
  id: totrans-12
  prefs: []
  type: TYPE_TB
  zh: '| Mask R-CNN | 图像分割 | [[paper](https://arxiv.org/abs/1703.06870)][[code](https://github.com/CharlesShang/FastMaskRCNN)]
    |'
- en: R-CNN
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: R-CNN
- en: R-CNN ([Girshick et al., 2014](https://arxiv.org/abs/1311.2524)) is short for
    “Region-based Convolutional Neural Networks”. The main idea is composed of two
    steps. First, using [selective search](https://lilianweng.github.io/posts/2017-10-29-object-recognition-part-1/#selective-search),
    it identifies a manageable number of bounding-box object region candidates (“region
    of interest” or “RoI”). And then it extracts CNN features from each region independently
    for classification.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: R-CNN（[Girshick等人，2014](https://arxiv.org/abs/1311.2524)）简称“基于区域的卷积神经网络”。其主要思想由两个步骤组成。首先，使用[selective
    search](https://lilianweng.github.io/posts/2017-10-29-object-recognition-part-1/#selective-search)，它识别出可管理的边界框对象区域候选项（“感兴趣区域”或“RoI”）。然后，它独立地从每个区域提取CNN特征进行分类。
- en: '![](../Images/51c2c5526f9841cc282fc014e71ed5a6.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/51c2c5526f9841cc282fc014e71ed5a6.png)'
- en: 'Fig. 1\. The architecture of R-CNN. (Image source: [Girshick et al., 2014](https://arxiv.org/abs/1311.2524))'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图1. R-CNN的架构。（图片来源：[Girshick等人，2014](https://arxiv.org/abs/1311.2524)）
- en: Model Workflow
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型工作流程
- en: 'How R-CNN works can be summarized as follows:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: R-CNN的工作原理可以总结如下：
- en: '**Pre-train** a CNN network on image classification tasks; for example, VGG
    or ResNet trained on [ImageNet](http://image-net.org/index) dataset. The classification
    task involves N classes.'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**预训练**一个CNN网络用于图像分类任务；例如，在[ImageNet](http://image-net.org/index)数据集上训练的VGG或ResNet。分类任务涉及N个类别。'
- en: 'NOTE: You can find a pre-trained [AlexNet](https://github.com/BVLC/caffe/tree/master/models/bvlc_alexnet)
    in Caffe Model [Zoo](https://github.com/caffe2/caffe2/wiki/Model-Zoo). I don’t
    think you can [find it](https://github.com/tensorflow/models/issues/1394) in Tensorflow,
    but Tensorflow-slim model [library](https://github.com/tensorflow/models/tree/master/research/slim)
    provides pre-trained ResNet, VGG, and others.'
  id: totrans-20
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 注意：您可以在 Caffe 模型 [Zoo](https://github.com/caffe2/caffe2/wiki/Model-Zoo) 中找到一个预训练的
    [AlexNet](https://github.com/BVLC/caffe/tree/master/models/bvlc_alexnet)。我不认为您可以在
    Tensorflow 中找到它，但是 Tensorflow-slim 模型 [library](https://github.com/tensorflow/models/tree/master/research/slim)
    提供了预训练的 ResNet、VGG 等模型。
- en: Propose category-independent regions of interest by selective search (~2k candidates
    per image). Those regions may contain target objects and they are of different
    sizes.
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过选择性搜索提出独立于类别的感兴趣区域（每个图像约 2k 个候选区域）。这些区域可能包含目标对象，它们的大小不同。
- en: Region candidates are **warped** to have a fixed size as required by CNN.
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 区域候选区域被**扭曲**以满足 CNN 所需的固定大小。
- en: Continue fine-tuning the CNN on warped proposal regions for K + 1 classes; The
    additional one class refers to the background (no object of interest). In the
    fine-tuning stage, we should use a much smaller learning rate and the mini-batch
    oversamples the positive cases because most proposed regions are just background.
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 继续在扭曲的提议区域上对 CNN 进行 K + 1 类的微调；额外的一个类指的是背景（没有感兴趣的对象）。在微调阶段，我们应该使用更小的学习率，并且小批量过采样正例，因为大多数提议的区域只是背景。
- en: Given every image region, one forward propagation through the CNN generates
    a feature vector. This feature vector is then consumed by a **binary SVM** trained
    for **each class** independently.
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 给定每个图像区域，CNN 通过一次前向传播生成一个特征向量。然后，这个特征向量被一个为**每个类别**独立训练的**二元 SVM**所使用。
- en: The positive samples are proposed regions with IoU (intersection over union)
    overlap threshold >= 0.3, and negative samples are irrelevant others.
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 正样本是具有 IoU（交并比）重叠阈值 >= 0.3 的提出区域，负样本是无关的其他区域。
- en: To reduce the localization errors, a regression model is trained to correct
    the predicted detection window on bounding box correction offset using CNN features.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了减少本地化错误，使用 CNN 特征训练回归模型来校正预测的检测窗口在边界框校正偏移上的错误。
- en: Bounding Box Regression
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 边界框回归
- en: Given a predicted bounding box coordinate $\mathbf{p} = (p_x, p_y, p_w, p_h)$
    (center coordinate, width, height) and its corresponding ground truth box coordinates
    $\mathbf{g} = (g_x, g_y, g_w, g_h)$ , the regressor is configured to learn scale-invariant
    transformation between two centers and log-scale transformation between widths
    and heights. All the transformation functions take $\mathbf{p}$ as input.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个预测的边界框坐标 $\mathbf{p} = (p_x, p_y, p_w, p_h)$（中心坐标、宽度、高度）及其对应的真实框坐标 $\mathbf{g}
    = (g_x, g_y, g_w, g_h)$，回归器被配置为学习两个中心之间的尺度不变变换和宽度和高度之间的对数尺度变换。所有的变换函数以 $\mathbf{p}$
    为输入。
- en: $$ \begin{aligned} \hat{g}_x &= p_w d_x(\mathbf{p}) + p_x \\ \hat{g}_y &= p_h
    d_y(\mathbf{p}) + p_y \\ \hat{g}_w &= p_w \exp({d_w(\mathbf{p})}) \\ \hat{g}_h
    &= p_h \exp({d_h(\mathbf{p})}) \end{aligned} $$![](../Images/f6f2f6a721863b9b53fa1270684b7014.png)
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \begin{aligned} \hat{g}_x &= p_w d_x(\mathbf{p}) + p_x \\ \hat{g}_y &= p_h
    d_y(\mathbf{p}) + p_y \\ \hat{g}_w &= p_w \exp({d_w(\mathbf{p})}) \\ \hat{g}_h
    &= p_h \exp({d_h(\mathbf{p})}) \end{aligned} $$![](../Images/f6f2f6a721863b9b53fa1270684b7014.png)
- en: Fig. 2\. Illustration of transformation between predicted and ground truth bounding
    boxes.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2\. 预测和真实边界框之间的变换示意图。
- en: 'An obvious benefit of applying such transformation is that all the bounding
    box correction functions, $d_i(\mathbf{p})$ where $i \in \{ x, y, w, h \}$, can
    take any value between [-∞, +∞]. The targets for them to learn are:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 应用这种转换的明显好处是，所有边界框校正函数 $d_i(\mathbf{p})$（其中 $i \in \{ x, y, w, h \}$）可以取任何值在
    [-∞, +∞] 之间。它们学习的目标是：
- en: $$ \begin{aligned} t_x &= (g_x - p_x) / p_w \\ t_y &= (g_y - p_y) / p_h \\ t_w
    &= \log(g_w/p_w) \\ t_h &= \log(g_h/p_h) \end{aligned} $$
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \begin{aligned} t_x &= (g_x - p_x) / p_w \\ t_y &= (g_y - p_y) / p_h \\ t_w
    &= \log(g_w/p_w) \\ t_h &= \log(g_h/p_h) \end{aligned} $$
- en: 'A standard regression model can solve the problem by minimizing the SSE loss
    with regularization:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 一个标准的回归模型可以通过最小化带有正则化的 SSE 损失来解决问题：
- en: $$ \mathcal{L}_\text{reg} = \sum_{i \in \{x, y, w, h\}} (t_i - d_i(\mathbf{p}))^2
    + \lambda \|\mathbf{w}\|^2 $$
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \mathcal{L}_\text{reg} = \sum_{i \in \{x, y, w, h\}} (t_i - d_i(\mathbf{p}))^2
    + \lambda \|\mathbf{w}\|^2 $$
- en: The regularization term is critical here and RCNN paper picked the best λ by
    cross validation. It is also noteworthy that not all the predicted bounding boxes
    have corresponding ground truth boxes. For example, if there is no overlap, it
    does not make sense to run bbox regression. Here, only a predicted box with a
    nearby ground truth box with at least 0.6 IoU is kept for training the bbox regression
    model.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化项在这里至关重要，RCNN论文通过交叉验证选择了最佳λ。值得注意的是，并非所有预测的边界框都有相应的真实边界框。例如，如果没有重叠，运行bbox回归就没有意义。在这里，只有与至少0.6
    IoU的附近真实边界框相匹配的预测边界框才会被保留用于训练bbox回归模型。
- en: Common Tricks
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 常见技巧
- en: Several tricks are commonly used in RCNN and other detection models.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在RCNN和其他检测模型中通常使用几种技巧。
- en: '**Non-Maximum Suppression**'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**非极大值抑制**'
- en: 'Likely the model is able to find multiple bounding boxes for the same object.
    Non-max suppression helps avoid repeated detection of the same instance. After
    we get a set of matched bounding boxes for the same object category: Sort all
    the bounding boxes by confidence score. Discard boxes with low confidence scores.
    *While* there is any remaining bounding box, repeat the following: Greedily select
    the one with the highest score. Skip the remaining boxes with high IoU (i.e. >
    0.5) with previously selected one.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 模型可能能够为同一对象找到多个边界框。非极大值抑制有助于避免重复检测同一实例。在我们为同一对象类别获得一组匹配的边界框之后：按置信度分数对所有边界框进行排序。丢弃置信度较低的边界框。*当*还有剩余边界框时，重复以下步骤：贪婪地选择得分最高的边界框。跳过与先前选择的边界框具有高IoU（即>
    0.5）的剩余边界框。
- en: '![](../Images/793408a39bd9610dc845e0dc1eaf3d60.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/793408a39bd9610dc845e0dc1eaf3d60.png)'
- en: 'Fig. 3\. Multiple bounding boxes detect the car in the image. After non-maximum
    suppression, only the best remains and the rest are ignored as they have large
    overlaps with the selected one. (Image source: [DPM paper](http://lear.inrialpes.fr/~oneata/reading_group/dpm.pdf))'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图3. 图像中检测到汽车的多个边界框。经过非极大值抑制后，只有最佳结果保留，其余被忽略，因为它们与选定的边界框有很大的重叠。（图片来源：[DPM paper](http://lear.inrialpes.fr/~oneata/reading_group/dpm.pdf)）
- en: '**Hard Negative Mining**'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**难负例挖掘**'
- en: We consider bounding boxes without objects as negative examples. Not all the
    negative examples are equally hard to be identified. For example, if it holds
    pure empty background, it is likely an “*easy negative*”; but if the box contains
    weird noisy texture or partial object, it could be hard to be recognized and these
    are “*hard negative*”.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将没有对象的边界框视为负例。并非所有负例都同样难以识别。例如，如果它只包含纯空白背景，则很可能是“*易负例*”；但如果边界框包含奇怪的嘈杂纹理或部分对象，则可能难以识别，这些是“*难负例*”。
- en: The hard negative examples are easily misclassified. We can explicitly find
    those false positive samples during the training loops and include them in the
    training data so as to improve the classifier.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 难负例很容易被错误分类。我们可以在训练循环中明确找到这些误分类的假阳性样本，并将它们包含在训练数据中，以改进分类器。
- en: Speed Bottleneck
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 速度瓶颈
- en: 'Looking through the R-CNN learning steps, you could easily find out that training
    an R-CNN model is expensive and slow, as the following steps involve a lot of
    work:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 通过查看R-CNN学习步骤，您可以轻松发现训练R-CNN模型是昂贵且缓慢的，因为以下步骤涉及大量工作：
- en: Running selective search to propose 2000 region candidates for every image;
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行选择性搜索为每个图像提出2000个区域候选项；
- en: Generating the CNN feature vector for every image region (N images * 2000).
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为每个图像区域生成CNN特征向量（N个图像 * 2000）。
- en: 'The whole process involves three models separately without much shared computation:
    the convolutional neural network for image classification and feature extraction;
    the top SVM classifier for identifying target objects; and the regression model
    for tightening region bounding boxes.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 整个过程涉及三个模型分别进行，没有太多共享计算：用于图像分类和特征提取的卷积神经网络；用于识别目标对象的顶部SVM分类器；以及用于收紧区域边界框的回归模型。
- en: Fast R-CNN
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 快速R-CNN
- en: To make R-CNN faster, Girshick ([2015](https://arxiv.org/pdf/1504.08083.pdf))
    improved the training procedure by unifying three independent models into one
    jointly trained framework and increasing shared computation results, named **Fast
    R-CNN**. Instead of extracting CNN feature vectors independently for each region
    proposal, this model aggregates them into one CNN forward pass over the entire
    image and the region proposals share this feature matrix. Then the same feature
    matrix is branched out to be used for learning the object classifier and the bounding-box
    regressor. In conclusion, computation sharing speeds up R-CNN.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 为了加快 R-CNN 的速度，Girshick（[2015](https://arxiv.org/pdf/1504.08083.pdf)）通过将三个独立模型统一到一个联合训练框架中并增加共享计算结果来改进训练过程，命名为**Fast
    R-CNN**。该模型不再独立提取每个区域提议的 CNN 特征向量，而是将它们聚合到整个图像的一个 CNN 前向传递中，区域提议共享这个特征矩阵。然后，相同的特征矩阵被分支出来用于学习对象分类器和边界框回归器。总之，共享计算加速了
    R-CNN。
- en: '![](../Images/b926ca2835bf54fa92ee318dccf8b907.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b926ca2835bf54fa92ee318dccf8b907.png)'
- en: 'Fig. 4\. The architecture of Fast R-CNN. (Image source: [Girshick, 2015](https://arxiv.org/pdf/1504.08083.pdf))'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图4\. Fast R-CNN 的架构。（图片来源：[Girshick, 2015](https://arxiv.org/pdf/1504.08083.pdf)）
- en: RoI Pooling
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RoI 池化
- en: It is a type of max pooling to convert features in the projected region of the
    image of any size, h x w, into a small fixed window, H x W. The input region is
    divided into H x W grids, approximately every subwindow of size h/H x w/W. Then
    apply max-pooling in each grid.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种最大池化，将图像的投影区域中的特征转换为一个小的固定窗口，H x W。输入区域被划分为 H x W 网格，大约每个大小为 h/H x w/W 的子窗口。然后在每个网格中应用最大池化。
- en: '![](../Images/435bd19fe9e77a08a031e68663f3e764.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/435bd19fe9e77a08a031e68663f3e764.png)'
- en: 'Fig. 5\. RoI pooling (Image source: [Stanford CS231n slides](http://cs231n.stanford.edu/slides/2016/winter1516_lecture8.pdf).)'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图5\. RoI 池化（图片来源：[斯坦福 CS231n 幻灯片](http://cs231n.stanford.edu/slides/2016/winter1516_lecture8.pdf)）
- en: Model Workflow
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型工作流程
- en: 'How Fast R-CNN works is summarized as follows; many steps are same as in R-CNN:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: Fast R-CNN 的工作原理总结如下；许多步骤与 R-CNN 中相同：
- en: First, pre-train a convolutional neural network on image classification tasks.
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，在图像分类任务上对卷积神经网络进行预训练。
- en: Propose regions by selective search (~2k candidates per image).
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过选择性搜索提出区域（每个图像约2k个候选区域）。
- en: 'Alter the pre-trained CNN:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 修改预训练的 CNN：
- en: Replace the last max pooling layer of the pre-trained CNN with a [RoI pooling](#roi-pooling)
    layer. The RoI pooling layer outputs fixed-length feature vectors of region proposals.
    Sharing the CNN computation makes a lot of sense, as many region proposals of
    the same images are highly overlapped.
  id: totrans-63
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用 [RoI 池化](#roi-pooling) 层替换预训练 CNN 的最后一个最大池化层。RoI 池化层输出区域提议的固定长度特征向量。共享 CNN
    计算是有意义的，因为同一图像的许多区域提议高度重叠。
- en: Replace the last fully connected layer and the last softmax layer (K classes)
    with a fully connected layer and softmax over K + 1 classes.
  id: totrans-64
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用全连接层和 K + 1 类别的 softmax 替换最后一个全连接层和最后一个 softmax 层（K 类别）。
- en: 'Finally the model branches into two output layers:'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最终模型分为两个输出层：
- en: A softmax estimator of K + 1 classes (same as in R-CNN, +1 is the “background”
    class), outputting a discrete probability distribution per RoI.
  id: totrans-66
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 K + 1 类别的 softmax 估计器（与 R-CNN 中相同，+1 是“背景”类别），为每个 RoI 输出一个离散概率分布。
- en: A bounding-box regression model which predicts offsets relative to the original
    RoI for each of K classes.
  id: totrans-67
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个边界框回归模型，为每个K类别预测相对于原始 RoI 的偏移量。
- en: Loss Function
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 损失函数
- en: 'The model is optimized for a loss combining two tasks (classification + localization):'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型针对结合两个任务（分类 + 定位）的损失进行优化：
- en: '| **Symbol** | **Explanation** | | $u$ | True class label, $ u \in 0, 1, \dots,
    K$; by convention, the catch-all background class has $u = 0$. | | $p$ | Discrete
    probability distribution (per RoI) over K + 1 classes: $p = (p_0, \dots, p_K)$,
    computed by a softmax over the K + 1 outputs of a fully connected layer. | | $v$
    | True bounding box $ v = (v_x, v_y, v_w, v_h) $. | | $t^u$ | Predicted bounding
    box correction, $t^u = (t^u_x, t^u_y, t^u_w, t^u_h)$. See [above](#bounding-box-regression).
    | {:.info}'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '| **符号** | **解释** | | $u$ | 真实类别标签，$ u \in 0, 1, \dots, K$；按照惯例，全面背景类别为 $u
    = 0$。 | | $p$ | K + 1 类别的离散概率分布（每个 RoI）：$p = (p_0, \dots, p_K)$，通过全连接层的 K + 1
    输出进行 softmax 计算。 | | $v$ | 真实边界框 $ v = (v_x, v_y, v_w, v_h) $。 | | $t^u$ | 预测的边界框修正，$t^u
    = (t^u_x, t^u_y, t^u_w, t^u_h)$。参见[上文](#bounding-box-regression)。 | {:.info}'
- en: 'The loss function sums up the cost of classification and bounding box prediction:
    $\mathcal{L} = \mathcal{L}_\text{cls} + \mathcal{L}_\text{box}$. For “background”
    RoI, $\mathcal{L}_\text{box}$ is ignored by the indicator function $\mathbb{1}
    [u \geq 1]$, defined as:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数将分类成本和边界框预测成本相加：$\mathcal{L} = \mathcal{L}_\text{cls} + \mathcal{L}_\text{box}$。对于“背景”RoI，$\mathcal{L}_\text{box}$会被指示函数$\mathbb{1}
    [u \geq 1]$忽略，定义如下：
- en: $$ \mathbb{1} [u >= 1] = \begin{cases} 1 & \text{if } u \geq 1\\ 0 & \text{otherwise}
    \end{cases} $$
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \mathbb{1} [u >= 1] = \begin{cases} 1 & \text{if } u \geq 1\\ 0 & \text{otherwise}
    \end{cases} $$
- en: 'The overall loss function is:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 整体损失函数为：
- en: $$ \begin{align*} \mathcal{L}(p, u, t^u, v) &= \mathcal{L}_\text{cls} (p, u)
    + \mathbb{1} [u \geq 1] \mathcal{L}_\text{box}(t^u, v) \\ \mathcal{L}_\text{cls}(p,
    u) &= -\log p_u \\ \mathcal{L}_\text{box}(t^u, v) &= \sum_{i \in \{x, y, w, h\}}
    L_1^\text{smooth} (t^u_i - v_i) \end{align*} $$
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \begin{align*} \mathcal{L}(p, u, t^u, v) &= \mathcal{L}_\text{cls} (p, u)
    + \mathbb{1} [u \geq 1] \mathcal{L}_\text{box}(t^u, v) \\ \mathcal{L}_\text{cls}(p,
    u) &= -\log p_u \\ \mathcal{L}_\text{box}(t^u, v) &= \sum_{i \in \{x, y, w, h\}}
    L_1^\text{smooth} (t^u_i - v_i) \end{align*} $$
- en: The bounding box loss $\mathcal{L}_{box}$ should measure the difference between
    $t^u_i$ and $v_i$ using a **robust** loss function. The [smooth L1 loss](https://github.com/rbgirshick/py-faster-rcnn/files/764206/SmoothL1Loss.1.pdf)
    is adopted here and it is claimed to be less sensitive to outliers.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 边界框损失$\mathcal{L}_{box}$应该使用**鲁棒**损失函数来衡量$t^u_i$和$v_i$之间的差异。这里采用了[smooth L1
    loss](https://github.com/rbgirshick/py-faster-rcnn/files/764206/SmoothL1Loss.1.pdf)，据称对异常值不太敏感。
- en: $$ L_1^\text{smooth}(x) = \begin{cases} 0.5 x^2 & \text{if } \vert x \vert <
    1\\ \vert x \vert - 0.5 & \text{otherwise} \end{cases} $$![](../Images/7d11a7ae31f6d1d8de930fec203f427b.png)
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: $$ L_1^\text{smooth}(x) = \begin{cases} 0.5 x^2 & \text{if } \vert x \vert <
    1\\ \vert x \vert - 0.5 & \text{otherwise} \end{cases} $$![](../Images/7d11a7ae31f6d1d8de930fec203f427b.png)
- en: 'Fig. 6\. The plot of smooth L1 loss, $y = L\_1^\text{smooth}(x)$. (Image source:
    [link](https://github.com/rbgirshick/py-faster-rcnn/files/764206/SmoothL1Loss.1.pdf))'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图6。平滑L1损失的绘图，$y = L\_1^\text{smooth}(x)$。（图片来源：[link](https://github.com/rbgirshick/py-faster-rcnn/files/764206/SmoothL1Loss.1.pdf)）
- en: Speed Bottleneck
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 速度瓶颈
- en: Fast R-CNN is much faster in both training and testing time. However, the improvement
    is not dramatic because the region proposals are generated separately by another
    model and that is very expensive.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: Fast R-CNN在训练和测试时间上都快得多。然而，改进并不是很显著，因为区域建议是由另一个模型单独生成的，这是非常昂贵的。
- en: Faster R-CNN
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Faster R-CNN
- en: 'An intuitive speedup solution is to integrate the region proposal algorithm
    into the CNN model. **Faster R-CNN** ([Ren et al., 2016](https://arxiv.org/pdf/1506.01497.pdf))
    is doing exactly this: construct a single, unified model composed of RPN (region
    proposal network) and fast R-CNN with shared convolutional feature layers.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 一个直观的加速解决方案是将区域建议算法集成到CNN模型中。**Faster R-CNN**（[Ren et al., 2016](https://arxiv.org/pdf/1506.01497.pdf)）正是这样做的：构建一个由RPN（区域建议网络）和fast
    R-CNN组成的单一统一模型，共享卷积特征层。
- en: '![](../Images/1f1d9fab72eb50db18ab81ddffc21984.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1f1d9fab72eb50db18ab81ddffc21984.png)'
- en: 'Fig. 7\. An illustration of Faster R-CNN model. (Image source: [Ren et al.,
    2016](https://arxiv.org/pdf/1506.01497.pdf))'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图7。Faster R-CNN模型示意图。（图片来源：[Ren et al., 2016](https://arxiv.org/pdf/1506.01497.pdf)）
- en: Model Workflow
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型工作流程
- en: Pre-train a CNN network on image classification tasks.
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在图像分类任务上预训练CNN网络。
- en: Fine-tune the RPN (region proposal network) end-to-end for the region proposal
    task, which is initialized by the pre-train image classifier. Positive samples
    have IoU (intersection-over-union) > 0.7, while negative samples have IoU < 0.3.
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对区域建议任务端到端微调RPN（区域建议网络），该任务由预训练图像分类器初始化。正样本的IoU（交并比）> 0.7，而负样本的IoU < 0.3。
- en: Slide a small n x n spatial window over the conv feature map of the entire image.
  id: totrans-87
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在整个图像的卷积特征图上滑动一个小的n x n空间窗口。
- en: At the center of each sliding window, we predict multiple regions of various
    scales and ratios simultaneously. An anchor is a combination of (sliding window
    center, scale, ratio). For example, 3 scales + 3 ratios => k=9 anchors at each
    sliding position.
  id: totrans-88
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在每个滑动窗口的中心，我们同时预测多个不同尺度和比例的区域。一个锚点是（滑动窗口中心，尺度，比例）的组合。例如，3个尺度 + 3个比例 => 每个滑动位置有k=9个锚点。
- en: Train a Fast R-CNN object detection model using the proposals generated by the
    current RPN
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用当前RPN生成的建议训练Fast R-CNN目标检测模型
- en: Then use the Fast R-CNN network to initialize RPN training. While keeping the
    shared convolutional layers, only fine-tune the RPN-specific layers. At this stage,
    RPN and the detection network have shared convolutional layers!
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后使用 Fast R-CNN 网络初始化 RPN 训练。保持共享的卷积层，只微调 RPN 特定的层。在这个阶段，RPN 和检测网络共享卷积层！
- en: Finally fine-tune the unique layers of Fast R-CNN
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后微调 Fast R-CNN 的唯一层
- en: Step 4-5 can be repeated to train RPN and Fast R-CNN alternatively if needed.
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果需要，步骤 4-5 可以重复训练 RPN 和 Fast R-CNN。
- en: Loss Function
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 损失函数
- en: Faster R-CNN is optimized for a multi-task loss function, similar to fast R-CNN.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: Faster R-CNN 针对多任务损失函数进行了优化，类似于 Fast R-CNN。
- en: '| **Symbol** | **Explanation** | | $p_i$ | Predicted probability of anchor
    i being an object. | | $p^*_i$ | Ground truth label (binary) of whether anchor
    i is an object. | | $t_i$ | Predicted four parameterized coordinates. | | $t^*_i$
    | Ground truth coordinates. | | $N_\text{cls}$ | Normalization term, set to be
    mini-batch size (~256) in the paper. | | $N_\text{box}$ | Normalization term,
    set to the number of anchor locations (~2400) in the paper. | | $\lambda$ | A
    balancing parameter, set to be ~10 in the paper (so that both $\mathcal{L}_\text{cls}$
    and $\mathcal{L}_\text{box}$ terms are roughly equally weighted). | {:.info}'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '| **符号** | **解释** | | $p_i$ | 预测锚点 i 是对象的概率。 | | $p^*_i$ | 锚点 i 是否为对象的基本真值标签（二元）。
    | | $t_i$ | 预测的四个参数化坐标。 | | $t^*_i$ | 基本真值坐标。 | | $N_\text{cls}$ | 规范化项，设为论文中的小批量大小（~256）。
    | | $N_\text{box}$ | 规范化项，设为锚点位置数量（~2400）。 | | $\lambda$ | 平衡参数，设为论文中的 ~10（使得
    $\mathcal{L}_\text{cls}$ 和 $\mathcal{L}_\text{box}$ 项大致等权）。 | {:.info}'
- en: 'The multi-task loss function combines the losses of classification and bounding
    box regression:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 多任务损失函数结合了分类和边界框回归的损失：
- en: $$ \begin{align*} \mathcal{L} &= \mathcal{L}_\text{cls} + \mathcal{L}_\text{box}
    \\ \mathcal{L}(\{p_i\}, \{t_i\}) &= \frac{1}{N_\text{cls}} \sum_i \mathcal{L}_\text{cls}
    (p_i, p^*_i) + \frac{\lambda}{N_\text{box}} \sum_i p^*_i \cdot L_1^\text{smooth}(t_i
    - t^*_i) \\ \end{align*} $$
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \begin{align*} \mathcal{L} &= \mathcal{L}_\text{cls} + \mathcal{L}_\text{box}
    \\ \mathcal{L}(\{p_i\}, \{t_i\}) &= \frac{1}{N_\text{cls}} \sum_i \mathcal{L}_\text{cls}
    (p_i, p^*_i) + \frac{\lambda}{N_\text{box}} \sum_i p^*_i \cdot L_1^\text{smooth}(t_i
    - t^*_i) \\ \end{align*} $$
- en: where $\mathcal{L}_\text{cls}$ is the log loss function over two classes, as
    we can easily translate a multi-class classification into a binary classification
    by predicting a sample being a target object versus not. $L_1^\text{smooth}$ is
    the smooth L1 loss.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathcal{L}_\text{cls}$ 是两类之间的对数损失函数，我们可以通过将多类分类简单地转换为二元分类来预测样本是目标对象还是非目标对象。$L_1^\text{smooth}$
    是平滑的 L1 损失。
- en: $$ \mathcal{L}_\text{cls} (p_i, p^*_i) = - p^*_i \log p_i - (1 - p^*_i) \log
    (1 - p_i) $$
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \mathcal{L}_\text{cls} (p_i, p^*_i) = - p^*_i \log p_i - (1 - p^*_i) \log
    (1 - p_i) $$
- en: Mask R-CNN
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Mask R-CNN
- en: Mask R-CNN ([He et al., 2017](https://arxiv.org/pdf/1703.06870.pdf)) extends
    Faster R-CNN to pixel-level [image segmentation](https://lilianweng.github.io/posts/2017-10-29-object-recognition-part-1/#image-segmentation-felzenszwalbs-algorithm).
    The key point is to decouple the classification and the pixel-level mask prediction
    tasks. Based on the framework of [Faster R-CNN](#faster-r-cnn), it added a third
    branch for predicting an object mask in parallel with the existing branches for
    classification and localization. The mask branch is a small fully-connected network
    applied to each RoI, predicting a segmentation mask in a pixel-to-pixel manner.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: Mask R-CNN（[He et al., 2017](https://arxiv.org/pdf/1703.06870.pdf)）将 Faster
    R-CNN 扩展到像素级[图像分割](https://lilianweng.github.io/posts/2017-10-29-object-recognition-part-1/#image-segmentation-felzenszwalbs-algorithm)。关键点在于将分类和像素级掩模预测任务解耦。基于[Faster
    R-CNN](#faster-r-cnn)框架，它添加了一个第三分支，用于预测对象掩模，与用于分类和定位的现有分支并行。掩模分支是一个小型全连接网络，应用于每个
    RoI，以像素到像素的方式预测分割掩模。
- en: '![](../Images/182d643f5e586857efe73e7c9d8a8493.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/182d643f5e586857efe73e7c9d8a8493.png)'
- en: 'Fig. 8\. Mask R-CNN is Faster R-CNN model with image segmentation. (Image source:
    [He et al., 2017](https://arxiv.org/pdf/1703.06870.pdf))'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8\. Mask R-CNN 是带有图像分割的 Faster R-CNN 模型。（图片来源：[He et al., 2017](https://arxiv.org/pdf/1703.06870.pdf)）
- en: Because pixel-level segmentation requires much more fine-grained alignment than
    bounding boxes, mask R-CNN improves the RoI pooling layer (named “RoIAlign layer”)
    so that RoI can be better and more precisely mapped to the regions of the original
    image.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 因为像素级分割需要比边界框更精细的对齐，Mask R-CNN 改进了 RoI 池化层（称为“RoIAlign 层”），使得 RoI 能够更好地、更精确地映射到原始图像的区域。
- en: '![](../Images/21f3f337ad79befc682ac337ff8415a9.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/21f3f337ad79befc682ac337ff8415a9.png)'
- en: 'Fig. 9\. Predictions by Mask R-CNN on COCO test set. (Image source: [He et
    al., 2017](https://arxiv.org/pdf/1703.06870.pdf))'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图9. Mask R-CNN在COCO测试集上的预测。（图片来源：[He等人，2017](https://arxiv.org/pdf/1703.06870.pdf)）
- en: RoIAlign
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RoIAlign
- en: The RoIAlign layer is designed to fix the location misalignment caused by quantization
    in the RoI pooling. RoIAlign removes the hash quantization, for example, by using
    x/16 instead of [x/16], so that the extracted features can be properly aligned
    with the input pixels. [Bilinear interpolation](https://en.wikipedia.org/wiki/Bilinear_interpolation)
    is used for computing the floating-point location values in the input.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: RoIAlign层旨在修复RoI池化中由量化引起的位置不对齐。RoIAlign消除了哈希量化，例如，通过使用x/16而不是[x/16]，以便提取的特征可以与输入像素正确对齐。用于计算输入中浮点位置值的双线性插值。
- en: '![](../Images/4a072df929e7e8453eb3ef190cc6c799.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4a072df929e7e8453eb3ef190cc6c799.png)'
- en: 'Fig. 10\. A region of interest is mapped **accurately** from the original image
    onto the feature map without rounding up to integers. (Image source: [link](https://blog.athelas.com/a-brief-history-of-cnns-in-image-segmentation-from-r-cnn-to-mask-r-cnn-34ea83205de4))'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图10. 一个感兴趣区域从原始图像精确映射到特征图，而不是四舍五入到整数。（图片来源：[链接](https://blog.athelas.com/a-brief-history-of-cnns-in-image-segmentation-from-r-cnn-to-mask-r-cnn-34ea83205de4)）
- en: Loss Function
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 损失函数
- en: 'The multi-task loss function of Mask R-CNN combines the loss of classification,
    localization and segmentation mask: $ \mathcal{L} = \mathcal{L}_\text{cls} + \mathcal{L}_\text{box}
    + \mathcal{L}_\text{mask}$, where $\mathcal{L}_\text{cls}$ and $\mathcal{L}_\text{box}$
    are same as in Faster R-CNN.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: Mask R-CNN的多任务损失函数结合了分类、定位和分割掩模的损失：$ \mathcal{L} = \mathcal{L}_\text{cls} +
    \mathcal{L}_\text{box} + \mathcal{L}_\text{mask}$，其中 $\mathcal{L}_\text{cls}$
    和 $\mathcal{L}_\text{box}$ 与Faster R-CNN中的相同。
- en: The mask branch generates a mask of dimension m x m for each RoI and each class;
    K classes in total. Thus, the total output is of size $K \cdot m^2$. Because the
    model is trying to learn a mask for each class, there is no competition among
    classes for generating masks.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 掩模分支为每个RoI和每个类别生成一个大小为m x m的掩模；总共有K个类别。因此，总输出大小为$K \cdot m^2$。因为模型试图为每个类别学习一个掩模，所以没有类别之间为生成掩模而竞争。
- en: $\mathcal{L}_\text{mask}$ is defined as the average binary cross-entropy loss,
    only including k-th mask if the region is associated with the ground truth class
    k.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: $\mathcal{L}_\text{mask}$ 被定义为平均二元交叉熵损失，仅在区域与地面真实类别k相关联时才包括第k个掩模。
- en: $$ \mathcal{L}_\text{mask} = - \frac{1}{m^2} \sum_{1 \leq i, j \leq m} \big[
    y_{ij} \log \hat{y}^k_{ij} + (1-y_{ij}) \log (1- \hat{y}^k_{ij}) \big] $$
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \mathcal{L}_\text{mask} = - \frac{1}{m^2} \sum_{1 \leq i, j \leq m} \big[
    y_{ij} \log \hat{y}^k_{ij} + (1-y_{ij}) \log (1- \hat{y}^k_{ij}) \big] $$
- en: where $y_{ij}$ is the label of a cell (i, j) in the true mask for the region
    of size m x m; $\hat{y}_{ij}^k$ is the predicted value of the same cell in the
    mask learned for the ground-truth class k.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $y_{ij}$ 是大小为m x m的真实掩模中单元格(i, j)的标签；$\hat{y}_{ij}^k$ 是为地面真实类别k学习的掩模中相同单元格的预测值。
- en: Summary of Models in the R-CNN family
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: R-CNN家族模型总结
- en: Here I illustrate model designs of R-CNN, Fast R-CNN, Faster R-CNN and Mask
    R-CNN. You can track how one model evolves to the next version by comparing the
    small differences.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我展示了R-CNN、Fast R-CNN、Faster R-CNN和Mask R-CNN的模型设计。通过比较微小的差异，您可以追踪一个模型如何演变为下一个版本。
- en: '![](../Images/a8a0a7466f3ee63a338ef5ebe9365638.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a8a0a7466f3ee63a338ef5ebe9365638.png)'
- en: '* * *'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 'Cited as:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 引用为：
- en: '[PRE0]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Reference
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. [“Rich
    feature hierarchies for accurate object detection and semantic segmentation.”](https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf)
    In Proc. IEEE Conf. on computer vision and pattern recognition (CVPR), pp. 580-587\.
    2014.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. [“准确目标检测和语义分割的丰富特征层次结构。”](https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf)
    在IEEE计算机视觉与模式识别会议（CVPR）论文集中，第580-587页。2014年。'
- en: '[2] Ross Girshick. [“Fast R-CNN.”](https://arxiv.org/pdf/1504.08083.pdf) In
    Proc. IEEE Intl. Conf. on computer vision, pp. 1440-1448\. 2015.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Ross Girshick. [“Fast R-CNN。”](https://arxiv.org/pdf/1504.08083.pdf) 在IEEE国际计算机视觉会议论文集中，第1440-1448页。2015年。'
- en: '[3] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. [“Faster R-CNN:
    Towards real-time object detection with region proposal networks.”](http://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks.pdf)
    In Advances in neural information processing systems (NIPS), pp. 91-99\. 2015.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Shaoqing Ren, Kaiming He, Ross Girshick 和 Jian Sun. [“Faster R-CNN: Towards
    real-time object detection with region proposal networks.”](http://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks.pdf)
    在神经信息处理系统 (NIPS) 进展中，第 91-99 页。2015.'
- en: '[4] Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick. [“Mask R-CNN.”](https://arxiv.org/pdf/1703.06870.pdf)
    arXiv preprint arXiv:1703.06870, 2017.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Kaiming He, Georgia Gkioxari, Piotr Dollár 和 Ross Girshick. [“Mask R-CNN.”](https://arxiv.org/pdf/1703.06870.pdf)
    arXiv 预印本 arXiv:1703.06870, 2017.'
- en: '[5] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. [“You only
    look once: Unified, real-time object detection.”](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf)
    In Proc. IEEE Conf. on computer vision and pattern recognition (CVPR), pp. 779-788\.
    2016.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Joseph Redmon, Santosh Divvala, Ross Girshick 和 Ali Farhadi. [“You only
    look once: Unified, real-time object detection.”](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf)
    在 IEEE 计算机视觉与模式识别 (CVPR) 会议论文集中，第 779-788 页。2016.'
- en: '[6] [“A Brief History of CNNs in Image Segmentation: From R-CNN to Mask R-CNN”](https://blog.athelas.com/a-brief-history-of-cnns-in-image-segmentation-from-r-cnn-to-mask-r-cnn-34ea83205de4)
    by Athelas.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] [“卷积神经网络在图像分割中的简要历史：从 R-CNN 到 Mask R-CNN”](https://blog.athelas.com/a-brief-history-of-cnns-in-image-segmentation-from-r-cnn-to-mask-r-cnn-34ea83205de4)
    作者 Athelas.'
- en: '[7] Smooth L1 Loss: [https://github.com/rbgirshick/py-faster-rcnn/files/764206/SmoothL1Loss.1.pdf](https://github.com/rbgirshick/py-faster-rcnn/files/764206/SmoothL1Loss.1.pdf)'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] 平滑 L1 损失：[https://github.com/rbgirshick/py-faster-rcnn/files/764206/SmoothL1Loss.1.pdf](https://github.com/rbgirshick/py-faster-rcnn/files/764206/SmoothL1Loss.1.pdf)'
