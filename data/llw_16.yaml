- en: Controllable Neural Text Generation
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可控神经文本生成
- en: 原文：[https://lilianweng.github.io/posts/2021-01-02-controllable-text-generation/](https://lilianweng.github.io/posts/2021-01-02-controllable-text-generation/)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://lilianweng.github.io/posts/2021-01-02-controllable-text-generation/](https://lilianweng.github.io/posts/2021-01-02-controllable-text-generation/)
- en: '[Updated on 2021-02-01: Updated to version 2.0 with several work added and
    many typos fixed.]'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[更新于 2021-02-01: 更新为 2.0 版本，添加了一些工作并修复了许多拼写错误。]'
- en: '[Updated on 2021-05-26: Add P-tuning and Prompt Tuning in the [“prompt design”](#gradient-based-search)
    section.]'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[更新于 2021-05-26: 在[“提示设计”](#gradient-based-search)部分添加了 P-tuning 和 Prompt Tuning。]'
- en: '[Updated on 2021-09-19: Add [“unlikelihood training”](##unlikelihood-training).]'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[更新于 2021-09-19: 添加[“非概然性训练”](##unlikelihood-training)。]'
- en: There is a gigantic amount of free text on the Web, several magnitude more than
    labelled benchmark datasets. The state-of-the-art language models (LM) are trained
    with unsupervised Web data in large scale. When generating samples from LM by
    iteratively sampling the next token, we do not have much control over attributes
    of the output text, such as the topic, the style, the sentiment, etc. Many applications
    would demand a good control over the model output. For example, if we plan to
    use LM to generate reading materials for kids, we would like to guide the output
    stories to be safe, educational and easily understood by children.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 网络上有大量的免费文本，比标记的基准数据集多几个数量级。最先进的语言模型（LM）是通过大规模的无监督网络数据进行训练的。当通过迭代地抽样下一个标记来从
    LM 生成样本时，我们对输出文本的属性（如主题、风格、情感等）没有太多控制。许多应用程序需要对模型输出进行良好的控制。例如，如果我们计划使用 LM 为孩子们生成阅读材料，我们希望引导输出的故事是安全的、教育性的，并且容易被孩子理解。
- en: How to steer a powerful unconditioned language model? In this post, we will
    delve into several approaches for controlled content generation with an unconditioned
    langage model. Note that model steerability is still an open research question.
    Each introduced method has certain pros & cons.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 如何引导一个强大的无条件语言模型？在本文中，我们将深入探讨几种用于受控内容生成的方法，其中包括无条件语言模型。请注意，模型的可操纵性仍然是一个开放的研究问题。每种介绍的方法都有一定的优缺点。
- en: Apply guided decoding strategies and select desired outputs at test time.
  id: totrans-7
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在测试时应用引导式解码策略并选择所需的输出。
- en: Optimize for the most desired outcomes via good prompt design.
  id: totrans-8
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过良好的提示设计来优化最理想的结果。
- en: Fine-tune the base model or steerable layers to do conditioned content generation.
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对基础模型或可操纵层进行微调，以进行有条件的内容生成。
- en: 'In the following discussion, we assume we have access to a pretrained generative
    language model $p_\theta$. The model has learned the distribution over token sequences
    by optimizing for the next token prediction: $ \mathcal{L}_\text{ML} = - \sum_t
    \log p_\theta(x_t \vert x_{<t}) $.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的讨论中，我们假设我们可以访问一个预训练的生成式语言模型 $p_\theta$。该模型通过优化下一个标记的预测学习了标记序列上的分布：$ \mathcal{L}_\text{ML}
    = - \sum_t \log p_\theta(x_t \vert x_{<t}) $。
- en: Decoding Strategies
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解码策略
- en: By adopting different decoding methods, we can place restrictions or preferences
    on the sampling process to alter the generated samples without modifying any model
    weights. Even though decoding strategies do not change the values of any trainable
    parameter, it is a quite important component.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 通过采用不同的解码方法，我们可以对抽样过程施加限制或偏好，以改变生成的样本，而不修改任何模型权重。尽管解码策略不会改变任何可训练参数的值，但它是一个非常重要的组成部分。
- en: Common Decoding Methods
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 常见的解码方法
- en: Since the final layer of the model predicts logits $o$ over the vocabulary space,
    the next token can be sampled by applying softmax with temperature $T$. The probability
    of sampling the $i$-th token is
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 由于模型的最终层预测了词汇空间上的 logits $o$，下一个标记可以通过应用温度 $T$ 的 softmax 进行抽样。抽样第 $i$ 个标记的概率是
- en: $$ p_i \propto \frac{\exp(o_i / T)}{\sum_j \exp(o_j/T)} $$
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: $$ p_i \propto \frac{\exp(o_i / T)}{\sum_j \exp(o_j/T)} $$
- en: A low temperature would make the distribution sharper and a high value makes
    it softer.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 低温度会使分布更尖锐，而高值会使其更柔和。
- en: '**Greedy search**: Always pick the next token with the *highest* probability,
    equivalent to setting temperature $T=0$. However, it tends to create repetitions
    of phrases, even for well-trained models.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**贪婪搜索**：始终选择具有*最高*概率的下一个标记，相当于设置温度 $T=0$。然而，它往往会创建短语的重复，即使对于训练良好的模型也是如此。'
- en: '**Beam search**: It essentially does breadth-first search, one token per tree
    level, but with a limited bandwidth. At each level of the search tree, beam search
    keeps track of $n$ (named “beam width”) best candidates and expands all the successors
    of these candidates in the next level. Beam search could stop expanding a node
    if it hits the EOS (end-of-sentence) token.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**Beam search**：本质上是广度优先搜索，每个树级别一个标记，但带有有限的带宽。在搜索树的每个级别，beam search跟踪$n$（称为“束宽”）个最佳候选项，并在下一个级别扩展这些候选项的所有后继。如果遇到EOS（句子结束）标记，beam
    search可能停止扩展一个节点。'
- en: However, maximization-based decoding does not guarantee high-quality generation.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，基于最大化的解码并不能保证高质量的生成。
- en: '![](../Images/4cdd1bde84d6ef1d91fad1b7323c8e32.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4cdd1bde84d6ef1d91fad1b7323c8e32.png)'
- en: 'Fig. 1\. The probability assigned to the next token by beam search versus by
    humans. The human selected tokens have much higher variance in predicted probability
    and thus more surprising. (Image source: [Holtzman et al. 2019](https://arxiv.org/abs/1904.09751))'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图1\. 由beam search和人类分配给下一个标记的概率。人类选择的标记在预测概率上具有更高的方差，因此更具惊喜性。（图片来源：[Holtzman
    et al. 2019](https://arxiv.org/abs/1904.09751)）
- en: '**Top-k sampling** ([Fan et al., 2018](https://arxiv.org/abs/1805.04833)):
    At each sampling step, only the top $k$ most likely tokens are selected and the
    probability mass is redistributed among them. In [Fan et al., 2018](https://arxiv.org/abs/1805.04833),
    the authors proposed to use *top-k random sampling* where the next token is randomly
    selected among the top $k$ most likely candidates and they argued that this approach
    can generate more novel and less repetitive content than beam search.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**Top-k抽样** ([Fan et al., 2018](https://arxiv.org/abs/1805.04833))：在每个抽样步骤中，只选择前$k$个最有可能的标记，并在它们之间重新分配概率质量。在[Fan
    et al., 2018](https://arxiv.org/abs/1805.04833)中，作者提出使用*top-k随机抽样*，其中下一个标记在前$k$个最有可能的候选项中随机选择，他们认为这种方法可以生成比beam
    search更多新颖且不那么重复的内容。'
- en: '**Nucleus sampling** ([Holtzman et al. 2019](https://arxiv.org/abs/1904.09751)):
    Also known as “Top-p sampling”. One drawback of top-k sampling is that the predefined
    number $k$ does not take into consideration how *skewed* the probability distribution
    might be. The nucleus sampling selects the smallest set of top candidates with
    the cumulative probability exceeding a threshold (e.g. 0.95) and then the distribution
    is rescaled among selected candidates.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**核抽样** ([Holtzman et al. 2019](https://arxiv.org/abs/1904.09751))：也称为“Top-p抽样”。top-k抽样的一个缺点是预定义的数字$k$并没有考虑到概率分布可能有多么*倾斜*。核抽样选择最小的一组累积概率超过阈值（例如0.95）的顶级候选项，然后在选定的候选项中重新缩放分布。'
- en: Both top-k and nucleus sampling have less repetitions with a proper set of hyperparameters.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 无论是top-k抽样还是核抽样，都具有较少的重复性和适当的一组超参数。
- en: '**Penalized sampling** ([Keskar et al. 2019](https://arxiv.org/abs/1909.05858)):
    To avoid the common failure case of generating duplicate substrings, the [CTRL](https://arxiv.org/abs/1909.05858)
    paper proposed a new sampling method to penalize repetitions by discounting the
    scores of previously generated tokens. The probability distribution for the next
    token with repetition penalty is defined as:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**惩罚抽样** ([Keskar et al. 2019](https://arxiv.org/abs/1909.05858))：为了避免生成重复子字符串的常见失败情况，[CTRL](https://arxiv.org/abs/1909.05858)论文提出了一种新的抽样方法，通过打折先前生成的标记的分数来惩罚重复。带有重复惩罚的下一个标记的概率分布定义如下：'
- en: $$ p_i = \frac{\exp(o_i / (T \cdot \mathbb{1}(i \in g)))}{\sum_j \exp(o_j /
    (T \cdot \mathbb{1}(j \in g)))} \quad \mathbb{1}(c) = \theta \text{ if the condition
    }c\text{ is True else }1 $$
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: $$ p_i = \frac{\exp(o_i / (T \cdot \mathbb{1}(i \in g)))}{\sum_j \exp(o_j /
    (T \cdot \mathbb{1}(j \in g)))} \quad \mathbb{1}(c) = \theta \text{ if the condition
    }c\text{ is True else }1 $$
- en: where $g$ contains a set of previously generated tokens, $\mathbb{1}(.)$ is
    an identity function. $\theta=1.2$ is found to yield a good balance between less
    repetition and truthful generation.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$g$包含一组先前生成的标记，$\mathbb{1}(.)$是一个恒等函数。$\theta=1.2$被发现能在减少重复和生成真实内容之间取得良好平衡。
- en: Guided Decoding
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 引导解码
- en: All the above standard decoding strategies sample tokens according to the predicted
    probability, with no additional information. Our preferences on topic or sentiment
    can be baked into the candidate ranking function to guide the sample generation
    by altering the candidate ranking score. The ranking score for token selection
    at each decoding step can be set as a combination of LM log-likelihood and a set
    of desired feature discriminators. The features are designed to quantify human
    preferences by heuristics ([Ghazvininejad et al., 2017](https://www.aclweb.org/anthology/P17-4008/)),
    supervised learning ([Holtzman et al., 2018](https://arxiv.org/abs/1805.06087))
    or RL ([Li et al., 2017](https://arxiv.org/abs/1701.06549)).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 所有上述标准解码策略根据预测概率对标记进行采样，没有额外信息。我们对主题或情感的偏好可以通过候选排名函数嵌入到其中，以通过改变候选排名分数来引导样本生成。在每个解码步骤中用于标记选择的排名分数可以设置为LM对数似然和一组期望特征鉴别器的组合。这些特征旨在通过启发式（[Ghazvininejad等人，2017](https://www.aclweb.org/anthology/P17-4008/)）、监督学习（[Holtzman等人，2018](https://arxiv.org/abs/1805.06087)）或RL（[Li等人，2017](https://arxiv.org/abs/1701.06549)）来量化人类偏好。
- en: '[Ghazvininejad et al. (2017)](https://www.aclweb.org/anthology/P17-4008/) built
    a system called “Hafez” for generating poetry in desired style by adjusting sampling
    weights in beam search at decoding steps. The likelihood of sampling for the next
    token $x_{t+1}$ at step $t$ is augmented by a scoring function:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '[Ghazvininejad等人（2017）](https://www.aclweb.org/anthology/P17-4008/)构建了一个名为“Hafez”的系统，通过在解码步骤中调整波束搜索中的采样权重来生成所需风格的诗歌。在步骤$t$中，下一个标记$x_{t+1}$的采样可能性通过一个评分函数增强：'
- en: $$ \text{score}(x_{t+1}, b_t) = \text{score}(b_t) + \log p(x_{t+1}) + \color{green}{\sum_i
    \alpha_i f_i(x_{t+1})} $$
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \text{score}(x_{t+1}, b_t) = \text{score}(b_t) + \log p(x_{t+1}) + \color{green}{\sum_i
    \alpha_i f_i(x_{t+1})} $$
- en: where $\log p(x_{t+1})$ is the log-likelihood predicted by LM. $\text{score}(b_t)$
    is the accumulated score of the already-generated words in the current beam state
    $b_t$. The green part can incorporate many different features for steering the
    style of the output. A set of feature functions $f_i(.)$ define the preferences
    and the associated weights $alpha_i$ work like “control knobs” that can be easily
    customized at decoding time. Features can measure a variety of attributes and
    can be easily combined; for example,
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\log p(x_{t+1})$是LM预测的对数似然。$\text{score}(b_t)$是当前波束状态$b_t$中已生成单词的累积分数。绿色部分可以包含许多不同的特征，用于引导输出的风格。一组特征函数$f_i(.)$定义了偏好和相关权重$alpha_i$的工作方式类似于“控制旋钮”，可以在解码时轻松定制。特征可以衡量各种属性，并且可以轻松组合；例如，
- en: whether $x_{t+1}$ exists in a bag of desired or banned topical words.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是否$x_{t+1}$存在于所需或禁止的主题词袋中。
- en: whether $x_{t+1}$ indicates certain sentiments.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是否$x_{t+1}$表示特定情绪。
- en: whether $x_{t+1}$ is a repeated token (and thus $f_i$ needs to take the history
    as input too).
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是否$x_{t+1}$是一个重复的标记（因此$f_i$需要将历史作为输入）。
- en: the length of $x_{t+1}$ if longer or shorter words are in particular preferred.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果更长或更短的单词特别受欢迎，$x_{t+1}$的长度。
- en: Similar to Hafez, [Baheti et al. (2018)](https://arxiv.org/abs/1809.01215) manually
    designed features for ranking and altered the sampling distribution by appending
    similarity scores between topic distribution or embeddings of the context and
    the completion.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 与Hafez类似，[Baheti等人（2018）](https://arxiv.org/abs/1809.01215)为排名手动设计了特征，并通过附加上下文和完成的主题分布或嵌入之间的相似性分数改变了采样分布。
- en: '[Holtzman et al. (2018)](https://arxiv.org/abs/1805.06087) adopted a set of
    learned discriminators, each specializing in a different principle of communication
    guided by [Grice’s maxims](https://en.wikipedia.org/wiki/Cooperative_principle):
    quality, quantity, relation and manner. The discriminators learn to encode these
    desired principles by measuring repetition, entailment, relevance, and lexical
    diversity, respectively. Given some ground truth completion, all the discriminator
    models are trained to minimize the ranking log-likelihood, $\log\sigma(f_i(y_g)
    - f_i(y))$, because the gold continuation $y_g$ is expected to obtain a higher
    score than the generated one $y$. Here the weight coefficients $\alpha_i$ are
    also learned to minimize the score difference between the golden standard and
    the generated completion. Discriminative Adversarial Search (DAS; [Scialom et
    al., 2020](https://arxiv.org/abs/2002.10375)) is inspired by GAN and trains the
    discriminator to tell apart human created text from machine generated text. The
    discriminator predicts a label for each token instead of for the entire sequence.
    The discriminator logprob is added to the score to guide sampling towards the
    human-written style.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '[Holtzman等人（2018）](https://arxiv.org/abs/1805.06087)采用了一组学习的鉴别器，每个鉴别器专门针对由[Grice的格言](https://en.wikipedia.org/wiki/Cooperative_principle)指导的不同沟通原则：质量、数量、关系和方式。鉴别器通过测量重复、蕴涵、相关性和词汇多样性来学习编码这些期望的原则。鉴别器模型都经过训练，以最小化排名对数似然，$\log\sigma(f_i(y_g)
    - f_i(y))$，因为预期金标准完成$y_g$的得分应高于生成的$y$。这里权重系数$\alpha_i$也被学习，以最小化金标准和生成完成之间的得分差异。鉴别性对抗搜索（DAS；[Scialom等人，2020](https://arxiv.org/abs/2002.10375)）受到GAN的启发，训练鉴别器区分人类创作的文本和机器生成的文本。鉴别器为每个标记预测一个标签，而不是整个序列。鉴别器logprob被添加到得分中，以引导采样朝向人类写作风格。'
- en: '[Meister et al. (2020)](https://arxiv.org/abs/2010.02650) studied beam search
    in a regularized decoding framework:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '[Meister等人（2020）](https://arxiv.org/abs/2010.02650)在正则化解码框架中研究了波束搜索：'
- en: $$ \mathbf{y}^* = \arg\max_{\mathbf{y}\in\mathcal{Y}} \big( \underbrace{\log
    p_\theta(\mathbf{y}\vert\mathbf{x})}_\text{MAP} - \underbrace{\lambda\mathcal{R}(\mathbf{y})}_\text{regularizer}
    \big) $$
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \mathbf{y}^* = \arg\max_{\mathbf{y}\in\mathcal{Y}} \big( \underbrace{\log
    p_\theta(\mathbf{y}\vert\mathbf{x})}_\text{MAP} - \underbrace{\lambda\mathcal{R}(\mathbf{y})}_\text{regularizer}
    \big) $$
- en: 'Since we expect maximum probability to have minimum surprise, the surprisal
    of a LM at time step $t$ can be defined as follows:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们期望最大概率具有最小惊奇度，LM在时间步$t$的惊奇度可以定义如下：
- en: $$ \begin{aligned} u_0(\texttt{BOS}) &= 0 \text{ ; BOS is a placeholder token
    for the beginning of a sentence.}\\ u_t(y) &= -\log P_\theta(y \vert \mathbf{x},
    \mathbf{y}_{
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \begin{aligned} u_0(\texttt{BOS}) &= 0 \text{ ; BOS是句子开头的占位符。}\\ u_t(y) &=
    -\log P_\theta(y \vert \mathbf{x}, \mathbf{y}_{
- en: The MAP (maximum a posteriori) part demands for sequences with maximum probability
    given context, while the regularizer introduces other constraints. It is possible
    a global optimal strategy may need to have a high-surprisal step occasionally
    so that it can shorten the output length or produce more low-surprisal steps afterwards.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: MAP（最大后验）部分要求在给定上下文的情况下具有最大概率的序列，而正则化器引入其他约束。可能需要全局最优策略偶尔进行高惊奇步骤，以便缩短输出长度或在之后产生更多低惊奇步骤。
- en: 'Beam search has gone through the test of time in the field of NLP. The question
    is: *If we want to model beam search as exact search in a regularized decoding
    framework, how should $\mathcal{R}(\mathbf{y})$ be modeled?* The paper proposed
    a connection between beam search and the *uniform information density* (UID) hypothesis.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 波束搜索在自然语言处理领域经受住了时间的考验。问题是：*如果我们想将波束搜索建模为正则化解码框架中的精确搜索，应该如何建模 $\mathcal{R}(\mathbf{y})$？*
    该论文提出了波束搜索与*均匀信息密度*（UID）假设之间的联系。
- en: “The uniform information density hypothesis (UID; Levy and Jaeger, 2007) states
    that—subject to the constraints of the grammar—humans prefer sentences that distribute
    information (in the sense of information theory) equally across the linguistic
    signal, e.g., a sentence.”
  id: totrans-45
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “均匀信息密度假设（UID；Levy和Jaeger，2007）指出——在语法的约束下——人类更喜欢将信息（在信息论意义上）均匀分布在语言信号中，例如，一句话。”
- en: In other words, it hypothesizes that humans prefer text with evenly distributed
    surprisal. Popular decoding methods like top-k sampling or nuclear sampling actually
    filter out high-surprisal options, thus implicitly encouraging the UID property
    in output sequences.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，它假设人类更喜欢具有均匀分布surprisal的文本。流行的解码方法如top-k采样或核采样实际上会过滤掉高surprisal选项，从而在输出序列中隐式地鼓励UID属性。
- en: 'The paper experimented with several forms of regularizers:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 该论文尝试了几种形式的正则化器：
- en: '*Greedy*: $\mathcal{R}_\text{greedy}(\mathbf{y}) = \sum_{t=1}^{\vert\mathbf{y}\vert}
    \big(u_t(y_t) - \min_{y’ \in \mathcal{V}} u_t(y’) \big)^2$; if set $\lambda \to
    \infty$, we have greedy search. Note that being greedy at each individual step
    does not guarantee global optimality.'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*贪婪*：$\mathcal{R}_\text{greedy}(\mathbf{y}) = \sum_{t=1}^{\vert\mathbf{y}\vert}
    \big(u_t(y_t) - \min_{y’ \in \mathcal{V}} u_t(y’) \big)^2$；如果设$\lambda \to \infty$，我们得到贪婪搜索。请注意，每个步骤贪婪并不保证全局最优性。'
- en: '*Variance regularizer*: $\mathcal{R}_\text{var}(\mathbf{y}) = \frac{1}{\vert\mathbf{y}\vert}\sum_{t=1}^{\vert\mathbf{y}\vert}
    \big(u_t(y_t) - \bar{u} \big)^2$ , where $\bar{u}$ is the average surprisal over
    all timesteps. It directly encodes the UID hypothesis.'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*方差正则化器*：$\mathcal{R}_\text{var}(\mathbf{y}) = \frac{1}{\vert\mathbf{y}\vert}\sum_{t=1}^{\vert\mathbf{y}\vert}
    \big(u_t(y_t) - \bar{u} \big)^2$，其中$\bar{u}$是所有时间步的平均surprisal。它直接编码了UID假设。'
- en: '*Local consistency*: $\mathcal{R}_\text{local}(\mathbf{y}) = \frac{1}{\vert\mathbf{y}\vert}\sum_{t=1}^{\vert\mathbf{y}\vert}
    \big(u_t(y_t) - u_{t-1}(y_{t-1}) \big)^2$; this decoding regularizer encourages
    adjacent tokens to have similar surprisal.'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*局部一致性*：$\mathcal{R}_\text{local}(\mathbf{y}) = \frac{1}{\vert\mathbf{y}\vert}\sum_{t=1}^{\vert\mathbf{y}\vert}
    \big(u_t(y_t) - u_{t-1}(y_{t-1}) \big)^2$；这种解码正则化器鼓励相邻标记具有相似的surprisal。'
- en: '*Max regularizer*: $\mathcal{R}_\text{max}(\mathbf{y}) = \max_t u_t(y_t)$ penalizes
    the maximum compensation of surprisal.'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*最大正则化器*：$\mathcal{R}_\text{max}(\mathbf{y}) = \max_t u_t(y_t)$惩罚surprisal的最大补偿。'
- en: '*Squared regularizer*: $\mathcal{R}_\text{square}(\mathbf{y}) = \sum_{t=1}^{\vert\mathbf{y}\vert}
    u_t(y_t)^2$ encourages all the tokens to have surprisal close to 0.'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*平方正则化器*：$\mathcal{R}_\text{square}(\mathbf{y}) = \sum_{t=1}^{\vert\mathbf{y}\vert}
    u_t(y_t)^2$鼓励所有标记的surprisal接近0。'
- en: An experiment with greedy regularizers showed that larger $\lambda$ results
    in better performance (e.g. measured by BLEU for NMT task) and lower std dev of
    surprisal.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 通过对贪婪正则化器进行实验表明，较大的$\lambda$会导致更好的性能（例如通过NMT任务的BLEU度量）和更低的surprisal标准差。
- en: '![](../Images/b551fe60ca594a773c44448fe9a913f2.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b551fe60ca594a773c44448fe9a913f2.png)'
- en: 'Fig. 2\. The plot of BLEU and std. dev of surprisals as functions of the strength
    of the regularizer $\lambda$. The subgraph in grey shows the relationship between
    BLEU and surprisal std. dev. (Image source: [Meister et al. 2020](https://arxiv.org/abs/2010.02650))'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图2。BLEU和surprisal标准差作为正则化器$\lambda$强度的函数的绘图。灰色子图显示了BLEU和surprisal标准差之间的关系。 （图片来源：[Meister
    et al. 2020](https://arxiv.org/abs/2010.02650)）
- en: A default beam search would have text generation of decreased quality when beam
    size increases. Regularized beam search greatly helps alleviate this issue. A
    combined regularizer further improves the performance. In their experiments for
    NMT, they found $\lambda=5$ for greedy and $\lambda=2$ for squared work out as
    the optimal combined regularizer.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 默认的波束搜索在波束大小增加时会导致文本生成质量下降。正则化的波束搜索极大地帮助缓解了这个问题。结合正则化器进一步提高了性能。在他们的NMT实验中，他们发现贪婪的$\lambda=5$和平方的$\lambda=2$是最佳的组合正则化器。
- en: '![](../Images/2366566442f2c038b61a447e5ed84c3a.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2366566442f2c038b61a447e5ed84c3a.png)'
- en: 'Fig. 3\. The plot of BLEU of a function of beam size (left) and BLEU scores
    for translations created by different regularized decoding strategies. (Image
    source: [Meister et al. 2020](https://arxiv.org/abs/2010.02650))'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图3。BLEU作为波束大小的函数的绘图（左）和不同正则化解码策略创建的翻译的BLEU分数。 （图片来源：[Meister et al. 2020](https://arxiv.org/abs/2010.02650)）
- en: Guided decoding essentially runs a more expensive beam search where the sampling
    probability distribution is altered by side information about human preferences.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 引导解码本质上是运行一个更昂贵的波束搜索，其中采样概率分布受到有关人类偏好的辅助信息的影响。
- en: Trainable Decoding
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可训练解码
- en: Given a trained language model, [Gu et al (2017)](https://arxiv.org/abs/1702.02429)
    proposed a **trainable greedy decoding** algorithm to maximize an arbitrary objective
    for sampling sequences. The idea is based on the *noisy, parallel approximate
    decoding* ([NPAD](https://arxiv.org/abs/1605.03835)). NPAD injects unstructured
    noise into the model hidden states and runs noisy decoding multiple times in parallel
    to avoid potential degradation. To take a step further, trainable greedy decoding
    replaces the unstructured noise with a learnable random variable, predicted by
    a RL agent that takes the previous hidden state, the previous decoded token and
    the context as input. In other words, the decoding algorithm learns a RL actor
    to manipulate the model hidden states for better outcomes.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在给定训练好的语言模型的情况下，[顾等人（2017）](https://arxiv.org/abs/1702.02429)提出了一个**可训练的贪婪解码**算法，用于最大化任意目标以对序列进行采样。这个想法基于*嘈杂的、并行的近似解码*（[NPAD](https://arxiv.org/abs/1605.03835)）。NPAD向模型隐藏状态注入非结构化噪声，并并行多次运行嘈杂解码，以避免潜在的退化。为了更进一步，可训练的贪婪解码用可学习的随机变量替换了非结构化噪声，由一个接受前一个隐藏状态、前一个解码标记和上下文作为输入的RL代理预测。换句话说，解码算法学习一个RL
    actor来操纵模型隐藏状态以获得更好的结果。
- en: '[Grover et al. (2019)](https://arxiv.org/abs/1906.09531) trained a binary classifier
    to distinguish samples from data distribution and samples from the generative
    model. This classifier is used to estimate *importance weights* for constructing
    a new unnormalized distribution. The proposed strategy is called **likelihood-free
    importance weighting (LFIW)**.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[Grover等人（2019）](https://arxiv.org/abs/1906.09531)训练了一个二元分类器来区分来自数据分布的样本和来自生成模型的样本。这个分类器用于估计*重要性权重*以构建一个新的非规范化分布。所提出的策略被称为**无似然重要性加权（LFIW）**。'
- en: Let $p$ be the real data distribution and $p_\theta$ be a learned generative
    model. A classical approach for evaluating the expectation of a given function
    $f$ under $p$ using samples from $p_\theta$ is to use importance sampling.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 让$p$表示真实数据分布，$p_\theta$表示学习到的生成模型。使用从$p_\theta$中获取的样本来评估给定函数$f$在$p$下的期望的经典方法是使用重要性抽样。
- en: $$ \mathbb{E}_{\mathbf{x}\sim p} [f(\mathbf{x})] = \mathbb{E}_{\mathbf{x}\sim
    p_\theta} \Big[\frac{p(\mathbf{x})}{p_\theta(\mathbf{x})} f(\mathbf{x})\Big] \approx
    \frac{1}{N} \sum_{i=1}^N w(\mathbf{x}_i)f(\mathbf{x}_i) $$
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \mathbb{E}_{\mathbf{x}\sim p} [f(\mathbf{x})] = \mathbb{E}_{\mathbf{x}\sim
    p_\theta} \Big[\frac{p(\mathbf{x})}{p_\theta(\mathbf{x})} f(\mathbf{x})\Big] \approx
    \frac{1}{N} \sum_{i=1}^N w(\mathbf{x}_i)f(\mathbf{x}_i) $$
- en: 'However, $p(\mathbf{x})$ can only be estimated via finite datasets. Let $c_\phi:
    \mathcal{X} \to [0,1]$ be a probabilistic binary classifier for predicting whether
    a sample $\mathbf{x}$ is from the true data distribution ($y=1$). The joint distribution
    over $\mathcal{X}\times\mathcal{Y}$ is denoted as $q(\mathbf{x}, y)$.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '然而，$p(\mathbf{x})$只能通过有限数据集来估计。让$c_\phi: \mathcal{X} \to [0,1]$是一个用于预测样本$\mathbf{x}$是否来自真实数据分布（$y=1$）的概率二元分类器。$\mathcal{X}\times\mathcal{Y}$上的联合分布被表示为$q(\mathbf{x},
    y)$。'
- en: $$ q(\mathbf{x}\vert y) = \begin{cases} p_\theta(\mathbf{x}) & \text{ if }y=0\text{;
    predicted to be generated data} \\ p(\mathbf{x}) & \text{ otherwise; from the
    true data distribution} \end{cases} $$
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: $$ q(\mathbf{x}\vert y) = \begin{cases} p_\theta(\mathbf{x}) & \text{ 如果 }y=0\text{；预测为生成数据}
    \\ p(\mathbf{x}) & \text{ 否则；来自真实数据分布} \end{cases} $$
- en: 'Then if $c_\phi$ is [Bayes optimal](https://svivek.com/teaching/lectures/slides/prob-learning/bayes-optimal-classifier.pdf),
    the importance weight can be estimated by:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 如果$c_\phi$是[贝叶斯最优](https://svivek.com/teaching/lectures/slides/prob-learning/bayes-optimal-classifier.pdf)，那么重要性权重可以通过以下方式估计：
- en: $$ w_\phi(\mathbf{x}) = \frac{p(\mathbf{x})}{p_\theta(\mathbf{x})} = \frac{q(\mathbf{x}
    \vert y=1)}{q(\mathbf{x} \vert y=0)} = \frac{q(y=0)}{q(y=1)} \frac{q(y=1 \vert
    \mathbf{x})}{q(y=0 \vert \mathbf{x})} = \gamma \frac{c_\phi(\mathbf{x})}{1 - c_\phi(\mathbf{x})}
    $$
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: $$ w_\phi(\mathbf{x}) = \frac{p(\mathbf{x})}{p_\theta(\mathbf{x})} = \frac{q(\mathbf{x}
    \vert y=1)}{q(\mathbf{x} \vert y=0)} = \frac{q(y=0)}{q(y=1)} \frac{q(y=1 \vert
    \mathbf{x})}{q(y=0 \vert \mathbf{x})} = \gamma \frac{c_\phi(\mathbf{x})}{1 - c_\phi(\mathbf{x})}
    $$
- en: where $\gamma = \frac{q(y=0)}{q(y=1)} > 0$ is a fixed odd ratio.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\gamma = \frac{q(y=0)}{q(y=1)} > 0$是一个固定的奇数比率。
- en: 'Since we cannot learn a perfect optimal classifier, the importance weight would
    be an estimation $\hat{w}_\phi$. A couple of practical tricks can be applied to
    offset cases when the classifier exploits artifacts in the generated samples to
    make very confident predictions (i.e. very small importance weights):'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们无法学习到一个完美的最优分类器，重要性权重将是一个估计值$\hat{w}_\phi$。可以应用一些实用技巧来抵消分类器利用生成样本中的人为因素进行非常自信的预测（即非常小的重要性权重）：
- en: 'Self-normalization: normalize the weight by the sum $\hat{w}_\phi(\mathbf{x}_i)
    / \sum_{j=1}^N \hat{w}_\phi(\mathbf{x}_j)$.'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 自标准化：通过总和 $\hat{w}_\phi(\mathbf{x}_i) / \sum_{j=1}^N \hat{w}_\phi(\mathbf{x}_j)$
    对权重进行归一化。
- en: 'Flattening: add a power scaling parameter $\alpha > 0$, $\hat{w}_\phi(\mathbf{x}_i)^\alpha$.'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 平坦化：添加一个幂缩放参数 $\alpha > 0$，$\hat{w}_\phi(\mathbf{x}_i)^\alpha$。
- en: 'Clipping: specify a lower bound $\max(\hat{w}_\phi(\mathbf{x}_i), \beta)$.'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 剪裁：指定一个下界 $\max(\hat{w}_\phi(\mathbf{x}_i), \beta)$。
- en: To sample from an importance resampled generative model, $\mathbf{x}\sim p_{\theta,
    \phi}(\mathbf{x}) \propto p_\theta(\mathbf{x})\hat{w}_\phi(\mathbf{x})$, they
    adopt SIR (Sampling-Importance-Resampling),
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 要从重要性重采样的生成模型中进行采样，$\mathbf{x}\sim p_{\theta, \phi}(\mathbf{x}) \propto p_\theta(\mathbf{x})\hat{w}_\phi(\mathbf{x})$，他们采用了
    SIR（采样-重要性-重采样），
- en: '![](../Images/e18ad421d5bdee037b6d8b51b6352779.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e18ad421d5bdee037b6d8b51b6352779.png)'
- en: 'Fig. 4\. The algorithm for sampling from a generative model according to importance
    weights $\hat{w}(\mathbf{x}\_i)$ using SIR. (Image source: [Grover et al., 2019)](https://arxiv.org/abs/1906.09531))'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4\. 使用 SIR 从生成模型中根据重要性权重 $\hat{w}(\mathbf{x}_i)$ 进行采样的算法。（图片来源：[Grover et
    al., 2019)](https://arxiv.org/abs/1906.09531)）
- en: '[Deng et al., 2020](https://arxiv.org/abs/2004.11714) proposed to learn a EBM
    to steer a LM in the [residual space](https://arxiv.org/abs/1906.03351), $P_\theta(x)
    \propto P_\text{LM}(x)\exp(-E_\theta(x))$, where $P_\theta$ is the joint model;
    $E_\theta$ is the residual energy function to be learned. If we know the partition
    function $Z$, we can model the generative model for generative a sequence $x_{p+1},
    \dots, x_T$ as:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '[Deng et al., 2020](https://arxiv.org/abs/2004.11714) 提出了学习一个 EBM 来引导 [残差空间](https://arxiv.org/abs/1906.03351)
    中的 LM，$P_\theta(x) \propto P_\text{LM}(x)\exp(-E_\theta(x))$，其中 $P_\theta$ 是联合模型；$E_\theta$
    是要学习的残差能量函数。如果我们知道分区函数 $Z$，我们可以对生成模型进行建模以生成序列 $x_{p+1}, \dots, x_T$：'
- en: $$ P_\theta(x_{p+1:T}\vert x_{1:p}) = \frac{P_\text{LM}(x_{p+1:T}\vert x_{1:p})
    \exp(-E_\theta(x_{1:T}))}{Z_\theta(x_{1:p})} $$
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: $$ P_\theta(x_{p+1:T}\vert x_{1:p}) = \frac{P_\text{LM}(x_{p+1:T}\vert x_{1:p})
    \exp(-E_\theta(x_{1:T}))}{Z_\theta(x_{1:p})} $$
- en: 'The goal is to learn the parameters of the energy function $E_\theta$ such
    that the joint model $P_\theta$ gets closer to the desired data distribution.
    The residual energy function is trained by noise contrastive estimation ([NCE](https://www.kdnuggets.com/2019/07/introduction-noise-contrastive-estimation.html)),
    considering $P_\theta$ as the model distribution and $P_\text{LM}$ as the noise
    distribution:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是学习能量函数 $E_\theta$ 的参数，使得联合模型 $P_\theta$ 更接近期望的数据分布。残差能量函数通过噪声对比估计（[NCE](https://www.kdnuggets.com/2019/07/introduction-noise-contrastive-estimation.html)）进行训练，考虑
    $P_\theta$ 作为模型分布，$P_\text{LM}$ 作为噪声分布：
- en: $$ \theta = \arg\max_{\theta} \mathbb{E}_{x^+ \sim P_\text{data}} \log\frac{1}{1+\exp(E_\theta(x^+))}
    + \mathbb{E}_{x^- \sim P_\text{LM}} \log\frac{1}{1+\exp(-E_\theta(x^-))} $$
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \theta = \arg\max_{\theta} \mathbb{E}_{x^+ \sim P_\text{data}} \log\frac{1}{1+\exp(E_\theta(x^+))}
    + \mathbb{E}_{x^- \sim P_\text{LM}} \log\frac{1}{1+\exp(-E_\theta(x^-))} $$
- en: However, the partition function is intractable in practice. The paper proposed
    a simple way to first sample from the original LM and then to resample from them
    according to the energy function. This is unfortunately quite expensive.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在实践中，分区函数是难以计算的。该论文提出了一个简单的方法，首先从原始 LM 中采样，然后根据能量函数重新采样。这很昂贵。
- en: '![](../Images/03d9d972e07e65226feeea2afa29da5d.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/03d9d972e07e65226feeea2afa29da5d.png)'
- en: 'Fig. 5\. Top k samples from the base LM are resampled according to the residual
    energy function. (Image source: [Deng et al., 2020](https://arxiv.org/abs/2004.11714))'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5\. 从基础 LM 中重新采样的前 k 个样本，根据残差能量函数。（图片来源：[Deng et al., 2020](https://arxiv.org/abs/2004.11714))
- en: Smart Prompt Design
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 智能提示设计
- en: Large language models have been shown to be very powerful on many NLP tasks,
    even with only *prompting* and no task-specific fine-tuning ([GPT2](https://lilianweng.github.io/posts/2019-01-31-lm/#gpt-2),
    [GPT3](https://lilianweng.github.io/posts/2019-01-31-lm/#gpt-3). The prompt design
    has a big impact on the performance on downstream tasks and often requires time-consuming
    manual crafting. For example, factual questions can gain a big boost with smart
    prompt design in “closed-book exam” ([Shin et al., 2020](https://arxiv.org/abs/2010.15980),
    [Jiang et al., 2020)](https://arxiv.org/abs/1911.12543)). I’m expecting to see
    an increasing amount of literature on automatic smart prompt design.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型已经在许多自然语言处理任务上表现出非常强大的能力，即使只有*提示*而没有特定任务的微调（[GPT2](https://lilianweng.github.io/posts/2019-01-31-lm/#gpt-2)，[GPT3](https://lilianweng.github.io/posts/2019-01-31-lm/#gpt-3)）。提示设计对下游任务的性能有很大影响，通常需要耗费大量时间进行手工制作。例如，事实性问题在“闭卷考试”中通过智能提示设计可以获得很大提升（[Shin等，2020](https://arxiv.org/abs/2010.15980)，[Jiang等，2020)](https://arxiv.org/abs/1911.12543)）。我期待看到越来越多关于自动智能提示设计的文献。
- en: Gradient-based Search
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于梯度的搜索
- en: '**AutoPrompt** ([Shin et al., 2020](https://arxiv.org/abs/2010.15980); [code](http://ucinlp.github.io/autoprompt))
    is a method to automatically create prompts for various tasks via gradient-based
    search. AutoPrompt constructs a prompt by combining the original task inputs $x$
    with a collection of trigger tokens $x_\text{trig}$ according to a template $\lambda$.
    The trigger tokens are shared across all inputs and thus *universally* effective.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '**AutoPrompt**（[Shin等，2020](https://arxiv.org/abs/2010.15980)；[code](http://ucinlp.github.io/autoprompt)）是一种通过基于梯度的搜索自动创建各种任务提示的方法。AutoPrompt通过将原始任务输入$x$与一组触发令牌$x_\text{trig}$根据模板$\lambda$组合来构建提示。这些触发令牌在所有输入中共享，因此*通用*有效。'
- en: '![](../Images/b120e880e2d9dece1307cf3dd7e5bc06.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b120e880e2d9dece1307cf3dd7e5bc06.png)'
- en: 'Fig. 6\. The overview of AutoPrompt. The trigger tokens are retrieved to optimize
    for the target outputs across all inputs. (Image source: [Shin et al., 2020](https://arxiv.org/abs/2010.15980))'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图6。AutoPrompt的概述。触发令牌被检索以优化所有输入的目标输出。（图片来源：[Shin等，2020](https://arxiv.org/abs/2010.15980)）
- en: 'The universal trigger tokens are identified using a gradient-guided search
    strategy same as in [Wallace et al., 2019](https://arxiv.org/abs/1908.07125).
    The *universal* setting means that the trigger tokens $x_\text{trig}$ can optimize
    for the target output $\tilde{y}$ for all inputs from a dataset:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 通用触发令牌是使用与[Wallace等，2019](https://arxiv.org/abs/1908.07125)中相同的梯度引导搜索策略来识别的。*通用*设置意味着触发令牌$x_\text{trig}$可以为数据集中所有输入的目标输出$\tilde{y}$进行优化：
- en: $$ x_\text{trig} = \arg\min_{x’_\text{trig}} \mathbb{E}_{x\sim\mathcal{X}} [\mathcal{L}(\tilde{y},
    f(x’_\text{trig}; x))] $$
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: $$ x_\text{trig} = \arg\min_{x’_\text{trig}} \mathbb{E}_{x\sim\mathcal{X}} [\mathcal{L}(\tilde{y},
    f(x’_\text{trig}; x))] $$
- en: 'The search operates in the embedding space. The embedding of every trigger
    token $e_{\text{trig}_i}$ is first initialized to some default value and then
    gets updated to minimize the first-order Taylor expansion of the task-specific
    loss around the current token embedding:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 搜索在嵌入空间中进行。每个触发令牌$e_{\text{trig}_i}$的嵌入首先初始化为某个默认值，然后更新以最小化围绕当前令牌嵌入的任务特定损失的一阶泰勒展开：
- en: $$ e^{(t+1)}_\text{trig} = \arg\min_{e\in\mathcal{V}} [e - e^{(t)}_{\text{trig}_i}]^\top
    \nabla_{e^{(t)}_{\text{trig}_i}} \mathcal{L} $$
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: $$ e^{(t+1)}_\text{trig} = \arg\min_{e\in\mathcal{V}} [e - e^{(t)}_{\text{trig}_i}]^\top
    \nabla_{e^{(t)}_{\text{trig}_i}} \mathcal{L} $$
- en: where $\mathcal{V}$ refers to the embedding matrix of all the tokens. $\nabla_{e^{(t)}_{\text{trig}_i}}
    \mathcal{L}$ is the average gradient of the task loss over a batch at iteration
    $t$. We can brute-force the optimal $e$ by a $\vert \mathcal{V} \vert d$-dimensional
    dot product, which is cheap and can be computed in parallel.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\mathcal{V}$是所有令牌的嵌入矩阵。$\nabla_{e^{(t)}_{\text{trig}_i}} \mathcal{L}$是在迭代$t$的批次上任务损失的平均梯度。我们可以通过一个$\vert
    \mathcal{V} \vert d$维点积来蛮力求解最优$e$，这是廉价的并且可以并行计算。
- en: '![](../Images/e5f1917177d4e0eccc2b1a2f912eeabd.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e5f1917177d4e0eccc2b1a2f912eeabd.png)'
- en: 'Fig. 7\. We search for trigger tokens by updating their embeddings with the
    gradient of the task loss per batch. (Image source: [Wallace et al., 2019](https://arxiv.org/abs/1908.07125))'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图7。我们通过更新每批次的任务损失梯度来搜索触发令牌。（图片来源：[Wallace等，2019](https://arxiv.org/abs/1908.07125)）
- en: The above token replacement method can be augmented with beam search. When looking
    for the optimal token embedding $e$, we can pick top-$k$ candidates instead of
    a single one, searching from left to right and score each beam by $\mathcal{L}$
    on the current data batch.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 上述的令牌替换方法可以通过束搜索进行增强。在寻找最佳的令牌嵌入$e$时，我们可以选择前$k$个候选项而不是单个，从左到右搜索，并在当前数据批次上通过$\mathcal{L}$对每个束进行评分。
- en: '![](../Images/9d70319fe02a6cf0a9b1621dce801006.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9d70319fe02a6cf0a9b1621dce801006.png)'
- en: 'Fig. 8\. Example prompts discovered by AutoPrompt for different tasks. (Image
    source: [Shin et al., 2020](https://arxiv.org/abs/2010.15980))'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图8. AutoPrompt 发现的不同任务的示例提示。（图片来源：[Shin et al., 2020](https://arxiv.org/abs/2010.15980)）
- en: 'Smart prompt design essentially produces efficient context that can lead to
    desired completion. Motivated by this observation, [Li & Liang (2021)](https://arxiv.org/abs/2101.00190)
    proposed **Prefix-Tuning** which assigns a small number of trainable parameters
    at the beginning of an input sequence (named “prefix”) to steer a LM, $[\text{PREFIX};
    x; y]$. Let $\mathcal{P}_\text{idx}$ be a set of prefix indices and $\text{dim}(h_i)$
    be the embedding size. The prefix parameters $P_\theta$ has the dimension $\vert\mathcal{P}_\text{idx}\vert
    \times \text{dim}(h_i) $ and the hidden state takes the form:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 聪明的提示设计本质上产生了可以导致期望完成的有效上下文。受到这一观察的启发，[Li & Liang (2021)](https://arxiv.org/abs/2101.00190)
    提出了**前缀调整**，它在输入序列的开头分配了少量可训练参数（称为“前缀”）来引导一个语言模型，$[\text{PREFIX}; x; y]$。设$\mathcal{P}_\text{idx}$为前缀索引集合，$\text{dim}(h_i)$为嵌入大小。前缀参数$P_\theta$的维度为$\vert\mathcal{P}_\text{idx}\vert
    \times \text{dim}(h_i)$，隐藏状态的形式为：
- en: $$ h_i = \begin{cases} P_\theta[i,:], & \text{if }i \in \mathcal{P}_\text{idx}\\
    \text{LM}_\phi(z_i, h_{
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: $$ h_i = \begin{cases} P_\theta[i,:], & \text{if }i \in \mathcal{P}_\text{idx}\\
    \text{LM}_\phi(z_i, h_{
- en: Note that only $P_\theta$ is trainable and the LM parameters $\phi$ is frozen
    during training.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，只有$P_\theta$是可训练的，语言模型参数$\phi$在训练过程中被冻结。
- en: '![](../Images/141b29c49aa74d4c669f4efde52c9b9e.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/141b29c49aa74d4c669f4efde52c9b9e.png)'
- en: 'Fig. 9\. Illustrations of fine-tuning versus prefix-tuning. (Image source:
    [Li & Liang 2021](https://arxiv.org/abs/2101.00190))'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图9. 微调与前缀调整的示意图。（图片来源：[Li & Liang 2021](https://arxiv.org/abs/2101.00190)）
- en: The prefix parameters do not tie to any embeddings associated with the real
    words and thus they are more *expressive* for steering the context. Direct optimizing
    $P_\theta$ unfortunately results in poor performance. To reduce the difficulty
    associated with high dimensionality training, the matrix $P_\theta$ is reparameterized
    by a smaller matrix $P’_\theta \in \mathbb{R}^{\vert\mathcal{P}_\text{idx}\vert
    \times c}$ and a large feed forward network $\text{MLP}_\theta \in \mathbb{R}^{c\times
    \text{dim}(h_i)}$.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 前缀参数与任何与真实单词相关联的嵌入不相关，因此它们对于引导上下文更具*表现力*。直接优化$P_\theta$不幸地导致性能不佳。为了减少与高维训练相关的困难，矩阵$P_\theta$通过一个较小的矩阵$P’_\theta
    \in \mathbb{R}^{\vert\mathcal{P}_\text{idx}\vert \times c}$和一个大型前馈网络$\text{MLP}_\theta
    \in \mathbb{R}^{c\times \text{dim}(h_i)}$重新参数化。
- en: The performance increases with the prefix length $\vert\mathcal{P}_\text{idx}\vert$
    up to some value. And this value varies with tasks.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 随着前缀长度$\vert\mathcal{P}_\text{idx}\vert$的增加，性能会提高，直到某个值。而这个值会随着任务的不同而变化。
- en: '![](../Images/edaa4395e484103740b6f28b7448560d.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/edaa4395e484103740b6f28b7448560d.png)'
- en: 'Fig. 10\. Task performance, summarization (left) and table-to-text (right),
    as a function of prefix length. (Image source: [Li & Liang 2021](https://arxiv.org/abs/2101.00190))'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图10. 任务性能，摘要（左）和表格到文本（右），作为前缀长度的函数。（图片来源：[Li & Liang 2021](https://arxiv.org/abs/2101.00190)）
- en: 'A few other interesting learnings from their ablation studies include:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 他们的消融研究中还有一些其他有趣的发现：
- en: Tuning only the embedding layer (without prefix) is not sufficiently expressive.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仅调整嵌入层（没有前缀）的表现不够具有表现力。
- en: Placing the trainable parameter between $x$ and $y$, $[x; \text{INFIX}; y]$,
    slightly underperforms prefix-tuning, likely because it only affects the context
    for $y$ while prefix affects both.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将可训练参数放置在$x$和$y$之间，$[x; \text{INFIX}; y]$，略微表现不佳于前缀调整，可能是因为它只影响$y$的上下文，而前缀影响两者。
- en: Random initialization of $P_\theta$ leads to low performance with high variance.
    In contrast, initializing $P_\theta$ with activations of real words improves generation,
    even the words are irrelevant to the task.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $P_\theta$的随机初始化会导致性能低且方差大。相比之下，使用真实单词的激活来初始化$P_\theta$会提高生成效果，即使这些单词与任务无关。
- en: Fine-tuned models achieve better task performance but they can fail in the low
    data regime. Both AutoPrompt and Prefix-Tuning were found to outperform fine-tuning
    in the regime where the training dataset is small (i.e. $10^2-10^3$ samples).
    As an alternative to fine-tuning, prompt design or learning the context embedding
    is much cheaper. AutoPrompt improves the accuracy for sentiment classification
    a lot more than manual prompts and achieves similar performance as linear probing.
    For the NLI task, AutoPrompt obtains higher accuracy than linear probing. It is
    able to retrieve facts more accurately than manual prompts too. In low data regime,
    Prefix-Tuning achieves performance comparable with fine-tuning on table-to-text
    generation and summarization.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 微调模型在任务性能上表现更好，但在数据稀缺的情况下可能会失败。发现AutoPrompt和Prefix-Tuning在训练数据集较小（即$10^2-10^3$个样本）的情况下优于微调。作为微调的替代方案，提示设计或学习上下文嵌入成本更低。AutoPrompt比手动提示大大提高了情感分类的准确性，并实现了与线性探测类似的性能。对于NLI任务，AutoPrompt比线性探测获得更高的准确性。它能够比手动提示更准确地检索事实。在数据稀缺的情况下，Prefix-Tuning在表格到文本生成和摘要上实现了与微调相媲美的性能。
- en: Two successive works, **P-tuning** ([Liu et al. 2021](https://arxiv.org/abs/2103.10385);
    [code](https://github.com/THUDM/P-tuning)) and **Prompt Tuning** ([Lester et al.
    2021](https://arxiv.org/abs/2104.08691)), follow the similar idea of explicit
    training continuous prompt embeddings but with a few different choices over the
    trainable parameters and architecture. Different from Prefix-Tuning which concatenates
    continuous prompt tokens in every hidden state layer of the transformer, both
    P-tuning and Prompt Tuning non-invasively add continuous prompts *only in the
    input* to work well.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 两个连续的工作，**P-tuning**（[Liu et al. 2021](https://arxiv.org/abs/2103.10385); [code](https://github.com/THUDM/P-tuning)）和**Prompt
    Tuning**（[Lester et al. 2021](https://arxiv.org/abs/2104.08691)），遵循了显式训练连续提示嵌入的类似思想，但在可训练参数和架构上有一些不同选择。与将连续提示令牌连接在变压器的每个隐藏状态层中的Prefix-Tuning不同，P-tuning和Prompt
    Tuning都是在*输入中*非侵入性地添加连续提示以达到良好效果。
- en: Let $[P_i]$ be the $i$-th token in the prompt template of **P-tuning** ([Liu
    et al. 2021](https://arxiv.org/abs/2103.10385)), we can denote a prompt as a sequence
    $T=\{[P_{0:i}], \mathbf{x}, [P_{i+1:m}], \mathbf{y}\}$. Each token $[P_i]$ does
    not have to be a real token in the model vocabulary (“pseudo-token”), and thus
    the encoded template $T^e$ looks like the following and the pseudo-token hidden
    state can be optimized with gradient descent.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 让$[P_i]$表示**P-tuning**（[Liu et al. 2021](https://arxiv.org/abs/2103.10385)）的提示模板中的第$i$个令牌，我们可以将提示表示为序列$T=\{[P_{0:i}],
    \mathbf{x}, [P_{i+1:m}], \mathbf{y}\}$。每个令牌$[P_i]$不必是模型词汇表中的真实令牌（“伪令牌”），因此编码的模板$T^e$如下所示，伪令牌的隐藏状态可以通过梯度下降进行优化。
- en: $$ T^e = \{ h_0, \dots, h_i, \text{embed}(\mathbf{x}), h_{i+1}, \dots, h_m,
    \text{embed}(\mathbf{y})\} $$![](../Images/20f964916c6caba90c270973fac17c7b.png)
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: $$ T^e = \{ h_0, \dots, h_i, \text{embed}(\mathbf{x}), h_{i+1}, \dots, h_m,
    \text{embed}(\mathbf{y})\} $$![](../Images/20f964916c6caba90c270973fac17c7b.png)
- en: 'Fig. 11\. The illustration of P-tuning. Sometimes, adding a few task-related
    anchor tokens, such as “capital” in the figure, can bring further improvement.
    (Image source: [Liu et al. 2021](https://arxiv.org/abs/2103.10385))'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图11\. P-tuning的示意图。有时，添加一些与任务相关的锚定令牌，如图中的“capital”，可以带来进一步的改进。（图片来源：[Liu et
    al. 2021](https://arxiv.org/abs/2103.10385)）
- en: 'There are two major optimization challenges in P-tuning:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: P-tuning中存在两个主要的优化挑战：
- en: 'Discreteness: The word embedding of a pretrained language model are highly
    discrete. It is hard to optimize $h_i$ if they are intialized at random.'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 离散性：预训练语言模型的词嵌入是高度离散的。如果它们是随机初始化的，很难优化$h_i$。
- en: 'Association: $h_i$ should be dependent on each other. Thus they develop a mechanism
    to model this dependency by training a light-weighted LSTM-based prompt encoder:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 关联性：$h_i$应该相互依赖。因此，他们开发了一种机制来通过训练轻量级基于LSTM的提示编码器来建模这种依赖关系：
- en: '$$ h_i = \text{MLP}([\text{LSTM}(h_{0:i}): \text{LSTM}(h_{i:m})]) $$'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '$$ h_i = \text{MLP}([\text{LSTM}(h_{0:i}): \text{LSTM}(h_{i:m})]) $$'
- en: P-tuning is more flexible than prefix-tuning, as it inserts trainable tokens
    in the middle of a prompt not just at the beginning. The usage of task-specific
    anchor tokens is like combining manual prompt engineering with trainable prompts.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: P-tuning比prefix-tuning更灵活，因为它不仅在提示开头插入可训练令牌，而且在提示中间插入。使用任务特定的锚定令牌就像将手动提示工程与可训练提示结合在一起。
- en: '**Prompt Tuning** ([Lester et al. 2021](https://arxiv.org/abs/2104.08691))
    largely simplifies the idea of prefix tuning by only allowing an additional $k$
    tunable tokens per downstream task to be prepended to the input text. The conditional
    generation is $p_{\theta, \theta_P}(Y \vert [P; X])$, where $P$ is the “pseudo
    prompt” with parameters $\theta_P$ trainable via back-propagation. Both $X$ and
    $P$ are embedding vectors and we have $X \in \mathbb{R}^{n \times d^e}, P \in
    \mathbb{R}^{k \times d^e}$ and $[P;X] \in \mathbb{R}^{(n+k) \times d^e}$, where
    $d^e$ is the embedding space dimensionality.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '**提示调整**（[Lester et al. 2021](https://arxiv.org/abs/2104.08691)）大大简化了前缀调整的概念，只允许每个下游任务在输入文本前添加额外的$k$个可调节标记。条件生成是$p_{\theta,
    \theta_P}(Y \vert [P; X])$，其中$P$是具有可通过反向传播训练的参数$\theta_P$的“伪提示”。$X$和$P$都是嵌入向量，我们有$X
    \in \mathbb{R}^{n \times d^e}, P \in \mathbb{R}^{k \times d^e}$和$[P;X] \in \mathbb{R}^{(n+k)
    \times d^e}$，其中$d^e$是嵌入空间的维度。'
- en: Prompt tuning produces competitive results as model fine-tuning when the model
    gets *large* (billions of parameters and up). This result is especially interesting
    given that large models are expensive to fine-tune and execute at inference time.
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当模型变得*庞大*（数十亿个参数及以上）时，提示调整产生了与模型微调竞争力相当的结果。这一结果尤其有趣，因为大型模型在微调和推理时执行都很昂贵。
- en: With learned task-specific parameters, prompt tuning achieves better transfer
    learning when adapting to new domains. It outperforms fine-tuning on domain shift
    problems.
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过学习的任务特定参数，提示调整在适应新领域时实现更好的迁移学习。在领域转移问题上，它优于微调。
- en: They also showed that prompt ensembling of multiple prompts for the same task
    introduces further improvement.
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 他们还表明，对于同一任务的多个提示的提示集成引入了进一步的改进。
- en: '![](../Images/be7e2c0f2ad6cf1c2fa704d9b534b86b.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/be7e2c0f2ad6cf1c2fa704d9b534b86b.png)'
- en: 'Fig. 12\. The illustration of how Prompt Tuning works. (Image source: [Lester
    et al. 2021](https://arxiv.org/abs/2104.08691))'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图12\. 提示调整的工作原理示意图。（图片来源：[Lester et al. 2021](https://arxiv.org/abs/2104.08691)）
- en: 'The experiments investigated several prompt initialization schemes:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 实验调查了几种提示初始化方案：
- en: Random initialization by uniformly sampling from [-0.5, 0.5];
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机初始化，从[-0.5, 0.5]均匀采样；
- en: Sample embeddings of top 5000 common tokens;
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 采样前5000个常见标记的嵌入；
- en: Use the embedding values of the class label strings. If we don’t have enough
    class labels to initialize the soft-prompt, we fall back to scheme 2. Random initialization
    performs noticeably worse than the other two options.
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用类标签字符串的嵌入值。如果我们没有足够的类标签来初始化软提示，我们将退回到方案2。随机初始化的表现明显不如其他两个选项。
- en: '![](../Images/58cd0e635c5ff12d91fef5dad060f672.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/58cd0e635c5ff12d91fef5dad060f672.png)'
- en: 'Fig. 13\. The effect of (a) different prompt initialization schemes and (b)
    different prompt lengths. (Image source: [Lester et al. 2021](https://arxiv.org/abs/2104.08691))'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图13\. (a)不同提示初始化方案和(b)不同提示长度的影响。（图片来源：[Lester et al. 2021](https://arxiv.org/abs/2104.08691)）
- en: The pre-training objectives also have a big impact on the quality of prompt
    tuning. T5’s “span corruption” is not a good option here.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练目标也对提示调整的质量产生重大影响。T5的“跨度破坏”在这里不是一个好选择。
- en: Prompt tuning is found to be less likely to overfit to a specific dataset. To
    evaluate the robustness to data shifting problem, they trained the model on one
    dataset of one task and evaluated it on the test dataset but in a *different domain*.
    Prompt tuning is more resilient and can generalize to different domains better.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 发现提示调整不太可能过度拟合特定数据集。为了评估对数据转移问题的鲁棒性，他们在一个任务的一个数据集上训练模型，并在*不同领域*的测试数据集上评估。提示调整更具弹性，能更好地泛化到不同领域。
- en: '![](../Images/59aca00445d1a5b2963ae9177c0e2a20.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/59aca00445d1a5b2963ae9177c0e2a20.png)'
- en: 'Fig. 14\. Prompt tuning is more resilient to domain shift between train and
    test sets. (Image source: [Lester et al. 2021](https://arxiv.org/abs/2104.08691))'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图14\. 提示调整在训练集和测试集之间的领域转移上更具弹性。（图片来源：[Lester et al. 2021](https://arxiv.org/abs/2104.08691)）
- en: Heuristic-based Search
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于启发式的搜索
- en: Paraphrasing is a quick way to explore more prompts similar to the known version,
    which can be done via *back-translation*. Using back-translation, the initial
    prompt is translated into $B$ candidates in another language and then each is
    translated back into $B$ candidates in the original language. The resulting total
    $B^2$ candidates are scored and ranked by their round-trip probabilities.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 释义是探索更多类似已知版本的提示的快速方法，可以通过*回译*来实现。使用回译，初始提示被翻译成另一种语言中的$B$个候选项，然后每个候选项再被翻译回原始语言中的$B$个候选项。得到的总共$B^2$个候选项通过其往返概率进行评分和排名。
- en: '[Ribeiro et al (2018)](https://www.aclweb.org/anthology/P18-1079/) identified
    *semantically equivalent adversaries (SEA)* by generating a variety of paraphrases
    $\{x’\}$ of input $x$ until it triggers a different prediction of target function
    $f$:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '[里贝罗等人（2018）](https://www.aclweb.org/anthology/P18-1079/)通过生成输入$x$的各种释义$\{x’\}$来识别*语义等效对手（SEA）*，直到触发目标函数$f$的不同预测：'
- en: $$ \begin{aligned} SEA(x, x') &= \mathbb{1}[\text{SemEq}(x, x') \land f(x) \neq
    f(x')] \\ \text{where SemEq}(x, x') &= \mathbb{1}[\min\Big(1, \frac{p(x'\vert
    x)}{p(x\vert x)} \Big) \geq \tau] \end{aligned} $$
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \begin{aligned} SEA(x, x') &= \mathbb{1}[\text{SemEq}(x, x') \land f(x) \neq
    f(x')] \\ \text{where SemEq}(x, x') &= \mathbb{1}[\min\Big(1, \frac{p(x'\vert
    x)}{p(x\vert x)} \Big) \geq \tau] \end{aligned} $$
- en: where the score $p(x’\vert x)$ is proportional to translating $x$ into multiple
    languages and then translating it back to the original language.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 其中得分$p(x’\vert x)$与将$x$翻译成多种语言，然后再翻译回原始语言成正比。
- en: Examples of SEA rules include (*What `NOUN`→Which `NOUN`*), (*`WP` is → `WP`’s’*),
    (*was→is*), etc. They are considered as “bugs” in the model. Applying those rules
    as data augmentation in model training helps robustify the model and fix bugs.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: SEA规则的示例包括（*What `NOUN`→Which `NOUN`*）、（*`WP` is → `WP`’s’*）、（*was→is*）等。它们被视为模型中的“错误”。在模型训练中将这些规则应用为数据增强有助于使模型更加健壮并修复错误。
- en: '[Jiang et al (2020)](https://arxiv.org/abs/1911.12543) attempts to validate
    whether a trained language model knows certain knowledge by automatically discovering
    better prompts to query. Within the scope of knowledge retrieval where factual
    knowledge is represented in the form of a triple $\langle x, r, y \rangle$ (subject,
    relation, object). The prompts can be mined from training sentences (e.g. Wikipedia
    description) or expanded by paraphrase.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '[蒋等人（2020）](https://arxiv.org/abs/1911.12543)试图验证训练过的语言模型是否了解某些知识，通过自动发现更好的提示来查询。在知识检索范围内，事实知识以三元组$\langle
    x, r, y \rangle$（主语，关系，客体）的形式表示。提示可以从训练句子（例如维基百科描述）中挖掘，也可以通过释义进行扩展。'
- en: Interestingly some small modifications in the prompts may lead to big gain,
    as shown in Fig. X.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，如图X所示，提示中的一些小修改可能会带来巨大的收益。
- en: '![](../Images/47abafea674e539ae93a04bcb2209957.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/47abafea674e539ae93a04bcb2209957.png)'
- en: 'Fig. 15\. Small modifications in prompt templates can lead to big performance
    gains: replacement in blue, insertion in green, deletion in red. (Image source:
    [Jiang et al., 2020](https://arxiv.org/abs/1911.12543))'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 图15. 在提示模板中进行小的修改可能会导致性能大幅提升：蓝色替换，绿色插入，红色删除。（图片来源：[蒋等人，2020](https://arxiv.org/abs/1911.12543)）
- en: Fine-tuning
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 微调
- en: Fine-tuning is an intuitive way to guide a LM to output desired content, commonly
    by training on supervised datasets or by RL. We can fine-tune all the weights
    in the model or restrict the fine-tuning to only top or additional layers.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 微调是引导语言模型输出所需内容的直观方法，通常通过在监督数据集上训练或通过RL进行。我们可以微调模型中的所有权重，也可以将微调限制在顶部或额外层。
- en: Conditional Training
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 有条件的训练
- en: Conditional training aims to learn a generative model conditioned on a control
    variable $z$, $p(y \vert x, z)$.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 有条件的训练旨在学习一个以控制变量$z$为条件的生成模型，$p(y \vert x, z)$。
- en: '[Fan et al (2018)](https://arxiv.org/abs/1805.04833) trained a conditional
    language model for 2-step story generation. First, a model outputs the story sketch
    and then a story writing model creates a story following that sketch. The mechanism
    of conditioning on the sketch is implemented by a *fusion* model architecture.
    The fusion model enforces a form of *residual learning* that allows the story
    writing model to focus on learning what the first sketch generation model is missing.
    Also for story generation, [Peng et al (2018)](https://www.aclweb.org/anthology/W18-1505/)
    experimented with an ending valence-conditioned story generator LM, $p(x_t \vert
    x_{<t}, z)$ where $z$ is the label of the story ending (sad, happy or neutral).
    Their language model is a bidirectional LSTM and the label is mapped into a learned
    embedding which then blends into the LSTM cell.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '[Fan et al (2018)](https://arxiv.org/abs/1805.04833) 训练了一个用于两步故事生成的条件语言模型。首先，模型输出故事草图，然后一个故事写作模型根据该草图创建故事。在草图上的条件机制是通过*融合*模型架构实现的。融合模型实施了一种*残差学习*形式，允许故事写作模型专注于学习第一个草图生成模型所遗漏的内容。此外，对于故事生成，[Peng
    et al (2018)](https://www.aclweb.org/anthology/W18-1505/) 尝试了一个以结局情感为条件的故事生成 LM，$p(x_t
    \vert x_{<t}, z)$，其中 $z$ 是故事结局的标签（悲伤、快乐或中性）。他们的语言模型是一个双向 LSTM，标签被映射到一个学习的嵌入中，然后融入
    LSTM 单元中。'
- en: '**CTRL** ([Keskar et al., 2019](https://arxiv.org/abs/1909.05858); [code](https://github.com/salesforce/ctrl))
    aims to train a language model conditioned control code $z$ using controllable
    datasets. CTRL learns the conditioned distribution $p(x \vert z)$ by training
    on raw text sequences with *control code prefixes*, such as `[horror]`, `[legal]`,
    etc. Then the learned model is able to generate text with respect to the prompt
    prefix. The training data contains Wikipedia, OpenWebText, books, Amazon reviews,
    reddit corpus and many more, where each dataset is assigned with a control code
    and subreddit in the reddit corpus has its own topic as control code.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '**CTRL**（[Keskar et al., 2019](https://arxiv.org/abs/1909.05858); [code](https://github.com/salesforce/ctrl)）旨在训练一个以控制代码
    $z$ 为条件的语言模型，使用可控数据集。 CTRL 通过在原始文本序列上训练带有*控制代码前缀*的模型，如 `[horror]`，`[legal]` 等，来学习条件分布
    $p(x \vert z)$。然后，学习的模型能够根据提示前缀生成文本。训练数据包括维基百科、OpenWebText、书籍、亚马逊评论、reddit 语料库等，其中每个数据集都分配有一个控制代码，reddit
    语料库中的 subreddit 有其自己的主题作为控制代码。'
- en: '![](../Images/a62ebf37741828c36f465bd79e7a4cd6.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a62ebf37741828c36f465bd79e7a4cd6.png)'
- en: 'Fig. 16\. Datasets used for training CTRL and associated control codes. (Image
    source: Edited from Table 7 in [Keskar et al., 2019](https://arxiv.org/abs/1909.05858))'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16\. 用于训练 CTRL 和相关控制代码的数据集。 (图片来源：[Keskar et al., 2019](https://arxiv.org/abs/1909.05858)
    中表 7 的编辑)
- en: The control code also can be used for *domain annotation* given tokens, because
    $p(z \vert x) \propto p(x \vert z) p(z)$, assuming the prior over domains is uniform.
    One limitation of CTRL is the lack of control for *what not to generate* (e.g.
    avoid toxicity).
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 控制代码也可以用于给定标记的*领域注释*，因为 $p(z \vert x) \propto p(x \vert z) p(z)$，假设领域的先验分布是均匀的。
    CTRL 的一个限制是缺乏对*不生成什么*的控制（例如，避免毒性）。
- en: '![](../Images/5fff8250b8b2362417721bae307d7031.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5fff8250b8b2362417721bae307d7031.png)'
- en: 'Fig. 17\. The examples of conditioned sample generation by CTRL. (Image source:
    [Keskar et al., 2019](https://arxiv.org/abs/1909.05858))'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17\. CTRL 生成的条件样本示例。 (图片来源：[Keskar et al., 2019](https://arxiv.org/abs/1909.05858))
- en: Note that CTRL trains a transformer model from scratch. However, labelling all
    the text within the same dataset with the same control code (e.g. All the wikipedia
    articles have “wikipedia” as control code) feels quite constrained. Considering
    that often we need highly customized control codes but only have a limited amount
    of labelled data, I would expect fine-tuning an unconditional LM with a small
    labelled dataset in the same way as CTRL to work out well too. Although how much
    data is needed and how good the sample quality might be are subject to experimentation.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，CTRL 从头开始训练一个 transformer 模型。然而，将同一数据集中的所有文本标记为相同的控制代码（例如，所有维基百科文章都有“维基百科”作为控制代码）感觉相当受限制。考虑到通常我们需要高度定制的控制代码，但只有有限数量的标记数据，我期望像
    CTRL 一样使用小型标记数据集对无条件 LM 进行微调也能取得良好的效果。尽管需要多少数据以及样本质量如何可能需要进行实验。
- en: RL Fine-tuning
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 强化学习微调
- en: Fine-tuning a sequential model with RL regarding any arbitrary and possibly
    non-differentiable reward function has been proved to work well years ago ([Ranzato
    et al., 2015](https://arxiv.org/abs/1511.06732)). RL fine-tuning can resolve several
    problems with *teacher forcing* method. With teacher forcing, the model only minimizes
    a maximum-likelihood loss at each individual decoding step during training but
    it is asked to predict the entire sequence from scratch at test time. Such a discrepancy
    between train and test could lead to exposure bias and accumulated error. In contrast,
    RL fine-tuning is able to directly optimize task-specific metrics on the sequence
    level, such as BLEU for translation ([Ranzato et al., 2015](https://arxiv.org/abs/1511.06732),
    [Wu et al., 2016](https://arxiv.org/abs/1609.08144), [Nguyen et al., 2017](https://arxiv.org/abs/1707.07402)),
    ROUGE for summarization ([Ranzato et al., 2015](https://arxiv.org/abs/1511.06732),
    [Paulus et al., 2017](https://arxiv.org/abs/1705.04304), [Wu and Hu, 2018](https://arxiv.org/abs/1804.07036))
    and customized metric for story generation ([Tambwekar et al., 2018](https://arxiv.org/abs/1809.10736)).
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 多年来，使用RL针对任意任意可能非可微分奖励函数微调顺序模型已被证明效果良好（[Ranzato et al., 2015](https://arxiv.org/abs/1511.06732)）。RL微调可以解决*教师强迫*方法的几个问题。使用教师强迫，模型在训练期间仅在每个解码步骤最小化最大似然损失，但在测试时要求从头开始预测整个序列。这种训练和测试之间的差异可能导致曝光偏差和累积误差。相比之下，RL微调能够直接在序列级别上优化任务特定的度量标准，如翻译中的BLEU（[Ranzato
    et al., 2015](https://arxiv.org/abs/1511.06732)、[Wu et al., 2016](https://arxiv.org/abs/1609.08144)、[Nguyen
    et al., 2017](https://arxiv.org/abs/1707.07402)）、摘要中的ROUGE（[Ranzato et al., 2015](https://arxiv.org/abs/1511.06732)、[Paulus
    et al., 2017](https://arxiv.org/abs/1705.04304)、[Wu and Hu, 2018](https://arxiv.org/abs/1804.07036)）以及故事生成中的自定义度量标准（[Tambwekar
    et al., 2018](https://arxiv.org/abs/1809.10736)）。
- en: '[Ranzato et al (2015)](https://arxiv.org/abs/1511.06732) applied REINFORCE
    to train RNN models for sequence generation tasks. The model is first trained
    to predict the next token using cross-entropy loss (ML loss) and then fine-tuned
    alternatively by both ML loss and REINFORCE (RL loss). At the second fine-tuning
    stage, the number of training steps for next-token prediction is gradually decreasing
    until none and eventually only RL loss is used. This sequence-level RL fine-tuning
    was shown by experiments to lead to great improvements over several supervised
    learning baselines back then.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '[Ranzato et al (2015)](https://arxiv.org/abs/1511.06732)应用REINFORCE来训练RNN模型进行序列生成任务。该模型首先通过交叉熵损失（ML损失）训练以预测下一个标记，然后通过ML损失和REINFORCE（RL损失）交替进行微调。在第二次微调阶段，用于下一个标记预测的训练步骤数量逐渐减少直至为零，最终仅使用RL损失。实验证明，这种序列级RL微调相对于当时的几个监督学习基线有了很大的改进。'
- en: Google implemented the similar approach in their neural machine translation
    system ([Wu et al., 2016](https://arxiv.org/abs/1609.08144)) and [Paulus et al
    (2017)](https://arxiv.org/abs/1705.04304) adopted such approach for summarization
    task. The training objective contains two parts, ML loss for next token prediction,
    $\mathcal{L}_\text{ML} = \sum_{(x, y^*)\sim\mathcal{D}} \log p_\theta(y^* \vert
    x)$, and RL loss $\mathcal{L}_\text{RL}$ for maximizing the expected reward where
    the reward per sequence is measured by BLEU or ROUGE. The model is first trained
    with $\mathcal{L}_\text{ML}$ until convergence and then fine-tuned with a linear
    combination of two losses, $\mathcal{L}_\text{mix} = \alpha \mathcal{L}_\text{ML}
    + (1 - \alpha)\mathcal{L}_\text{RL}$.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 谷歌在他们的神经机器翻译系统中实现了类似的方法（[Wu et al., 2016](https://arxiv.org/abs/1609.08144)），而[Paulus
    et al (2017)](https://arxiv.org/abs/1705.04304)则采用了这种方法来进行摘要任务。训练目标包含两部分，ML损失用于下一个标记的预测，$\mathcal{L}_\text{ML}
    = \sum_{(x, y^*)\sim\mathcal{D}} \log p_\theta(y^* \vert x)$，以及RL损失$\mathcal{L}_\text{RL}$用于最大化期望奖励，其中每个序列的奖励由BLEU或ROUGE来衡量。该模型首先使用$\mathcal{L}_\text{ML}$进行训练直至收敛，然后用两个损失的线性组合进行微调，$\mathcal{L}_\text{mix}
    = \alpha \mathcal{L}_\text{ML} + (1 - \alpha)\mathcal{L}_\text{RL}$。
- en: 'The RL loss of Google NMT is to maximize the expected BLEU score:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 谷歌NMT的RL损失是为了最大化期望的BLEU分数：
- en: $$ \mathcal{L}_\text{RL} = - \sum_{(x, y^*)\sim\mathcal{D}} \mathbb{E}_{y\sim
    p_\theta(.\vert x)} [R(y, y^*)] $$
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \mathcal{L}_\text{RL} = - \sum_{(x, y^*)\sim\mathcal{D}} \mathbb{E}_{y\sim
    p_\theta(.\vert x)} [R(y, y^*)] $$
- en: where $y$ is the predicted sequence and $y^*$ is the ground truth.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$y$是预测的序列，$y^*$是真实标准。
- en: '[Paulus et al (2017)](https://arxiv.org/abs/1705.04304) added an extra weighting
    term based on the reward difference between two output sequences, $y$ by sampling
    the next token according to the predicted probability and $\hat{y}$ by greedily
    taking the most likely token. This RL loss maximizes the conditional likelihood
    of the sampled sequence $y$ if it obtains a higher reward than the greedy baseline
    $\hat{y}$:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '[Paulus et al (2017)](https://arxiv.org/abs/1705.04304)在基于两个输出序列之间奖励差异的额外加权项上进行了添加，$y$通过根据预测概率采样下一个标记和$\hat{y}$通过贪婪地选择最可能的标记。如果采样序列$y$获得比贪婪基线$\hat{y}$更高的奖励，则这种RL损失最大化了采样序列$y$的条件概率：'
- en: $$ \mathcal{L}_\text{RL} = \sum_{(x, y^*)\sim\mathcal{D}} (R(\hat{y}, y^*) -
    R(y, y^*)) \sum_{t=1}^{n'} \log p(y_t \vert y_{
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \mathcal{L}_\text{RL} = \sum_{(x, y^*)\sim\mathcal{D}} (R(\hat{y}, y^*) -
    R(y, y^*)) \sum_{t=1}^{n'} \log p(y_t \vert y_{
- en: RL Fine-tuning with Human Preferences
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用人类偏好进行RL微调
- en: Reward learning is critical for defining human preferences. Quantitative measurement
    like BLEU or ROUGE computes the overlap of words and n-gram phrases between sequences
    and does not always correlate with better quality by human judges. Reward learning
    from human feedback ([Christiano et al., 2017](https://arxiv.org/abs/1706.03741))
    is a better way to align what we measure with what we actually care about. Human
    feedback has been applied to learn a reward function for applications like story
    generation ([Yi et al., 2019](https://arxiv.org/abs/1904.13015)) and summarization
    ([Böhm et al., 2019](https://arxiv.org/abs/1909.01214), [Ziegler et al., 2019](https://arxiv.org/abs/1909.08593),
    [Stiennon et al., 2020](https://arxiv.org/abs/2009.01325)).
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 奖励学习对于定义人类偏好至关重要。像BLEU或ROUGE这样的定量测量计算了序列之间单词和n-gram短语的重叠，并不总是与人类评委的更好质量相关。来自人类反馈的奖励学习([Christiano
    et al., 2017](https://arxiv.org/abs/1706.03741))是将我们测量的内容与我们实际关心的内容对齐的更好方式。人类反馈已被应用于学习应用程序的奖励函数，如故事生成([Yi
    et al., 2019](https://arxiv.org/abs/1904.13015))和摘要([Böhm et al., 2019](https://arxiv.org/abs/1909.01214),
    [Ziegler et al., 2019](https://arxiv.org/abs/1909.08593), [Stiennon et al., 2020](https://arxiv.org/abs/2009.01325))。
- en: In order to generate more coherent conversation, [Yi et al (2019)](https://arxiv.org/abs/1904.13015)
    collected 4 types of binary human feedback given a conversation pair (user utterance,
    system response), whether the system response is (1) comprehensive, (2) on topic,
    (3) interesting and (4) leading to continuation of the conversation. An evaluator
    is trained to predict human feedback and then is used to rerank the beam search
    samples, to finetune the model or to do both. (Actually they didn’t use RL fine-tuning
    but rather use the evaluator to provide a discriminator loss in supervised fine-tuning.)
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 为了生成更连贯的对话，[Yi et al (2019)](https://arxiv.org/abs/1904.13015)收集了4种类型的二进制人类反馈，给定一个对话对（用户话语，系统回应），系统回应是否(1)全面，(2)主题相关，(3)有趣和(4)导致对话继续。评估器被训练以预测人类反馈，然后用于重新排列波束搜索样本，微调模型或两者兼而有之。（实际上，他们没有使用RL微调，而是使用评估器在监督微调中提供鉴别器损失。）
- en: Let’s define a learned reward function $R_\psi(x, y)$ parameterized by $\psi$
    as a measurement for the quality of output $y$ given the input $x$.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义一个由$\psi$参数化的学习奖励函数$R_\psi(x, y)$，作为给定输入$x$的输出$y$质量的衡量标准。
- en: 'To learn the ground truth reward $R^*$ defined by human judgements, [Böhm et
    al (2019)](https://arxiv.org/abs/1909.01214) compared two loss functions:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 为了学习由人类判断定义的地面真实奖励$R^*$，[Böhm et al (2019)](https://arxiv.org/abs/1909.01214)比较了两种损失函数：
- en: '(1) Regression loss: simply minimizing the mean squared error.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: (1) 回归损失：简单地最小化均方误差。
- en: $$ \mathcal{L}^\text{MSE}_\text{rm} = [R^*(x, y) - R_\psi(x, y)]^2 $$
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \mathcal{L}^\text{MSE}_\text{rm} = [R^*(x, y) - R_\psi(x, y)]^2 $$
- en: '(2) Preference loss: learning to agree with the ground truth reward,'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: (2) 偏好损失：学习与地面真实奖励一致，
- en: $$ \begin{aligned} \mathcal{L}^\text{pref}_\text{rm} =& - \sum_{i,j} \big(\mathbb{1}[R^*(x,
    y_i) > R^*(x, y_j)] \log P(y_i \succ y_j) + \\ &\mathbb{1}[R^*(x, y_j) > R^*(x,
    y_i)] \log P(y_j \succ y_i) \big)\\ \text{where }P(y_i \succ y_j) =& \frac{\exp(R_\psi(x,
    y_i))}{\exp(R_\psi(x, y_i)) + \exp(R_\psi(x, y_j))} \end{aligned} $$
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \begin{aligned} \mathcal{L}^\text{pref}_\text{rm} =& - \sum_{i,j} \big(\mathbb{1}[R^*(x,
    y_i) > R^*(x, y_j)] \log P(y_i \succ y_j) + \\ &\mathbb{1}[R^*(x, y_j) > R^*(x,
    y_i)] \log P(y_j \succ y_i) \big)\\ \text{where }P(y_i \succ y_j) =& \frac{\exp(R_\psi(x,
    y_i))}{\exp(R_\psi(x, y_i)) + \exp(R_\psi(x, y_j))} \end{aligned} $$
- en: Their experiments showed that the *preference loss* achieves the best performance,
    where the reward model is a thin MLP layer on top of BERT sentence embedding.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 他们的实验表明*偏好损失*取得了最佳性能，其中奖励模型是BERT句子嵌入顶部的一层薄MLP层。
- en: '[Ziegler et al (2019)](https://arxiv.org/abs/1909.08593) collected human labels
    by asking humans to select the best candidate $y_b$ out of a few options $\{y_i\}$
    given the input $x \sim \mathcal{D}$. The candidates are sampled by $y_0, y_1
    \sim p(.\vert x), y_2, y_3 \sim \pi(.\vert x)$. We should be aware that human
    labeling might have very high disagreement when the ground truth is fuzzy.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '[Ziegler et al (2019)](https://arxiv.org/abs/1909.08593)通过要求人类从给定输入$x \sim
    \mathcal{D}$中的几个选项$\{y_i\}$中选择最佳候选$y_b$来收集人类标签。候选项由$y_0, y_1 \sim p(.\vert x),
    y_2, y_3 \sim \pi(.\vert x)$进行采样。我们应该意识到，当地面真相模糊时，人类标注可能存在很高的分歧。'
- en: '![](../Images/acebd0e25444d23fd33fe9eeebd592e8.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/acebd0e25444d23fd33fe9eeebd592e8.png)'
- en: 'Fig. 18\. The overview of the training framework for fine-tuning a language
    model policy with reward learned from human feedback. (Image source: [Ziegler
    et al., 2019](https://arxiv.org/abs/1909.08593))'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 图18。使用从人类反馈中学习的奖励微调语言模型策略的训练框架概述。（图片来源：[Ziegler et al., 2019](https://arxiv.org/abs/1909.08593)）
- en: 'The reward model is implemented by a pretrained language model with an extra
    random linear layer of the final embedding output. It it trained to minimize the
    loss:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 奖励模型由一个预训练语言模型实现，具有额外的最终嵌入输出的随机线性层。它被训练以最小化损失：
- en: $$ \mathcal{L}_\text{rm} = -\mathbb{E}_{(x, \{y_i\}, b) \sim \mathcal{D}} \Big[
    \log \frac{\exp(R_\psi(x, y_b))}{\sum_i \exp(R_\psi(x, y_i))} \Big] $$
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \mathcal{L}_\text{rm} = -\mathbb{E}_{(x, \{y_i\}, b) \sim \mathcal{D}} \Big[
    \log \frac{\exp(R_\psi(x, y_b))}{\sum_i \exp(R_\psi(x, y_i))} \Big] $$
- en: To keep the scale consistent during training, the reward model is normalized
    to have mean 0 and variance 1.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在训练过程中保持尺度一致，奖励模型被归一化为均值为0，方差为1。
- en: 'During RL fine-tuning, the policy $\pi$, initialized by a pretrained language
    model $p$, is optimized via [PPO](https://lilianweng.github.io/posts/2018-04-08-policy-gradient/#ppo)
    with the above learned reward model. To avoid the policy’s deviating from its
    original behavior too much, a **KL penalty** is added:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习微调期间，由预训练语言模型$p$初始化的策略$\pi$通过[PPO](https://lilianweng.github.io/posts/2018-04-08-policy-gradient/#ppo)与上述学习的奖励模型进行优化。为避免策略偏离其原始行为太多，添加了**KL
    惩罚**：
- en: $$ R(x, y) = R_\psi(x, y) - \beta\log\frac{\pi(y \vert x)}{p(y \vert x)} $$
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: $$ R(x, y) = R_\psi(x, y) - \beta\log\frac{\pi(y \vert x)}{p(y \vert x)} $$
- en: If running online data collection, human label collection process is continued
    during RL fine-tuning and thus the human labelers can review results generated
    by the latest policy. The number of human labels are evenly spread out during
    the training process. Meanwhile the reward model is also retrained periodically.
    Online data collection turns out to be important for the summarization task but
    not for the text continuation task. In their experiments, jointly training the
    reward model and the policy with shared parameters did not work well and can lead
    to overfitting due to the big imbalance between dataset sizes.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 如果进行在线数据收集，人类标签收集过程将在强化学习微调期间继续进行，因此人类标注者可以审查最新策略生成的结果。在训练过程中，人类标签的数量均匀分布。同时，奖励模型也定期重新训练。在线数据收集对摘要任务很重要，但对文本延续任务则不重要。在他们的实验中，共同训练奖励模型和具有共享参数的策略并不奏效，可能会导致由于数据集大小之间的巨大不平衡而过拟合。
- en: 'In the following work ([Stiennon et al., 2020](https://arxiv.org/abs/2009.01325)),
    the human label collection was further simplified to select the best option between
    a pair of summaries, $y_b \in\{y_0, y_1\}$ The reward model loss was updated to
    optimize the log odds of the selected summary:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下工作中（[Stiennon et al., 2020](https://arxiv.org/abs/2009.01325)），人类标签收集进一步简化为在一对摘要中选择最佳选项$y_b
    \in\{y_0, y_1\}$。奖励模型损失被更新以优化所选摘要的对数几率：
- en: $$ \mathcal{L}_\text{rm} = \mathbb{E}_{(x, y_0, y_1, b)\sim\mathcal{D}} [\log(\sigma(r_\theta(x,
    y_b) − r_\theta(x, y_{1−b})))] $$![](../Images/82881508eb3d02b15f32bf8e8a833dd3.png)
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \mathcal{L}_\text{rm} = \mathbb{E}_{(x, y_0, y_1, b)\sim\mathcal{D}} [\log(\sigma(r_\theta(x,
    y_b) − r_\theta(x, y_{1−b})))] $$![](../Images/82881508eb3d02b15f32bf8e8a833dd3.png)
- en: 'Fig. 19\. The overview of fine-tuning the language model policy from human
    feedback for summarization, including (1) human feedback collection, (2) reward
    model training, and (3) policy training. (Image source: [Stiennon et al., 2020](https://arxiv.org/abs/2009.01325))'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 图19。从人类反馈中微调语言模型策略的概述，包括（1）人类反馈收集，（2）奖励模型训练和（3）策略训练。（图片来源：[Stiennon et al.,
    2020](https://arxiv.org/abs/2009.01325)）
- en: Guided Fine-tuning with Steerable Layer
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 带有可导层的引导微调
- en: Instead of fine-tuning the entire model, only fine-tuning a small extra set
    of parameters while the base model stays fixed is computationally cheaper.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是微调整个模型，只微调额外的一小组参数，而基本模型保持不变，这在计算上更便宜。
- en: In computer vision, plug-and-play generative networks (PPGN; [Nguyen et al.,
    2017](https://arxiv.org/abs/1612.00005)) generate images with different attributes
    by plugging a discriminator $p(a \vert x)$ into a base generative model $p(x)$.
    Then the sample with a desired attribute $a$ can be sampled from $p(x \vert a)
    \propto p(a \vert x)p(x)$. Inspired by PPGN, the **plug-and-play language model**
    (**PPLM**; [Dathathri et al., 2019](https://arxiv.org/abs/1912.02164)) combines
    one or multiple simple attribute models with a pretrained language model for controllable
    text generation.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算机视觉中，即插即用生成网络（PPGN；[Nguyen等，2017](https://arxiv.org/abs/1612.00005)）通过将鉴别器$p(a
    \vert x)$插入基本生成模型$p(x)$来生成具有不同属性的图像。然后，可以从$p(x \vert a) \propto p(a \vert x)p(x)$中采样具有所需属性$a$的样本。受PPGN启发，**即插即用语言模型**（**PPLM**；[Dathathri等，2019](https://arxiv.org/abs/1912.02164)）将一个或多个简单属性模型与预训练语言模型结合起来，用于可控文本生成。
- en: 'Given an attribute $a$ and the generated sample $x$, let an attribute model
    be $p(a\vert x)$. To control content generation, the current latent representation
    at time $t$, $H_t$ (containing a list of key-value pairs per layer), can be shifted
    by $\Delta H_t$ in the direction of the sum of two gradients:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 给定属性$a$和生成的样本$x$，让属性模型为$p(a\vert x)$。为了控制内容生成，当前时间$t$的潜在表示$H_t$（每层包含一组键-值对）可以通过$\Delta
    H_t$向两个梯度的和的方向移动：
- en: One toward higher log-likelihood of the attribute $a$ under $p(a \vert x)$ —
    so that the output content acquires a desired attribute.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一种朝着使输出内容获得所需属性$a$在$p(a \vert x)$下更高对数似然的方向——以便输出内容获得所需属性。
- en: The other toward higher log-likelihood of the unmodified language model $p(x)$
    — so that the generated text is still in fluent and smooth natural language.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一种朝着使未修改的语言模型$p(x)$下更高对数似然的方向——以便生成的文本仍然是流畅和自然的语言。
- en: 'To shift the output, at decoding time, PPLM runs one forward → one backward
    → one forward, three passes in total:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 为了转移输出，在解码时，PPLM运行一次前向传递→一次后向传递→一次前向传递，总共三次传递：
- en: First a forward pass is performed to compute the likelihood of attribute $a$
    by $p(a\vert x)$;
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先执行前向传递以计算属性$a$的可能性$p(a\vert x)$；
- en: Let $\Delta H_t$ be a stepwise update to the hidden state $H_t$ such that $(H_t
    + \Delta H_t)$ shifts the distribution of generated text closer to having the
    attribute $a$. $\Delta H_t$ is initialized at zero. Then a backward pass updates
    the LM hidden states using normalized gradients from the attribute model $\nabla_{\Delta
    H_t} \log p(a \vert H_t + \Delta H_t)$ as
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设$\Delta H_t$是隐藏状态$H_t$的逐步更新，使得$(H_t + \Delta H_t)$将生成文本的分布移向具有属性$a$。$\Delta
    H_t$初始化为零。然后，通过使用属性模型$\nabla_{\Delta H_t} \log p(a \vert H_t + \Delta H_t)$的归一化梯度来更新LM隐藏状态的后向传递如下
- en: $$ \Delta H_t \leftarrow \Delta H_t + \alpha \frac{\nabla_{\Delta H_t} \log
    p(a|H_t + \Delta H_t)}{\| \nabla_{\Delta H_t} \log p(a|H_t + \Delta H_t) \|^\gamma}
    $$
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \Delta H_t \leftarrow \Delta H_t + \alpha \frac{\nabla_{\Delta H_t} \log
    p(a|H_t + \Delta H_t)}{\| \nabla_{\Delta H_t} \log p(a|H_t + \Delta H_t) \|^\gamma}
    $$
- en: where $\gamma$ is a normalization scaling coefficient, set per layer. $\alpha$
    is step size. This update can be repeated $m \in [3, 10]$ times 3\. The final
    forward pass recomputes a new distribution over the vocabulary, generated from
    the updated latents $\tilde{H}_t = H_t + \Delta H_t$. The next token is sampled
    from the updated distribution.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\gamma$是一个每层设置的归一化缩放系数。$\alpha$是步长。这个更新可以重复$m \in [3, 10]$次。最终的前向传递重新计算从更新的潜在$\tilde{H}_t
    = H_t + \Delta H_t$生成的词汇分布。下一个标记从更新的分布中采样。
- en: '![](../Images/7bc4096d90241c80e0297218f8a431ad.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7bc4096d90241c80e0297218f8a431ad.png)'
- en: 'Fig. 20\. The overview of how PPLM runs three passes to update the model output
    to increase the likelihood of a desired attribute. (Image source: [Dathathri et
    al., 2019](https://arxiv.org/abs/1912.02164))'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 图20。PPLM运行三次传递以更新模型输出，增加所需属性的可能性的概述。（图片来源：[Dathathri等，2019](https://arxiv.org/abs/1912.02164)）
- en: 'Multiple attribute models can be mix-and-matched during generation with customized
    weights, acting as a set of “control knobs”. The PPLM paper explored two types
    of attribute models:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成过程中，可以混合匹配多个属性模型，并使用自定义权重，充当一组“控制旋钮”。PPLM论文探讨了两种属性模型：
- en: The simplest attribution model is based on a predefined *bag of words* (BoW),
    $\{w_1, \dots, w_k\}$, that specifies a topic of interest.
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最简单的属性模型基于预定义的*词袋*（BoW），$\{w_1, \dots, w_k\}$，指定了感兴趣的主题。
- en: $$ \log p(a \vert x) = \log\big( \sum_{i=1}^k p_{t+1} [w_i] \big) $$
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \log p(a \vert x) = \log\big( \sum_{i=1}^k p_{t+1} [w_i] \big) $$
- en: To encourage the model to output the desired words at least once but not at
    every step, they normalize the gradient by the maximum gradient norm.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 为了鼓励模型至少在某个步骤输出所需的单词，但不是每个步骤都输出，他们通过最大梯度范数对梯度进行归一化。
- en: Interestingly, they found that increasing the probability of generating words
    in the bag also increases the probability of generating *related* but not identical
    words about the same topic. 2\. The discriminator attribute models are based on
    learned classifiers which define preferences by a distribution instead of hard
    samples.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，他们发现增加生成包中单词的概率也会增加生成关于同一主题的*相关*但不完全相同的单词的概率。2\. 判别器属性模型基于学习的分类器，通过分布而不是硬样本定义偏好。
- en: 'To ensure the fluency in language, PPLM applied two additional designs:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保语言流畅性，PPLM 应用了两个额外的设计：
- en: Minimizing the KL diverge between modified and unmodified LM, commonly seen
    in other RL fine-tuning approaches (see [above](#kl-penalty)).
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最小化修改和未修改 LM 之间的 KL 散度，这在其他 RL 微调方法中很常见（见[上文](#kl-penalty)）。
- en: It performs [post-norm fusion](https://arxiv.org/abs/1809.00125) to constantly
    tie the generated text to the unconditional LM $p(x)$, $x_{t+1} \sim \frac{1}{\beta}(\tilde{p}_{t+1}^{\gamma_\text{gm}}
    p_{t+1}^{1-\gamma_\text{gm}})$, where $p_{t+1}$ and $\tilde{p}_{t+1}$ are the
    unmodified and modified output distributions, respectively. $\beta$ is a normalizing
    factor. $\gamma_\text{gm} \in [0.8, 0.95]$ balances between prediction from before
    and after models.
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它执行[后归一化融合](https://arxiv.org/abs/1809.00125)以始终将生成的文本与无条件 LM $p(x)$ 相关联，$x_{t+1}
    \sim \frac{1}{\beta}(\tilde{p}_{t+1}^{\gamma_\text{gm}} p_{t+1}^{1-\gamma_\text{gm}})$，其中
    $p_{t+1}$ 和 $\tilde{p}_{t+1}$ 分别是未修改和修改后的输出分布。$\beta$ 是一个归一化因子。$\gamma_\text{gm}
    \in [0.8, 0.95]$ 在前后模型的预测之间取得平衡。
- en: '![](../Images/23caaf09b25f01ab45652c9cc23f031b.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/23caaf09b25f01ab45652c9cc23f031b.png)'
- en: 'Fig. 21\. Examples of controllable text generation by PPLM. (Image source:
    [Dathathri et al., 2019](https://arxiv.org/abs/1912.02164))'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 图 21\. PPLM 实现的可控文本生成示例。（图片来源：[Dathathri 等人，2019](https://arxiv.org/abs/1912.02164)）
- en: Interestingly, they found a large variance in the extent of controllability
    across topics. Some topics (religion, science, politics) are easier to control
    for compared to others (computers, space).
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，他们发现在不同主题之间的可控性程度存在很大差异。一些主题（宗教、科学、政治）比其他主题（计算机、空间）更容易控制。
- en: One obvious drawback of PPLM is that due to multiple passes at every decoding
    step, the test time computation becomes much more expensive.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: PPLM 的一个明显缺点是由于在每个解码步骤进行多次传递，测试时间计算变得更加昂贵。
- en: Similar to PPLM, **DELOREAN** (DEcoding for nonmonotonic LOgical REAsoNing;
    [Qin et al., 2020](https://arxiv.org/abs/2010.05906)) incorporates the future
    context by back-propagation. Given input text $\mathbf{x}$, DELOREAN aims to generate
    continuation completion $\mathbf{y} = [y_1, \dots, y_N]$ such that $y$ satisfies
    certain constraints defined by a context $z$. To keep the generation differentiable,
    a soft representation of $y$ is tracked, $\tilde{\mathbf{y}}=(\tilde{y}_1, \dots,
    \tilde{y}_N)$ where $\tilde{y}_i \in \mathbb{R}^V$ are logits over the vocabulary.
    $\tilde{\mathbf{y}}^{(t)}$ is the soft representation at iteration $t$.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 与 PPLM 类似，**DELOREAN**（DEcoding for nonmonotonic LOgical REAsoNing；[Qin 等人，2020](https://arxiv.org/abs/2010.05906)）通过反向传播将未来上下文纳入考虑。给定输入文本
    $\mathbf{x}$，DELOREAN 旨在生成满足由上下文 $z$ 定义的某些约束的续写完成 $\mathbf{y} = [y_1, \dots, y_N]$。为了保持生成的可微分性，跟踪
    $y$ 的软表示，$\tilde{\mathbf{y}}=(\tilde{y}_1, \dots, \tilde{y}_N)$，其中 $\tilde{y}_i
    \in \mathbb{R}^V$ 是词汇表上的对数。$\tilde{\mathbf{y}}^{(t)}$ 是迭代 $t$ 时的软表示。
- en: 'Given the representation $\tilde{y}^{(t-1)}$ at iteration $t$, it runs the
    following procedures:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 给定迭代 $t$ 时的表示 $\tilde{y}^{(t-1)}$，它执行以下过程：
- en: '**Backward**: The constraint is represented as a loss function $\mathcal{L}(\mathbf{x},
    \tilde{\mathbf{y}}^{(t-1)}, z))$. The logits are updated via gradient descent:
    $\tilde{y}^{(t), b}_n = \tilde{y}_n^{(t-1)} - \lambda \nabla_{\tilde{y}_n} \mathcal{L}(\mathbf{x},
    \tilde{\mathbf{y}}^{(t-1)}, z)$.'
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**反向**：约束表示为损失函数 $\mathcal{L}(\mathbf{x}, \tilde{\mathbf{y}}^{(t-1)}, z))$。通过梯度下降更新对数：$\tilde{y}^{(t),
    b}_n = \tilde{y}_n^{(t-1)} - \lambda \nabla_{\tilde{y}_n} \mathcal{L}(\mathbf{x},
    \tilde{\mathbf{y}}^{(t-1)}, z)$。'
- en: '**Forward**: Run forward pass to ensure the generated text is fluent. $\tilde{y}^{(t),f}_n
    = \text{LM}(\mathbf{x}, \tilde{\mathbf{y}}^{(t)}_{1:n-1})$.'
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**正向**：运行正向传递以确保生成的文本流畅。$\tilde{y}^{(t),f}_n = \text{LM}(\mathbf{x}, \tilde{\mathbf{y}}^{(t)}_{1:n-1})$。'
- en: Then linearly combine two logits together to create a new representation $\tilde{y}^{(t)}_n
    = \gamma \tilde{y}^{(t), f}_n + (1-\gamma) \tilde{y}^{(t), b}_n$. Note that each
    $\tilde{y}^{(t)}_n$ is needed to sample the next $\tilde{y}^{(t),f}_{n+1}$.
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后线性组合两个logits以创建一个新的表示$\tilde{y}^{(t)}_n = \gamma \tilde{y}^{(t), f}_n + (1-\gamma)
    \tilde{y}^{(t), b}_n$。注意，每个$\tilde{y}^{(t)}_n$都需要对下一个$\tilde{y}^{(t),f}_{n+1}$进行采样。
- en: '**Side-tuning** ([Zhang et al., 2019](https://arxiv.org/abs/1912.13503)) trains
    a light-weighted side network that learns a residual on top of the original model
    outputs without modifying the pre-trained model weights. Unlike PPLM, no gradient
    update is applied on the hidden states. It is a simple yet effective approach
    for incremental learning. The base model is treated as a black-box model and does
    not necessarily have to be a neural network. Side-tuning setup assumes the base
    and side models are fed exactly the same input and the side model is independently
    learned.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '**边缘调整**（[Zhang等人，2019](https://arxiv.org/abs/1912.13503)）训练一个轻量级的边缘网络，学习在不修改预训练模型权重的情况下在原始模型输出之上学习残差。与PPLM不同，隐藏状态上不应用梯度更新。这是一种简单而有效的增量学习方法。基础模型被视为黑盒模型，不一定是神经网络。边缘调整设置假设基础模型和边缘模型接收完全相同的输入，并且边缘模型是独立学习的。'
- en: '![](../Images/a0fc6c46461f8c2a518d0cb7b13ac50f.png)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a0fc6c46461f8c2a518d0cb7b13ac50f.png)'
- en: 'Fig. 22\. Comparison of fixed weights, fine-tuning and side-tuning. (Image
    source: [Zhang et al., 2019](https://arxiv.org/abs/1912.13503))'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 图22\. 固定权重、微调和边缘调整的比较。（图片来源：[Zhang等人，2019](https://arxiv.org/abs/1912.13503)）
- en: 'The paper explored different strategies of fusing predictions from the base
    and side models: `product` is the worst while `sum` ($\alpha$-blending), MLP,
    and [FiLM](https://arxiv.org/abs/1709.07871) are comparable. Side-tuning is able
    to achieve better performance, when it is trained with intermediate amounts of
    data and when the base network is large.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 该论文探讨了融合基础模型和边缘模型预测的不同策略：`product`是最差的，而`sum`（$\alpha$-混合）、MLP和[FiLM](https://arxiv.org/abs/1709.07871)是可比的。当边缘调整使用中等数量的数据进行训练且基础网络较大时，能够实现更好的性能。
- en: '**Auxiliary tuning** ([Zeldes et al., 2020](https://arxiv.org/abs/2006.16823))
    supplements the original pre-trained model with an *auxiliary* model that shifts
    the output distribution according to the target task. The base and auxiliary model
    outputs are merged on the logits level. The combined model is trained to maximize
    the likelihood $p(x_t\vert x_{<t}, z)$ of target output.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '**辅助调整**（[Zeldes等人，2020](https://arxiv.org/abs/2006.16823)）通过*辅助*模型补充原始预训练模型，根据目标任务调整输出分布。基础模型和辅助模型的输出在logits级别上合并。组合模型被训练以最大化目标输出的似然$p(x_t\vert
    x_{<t}, z)$。'
- en: 'The conditional probability of $p(x_t\vert x_{<t}, z)$ can be decomposed into
    two parts:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: $p(x_t\vert x_{<t}, z)$的条件概率可以分解为两部分：
- en: $p(x_t\vert x_{<t})$ assigns high probabilities to fluent sequences of tokens;
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: $p(x_t\vert x_{<t})$为标记流畅序列分配高概率；
- en: a shift on $p(x_t\vert x_{<t})$ towards $p(x_t\vert x_{<t}, z)$.
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将$p(x_t\vert x_{<t})$的偏移转向$p(x_t\vert x_{<t}, z)$。
- en: $$ p(x_t\vert x_{
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: $$ p(x_t\vert x_{
- en: By Bayesian rule, we have
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 根据贝叶斯规则，我们有
- en: $$ p(x_t\vert x_{
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: $$ p(x_t\vert x_{
- en: And therefore the auxiliary model $\text{logits}_\text{aux}(x_t \vert x_{<t},
    z))$ effectively should learn to predict $p(z \vert x_{\leq t})$. In the experiments
    of [Zeldes et al., 2020](https://arxiv.org/abs/2006.16823), the auxiliary model
    can re-use the intermediate layers of the pre-trained LM for feature extraction.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，辅助模型$\text{logits}_\text{aux}(x_t \vert x_{<t}, z))$有效地应该学会预测$p(z \vert x_{\leq
    t})$。在[Zeldes等人，2020](https://arxiv.org/abs/2006.16823)的实验中，辅助模型可以重复使用预训练语言模型的中间层进行特征提取。
- en: '![](../Images/76737c8fbd909dc4e17af802735d5209.png)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/76737c8fbd909dc4e17af802735d5209.png)'
- en: 'Fig. 23\. The auxiliary model is trained by reusing features extracted from
    multiple layers of the base model. (Image source: [Zeldes et al., 2020](https://arxiv.org/abs/2006.16823))'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 图23\. 辅助模型通过重用从基础模型的多个层中提取的特征进行训练。（图片来源：[Zeldes等人，2020](https://arxiv.org/abs/2006.16823)）
- en: '**GeDi** ([Kruse et al., 2020](https://arxiv.org/abs/2009.06367)) guides the
    text generation by *Generative Discriminator*. The discriminator is implemented
    as a class conditional language model (CC-LM), $p_\theta(x_{1:t} \vert z)$. The
    discriminator guides generation at each decoding step by computing classification
    probabilities for all possible next tokens via Bayes rule by normalizing over
    *two* contrastive class-conditional distributions:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '**GeDi**（[Kruse et al., 2020](https://arxiv.org/abs/2009.06367)）通过 *生成鉴别器*（Generative
    Discriminator）指导文本生成。该鉴别器被实现为一个类条件语言模型（CC-LM），$p_\theta(x_{1:t} \vert z)$。鉴别器通过贝叶斯规则计算所有可能的下一个标记的分类概率，通过对
    *两个* 对比类条件分布进行归一化来指导每个解码步骤的生成：'
- en: One conditioned on the control code $z$ for desired attribute.
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个在所需属性的控制代码 $z$ 上进行条件的。
- en: The other conditioned on the anti-control code $\bar{z}$ for undesired attributes.
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 另一个在反控制代码 $\bar{z}$ 上进行条件的，用于不需要的属性。
- en: 'GeDi relies on the contract between $p_\theta(x_{1:t} \vert z)$ and $p_\theta(x_{1:t}
    \vert \bar{z})$ to compute the probability of the sequence belonging to the desired
    class. The discriminator loss is to maximize the probability of desired attribute
    $z$:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: GeDi 依赖于 $p_\theta(x_{1:t} \vert z)$ 和 $p_\theta(x_{1:t} \vert \bar{z})$ 之间的对比来计算序列属于所需类别的概率。鉴别器损失是为了最大化所需属性
    $z$ 的概率：
- en: $$ \begin{aligned} p_\theta(z \vert x_{1:t}) &= \frac{p(z) p_\theta(x_{1:\tau}
    \vert z)^{\alpha/\tau}}{\sum_{z' \in \{z, \bar{z}\}} p(z') p_\theta(x_{1:\tau}
    \vert z')^{\alpha/\tau} } \\ \mathcal{L}_\text{desc} &= -\frac{1}{N} \sum_{i=1}^N
    \log p_\theta(z^{(i)} \vert x^{(i)}_{1:\tau_i}) \\ &= -\frac{1}{N} \sum_{i=1}^N
    \log \frac{p(z) p_\theta(x^{(i)}_{1:\tau_i} \vert z^{(i)})^{\alpha/t_i}}{\sum_{z'
    \in \{z, \bar{z}\} } p(z')p_\theta(x^{(i)}_{1:\tau_i} \vert z')^{\alpha/\tau_i}}
    \end{aligned} $$
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \begin{aligned} p_\theta(z \vert x_{1:t}) &= \frac{p(z) p_\theta(x_{1:\tau}
    \vert z)^{\alpha/\tau}}{\sum_{z' \in \{z, \bar{z}\}} p(z') p_\theta(x_{1:\tau}
    \vert z')^{\alpha/\tau} } \\ \mathcal{L}_\text{desc} &= -\frac{1}{N} \sum_{i=1}^N
    \log p_\theta(z^{(i)} \vert x^{(i)}_{1:\tau_i}) \\ &= -\frac{1}{N} \sum_{i=1}^N
    \log \frac{p(z) p_\theta(x^{(i)}_{1:\tau_i} \vert z^{(i)})^{\alpha/t_i}}{\sum_{z'
    \in \{z, \bar{z}\} } p(z')p_\theta(x^{(i)}_{1:\tau_i} \vert z')^{\alpha/\tau_i}}
    \end{aligned} $$
- en: where $p(z) = \exp(b_z) / \sum_{z’} \exp(b_{z’})$ and $b_z$ is a learned class
    prior. The probabilities are normalized by the current sequence length $\tau$
    to robustify generation sequences of variable lengths. $\tau_i$ is the sequence
    length of the $i$-th input $x^{(i)}$ in the dataset.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $p(z) = \exp(b_z) / \sum_{z’} \exp(b_{z’})$，$b_z$ 是一个学习到的类先验。概率通过当前序列长度 $\tau$
    进行归一化，以增强可变长度生成序列的鲁棒性。$\tau_i$ 是数据集中第 $i$ 个输入 $x^{(i)}$ 的序列长度。
- en: '![](../Images/e36931f0b17cb053ed843939d7487c1f.png)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e36931f0b17cb053ed843939d7487c1f.png)'
- en: 'Fig. 24\. An illustration of how GeDi works via Bayesian rule. (Image source:
    [Kruse et al., 2020](https://arxiv.org/abs/2009.06367))'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 图 24\. 通过贝叶斯规则展示 GeDi 的工作原理。（图片来源：[Kruse et al., 2020](https://arxiv.org/abs/2009.06367)）
- en: They finetuned a GPT2-medium model with control code similar to how [CTRL](#ctrl)
    is trained to form a CC-LM using a linear combination of discriminative loss and
    generative loss. This discriminator model is then used as GiDe to guide generation
    by a larger language model like GPT2-XL.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 他们对一个类似于 [CTRL](#ctrl) 训练的控制代码进行了微调的 GPT2-medium 模型，以形成一个使用鉴别损失和生成损失的线性组合的类条件语言模型（CC-LM）。然后将这个鉴别器模型用作
    GiDe，通过类似于 GPT2-XL 这样的更大语言模型来指导生成。
- en: One way of decoding from GeDi is to sample from a weighted posterior $p^w(x_{t+1}\vert
    x_{1:t}, z) \propto p(z \vert x_{1:t+1})^w p(x_{t+1} \vert x_{1:t})$ where $w>1$
    applies additional bias toward the desired class $z$. In the sampling process,
    only tokens with the class or next-token probability larger than a certain threshold
    are selected.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 从 GeDi 解码的一种方式是从加权后验中进行采样 $p^w(x_{t+1}\vert x_{1:t}, z) \propto p(z \vert x_{1:t+1})^w
    p(x_{t+1} \vert x_{1:t})$，其中 $w>1$ 对所需类别 $z$ 应用额外偏向。在采样过程中，只选择具有大于一定阈值的类别或下一个标记概率的标记。
- en: GeDi guided generation in their experiments showed strong controllability and
    ran 30x faster than [PPLM](#pplm).
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 他们在实验中展示的 GeDi 指导生成表现出很强的可控性，并且比 [PPLM](#pplm) 快 30 倍。
- en: Distributional Approach
  id: totrans-247
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分布式方法
- en: '**Generation with Distributional Control** (GDC; [Khalifa, et al. 2020](https://arxiv.org/abs/2012.11635))
    frames controlled text generation as the optimization of a probability distribution
    with a constraint. It involves two major steps.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '**分布式控制生成**（GDC；[Khalifa, et al. 2020](https://arxiv.org/abs/2012.11635)）将受控文本生成框架化为带有约束的概率分布的优化。它包括两个主要步骤。'
- en: '**Step 1: Learn a EBM of the target model**'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 1：学习目标模型的 EBM**'
- en: Let’s label a pretrained LM as $a$ and a target LM with desired features as
    $p$. The desired features can be defined by a set of pre-defined real-valued feature
    functions $\phi_i(x), i=1,\dots,k$ over $x \in X$, denoted as a vector $\boldsymbol{\phi}$.
    When sequences $x \in X$ are sampled according to the desired model $p$, the expectations
    of features $\mathbb{E}_{x\sim p}\boldsymbol{\phi}(x)$ should be close to $\bar{\boldsymbol{\mu}}$
    , named “*moment constraints*”. The feature function $\phi_i$ can have distinct
    values (e.g. identity function for binary classifier) or continuous probabilities.
    In the meantime, the fine-tuned model $p$ should not diverge from $a$ too much
    by maintaining a small KL divergence measure.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将预训练的语言模型标记为$a$，将具有所需特征的目标语言模型标记为$p$。所需特征可以由一组预定义的实值特征函数$\phi_i(x), i=1,\dots,k$定义在$x
    \in X$上，表示为向量$\boldsymbol{\phi}$。当根据所需模型$p$对序列$x \in X$进行采样时，特征的期望$\mathbb{E}_{x\sim
    p}\boldsymbol{\phi}(x)$应该接近$\bar{\boldsymbol{\mu}}$，称为“*矩约束*”。特征函数$\phi_i$可以具有不同的值（例如，对于二元分类器的恒等函数）或连续概率。同时，微调模型$p$不应该与$a$相差太远，通过保持较小的KL散度度量。
- en: 'In summary, given a pretrained model $a$, we would like to find a target model
    $p$ such that:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，给定一个预训练模型$a$，我们希望找到一个目标模型$p$，使得：
- en: $$ \begin{aligned} \bar{\boldsymbol{\mu}} &= \mathbb{E}_{x\sim p}\boldsymbol{\phi}(x)
    \\ p &= \arg\min_{c \in \mathcal{C}} D_\text{KL}(c, a) \end{aligned} $$
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \begin{aligned} \bar{\boldsymbol{\mu}} &= \mathbb{E}_{x\sim p}\boldsymbol{\phi}(x)
    \\ p &= \arg\min_{c \in \mathcal{C}} D_\text{KL}(c, a) \end{aligned} $$
- en: where $\mathcal{C}$ is the set of all distributions over $X$ that satisfy the
    moment constraints.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\mathcal{C}$是满足矩约束的所有分布集合。
- en: 'According to theorems in Information Geometry, $p$ can be approximated by an
    EBM (energy-based model; an unnormalized probability distribution) $P$ in the
    form of exponential function, such that $p(x) \propto P(x)$ and $p(x)=\frac{1}{Z}P(x)$
    where $Z=\sum_x P(x)$. The energy-based model can be approximated by:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 根据信息几何学中的定理，$p$可以通过形式为指数函数的EBM（基于能量的模型；未归一化的概率分布）$P$来近似，使得$p(x) \propto P(x)$和$p(x)=\frac{1}{Z}P(x)$，其中$Z=\sum_x
    P(x)$。EBM可以通过以下方式近似：
- en: $$ P(x)=a(x)\exp\big(\sum_i \lambda_i \phi_i(x)\big)=a(x)\exp(\boldsymbol{\lambda}\cdot\boldsymbol{\phi}(x))
    $$
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: $$ P(x)=a(x)\exp\big(\sum_i \lambda_i \phi_i(x)\big)=a(x)\exp(\boldsymbol{\lambda}\cdot\boldsymbol{\phi}(x))
    $$
- en: Let’s define *importance weight* $w(x, \boldsymbol{\lambda}) = \frac{P(x)}{a(x)}
    = \exp\langle\boldsymbol{\lambda}\cdot\boldsymbol{\phi}(x)\rangle$. Given a large
    number of sequences sampled from the pretrained model $x_1, \dots, x_N \sim a(x)$,
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义*重要权重* $w(x, \boldsymbol{\lambda}) = \frac{P(x)}{a(x)} = \exp\langle\boldsymbol{\lambda}\cdot\boldsymbol{\phi}(x)\rangle$。给定从预训练模型中采样的大量序列$x_1,
    \dots, x_N \sim a(x)$，
- en: $$ \begin{aligned} \mu(\boldsymbol{\lambda}) &= \mathbb{E}_{x\sim p}\boldsymbol{\phi}(x)
    = \mathbb{E}_{x\sim a} \frac{p(x)}{a(x)}\boldsymbol{\phi}(x) = \frac{1}{Z}\mathbb{E}_{x\sim
    a} w(x, \boldsymbol{\lambda}) \boldsymbol{\phi}(x) \\ &= \frac{\mathbb{E}_{x\sim
    a} w(x, \boldsymbol{\lambda}) \boldsymbol{\phi}(x)}{\sum_{x\in X} P(x)} = \frac{\mathbb{E}_{x\sim
    a} w(x, \boldsymbol{\lambda}) \boldsymbol{\phi}(x)}{\sum_{x\in X} w(x, \boldsymbol{\lambda})a(x)}
    = \frac{\mathbb{E}_{x\sim a} w(x, \boldsymbol{\lambda}) \boldsymbol{\phi}(x)}{\mathbb{E}_{x\sim
    a} w(x, \boldsymbol{\lambda})} \\ &\simeq \frac{\sum_{i=1}^N w(x_i,\boldsymbol{\lambda})
    \boldsymbol{\phi}(x_i)}{\sum_{i=1}^N w(x_i, \boldsymbol{\lambda})} = \frac{\sum_{i=1}^N
    \exp\langle\boldsymbol{\lambda}\cdot\boldsymbol{\phi}(x)\rangle \boldsymbol{\phi}(x_i)}{\sum_{i=1}^N
    \exp\langle\boldsymbol{\lambda}\cdot\boldsymbol{\phi}(x)\rangle} \end{aligned}
    $$
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \begin{aligned} \mu(\boldsymbol{\lambda}) &= \mathbb{E}_{x\sim p}\boldsymbol{\phi}(x)
    = \mathbb{E}_{x\sim a} \frac{p(x)}{a(x)}\boldsymbol{\phi}(x) = \frac{1}{Z}\mathbb{E}_{x\sim
    a} w(x, \boldsymbol{\lambda}) \boldsymbol{\phi}(x) \\ &= \frac{\mathbb{E}_{x\sim
    a} w(x, \boldsymbol{\lambda}) \boldsymbol{\phi}(x)}{\sum_{x\in X} P(x)} = \frac{\mathbb{E}_{x\sim
    a} w(x, \boldsymbol{\lambda}) \boldsymbol{\phi}(x)}{\sum_{x\in X} w(x, \boldsymbol{\lambda})a(x)}
    = \frac{\mathbb{E}_{x\sim a} w(x, \boldsymbol{\lambda}) \boldsymbol{\phi}(x)}{\mathbb{E}_{x\sim
    a} w(x, \boldsymbol{\lambda})} \\ &\simeq \frac{\sum_{i=1}^N w(x_i,\boldsymbol{\lambda})
    \boldsymbol{\phi}(x_i)}{\sum_{i=1}^N w(x_i, \boldsymbol{\lambda})} = \frac{\sum_{i=1}^N
    \exp\langle\boldsymbol{\lambda}\cdot\boldsymbol{\phi}(x)\rangle \boldsymbol{\phi}(x_i)}{\sum_{i=1}^N
    \exp\langle\boldsymbol{\lambda}\cdot\boldsymbol{\phi}(x)\rangle} \end{aligned}
    $$
- en: Using SGD over the objective $|\boldsymbol{\mu}(\boldsymbol{\lambda}) - \bar{\boldsymbol{\mu}}|^2_2$,
    we can obtain an estimated value for $\boldsymbol{\lambda}$ and a representation
    of $P(x)=a(x)\exp\langle\boldsymbol{\lambda}\cdot\boldsymbol{\phi}(x)\rangle$.
    $P(x)$ is a sequential EBM because $a$ is an autoregressive model.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 使用目标函数$|\boldsymbol{\mu}(\boldsymbol{\lambda}) - \bar{\boldsymbol{\mu}}|^2_2$上的SGD，我们可以得到$\boldsymbol{\lambda}$的估计值和$P(x)=a(x)\exp\langle\boldsymbol{\lambda}\cdot\boldsymbol{\phi}(x)\rangle$的表示。$P(x)$是一个序列EBM，因为$a$是一个自回归模型。
- en: '**Step 2: Learn the target probability distribution**'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 2：学习目标概率分布**'
- en: 'The EBM $P(x)$ can compute ratios of probabilities of two sequences, but cannot
    sample from $p(x)$ with knowing $Z$. In order to sample from a sequential EBM,
    the paper proposed to use [Distributional Policy Gradient](https://arxiv.org/abs/1912.08517)
    (DPG; but not this [DPG](https://lilianweng.github.io/posts/2018-04-08-policy-gradient/#dpg))
    with the objective to obtain an autoregressive policy $\pi_\theta$ to approximate
    a target distribution $p$ by minimizing the cross entropy $H(p, \pi_\theta)$.
    DPG runs through a sequence of iterations. Within each iteration, the proposed
    distribution $q$ is used for sampling and we can correct the cross entropy loss
    with importance weights too:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: EBM $P(x)$ 可以计算两个序列概率的比率，但不能在知道 $Z$ 的情况下从 $p(x)$ 中采样。为了从序列 EBM 中采样，论文提出使用 [分布策略梯度](https://arxiv.org/abs/1912.08517)（DPG；但不是这个
    [DPG](https://lilianweng.github.io/posts/2018-04-08-policy-gradient/#dpg)）来获得一个自回归策略
    $\pi_\theta$，通过最小化交叉熵 $H(p, \pi_\theta)$ 来逼近目标分布 $p$。DPG 经过一系列迭代。在每次迭代中，提出的分布
    $q$ 用于采样，我们也可以用重要性权重来校正交叉熵损失：
- en: $$ \begin{aligned} \nabla_\theta H(p, \pi_\theta) &= - \nabla_\theta \mathbb{E}_{x\sim
    p} \log \pi_\theta(x) = - \mathbb{E}_{x\sim p} \nabla_\theta \log \pi_\theta(x)
    \\ &= - \mathbb{E}_{x\sim q} \frac{p(x)}{q(x)} \nabla_\theta \log \pi_\theta(x)
    = - \frac{1}{Z}\mathbb{E}_{x\sim q} \frac{P(x)}{q(x)} \nabla_\theta \log \pi_\theta(x)
    \end{aligned} $$
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \begin{aligned} \nabla_\theta H(p, \pi_\theta) &= - \nabla_\theta \mathbb{E}_{x\sim
    p} \log \pi_\theta(x) = - \mathbb{E}_{x\sim p} \nabla_\theta \log \pi_\theta(x)
    \\ &= - \mathbb{E}_{x\sim q} \frac{p(x)}{q(x)} \nabla_\theta \log \pi_\theta(x)
    = - \frac{1}{Z}\mathbb{E}_{x\sim q} \frac{P(x)}{q(x)} \nabla_\theta \log \pi_\theta(x)
    \end{aligned} $$
- en: 'To learn such a $\pi_\theta$, the paper adopts a KL-adaptive version of DPG:
    It only updates $q$ when the estimated policy $\pi_\theta$ gets closer to $p$.
    This adaptive step is important for fast convergence.'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 为了学习这样一个 $\pi_\theta$，论文采用了 KL 自适应版本的 DPG：只有在估计的策略 $\pi_\theta$ 接近 $p$ 时才更新
    $q$。这种自适应步骤对于快速收敛很重要。
- en: '![](../Images/e410eeee9770600e9811ddd725df732e.png)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e410eeee9770600e9811ddd725df732e.png)'
- en: 'Fig. 25\. The algorithm of distributional policy gradient to make it possible
    to sample from a EBM $P(x)$, where $q$ is initialized to be $a$. (Image source:
    [Khalifa, et al. 2020](https://arxiv.org/abs/2012.11635))'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 图 25\. 分布策略梯度算法，使得从 EBM $P(x)$ 中采样成为可能，其中 $q$ 初始化为 $a$。（图片来源：[Khalifa, et al.
    2020](https://arxiv.org/abs/2012.11635)）
- en: 'This approach can be used to model various constraints in controllable text
    generation:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法可以用于建模可控文本生成中的各种约束：
- en: 'Pointwise constraints: $\phi_i$ is a binary feature; such as constraining the
    presence or absence of words, or classifier-based constraints.'
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点对点约束：$\phi_i$ 是一个二元特征；例如，约束单词的存在或缺失，或基于分类器的约束。
- en: 'Distributional constraints: $\phi_i$ represents a probability distribution;
    such as constraining the probability of gender, topic, etc. Their experiments
    showed great progress in debiasing a GPT-2 model that was trained on Wikipedia
    Biographies corpus. The percentage of generated biographies on females increased
    from 7.4% to 35.6%.'
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分布约束：$\phi_i$ 表示一个概率分布；例如，约束性别、主题等的概率。他们的实验显示，在训练于维基百科传记语料库的 GPT-2 模型中，去偏见取得了巨大进展。生成的女性传记的比例从
    7.4% 增加到 35.6%。
- en: 'Hybrid constraints: combine multiple constraints by simply summing them up.'
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 混合约束：通过简单求和结合多个约束。
- en: '![](../Images/6a6859110052e5b15d26e19a3f150bb0.png)'
  id: totrans-269
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6a6859110052e5b15d26e19a3f150bb0.png)'
- en: 'Fig. 26\. Debiasing experiments using GDC with various constraints. (Image
    source: [Khalifa, et al. 2020](https://arxiv.org/abs/2012.11635))'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 图 26\. 使用各种约束进行去偏见实验的 GDC。（图片来源：[Khalifa, et al. 2020](https://arxiv.org/abs/2012.11635)）
- en: Compared to other baselines, GDC using pointwise constraints diverges less from
    the base model $a$ and produces smoother curves.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他基线相比，使用点对点约束的 GDC 与基准模型 $a$ 的偏离较小，并产生更平滑的曲线。
- en: '![](../Images/a11ead0fcc2fafe29e40a1437acd4cf3.png)'
  id: totrans-272
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a11ead0fcc2fafe29e40a1437acd4cf3.png)'
- en: 'Fig. 27\. Compare pointwise constrained GDC with several baselines. Low Self-BLEU-5
    and high Dist-1 indicate high diversity. (Image source: [Khalifa, et al. 2020](https://arxiv.org/abs/2012.11635))'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 图 27\. 将点对点约束的 GDC 与几个基线进行比较。低 Self-BLEU-5 和高 Dist-1 表示高多样性。（图片来源：[Khalifa,
    et al. 2020](https://arxiv.org/abs/2012.11635)）
- en: REINFORCE that optimizes the reward $\phi$ directly ($\text{REINFORCE}$ in Fig.
    X.) without constraints converges fast but has a high deviation from the original
    model.
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 直接优化奖励 $\phi$（图 X 中的 $\text{REINFORCE}$）而没有约束的 REINFORCE 收敛速度快，但与原始模型的偏差较大。
- en: REINFORCE that optimizes $P(x)$ ($\text{REINFORCE}_{P(x)}$ in Fig. X.) has low
    sample diversity.
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化$P(x)$（$\text{REINFORCE}_{P(x)}$在图X中）的REINFORCE具有较低的样本多样性。
- en: Compared to [Ziegler et al., 2019](https://arxiv.org/abs/1909.08593) GDC has
    smoother learning curves and produces a richer vocabulary.
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与[Ziegler等人，2019年](https://arxiv.org/abs/1909.08593)相比，GDC具有更平滑的学习曲线，并产生更丰富的词汇。
- en: Unlikelihood Training
  id: totrans-277
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 不可能性训练
- en: The standard way of maximizing the log-likelihood loss in language model training
    leads to [incorrect token distribution](#beam-search-surprise), which cannot be
    fixed with only smart decoding methods. Such models tend to output high-frequency
    words too often and low-frequency words too rarely, especially when using deterministic
    decoding (e.g. greedy, beam search). In other words, they are overconfident in
    their predictions.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 在语言模型训练中，通过最大化对数似然损失的标准方法会导致[不正确的标记分布](#beam-search-surprise)，这不能仅通过智能解码方法来修复。这种模型往往会过于频繁地输出高频词汇，而很少输出低频词汇，特别是在使用确定性解码（例如贪婪、波束搜索）时。换句话说，它们对自己的预测过于自信。
- en: 'Unlikelihood training ([Welleck & Kulikov et al. 2019](https://arxiv.org/abs/1908.04319)]
    tries to combat this and incorporates preference to *unwanted* content into the
    training objective directly. It combines two updates:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 不可能性训练（[Welleck & Kulikov等人，2019年](https://arxiv.org/abs/1908.04319)）试图解决这个问题，并将对*不需要的*内容的偏好直接纳入训练目标中。它结合了两种更新：
- en: A routine maximized likelihood update to assign true tokens with high probability;
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一种常规的最大似然更新，为真实标记分配高概率；
- en: A new type of unlikelihood update to avoid unwanted tokens with high probability.
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一种新型的不可能性更新，以避免高概率的不需要的标记。
- en: 'Given a sequence of tokens $(x_1, \dots, x_T)$ and a set of negative candidate
    tokens $\mathcal{C}^t = \{c_1, \dots , c_m\}$ at step $t$, where each token $x_i,
    c_j \in \mathcal{V}$, the combined loss for step $t$ is defined as:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一系列标记$(x_1, \dots, x_T)$和步骤$t$处的一组负候选标记$\mathcal{C}^t = \{c_1, \dots , c_m\}$，其中每个标记$x_i,
    c_j \in \mathcal{V}$，步骤$t$的组合损失定义如下：
- en: $$ \mathcal{L}^t_\text{UL}(p_\theta (. \vert x_{
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \mathcal{L}^t_\text{UL}(p_\theta (. \vert x_{
- en: One approach for constructing $\mathcal{C}^t$ is to randomly select candidates
    from model-generated sequences.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 构建$\mathcal{C}^t$的一种方法是从模型生成的序列中随机选择候选项。
- en: 'The unlikelihood training can be extended to be on the *sequence*-level, where
    the negative continuation is defined by a sequence of per-step negative candidate
    sets. They should be designed to penalize properties that we don’t like. For example,
    we can penalize repeating n-grams as follows:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 不可能性训练可以扩展到*序列*级别，其中负继续由每步负候选集合序列定义。它们应该被设计为惩罚我们不喜欢的属性。例如，我们可以如下惩罚重复的n-gram：
- en: $$ \mathcal{C}^t_\text{repeat-n} = \{x_t\} \text{ if }(x_{t-i}, \dots, x_{t+j})
    \in x_{
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \mathcal{C}^t_\text{repeat-n} = \{x_t\} \text{ if }(x_{t-i}, \dots, x_{t+j})
    \in x_{
- en: Their experiments used unlikelihood training to avoid repetitions in language
    model outputs and indeed showed better results on less repetition and more unique
    tokens compared to standard MLE training.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '他们的实验使用不可能性训练来避免语言模型输出中的重复，并确实显示出与标准MLE训练相比，重复较少且唯一标记更多的更好结果。 '
- en: Citation
  id: totrans-288
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引用
- en: 'Cited as:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 被引用为：
- en: Weng, Lilian. (Jan 2021). Controllable neural text generation. Lil’Log. https://lilianweng.github.io/posts/2021-01-02-controllable-text-generation/.
  id: totrans-290
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Weng，Lilian。 （2021年1月）。可控神经文本生成。Lil’Log。 https://lilianweng.github.io/posts/2021-01-02-controllable-text-generation/。
- en: Or
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 或
- en: '[PRE0]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: References
  id: totrans-293
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Patrick von Platen. [“How to generate text: using different decoding methods
    for language generation with Transformers”](https://huggingface.co/blog/how-to-generate)
    Hugging face blog, March 18, 2020.'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Patrick von Platen。[“如何生成文本：使用不同的解码方法进行变压器语言生成”](https://huggingface.co/blog/how-to-generate)
    Hugging face博客，2020年3月18日。'
- en: '[2] Angela Fan, et al. [“Hierarchical Neural Story Generation/”](https://arxiv.org/abs/1805.04833)
    arXiv preprint arXiv:1805.04833 (2018).'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Angela Fan等人。[“分层神经故事生成”](https://arxiv.org/abs/1805.04833) arXiv预印本 arXiv:1805.04833
    (2018)。'
- en: '[3] Ari Holtzman et al. [“The Curious Case of Neural Text Degeneration.”](https://arxiv.org/abs/1904.09751)
    ICLR 2020.'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Ari Holtzman等人。[“神经文本退化的好奇案例。”](https://arxiv.org/abs/1904.09751) ICLR
    2020。'
- en: '[4] Marjan Ghazvininejad et al. [“Hafez: an interactive poetry generation system.”](https://www.aclweb.org/anthology/P17-4008)
    ACL 2017.'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Marjan Ghazvininejad等人。[“Hafez：一个交互式诗歌生成系统。”](https://www.aclweb.org/anthology/P17-4008)
    ACL 2017。'
- en: '[5] Ari Holtzman et al. [“Learning to write with cooperative discriminators.”](https://arxiv.org/abs/1805.06087)
    ACL 2018.'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Ari Holtzman等人。[“学习与合作鉴别器写作。”](https://arxiv.org/abs/1805.06087) ACL 2018。'
- en: '[6] Ashutosh Baheti et al. [“Generating More Interesting Responses in Neural
    Conversation Models with Distributional Constraints.”](https://arxiv.org/abs/1809.01215)
    EMNLP 2018.'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Ashutosh Baheti等人 [“通过分布约束在神经对话模型中生成更有趣的回复。”](https://arxiv.org/abs/1809.01215)
    EMNLP 2018.'
- en: '[7] Jiatao Gu et al. [“Trainable greedy decoding for neural machine translation.”](https://arxiv.org/abs/1702.02429)
    EMNLP 2017.'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] 顾家涛等人 [“神经机器翻译的可训练贪婪解码。”](https://arxiv.org/abs/1702.02429) EMNLP 2017.'
- en: '[8] Kyunghyun Cho. [“Noisy Parallel Approximate Decoding for Conditional Recurrent
    Language Model.”](https://arxiv.org/abs/1605.03835) arXiv preprint arXiv:1605.03835\.
    (2016).'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] Kyunghyun Cho. [“用于条件递归语言模型的嘈杂并行近似解码。”](https://arxiv.org/abs/1605.03835)
    arXiv预印本 arXiv:1605.03835\. (2016).'
- en: '[9] Marco Tulio Ribeiro et al. [“Semantically equivalent adversarial rules
    for debugging NLP models.”](https://www.aclweb.org/anthology/P18-1079/) ACL 2018.'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] Marco Tulio Ribeiro等人 [“用于调试NLP模型的语义等效对抗规则。”](https://www.aclweb.org/anthology/P18-1079/)
    ACL 2018.'
- en: '[10] Eric Wallace et al. [“Universal Adversarial Triggers for Attacking and
    Analyzing NLP.”](https://arxiv.org/abs/1908.07125) EMNLP 2019\. [[code](https://github.com/Eric-Wallace/universal-triggers)]'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] Eric Wallace等人 [“用于攻击和分析NLP的通用对抗触发器。”](https://arxiv.org/abs/1908.07125)
    EMNLP 2019\. [[code](https://github.com/Eric-Wallace/universal-triggers)]'
- en: '[11] Taylor Shin et al. [“AutoPrompt: Eliciting Knowledge from Language Models
    with Automatically Generated Prompts.”](https://arxiv.org/abs/2010.15980) EMNLP
    2020\. [[code](http://ucinlp.github.io/autoprompt)]'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] Taylor Shin等人 [“AutoPrompt: 通过自动生成提示从语言模型中引出知识。”](https://arxiv.org/abs/2010.15980)
    EMNLP 2020\. [[code](http://ucinlp.github.io/autoprompt)]'
- en: '[12] Zhengbao Jiang et al. [“How Can We Know What Language Models Know?”](https://arxiv.org/abs/1911.12543)
    TACL 2020.'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: '[12] 蒋正宝等人 [“我们如何知道语言模型知道什么？”](https://arxiv.org/abs/1911.12543) TACL 2020.'
- en: '[13] Nanyun Peng et al. [“Towards Controllable Story Generation.”](https://www.aclweb.org/anthology/W18-1505/)
    NAACL 2018.'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: '[13] 彭南云等人 [“可控故事生成的探索。”](https://www.aclweb.org/anthology/W18-1505/) NAACL
    2018.'
- en: '[14] Nitish Shirish Keskar, et al. [“CTRL: A Conditional Transformer Language
    Model for Controllable Generation”](https://arxiv.org/abs/1909.05858) arXiv preprint
    arXiv:1909.05858 (2019).[[code](https://github.com/salesforce/ctrl)]'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: '[14] Nitish Shirish Keskar等人 [“CTRL：用于可控生成的条件变压器语言模型”](https://arxiv.org/abs/1909.05858)
    arXiv预印本 arXiv:1909.05858 (2019).[[code](https://github.com/salesforce/ctrl)]'
- en: '[15] Marc’Aurelio Ranzato et al. [“Sequence Level Training with Recurrent Neural
    Networks.”](https://arxiv.org/abs/1511.06732) ICLR 2016.'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: '[15] Marc’Aurelio Ranzato等人 [“使用循环神经网络进行序列级训练。”](https://arxiv.org/abs/1511.06732)
    ICLR 2016.'
- en: '[16] Yonghui Wu et al. [“Google’s Neural Machine Translation System: Bridging
    the Gap between Human and Machine Translation.”](https://arxiv.org/abs/1609.08144)
    CoRR 2016.'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: '[16] Yonghui Wu等人 [“谷歌的神经机器翻译系统：弥合人类和机器翻译之间的差距。”](https://arxiv.org/abs/1609.08144)
    CoRR 2016.'
- en: '[17] Romain Paulus et al. [“A Deep Reinforced Model for Abstractive Summarization.”](https://arxiv.org/abs/1705.04304)
    ICLR 2018.'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: '[17] Romain Paulus等人 [“用于抽象摘要的深度强化模型。”](https://arxiv.org/abs/1705.04304) ICLR
    2018.'
- en: '[18] Paul Christiano et al. [“Deep Reinforcement Learning from Human Preferences.”](https://arxiv.org/abs/1706.03741)
    NIPS 2017.'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '[18] Paul Christiano等人 [“从人类偏好中进行深度强化学习。”](https://arxiv.org/abs/1706.03741)
    NIPS 2017.'
- en: '[19] Sanghyun Yi et al. [“Towards coherent and engaging spoken dialog response
    generation using automatic conversation evaluators.”](https://arxiv.org/abs/1904.13015)
    INLG 2019.'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: '[19] Sanghyun Yi等人 [“利用自动对话评估器进行连贯而引人入胜的口语对话响应生成。”](https://arxiv.org/abs/1904.13015)
    INLG 2019.'
- en: '[20] Florian Böhm et al. [“Better rewards yield better summaries: Learning
    to summarise without references.”](https://arxiv.org/abs/1909.01214) EMNLP 2019\.
    [[code](https://github.com/yg211/summary-reward-no-reference)]'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: '[20] Florian Böhm等人 [“更好的奖励带来更好的摘要：学习在没有参考文献的情况下进行摘要。”](https://arxiv.org/abs/1909.01214)
    EMNLP 2019\. [[code](https://github.com/yg211/summary-reward-no-reference)]'
- en: '[21] Daniel M Ziegler et al. [“Fine-tuning language models from human preferences.”](https://arxiv.org/abs/1909.08593)
    arXiv preprint arXiv:1909.08593 (2019). [[code](https://github.com/openai/lm-human-preferences)]'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '[21] Daniel M Ziegler等人 [“从人类偏好中微调语言模型。”](https://arxiv.org/abs/1909.08593)
    arXiv预印本 arXiv:1909.08593 (2019). [[code](https://github.com/openai/lm-human-preferences)]'
- en: '[22] Nisan Stiennon, et al. [“Learning to summarize from human feedback.”](https://arxiv.org/abs/2009.01325)
    arXiv preprint arXiv:2009.01325 (2020).'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: '[22] Nisan Stiennon等人 [“从人类反馈中学习总结。”](https://arxiv.org/abs/2009.01325) arXiv预印本
    arXiv:2009.01325 (2020).'
- en: '[23] Sumanth Dathathri et al. [“Plug and play language models: a simple approach
    to controlled text generation.”](https://arxiv.org/abs/1912.02164) ICLR 2020\.
    [[code](https://github.com/uber-research/PPLM)]'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: '[23] Sumanth Dathathri 等人 [“即插即用语言模型：一种简单的受控文本生成方法。”](https://arxiv.org/abs/1912.02164)
    ICLR 2020\. [[code](https://github.com/uber-research/PPLM)]'
- en: '[24] Jeffrey O Zhang et al. [“Side-tuning: Network adaptation via additive
    side networks”](https://arxiv.org/abs/1912.13503) ECCV 2020.'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: '[24] Jeffrey O Zhang 等人 [“侧调整：通过附加侧网络进行网络适应”](https://arxiv.org/abs/1912.13503)
    ECCV 2020.'
- en: '[25] Ben Kruse et al. [“GeDi: Generative Discriminator Guided Sequence Generation.”](https://arxiv.org/abs/2009.06367)
    arXiv preprint arXiv:2009.06367.'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: '[25] Ben Kruse 等人 [“GeDi：生成鉴别器引导的序列生成。”](https://arxiv.org/abs/2009.06367)
    arXiv 预印本 arXiv:2009.06367.'
- en: '[26] Yoel Zeldes et al. [“Technical Report: Auxiliary Tuning and its Application
    to Conditional Text Generatio.”](https://arxiv.org/abs/2006.16823) arXiv preprint
    arXiv:2006.16823.'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: '[26] Yoel Zeldes 等人 [“技术报告：辅助调整及其在条件文本生成中的应用。”](https://arxiv.org/abs/2006.16823)
    arXiv 预印本 arXiv:2006.16823.'
- en: '[27] Thomas Scialom, et al. [“Discriminative Adversarial Search for Abstractive
    Summarization”](https://arxiv.org/abs/2002.10375) ICML 2020.'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: '[27] Thomas Scialom 等人 [“用于抽象摘要的判别式对抗搜索”](https://arxiv.org/abs/2002.10375)
    ICML 2020.'
- en: '[28] Clara Meister, et al. [“If beam search is the answer, what was the question?”](https://arxiv.org/abs/2010.02650)
    EMNLP 2020.'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: '[28] Clara Meister 等人 [“如果束搜索是答案，问题是什么？”](https://arxiv.org/abs/2010.02650)
    EMNLP 2020.'
- en: '[29] Xiang Lisa Li and Percy Liang. [“Prefix-Tuning: Optimizing Continuous
    Prompts for Generation.”](https://arxiv.org/abs/2101.00190) arXiv preprint arXiv:2101.00190
    (2021).'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: '[29] Xiang Lisa Li 和 Percy Liang. [“前缀调整：优化连续提示以进行生成。”](https://arxiv.org/abs/2101.00190)
    arXiv 预印本 arXiv:2101.00190 (2021).'
- en: '[30] Lianhui Qin, et al. [“Back to the Future: Unsupervised Backprop-based
    Decoding for Counterfactual and Abductive Commonsense Reasoning.”](https://arxiv.org/abs/2010.05906)
    arXiv preprint arXiv:2010.05906 (2020).'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: '[30] Lianhui Qin 等人 [“回到未来：无监督反向传播解码用于反事实和推理常识推理。”](https://arxiv.org/abs/2010.05906)
    arXiv 预印本 arXiv:2010.05906 (2020).'
- en: '[31] Muhammad Khalifa, et al. [“A Distributional Approach to Controlled Text
    Generation”](https://arxiv.org/abs/2012.11635) Accepted by ICLR 2021.'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: '[31] Muhammad Khalifa 等人 [“一种分布式方法用于受控文本生成”](https://arxiv.org/abs/2012.11635)
    ICLR 2021 接受.'
- en: '[32] Aditya Grover, et al. [“Bias correction of learned generative models using
    likelihood-free importance weighting.”](https://arxiv.org/abs/1906.09531) NeuriPS
    2019.'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: '[32] Aditya Grover 等人 [“使用无似然重要性加权对学习生成模型进行偏差校正。”](https://arxiv.org/abs/1906.09531)
    NeuriPS 2019.'
- en: '[33] Yuntian Deng et al. [“Residual Energy-Based Models for Text Generation.”](https://arxiv.org/abs/2004.11714)
    ICLR 2020.'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: '[33] Yuntian Deng 等人 [“基于残余能量的文本生成模型。”](https://arxiv.org/abs/2004.11714) ICLR
    2020.'
- en: '[34] Brian Lester et al. [“The Power of Scale for Parameter-Efficient Prompt
    Tuning.”](https://arxiv.org/abs/2104.08691) arXiv preprint arXiv:2104.08691 (2021).'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: '[34] Brian Lester 等人 [“参数高效提示调整的规模优势。”](https://arxiv.org/abs/2104.08691) arXiv
    预印本 arXiv:2104.08691 (2021).'
- en: '[35] Xiao Liu et al. [“GPT Understands, Too.”](https://arxiv.org/abs/2103.10385)
    arXiv preprint arXiv:2103.10385 (2021).'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: '[35] Xiao Liu 等人 [“GPT 也能理解。”](https://arxiv.org/abs/2103.10385) arXiv 预印本
    arXiv:2103.10385 (2021).'
- en: '[36] Welleck & Kulikov et al. [“Neural Text Generation with Unlikelihood Training”](https://arxiv.org/abs/1908.04319)
    arXiv:1908.04319 (2019).'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: '[36] Welleck & Kulikov 等人 [“使用不可能性训练的神经文本生成”](https://arxiv.org/abs/1908.04319)
    arXiv:1908.04319 (2019).'
- en: '[nlp](https://lilianweng.github.io/tags/nlp/)'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[nlp](https://lilianweng.github.io/tags/nlp/)'
- en: '[language-model](https://lilianweng.github.io/tags/language-model/)'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[language-model](https://lilianweng.github.io/tags/language-model/)'
- en: '[alignment](https://lilianweng.github.io/tags/alignment/)'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[alignment](https://lilianweng.github.io/tags/alignment/)'
- en: '[steerability](https://lilianweng.github.io/tags/steerability/)'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[steerability](https://lilianweng.github.io/tags/steerability/)'
- en: '[reinforcement-learning](https://lilianweng.github.io/tags/reinforcement-learning/)'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[reinforcement-learning](https://lilianweng.github.io/tags/reinforcement-learning/)'
- en: '[long-read](https://lilianweng.github.io/tags/long-read/)'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[long-read](https://lilianweng.github.io/tags/long-read/)'
- en: '[«'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: '[«'
- en: Reducing Toxicity in Language Models](https://lilianweng.github.io/posts/2021-03-21-lm-toxicity/)
    [»
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 减少语言模型中的毒性](https://lilianweng.github.io/posts/2021-03-21-lm-toxicity/) [»
- en: How to Build an Open-Domain Question Answering System?](https://lilianweng.github.io/posts/2020-10-29-odqa/)[](https://twitter.com/intent/tweet/?text=Controllable%20Neural%20Text%20Generation&url=https%3a%2f%2flilianweng.github.io%2fposts%2f2021-01-02-controllable-text-generation%2f&hashtags=nlp%2clanguage-model%2calignment%2csteerability%2creinforcement-learning%22%2clong-read)[](https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2flilianweng.github.io%2fposts%2f2021-01-02-controllable-text-generation%2f&title=Controllable%20Neural%20Text%20Generation&summary=Controllable%20Neural%20Text%20Generation&source=https%3a%2f%2flilianweng.github.io%2fposts%2f2021-01-02-controllable-text-generation%2f)[](https://reddit.com/submit?url=https%3a%2f%2flilianweng.github.io%2fposts%2f2021-01-02-controllable-text-generation%2f&title=Controllable%20Neural%20Text%20Generation)[](https://facebook.com/sharer/sharer.php?u=https%3a%2f%2flilianweng.github.io%2fposts%2f2021-01-02-controllable-text-generation%2f)[](https://api.whatsapp.com/send?text=Controllable%20Neural%20Text%20Generation%20-%20https%3a%2f%2flilianweng.github.io%2fposts%2f2021-01-02-controllable-text-generation%2f)[](https://telegram.me/share/url?text=Controllable%20Neural%20Text%20Generation&url=https%3a%2f%2flilianweng.github.io%2fposts%2f2021-01-02-controllable-text-generation%2f)©
    2024 [Lil'Log](https://lilianweng.github.io/) Powered by [Hugo](https://gohugo.io/)
    & [PaperMod](https://git.io/hugopapermod)[](#top "Go to Top (Alt + G)")
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 如何构建一个开放领域问答系统？](https://lilianweng.github.io/posts/2020-10-29-odqa/)[](https://twitter.com/intent/tweet/?text=可控神经文本生成&url=https%3a%2f%2flilianweng.github.io%2fposts%2f2021-01-02-controllable-text-generation%2f&hashtags=nlp%2clanguage-model%2calignment%2csteerability%2creinforcement-learning%22%2clong-read)[](https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2flilianweng.github.io%2fposts%2f2021-01-02-controllable-text-generation%2f&title=可控神经文本生成&summary=可控神经文本生成&source=https%3a%2f%2flilianweng.github.io%2fposts%2f2021-01-02-controllable-text-generation%2f)[](https://reddit.com/submit?url=https%3a%2f%2flilianweng.github.io%2fposts%2f2021-01-02-controllable-text-generation%2f&title=可控神经文本生成)[](https://facebook.com/sharer/sharer.php?u=https%3a%2f%2flilianweng.github.io%2fposts%2f2021-01-02-controllable-text-generation%2f)[](https://api.whatsapp.com/send?text=可控神经文本生成%20-%20https%3a%2f%2flilianweng.github.io%2fposts%2f2021-01-02-controllable-text-generation%2f)[](https://telegram.me/share/url?text=可控神经文本生成&url=https%3a%2f%2flilianweng.github.io%2fposts%2f2021-01-02-controllable-text-generation%2f)©
    2024 [Lil'Log](https://lilianweng.github.io/) Powered by [Hugo](https://gohugo.io/)
    & [PaperMod](https://git.io/hugopapermod)[](#top "返回顶部 (Alt + G)")
