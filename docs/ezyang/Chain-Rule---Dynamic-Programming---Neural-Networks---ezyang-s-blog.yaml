- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-07-01 18:17:46'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-07-01 18:17:46'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Chain Rule + Dynamic Programming = Neural Networks : ezyang’s blog'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 链式法则 + 动态规划 = 神经网络：ezyang的博客
- en: 来源：[http://blog.ezyang.com/2011/05/neural-networks/](http://blog.ezyang.com/2011/05/neural-networks/)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[http://blog.ezyang.com/2011/05/neural-networks/](http://blog.ezyang.com/2011/05/neural-networks/)
- en: '(Guess what Edward has in a week: Exams! The theming of these posts might have
    something to do with that...)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: （猜猜 Edward 一周有什么事情：考试！这些帖子的主题可能与此有关...）
- en: 'At this point in my life, I’ve taken a course on introductory artificial intelligence
    twice. (Not my fault: I happened to have taken MIT’s version before going to Cambridge,
    which also administers this material as part of the year 2 curriculum.) My first
    spin through 6.034 was a mixture of disbelief at how simple the algorithms were,
    indignation at their examination methods, and the vague sense at the end that
    I really should have paid more attention. My second time through, I managed to
    distill a lot more algorithmic content out of the course, since I wasn’t worrying
    as much about the details.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在我生命中的这一阶段，我已经两次上过介绍人工智能的课程。（不是我的错：在去剑桥之前，我碰巧已经上过 MIT 的版本，剑桥也将这些内容作为第二年课程的一部分进行教授。）我第一次学习
    6.034 时，对算法的简单感到难以置信，对考试方法感到愤怒，并在最后模糊感到我真的应该更加关注。第二次学习时，我设法从课程中提炼出更多的算法内容，因为我不再过多担心细节。
- en: 'The topic of today’s post is one such distillation of algorithmic content from
    the neural network learning process. Well, at least, for multilayer perceptrons—since
    that’s what usually gets studied as a case of neural networks. It should be noted
    that the perceptron is a really simple sort of mathematical function: it’s a multivariable
    function that takes as arguments a weight vector and an input vector, takes their
    dot product and runs the result through an activation function (which is usually
    chosen so that it has nice properties when differentiated.) “Learning” in this
    case is first-order optimization via gradient descent, and the primarily computational
    content involves calculating the partial derivative of the function with respect
    to the weight vector—something that anyone who has taken multivariable calculus
    ought to be able to do in his sleep.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 今天的帖子主题是从神经网络学习过程中提炼出的算法内容。嗯，至少是对于多层感知器来说是这样——因为这通常是作为神经网络案例研究的一部分被研究的。值得注意的是，感知器实际上是一种非常简单的数学函数：它是一个多变量函数，它以一个权重向量和一个输入向量为参数，取它们的点积并通过激活函数（通常选择使其在微分时具有良好性质的函数）运行结果。“学习”在这种情况下是通过梯度下降进行的一阶优化，主要的计算内容涉及计算函数对权重向量的偏导数——这是任何学过多元微积分的人应该能够轻而易举做到的事情。
- en: 'Note that I say *ought*. Actually, neural networks gave me a pretty horrendous
    time both times I had to learn it. Part of the trouble is that once you’ve worked
    out the update formulas, you don’t actually need to understand the derivation:
    they “just work.” Of course, no self-respecting course would want to quiz you
    on your ability to memorize the relevant equations, so they’ll usually ask you
    to write out the derivation. There you run into the second trouble: most presentations
    of the derivation are quite long and don’t “compress” well.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我说的是*应该*。实际上，神经网络两次让我非常头痛，因为每次学习它时都很艰难。问题的一部分在于，一旦你计算出更新公式，你其实不需要理解推导过程：它们“就这样运行”。当然，任何值得尊敬的课程都不会只问你记忆相关方程的能力，所以它们通常会要求你写出推导过程。在这里，你会遇到第二个问题：大多数推导的呈现都相当冗长，不太容易“压缩”。
- en: 'The first insight into the process, which I (eventually) picked up the first
    time I took the course, was that these derivations were actually just repeatedly
    applying the chain rule. Thus, the laborious analysis of all of the partial derivatives
    can be replaced with the following algorithm: “Chop the perceptron into smaller
    functions, calculate the derivative of each function, and then multiply the results
    back together.” Now, this does require a little bit of care: one normally visualizes
    the perceptron network as a function on the input values, but the derivative is
    with respect to the weights. Furthermore, the perceptron network is a much more
    involved partial differentiation problem than one usually finds on a multivariable
    calculus exam, so if you don’t have your variable indexing sorted out it’s very
    easy to get confused. (Here, a notion of fresh names and global names comes in
    handy, because it sets the ground rules for notational sleights of hands that
    mathematicians do freely and confusingly.) If you have the chain rule in your
    arsenal, you have a fairly convincing story for the output perceptron, and with
    a little bit more confusion you might manage the the internal perceptrons too.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 首次深入了解到的过程，这是我第一次学习这门课程时（最终）掌握的内容，是这些推导实际上只是重复应用链规则。因此，所有偏导数的繁琐分析可以用以下算法替代：“将感知机切割成较小的函数，计算每个函数的导数，然后将结果相乘。”
    现在，这确实需要一点小心：人们通常将感知机网络视为输入值的函数，但导数是相对于权重的。此外，感知机网络是一个比通常在多变量微积分考试中找到的更为复杂的偏导数问题，因此，如果您的变量索引没有弄清楚，很容易感到困惑。（在这里，新名称和全局名称的概念非常有用，因为它为数学家自由且令人困惑的符号戏法设定了基本规则。）如果您掌握了链规则，您对输出感知机就有了一个相当令人信服的解释，并且再加上一点混乱，您可能也能应对内部感知机。
- en: 'The second insight into the process I didn’t pick up until my second time around:
    it is the resemblance of backpropagation to dynamic programming. This involved
    the realization that, in principle, I could calculate the partial derivative of
    the function with respect to any weight simply by tracing out the nodes “downstream”
    from it, and calculating the (longer) derivative chains manually. I could do this
    for every node, although it might get a bit tedious: the key idea of “backpropagation”
    is that you can reuse results for an efficiency increase, just as you do for dynamic
    programming. It is also gratifying to see that this explains why both treatments
    I’ve seen of neural nets obsess over δ, a seemingly innocuous derivative that
    really shouldn’t get its own symbol. The reason is this value is precisely what
    is stored in the dynamic programming table (in this case, shaped the same way
    as the input neural net); the actual partial derivative for a weight isn’t actually
    what we need. This is actually fairly common, as far as contest dynamic programming
    problems go—part of the trick is figuring out what intermediate calculations you
    also need to store in your table. Backpropagation is then just filling out the
    table from the output node to the input nodes.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 第二次深入了解到的过程直到第二次我才明白：反向传播与动态规划的相似性。这涉及到一种认识，即原则上，我可以通过跟踪“下游”节点并手动计算（更长的）导数链来计算函数对任何权重的偏导数。我可以为每个节点这样做，尽管这可能有点乏味：“反向传播”的关键思想是您可以重复使用结果以提高效率，就像动态规划一样。看到这一点也很令人满意，这解释了为什么我看过的神经网络两种处理都过于强调δ，一个看似无害的导数，实际上不应该有自己的符号。原因是这个值恰好存储在动态规划表中（在这种情况下，形状与输入神经网络相同）；权重的实际偏导数实际上并不是我们所需要的。这在竞赛动态规划问题中是相当常见的一点——其中一部分技巧是找出还需要在表中存储的中间计算。然后，反向传播只是从输出节点向输入节点填写表格。
- en: 'So there you have it: chain rule + dynamic programming = neural network backpropagation
    algorithm. Of course, this formulation requires you to know how to do the chain
    rule, and know how to do dynamic programming, but I find these concepts much easier
    to keep in my brain, and their combination pleasantly trivial.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你明白了：链规则 + 动态规划 = 神经网络反向传播算法。当然，这种表述要求您知道如何执行链规则，以及如何进行动态规划，但我发现这些概念要更容易记住，它们的结合也非常平凡。
- en: '*Postscript.* No lecturer can resist the temptation to expound on what they
    think “artificial intelligence” is. I’ll take this opportunity to chime in: I
    believe that AI is both a problem and an approach:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '*后记.* 没有讲师能抵挡住阐述他们对“人工智能”看法的诱惑。我会趁此机会插话一句：我认为人工智能既是一个问题也是一种方法：'
- en: Artificial intelligence is a problem, insofar as asking the question “What can
    humans do that computers cannot” is a tremendous way of digging up computationally
    interesting problems, and
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人工智能是一个问题，因为问“人类能做什么计算机做不了”这个问题是挖掘计算上有趣问题的一种重要方式，和
- en: Artificial intelligence is an approach, insofar as instances of intelligence
    in nature suggest possible solutions to computational problems.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人工智能是一种方法，因为自然界的智能实例暗示了计算问题的可能解决方案。
- en: I have tremendous respect for the power of AI to frame what questions researchers
    should be asking, and if we say an approach is AI because it handles a problem
    in this domain quite well, AI is everywhere. (It also explains why AI thrives
    at MIT, a very engineering oriented school.) I am still, however, skeptical about
    “biological inspiration”, since these approaches doesn’t actually seem to work
    that well (e.g. the fall of “traditional” AI and the rise of statistical NLP methods),
    and the fact that the resulting methods are a far cry from their biological counterparts,
    as any neuroscientist who is familiar with “neural” networks may attest. In some
    cases, the biological analogies may be actively harmful, obscuring the core mathematical
    issues.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我非常尊重人工智能的力量，它能够指导研究者提出应该问什么问题，如果我们说一种方法是人工智能，因为它在这个领域处理问题的能力相当不错，那么人工智能无处不在。（这也解释了为什么人工智能在麻省理工学院，一个非常工程导向的学校，如此蓬勃发展。）然而，我对“生物启发”仍然持怀疑态度，因为这些方法似乎并不那么有效（例如，“传统”人工智能的衰落和统计自然语言处理方法的崛起），而且由此产生的方法与其生物学对应物大相径庭，任何熟悉“神经”网络的神经科学家都会证明这一点。在某些情况下，生物类比可能会有积极有害作用，掩盖了核心的数学问题。
