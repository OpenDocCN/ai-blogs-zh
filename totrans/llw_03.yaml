- en: LLM Powered Autonomous Agents
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 以LLM为核心控制器的自主代理
- en: 原文：[https://lilianweng.github.io/posts/2023-06-23-agent/](https://lilianweng.github.io/posts/2023-06-23-agent/)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://lilianweng.github.io/posts/2023-06-23-agent/](https://lilianweng.github.io/posts/2023-06-23-agent/)
- en: Building agents with LLM (large language model) as its core controller is a
    cool concept. Several proof-of-concepts demos, such as [AutoGPT](https://github.com/Significant-Gravitas/Auto-GPT),
    [GPT-Engineer](https://github.com/AntonOsika/gpt-engineer) and [BabyAGI](https://github.com/yoheinakajima/babyagi),
    serve as inspiring examples. The potentiality of LLM extends beyond generating
    well-written copies, stories, essays and programs; it can be framed as a powerful
    general problem solver.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 以LLM（大型语言模型）为核心控制器构建代理是一个很酷的概念。几个概念验证演示，如[AutoGPT](https://github.com/Significant-Gravitas/Auto-GPT)、[GPT-Engineer](https://github.com/AntonOsika/gpt-engineer)和[BabyAGI](https://github.com/yoheinakajima/babyagi)，都是鼓舞人心的例子。LLM的潜力不仅限于生成写作精良的副本、故事、论文和程序；它可以被构想为一个强大的通用问题解决者。
- en: Agent System Overview
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 代理系统概述
- en: 'In a LLM-powered autonomous agent system, LLM functions as the agent’s brain,
    complemented by several key components:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在LLM驱动的自主代理系统中，LLM充当代理的大脑，辅以几个关键组件：
- en: '**Planning**'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**规划**'
- en: 'Subgoal and decomposition: The agent breaks down large tasks into smaller,
    manageable subgoals, enabling efficient handling of complex tasks.'
  id: totrans-6
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 子目标和分解：代理将大任务分解为更小、可管理的子目标，实现对复杂任务的高效处理。
- en: 'Reflection and refinement: The agent can do self-criticism and self-reflection
    over past actions, learn from mistakes and refine them for future steps, thereby
    improving the quality of final results.'
  id: totrans-7
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 反思和完善：代理可以对过去的行动进行自我批评和反思，从错误中学习并为未来步骤完善它们，从而提高最终结果的质量。
- en: '**Memory**'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**记忆**'
- en: 'Short-term memory: I would consider all the in-context learning (See [Prompt
    Engineering](https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/))
    as utilizing short-term memory of the model to learn.'
  id: totrans-9
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 短期记忆：我认为所有上下文学习（见[提示工程](https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/)）都是利用模型的短期记忆来学习。
- en: 'Long-term memory: This provides the agent with the capability to retain and
    recall (infinite) information over extended periods, often by leveraging an external
    vector store and fast retrieval.'
  id: totrans-10
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 长期记忆：这为代理提供了在较长时间内保留和召回（无限）信息的能力，通常通过利用外部向量存储和快速检索来实现。
- en: '**Tool use**'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**工具使用**'
- en: The agent learns to call external APIs for extra information that is missing
    from the model weights (often hard to change after pre-training), including current
    information, code execution capability, access to proprietary information sources
    and more.
  id: totrans-12
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代理学会调用外部API获取模型权重中缺失的额外信息（通常在预训练后难以更改），包括当前信息、代码执行能力、访问专有信息源等。
- en: '![](../Images/03ce43193968293a7a78e9c711977f47.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/03ce43193968293a7a78e9c711977f47.png)'
- en: Fig. 1\. Overview of a LLM-powered autonomous agent system.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图1. LLM驱动的自主代理系统概述。
- en: 'Component One: Planning'
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 组件一：规划
- en: A complicated task usually involves many steps. An agent needs to know what
    they are and plan ahead.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 一个复杂的任务通常涉及许多步骤。代理需要知道它们并提前计划。
- en: Task Decomposition
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 任务分解
- en: '[**Chain of thought**](https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/#chain-of-thought-cot)
    (CoT; [Wei et al. 2022](https://arxiv.org/abs/2201.11903)) has become a standard
    prompting technique for enhancing model performance on complex tasks. The model
    is instructed to “think step by step” to utilize more test-time computation to
    decompose hard tasks into smaller and simpler steps. CoT transforms big tasks
    into multiple manageable tasks and shed lights into an interpretation of the model’s
    thinking process.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[**思维链**](https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/#chain-of-thought-cot)（CoT；[魏等人
    2022](https://arxiv.org/abs/2201.11903)）已成为增强模型在复杂任务上性能的标准提示技术。该模型被指示“逐步思考”，利用更多的测试时间计算将困难任务分解为更小更简单的步骤。CoT将大任务转化为多个可管理的任务，并揭示了模型思考过程的解释。'
- en: '**Tree of Thoughts** ([Yao et al. 2023](https://arxiv.org/abs/2305.10601))
    extends CoT by exploring multiple reasoning possibilities at each step. It first
    decomposes the problem into multiple thought steps and generates multiple thoughts
    per step, creating a tree structure. The search process can be BFS (breadth-first
    search) or DFS (depth-first search) with each state evaluated by a classifier
    (via a prompt) or majority vote.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '**思维树**（[Yao et al. 2023](https://arxiv.org/abs/2305.10601)）通过在每一步探索多种推理可能性来扩展CoT。它首先将问题分解为多个思考步骤，并在每一步生成多个思考，创建一个树结构。搜索过程可以是BFS（广度优先搜索）或DFS（深度优先搜索），每个状态由分类器（通过提示）或多数投票评估。'
- en: Task decomposition can be done (1) by LLM with simple prompting like `"Steps
    for XYZ.\n1."`, `"What are the subgoals for achieving XYZ?"`, (2) by using task-specific
    instructions; e.g. `"Write a story outline."` for writing a novel, or (3) with
    human inputs.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 任务分解可以通过以下方式进行：（1）LLM通过简单提示如`"XYZ的步骤。\n1."`，`"实现XYZ的子目标是什么？"`，（2）使用任务特定的说明；例如，为写小说而写的`"写一个故事大纲。"`，或（3）通过人类输入。
- en: Another quite distinct approach, **LLM+P** ([Liu et al. 2023](https://arxiv.org/abs/2304.11477)),
    involves relying on an external classical planner to do long-horizon planning.
    This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate
    interface to describe the planning problem. In this process, LLM (1) translates
    the problem into “Problem PDDL”, then (2) requests a classical planner to generate
    a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the
    PDDL plan back into natural language. Essentially, the planning step is outsourced
    to an external tool, assuming the availability of domain-specific PDDL and a suitable
    planner which is common in certain robotic setups but not in many other domains.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种截然不同的方法，**LLM+P**（[Liu et al. 2023](https://arxiv.org/abs/2304.11477)）涉及依赖外部经典规划器进行长期规划。这种方法利用规划领域定义语言（PDDL）作为描述规划问题的中间接口。在这个过程中，LLM（1）将问题转化为“问题PDDL”，然后（2）请求经典规划器基于现有的“领域PDDL”生成PDDL计划，最后（3）将PDDL计划转化回自然语言。基本上，规划步骤被外部工具外包，假设领域特定的PDDL和适当的规划器是常见的，但在许多其他领域并非如此。
- en: Self-Reflection
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自我反思
- en: Self-reflection is a vital aspect that allows autonomous agents to improve iteratively
    by refining past action decisions and correcting previous mistakes. It plays a
    crucial role in real-world tasks where trial and error are inevitable.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 自我反思是一个至关重要的方面，它使自主代理能够通过改进过去的行动决策和纠正以前的错误来迭代改进。在试错不可避免的现实任务中，自我反思起着至关重要的作用。
- en: '**ReAct** ([Yao et al. 2023](https://arxiv.org/abs/2210.03629)) integrates
    reasoning and acting within LLM by extending the action space to be a combination
    of task-specific discrete actions and the language space. The former enables LLM
    to interact with the environment (e.g. use Wikipedia search API), while the latter
    prompting LLM to generate reasoning traces in natural language.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**ReAct**（[Yao et al. 2023](https://arxiv.org/abs/2210.03629)）通过将行动空间扩展为任务特定的离散动作和语言空间的组合，将推理和行动集成到LLM中。前者使LLM能够与环境互动（例如使用维基百科搜索API），而后者促使LLM生成自然语言中的推理轨迹。'
- en: 'The ReAct prompt template incorporates explicit steps for LLM to think, roughly
    formatted as:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ReAct提示模板包含LLM思考的明确步骤，大致格式如下：
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![](../Images/ee6b8b0938c86c2409e3a510635d06b0.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ee6b8b0938c86c2409e3a510635d06b0.png)'
- en: 'Fig. 2\. Examples of reasoning trajectories for knowledge-intensive tasks (e.g.
    HotpotQA, FEVER) and decision-making tasks (e.g. AlfWorld Env, WebShop). (Image
    source: [Yao et al. 2023](https://arxiv.org/abs/2210.03629)).'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图2。知识密集型任务（例如HotpotQA，FEVER）和决策任务（例如AlfWorld Env，WebShop）的推理轨迹示例（图片来源：[Yao et
    al. 2023](https://arxiv.org/abs/2210.03629)）。
- en: 'In both experiments on knowledge-intensive tasks and decision-making tasks,
    `ReAct` works better than the `Act`-only baseline where `Thought: …` step is removed.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '在知识密集型任务和决策任务的两个实验中，`ReAct`比仅有`Act`的基线效果更好，其中删除了`Thought: …`步骤。'
- en: '**Reflexion** ([Shinn & Labash 2023](https://arxiv.org/abs/2303.11366)) is
    a framework to equips agents with dynamic memory and self-reflection capabilities
    to improve reasoning skills. Reflexion has a standard RL setup, in which the reward
    model provides a simple binary reward and the action space follows the setup in
    ReAct where the task-specific action space is augmented with language to enable
    complex reasoning steps. After each action $a_t$, the agent computes a heuristic
    $h_t$ and optionally may *decide to reset* the environment to start a new trial
    depending on the self-reflection results.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**反思**（[Shinn & Labash 2023](https://arxiv.org/abs/2303.11366)）是一个框架，为代理提供动态记忆和自我反思能力，以提高推理能力。反思具有标准的RL设置，其中奖励模型提供简单的二进制奖励，行动空间遵循ReAct中的设置，其中任务特定的行动空间通过语言扩展以实现复杂的推理步骤。在每个动作$a_t$之后，代理计算启发式$h_t$，并根据自我反思结果可能*决定重置*环境以开始新的试验。'
- en: '![](../Images/089b2488d827b54841485d32f10d7790.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/089b2488d827b54841485d32f10d7790.png)'
- en: 'Fig. 3\. Illustration of the Reflexion framework. (Image source: [Shinn & Labash,
    2023](https://arxiv.org/abs/2303.11366))'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图3. 反思框架的示意图。（图片来源：[Shinn & Labash, 2023](https://arxiv.org/abs/2303.11366)）
- en: The heuristic function determines when the trajectory is inefficient or contains
    hallucination and should be stopped. Inefficient planning refers to trajectories
    that take too long without success. Hallucination is defined as encountering a
    sequence of consecutive identical actions that lead to the same observation in
    the environment.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 启发式函数确定轨迹是否低效或包含幻觉，并应停止。低效规划指的是花费太长时间而没有成功的轨迹。幻觉被定义为遇到一系列连续相同的动作，导致环境中出现相同的观察。
- en: Self-reflection is created by showing two-shot examples to LLM and each example
    is a pair of (failed trajectory, ideal reflection for guiding future changes in
    the plan). Then reflections are added into the agent’s working memory, up to three,
    to be used as context for querying LLM.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 自我反思是通过向LLM展示两个示例来创建的，每个示例是一对（失败的轨迹，用于指导未来计划变化的理想反思）。然后将反思添加到代理的工作记忆中，最多三个，用作查询LLM的上下文。
- en: '![](../Images/66dd5209dd47ec273c86c065d3c16312.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/66dd5209dd47ec273c86c065d3c16312.png)'
- en: 'Fig. 4\. Experiments on AlfWorld Env and HotpotQA. Hallucination is a more
    common failure than inefficient planning in AlfWorld. (Image source: [Shinn &
    Labash, 2023](https://arxiv.org/abs/2303.11366))'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图4. 在AlfWorld Env和HotpotQA上的实验。在AlfWorld中，幻觉比低效规划更常见。（图片来源：[Shinn & Labash,
    2023](https://arxiv.org/abs/2303.11366)）
- en: '**Chain of Hindsight** (CoH; [Liu et al. 2023](https://arxiv.org/abs/2302.02676))
    encourages the model to improve on its own outputs by explicitly presenting it
    with a sequence of past outputs, each annotated with feedback. Human feedback
    data is a collection of $D_h = \{(x, y_i , r_i , z_i)\}_{i=1}^n$, where $x$ is
    the prompt, each $y_i$ is a model completion, $r_i$ is the human rating of $y_i$,
    and $z_i$ is the corresponding human-provided hindsight feedback. Assume the feedback
    tuples are ranked by reward, $r_n \geq r_{n-1} \geq \dots \geq r_1$ The process
    is supervised fine-tuning where the data is a sequence in the form of $\tau_h
    = (x, z_i, y_i, z_j, y_j, \dots, z_n, y_n)$, where $\leq i \leq j \leq n$. The
    model is finetuned to only predict $y_n$ where conditioned on the sequence prefix,
    such that the model can self-reflect to produce better output based on the feedback
    sequence. The model can optionally receive multiple rounds of instructions with
    human annotators at test time.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**反事实链**（CoH；[Liu et al. 2023](https://arxiv.org/abs/2302.02676)）鼓励模型通过明确呈现一系列过去输出来改进自身输出，每个输出都附有反馈。人类反馈数据是一个集合$D_h
    = \{(x, y_i , r_i , z_i)\}_{i=1}^n$，其中$x$是提示，每个$y_i$是模型完成，$r_i$是人类对$y_i$的评分，$z_i$是相应的人类提供的反事实反馈。假设反馈元组按奖励排序，$r_n
    \geq r_{n-1} \geq \dots \geq r_1$。该过程是监督微调，数据是形式为$\tau_h = (x, z_i, y_i, z_j,
    y_j, \dots, z_n, y_n)$的序列，其中$\leq i \leq j \leq n$。模型被微调为仅预测$y_n$，在给定序列前缀的条件下，使得模型可以自我反思，根据反馈序列产生更好的输出。模型可以在测试时选择接收多轮指令与人类注释者。'
- en: To avoid overfitting, CoH adds a regularization term to maximize the log-likelihood
    of the pre-training dataset. To avoid shortcutting and copying (because there
    are many common words in feedback sequences), they randomly mask 0% - 5% of past
    tokens during training.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 为避免过拟合，CoH添加了一个正则化项，以最大化预训练数据集的对数似然。为避免捷径和复制（因为反馈序列中有许多常见词），他们在训练过程中随机屏蔽0% -
    5%的过去标记。
- en: The training dataset in their experiments is a combination of [WebGPT comparisons](https://huggingface.co/datasets/openai/webgpt_comparisons),
    [summarization from human feedback](https://github.com/openai/summarize-from-feedback)
    and [human preference dataset](https://github.com/anthropics/hh-rlhf).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在他们的实验中，训练数据集是[WebGPT比较](https://huggingface.co/datasets/openai/webgpt_comparisons)、[来自人类反馈的摘要](https://github.com/openai/summarize-from-feedback)和[人类偏好数据集](https://github.com/anthropics/hh-rlhf)的组合。
- en: '![](../Images/a4f010fef9ccf763d36f15fd11c0aabf.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a4f010fef9ccf763d36f15fd11c0aabf.png)'
- en: 'Fig. 5\. After fine-tuning with CoH, the model can follow instructions to produce
    outputs with incremental improvement in a sequence. (Image source: [Liu et al.
    2023](https://arxiv.org/abs/2302.02676))'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图5。在使用CoH进行微调后，模型可以按照指令产生逐步改进的输出序列。 （图片来源：[Liu等人，2023](https://arxiv.org/abs/2302.02676)）
- en: The idea of CoH is to present a history of sequentially improved outputs in
    context and train the model to take on the trend to produce better outputs. **Algorithm
    Distillation** (AD; [Laskin et al. 2023](https://arxiv.org/abs/2210.14215)) applies
    the same idea to cross-episode trajectories in reinforcement learning tasks, where
    an *algorithm* is encapsulated in a long history-conditioned policy. Considering
    that an agent interacts with the environment many times and in each episode the
    agent gets a little better, AD concatenates this learning history and feeds that
    into the model. Hence we should expect the next predicted action to lead to better
    performance than previous trials. The goal is to learn the process of RL instead
    of training a task-specific policy itself.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: CoH的想法是在上下文中呈现一系列逐步改进的输出历史，并训练模型跟随这种趋势产生更好的输出。**算法蒸馏**（AD；[Laskin等人，2023](https://arxiv.org/abs/2210.14215)）将相同的想法应用于强化学习任务中的跨集轨迹，其中一个*算法*被封装在一个长历史条件策略中。考虑到代理与环境的多次交互，每集中代理都会略微改进，AD将这些学习历史串联起来并将其馈送到模型中。因此，我们应该期望下一个预测的动作比之前的试验导致更好的性能。目标是学习RL的过程，而不是训练一个特定任务的策略本身。
- en: '![](../Images/45762432bbcfa0d9bc8f33552a9c6bb3.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/45762432bbcfa0d9bc8f33552a9c6bb3.png)'
- en: Fig. 6\. Illustration of how Algorithm Distillation (AD) works.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图6。展示了算法蒸馏（AD）的工作原理。
- en: '(Image source: [Laskin et al. 2023](https://arxiv.org/abs/2210.14215)).'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: （图片来源：[Laskin等人，2023](https://arxiv.org/abs/2210.14215)）。
- en: The paper hypothesizes that any algorithm that generates a set of learning histories
    can be distilled into a neural network by performing behavioral cloning over actions.
    The history data is generated by a set of source policies, each trained for a
    specific task. At the training stage, during each RL run, a random task is sampled
    and a subsequence of multi-episode history is used for training, such that the
    learned policy is task-agnostic.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 该论文假设，任何生成一组学习历史的算法都可以通过对动作执行行为克隆来蒸馏成神经网络。历史数据由一组源策略生成，每个策略针对特定任务进行训练。在训练阶段，每次RL运行时，会随机抽样一个任务，并使用多集历史的子序列进行训练，以便学到的策略是与任务无关的。
- en: In reality, the model has limited context window length, so episodes should
    be short enough to construct multi-episode history. Multi-episodic contexts of
    2-4 episodes are necessary to learn a near-optimal in-context RL algorithm. The
    emergence of in-context RL requires long enough context.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，模型具有有限的上下文窗口长度，因此每集应该足够短，以构建多集历史。2-4集的多集上下文对于学习接近最优的上下文RL算法是必要的。上下文RL的出现需要足够长的上下文。
- en: In comparison with three baselines, including ED (expert distillation, behavior
    cloning with expert trajectories instead of learning history), source policy (used
    for generating trajectories for distillation by [UCB](https://lilianweng.github.io/posts/2018-01-23-multi-armed-bandit/#upper-confidence-bounds)),
    RL^2 ([Duan et al. 2017](https://arxiv.org/abs/1611.02779); used as upper bound
    since it needs online RL), AD demonstrates in-context RL with performance getting
    close to RL^2 despite only using offline RL and learns much faster than other
    baselines. When conditioned on partial training history of the source policy,
    AD also improves much faster than ED baseline.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 与包括ED（专家蒸馏，使用专家轨迹而不是学习历史进行行为克隆）、源策略（用于通过[UCB](https://lilianweng.github.io/posts/2018-01-23-multi-armed-bandit/#upper-confidence-bounds)生成蒸馏轨迹的源策略）、RL^2（[Duan等人，2017](https://arxiv.org/abs/1611.02779)；作为上限，因为它需要在线RL）等三个基线相比，AD展示了在上下文中的RL，性能接近RL^2，尽管只使用离线RL，并且比其他基线学习速度更快。在部分源策略训练历史的条件下，AD也比ED基线改进得更快。
- en: '![](../Images/f6b0ce27ea6d0a5843cc0694709a0e86.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f6b0ce27ea6d0a5843cc0694709a0e86.png)'
- en: Fig. 7\. Comparison of AD, ED, source policy and RL^2 on environments that require
    memory and exploration. Only binary reward is assigned. The source policies are
    trained with [A3C](https://lilianweng.github.io/posts/2018-04-08-policy-gradient/#a3c)
    for "dark" environments and [DQN](http://lilianweng.github.io/posts/2018-02-19-rl-overview/#deep-q-network)
    for watermaze.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图7。在需要记忆和探索的环境中，AD、ED、源策略和RL^2的比较。仅分配二进制奖励。源策略是使用[A3C](https://lilianweng.github.io/posts/2018-04-08-policy-gradient/#a3c)在“黑暗”环境中训练的，水迷宫使用[DQN](http://lilianweng.github.io/posts/2018-02-19-rl-overview/#deep-q-network)。
- en: '(Image source: [Laskin et al. 2023](https://arxiv.org/abs/2210.14215))'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: （图片来源：[Laskin et al. 2023](https://arxiv.org/abs/2210.14215)）
- en: 'Component Two: Memory'
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二部分：记忆
- en: (Big thank you to ChatGPT for helping me draft this section. I’ve learned a
    lot about the human brain and data structure for fast MIPS in my [conversations](https://chat.openai.com/share/46ff149e-a4c7-4dd7-a800-fc4a642ea389)
    with ChatGPT.)
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: （非常感谢ChatGPT帮助我起草这一部分。在与ChatGPT的[对话](https://chat.openai.com/share/46ff149e-a4c7-4dd7-a800-fc4a642ea389)中，我学到了很多关于人类大脑和用于快速MIPS的数据结构。）
- en: Types of Memory
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 记忆类型
- en: Memory can be defined as the processes used to acquire, store, retain, and later
    retrieve information. There are several types of memory in human brains.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 记忆可以定义为用于获取、存储、保留和以后检索信息的过程。人类大脑中有几种类型的记忆。
- en: '**Sensory Memory**: This is the earliest stage of memory, providing the ability
    to retain impressions of sensory information (visual, auditory, etc) after the
    original stimuli have ended. Sensory memory typically only lasts for up to a few
    seconds. Subcategories include iconic memory (visual), echoic memory (auditory),
    and haptic memory (touch).'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**感觉记忆**：这是记忆的最早阶段，提供在原始刺激结束后保留感官信息（视觉、听觉等）的能力。感觉记忆通常只持续几秒钟。子类包括图像记忆（视觉）、回声记忆（听觉）和触觉记忆（触觉）。'
- en: '**Short-Term Memory** (STM) or **Working Memory**: It stores information that
    we are currently aware of and needed to carry out complex cognitive tasks such
    as learning and reasoning. Short-term memory is believed to have the capacity
    of about 7 items ([Miller 1956](psychclassics.yorku.ca/Miller/)) and lasts for
    20-30 seconds.'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**短期记忆**（STM）或**工作记忆**：它存储我们目前意识到并需要执行复杂认知任务的信息。据信，短期记忆的容量约为7个项目（[Miller 1956](psychclassics.yorku.ca/Miller/)），持续时间为20-30秒。'
- en: '**Long-Term Memory** (LTM): Long-term memory can store information for a remarkably
    long time, ranging from a few days to decades, with an essentially unlimited storage
    capacity. There are two subtypes of LTM:'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**长期记忆**（LTM）：长期记忆可以存储信息长达几天到几十年，具有基本无限的存储容量。长期记忆有两个子类型：'
- en: 'Explicit / declarative memory: This is memory of facts and events, and refers
    to those memories that can be consciously recalled, including episodic memory
    (events and experiences) and semantic memory (facts and concepts).'
  id: totrans-59
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 显性/陈述性记忆：这是关于事实和事件的记忆，指的是那些可以有意识地回忆起的记忆，包括情景记忆（事件和经历）和语义记忆（事实和概念）。
- en: 'Implicit / procedural memory: This type of memory is unconscious and involves
    skills and routines that are performed automatically, like riding a bike or typing
    on a keyboard.'
  id: totrans-60
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 隐性/程序性记忆：这种记忆是无意识的，涉及自动执行的技能和例行程序，如骑自行车或在键盘上打字。
- en: '![](../Images/860231b5c0b6aa8164971d3441fe3803.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/860231b5c0b6aa8164971d3441fe3803.png)'
- en: Fig. 8\. Categorization of human memory.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图8。人类记忆的分类。
- en: 'We can roughly consider the following mappings:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以粗略地考虑以下映射：
- en: Sensory memory as learning embedding representations for raw inputs, including
    text, image or other modalities;
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 感觉记忆作为学习原始输入的嵌入表示，包括文本、图像或其他模态；
- en: Short-term memory as in-context learning. It is short and finite, as it is restricted
    by the finite context window length of Transformer.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 短期记忆作为上下文学习。由于受Transformer有限上下文窗口长度的限制，它是短暂且有限的。
- en: Long-term memory as the external vector store that the agent can attend to at
    query time, accessible via fast retrieval.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 长期记忆作为外部向量存储，代理可以在查询时访问，通过快速检索可获得。
- en: Maximum Inner Product Search (MIPS)
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最大内积搜索（MIPS）
- en: The external memory can alleviate the restriction of finite attention span.
    A standard practice is to save the embedding representation of information into
    a vector store database that can support fast maximum inner-product search ([MIPS](https://en.wikipedia.org/wiki/Maximum_inner-product_search)).
    To optimize the retrieval speed, the common choice is the *approximate nearest
    neighbors (ANN)​* algorithm to return approximately top k nearest neighbors to
    trade off a little accuracy lost for a huge speedup.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 外部存储器可以缓解有限注意力跨度的限制。一个标准做法是将信息的嵌入表示保存到一个支持快速最大内积搜索（[MIPS](https://en.wikipedia.org/wiki/Maximum_inner-product_search)）的向量存储数据库中。为了优化检索速度，常见选择是*近似最近邻（ANN）*算法，返回大约前k个最近邻来权衡一点精度损失以换取巨大的加速度。
- en: 'A couple common choices of ANN algorithms for fast MIPS:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 用于快速MIPS的一些常见ANN算法：
- en: '[**LSH**](https://en.wikipedia.org/wiki/Locality-sensitive_hashing) (Locality-Sensitive
    Hashing): It introduces a *hashing* function such that similar input items are
    mapped to the same buckets with high probability, where the number of buckets
    is much smaller than the number of inputs.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**LSH**](https://en.wikipedia.org/wiki/Locality-sensitive_hashing)（Locality-Sensitive
    Hashing）：引入了一个*哈希*函数，使得相似的输入项以高概率映射到相同的桶中，其中桶的数量远远小于输入的数量。'
- en: '[**ANNOY**](https://github.com/spotify/annoy) (Approximate Nearest Neighbors
    Oh Yeah): The core data structure are *random projection trees*, a set of binary
    trees where each non-leaf node represents a hyperplane splitting the input space
    into half and each leaf stores one data point. Trees are built independently and
    at random, so to some extent, it mimics a hashing function. ANNOY search happens
    in all the trees to iteratively search through the half that is closest to the
    query and then aggregates the results. The idea is quite related to KD tree but
    a lot more scalable.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**ANNOY**](https://github.com/spotify/annoy)（Approximate Nearest Neighbors
    Oh Yeah）：核心数据结构是*随机投影树*，一组二叉树，其中每个非叶节点表示将输入空间分成两半的超平面，每个叶节点存储一个数据点。树是独立且随机构建的，因此在某种程度上，它模拟了一个哈希函数。ANNOY搜索发生在所有树中，通过迭代搜索最接近查询的一半，然后聚合结果。这个想法与KD树相关，但规模更大。'
- en: '[**HNSW**](https://arxiv.org/abs/1603.09320) (Hierarchical Navigable Small
    World): It is inspired by the idea of [small world networks](https://en.wikipedia.org/wiki/Small-world_network)
    where most nodes can be reached by any other nodes within a small number of steps;
    e.g. “six degrees of separation” feature of social networks. HNSW builds hierarchical
    layers of these small-world graphs, where the bottom layers contain the actual
    data points. The layers in the middle create shortcuts to speed up search. When
    performing a search, HNSW starts from a random node in the top layer and navigates
    towards the target. When it can’t get any closer, it moves down to the next layer,
    until it reaches the bottom layer. Each move in the upper layers can potentially
    cover a large distance in the data space, and each move in the lower layers refines
    the search quality.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**HNSW**](https://arxiv.org/abs/1603.09320)（Hierarchical Navigable Small World）：灵感来自于[小世界网络](https://en.wikipedia.org/wiki/Small-world_network)，其中大多数节点可以在很少的步骤内到达任何其他节点；例如社交网络的“六度分隔”特征。HNSW构建这些小世界图的分层结构，底层包含实际数据点。中间层创建快速搜索的快捷方式。在执行搜索时，HNSW从顶层的随机节点开始向目标导航。当无法更接近时，它移动到下一层，直到达到底层。上层的每次移动可能在数据空间中覆盖较大的距离，而下层的每次移动则提高了搜索质量。'
- en: '[**FAISS**](https://github.com/facebookresearch/faiss) (Facebook AI Similarity
    Search): It operates on the assumption that in high dimensional space, distances
    between nodes follow a Gaussian distribution and thus there should exist *clustering*
    of data points. FAISS applies vector quantization by partitioning the vector space
    into clusters and then refining the quantization within clusters. Search first
    looks for cluster candidates with coarse quantization and then further looks into
    each cluster with finer quantization.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**FAISS**](https://github.com/facebookresearch/faiss)（Facebook AI Similarity
    Search）：它基于这样的假设运行，即在高维空间中，节点之间的距离遵循高斯分布，因此应该存在数据点的*聚类*。FAISS通过将向量空间划分为簇并在簇内细化量化来应用向量量化。搜索首先寻找具有粗量化的簇候选，然后进一步查看每个簇的细量化。'
- en: '[**ScaNN**](https://github.com/google-research/google-research/tree/master/scann)
    (Scalable Nearest Neighbors): The main innovation in ScaNN is *anisotropic vector
    quantization*. It quantizes a data point $x_i$ to $\tilde{x}_i$ such that the
    inner product $\langle q, x_i \rangle$ is as similar to the original distance
    of $\angle q, \tilde{x}_i$ as possible, instead of picking the closet quantization
    centroid points.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**ScaNN**](https://github.com/google-research/google-research/tree/master/scann)（可扩展最近邻）：ScaNN
    中的主要创新是*各向异性向量量化*。它将数据点 $x_i$ 量化为 $\tilde{x}_i$，使得内积 $\langle q, x_i \rangle$
    尽可能与 $\angle q, \tilde{x}_i$ 的原始距离相似，而不是选择最接近的量化中心点。'
- en: '![](../Images/926f67eeac4a2afff0db630f84eb4d10.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/926f67eeac4a2afff0db630f84eb4d10.png)'
- en: 'Fig. 9\. Comparison of MIPS algorithms, measured in recall@10\. (Image source:
    [Google Blog, 2020](https://ai.googleblog.com/2020/07/announcing-scann-efficient-vector.html))'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9\. MIPS 算法的召回率@10比较。 （图片来源：[Google Blog, 2020](https://ai.googleblog.com/2020/07/announcing-scann-efficient-vector.html)）
- en: Check more MIPS algorithms and performance comparison in [ann-benchmarks.com](https://ann-benchmarks.com/).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在[ann-benchmarks.com](https://ann-benchmarks.com/)上查看更多 MIPS 算法和性能比较。
- en: 'Component Three: Tool Use'
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第三部分：工具使用
- en: Tool use is a remarkable and distinguishing characteristic of human beings.
    We create, modify and utilize external objects to do things that go beyond our
    physical and cognitive limits. Equipping LLMs with external tools can significantly
    extend the model capabilities.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 工具使用是人类的一个显著且独特的特征。我们创造、修改和利用外部物体来做超越我们身体和认知极限的事情。为LLM配备外部工具可以显著扩展模型的能力。
- en: '![](../Images/f324d3595b3c7bd413699d23bcb751c5.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f324d3595b3c7bd413699d23bcb751c5.png)'
- en: 'Fig. 10\. A picture of a sea otter using rock to crack open a seashell, while
    floating in the water. While some other animals can use tools, the complexity
    is not comparable with humans. (Image source: [Animals using tools](https://www.popularmechanics.com/science/animals/g39714258/animals-using-tools/))'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10\. 一只海獭使用石头打开贝壳的图片，同时漂浮在水中。虽然一些其他动物也能使用工具，但其复杂性无法与人类相比。（图片来源：[动物使用工具](https://www.popularmechanics.com/science/animals/g39714258/animals-using-tools/)）
- en: '**MRKL** ([Karpas et al. 2022](https://arxiv.org/abs/2205.00445)), short for
    “Modular Reasoning, Knowledge and Language”, is a neuro-symbolic architecture
    for autonomous agents. A MRKL system is proposed to contain a collection of “expert”
    modules and the general-purpose LLM works as a router to route inquiries to the
    best suitable expert module. These modules can be neural (e.g. deep learning models)
    or symbolic (e.g. math calculator, currency converter, weather API).'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '**MRKL**（[Karpas et al. 2022](https://arxiv.org/abs/2205.00445)），简称“模块化推理、知识和语言”，是用于自主代理的神经符号结构。
    提出了一个 MRKL 系统，其中包含一组“专家”模块，通用的 LLM 作为路由器将查询路由到最适合的专家模块。 这些模块可以是神经的（例如深度学习模型）或符号的（例如数学计算器、货币转换器、天气
    API）。'
- en: They did an experiment on fine-tuning LLM to call a calculator, using arithmetic
    as a test case. Their experiments showed that it was harder to solve verbal math
    problems than explicitly stated math problems because LLMs (7B Jurassic1-large
    model) failed to extract the right arguments for the basic arithmetic reliably.
    The results highlight when the external symbolic tools can work reliably, *knowing
    when to and how to use the tools are crucial*, determined by the LLM capability.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 他们对微调 LLM 以调用计算器进行了实验，以算术作为测试案例。 他们的实验表明，解决口头数学问题比明确陈述的数学问题更困难，因为 LLMs（7B Jurassic1-large
    模型）未能可靠地提取基本算术的正确参数。 结果突出了外部符号工具何时能够可靠工作，*知道何时以及如何使用工具至关重要*，由LLM的能力决定。
- en: Both **TALM** (Tool Augmented Language Models; [Parisi et al. 2022](https://arxiv.org/abs/2205.12255))
    and **Toolformer** ([Schick et al. 2023](https://arxiv.org/abs/2302.04761)) fine-tune
    a LM to learn to use external tool APIs. The dataset is expanded based on whether
    a newly added API call annotation can improve the quality of model outputs. See
    more details in the [“External APIs” section](https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/#external-apis)
    of Prompt Engineering.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '**TALM**（工具增强语言模型；[Parisi et al. 2022](https://arxiv.org/abs/2205.12255)）和
    **Toolformer**（[Schick et al. 2023](https://arxiv.org/abs/2302.04761)）都对 LM 进行微调，以学习使用外部工具
    API。 数据集根据新增的 API 调用注释是否能提高模型输出质量而扩展。 在 Prompt Engineering 的[“外部 API”部分](https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/#external-apis)中查看更多细节。'
- en: ChatGPT [Plugins](https://openai.com/blog/chatgpt-plugins) and OpenAI API [function
    calling](https://platform.openai.com/docs/guides/gpt/function-calling) are good
    examples of LLMs augmented with tool use capability working in practice. The collection
    of tool APIs can be provided by other developers (as in Plugins) or self-defined
    (as in function calls).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT [插件](https://openai.com/blog/chatgpt-plugins) 和 OpenAI API [函数调用](https://platform.openai.com/docs/guides/gpt/function-calling)
    是LLM增强了工具使用能力的实践示例。工具API的集合可以由其他开发者提供（如插件）或自定义（如函数调用）。
- en: '**HuggingGPT** ([Shen et al. 2023](https://arxiv.org/abs/2303.17580)) is a
    framework to use ChatGPT as the task planner to select models available in HuggingFace
    platform according to the model descriptions and summarize the response based
    on the execution results.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '**HuggingGPT**（[Shen等人，2023](https://arxiv.org/abs/2303.17580)）是一个框架，使用ChatGPT作为任务规划器，根据模型描述选择HuggingFace平台上可用的模型，并根据执行结果总结响应。'
- en: '![](../Images/b1c850628f87c5dd1195826d21a39715.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b1c850628f87c5dd1195826d21a39715.png)'
- en: 'Fig. 11\. Illustration of how HuggingGPT works. (Image source: [Shen et al.
    2023](https://arxiv.org/abs/2303.17580))'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图11。HuggingGPT工作原理示意图（图片来源：[Shen等人，2023](https://arxiv.org/abs/2303.17580)）
- en: 'The system comprises of 4 stages:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 系统包括4个阶段：
- en: '**(1) Task planning**: LLM works as the brain and parses the user requests
    into multiple tasks. There are four attributes associated with each task: task
    type, ID, dependencies, and arguments. They use few-shot examples to guide LLM
    to do task parsing and planning.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '**(1) 任务规划**：LLM作为大脑，将用户请求解析为多个任务。每个任务都有四个属性：任务类型、ID、依赖关系和参数。他们使用少量示例来指导LLM进行任务解析和规划。'
- en: 'Instruction:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 指导：
- en: 'The AI assistant can parse user input to several tasks: [{"task": task, "id",
    task_id, "dep": dependency_task_ids, "args": {"text": text, "image": URL, "audio":
    URL, "video": URL}}]. The "dep" field denotes the id of the previous task which
    generates a new resource that the current task relies on. A special tag "<resource>-task_id"
    refers to the generated text image, audio and video in the dependency task with
    id as task_id. The task MUST be selected from the following options: {{ Available
    Task List }}. There is a logical relationship between tasks, please note their
    order. If the user input can''t be parsed, you need to reply empty JSON. Here
    are several cases for your reference: {{ Demonstrations }}. The chat history is
    recorded as {{ Chat History }}. From this chat history, you can find the path
    of the user-mentioned resources for your task planning.</resource>'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 'AI助手可以将用户输入解析为多个任务：[{"task": task, "id", task_id, "dep": dependency_task_ids,
    "args": {"text": text, "image": URL, "audio": URL, "video": URL}}]。"dep"字段表示前一个任务的ID，生成当前任务依赖的新资源。特殊标签"<resource>-task_id"指的是依赖任务中生成的文本图像、音频和视频，其ID为task_id。任务必须从以下选项中选择：{{可用任务列表}}。任务之间存在逻辑关系，请注意它们的顺序。如果无法解析用户输入，您需要回复空的JSON。这里有几个案例供您参考：{{演示}}。聊天记录被记录为{{聊天记录}}。从这个聊天记录中，您可以找到用户提到的资源路径，用于您的任务规划。'
- en: '**(2) Model selection**: LLM distributes the tasks to expert models, where
    the request is framed as a multiple-choice question. LLM is presented with a list
    of models to choose from. Due to the limited context length, task type based filtration
    is needed.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '**(2) 模型选择**：LLM将任务分配给专家模型，其中请求被构建为一个多项选择问题。LLM被呈现一个模型列表供选择。由于上下文长度有限，需要基于任务类型进行过滤。'
- en: 'Instruction:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 指导：
- en: 'Given the user request and the call command, the AI assistant helps the user
    to select a suitable model from a list of models to process the user request.
    The AI assistant merely outputs the model id of the most appropriate model. The
    output must be in a strict JSON format: "id": "id", "reason": "your detail reason
    for the choice". We have a list of models for you to choose from {{ Candidate
    Models }}. Please select one model from the list.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '鉴于用户请求和调用命令，AI助手帮助用户从模型列表中选择一个合适的模型来处理用户请求。AI助手仅输出最合适模型的模型ID。输出必须采用严格的JSON格式："id":
    "id"，"reason": "您选择的原因"。我们为您提供了一个模型列表供您选择{{候选模型}}。请从列表中选择一个模型。'
- en: '**(3) Task execution**: Expert models execute on the specific tasks and log
    results.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '**(3) 任务执行**：专家模型在特定任务上执行并记录结果。'
- en: 'Instruction:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 指导：
- en: 'With the input and the inference results, the AI assistant needs to describe
    the process and results. The previous stages can be formed as - User Input: {{
    User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment
    }}, Task Execution: {{ Predictions }}. You must first answer the user''s request
    in a straightforward manner. Then describe the task process and show your analysis
    and model inference results to the user in the first person. If inference results
    contain a file path, must tell the user the complete file path.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 带有输入和推理结果，AI 助手需要描述过程和结果。前几个阶段可以形成为 - 用户输入：{{ 用户输入 }}, 任务规划：{{ 任务 }}, 模型选择：{{
    模型分配 }}, 任务执行：{{ 预测 }}。必须首先直接回答用户的请求。然后描述任务过程，并以第一人称向用户展示分析和模型推理结果。如果推理结果包含文件路径，必须告诉用户完整的文件路径。
- en: '**(4) Response generation**: LLM receives the execution results and provides
    summarized results to users.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '**(4) 响应生成**：LLM 接收执行结果并向用户提供总结结果。'
- en: 'To put HuggingGPT into real world usage, a couple challenges need to solve:
    (1) Efficiency improvement is needed as both LLM inference rounds and interactions
    with other models slow down the process; (2) It relies on a long context window
    to communicate over complicated task content; (3) Stability improvement of LLM
    outputs and external model services.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 要将 HuggingGPT 应用于现实世界中，需要解决一些挑战：（1）需要提高效率，因为 LLM 推理轮次和与其他模型的交互都会减慢流程；（2）它依赖于长上下文窗口来沟通复杂的任务内容；（3）需要改进
    LLM 输出和外部模型服务的稳定性。
- en: '**API-Bank** ([Li et al. 2023](https://arxiv.org/abs/2304.08244)) is a benchmark
    for evaluating the performance of tool-augmented LLMs. It contains 53 commonly
    used API tools, a complete tool-augmented LLM workflow, and 264 annotated dialogues
    that involve 568 API calls. The selection of APIs is quite diverse, including
    search engines, calculator, calendar queries, smart home control, schedule management,
    health data management, account authentication workflow and more. Because there
    are a large number of APIs, LLM first has access to API search engine to find
    the right API to call and then uses the corresponding documentation to make a
    call.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '**API-Bank**（[Li 等人 2023](https://arxiv.org/abs/2304.08244)）是用于评估工具增强 LLM 性能的基准。它包含
    53 个常用的 API 工具，完整的工具增强 LLM 工作流程，以及涉及 568 次 API 调用的 264 个带注释的对话。所选的 API 非常多样化，包括搜索引擎、计算器、日历查询、智能家居控制、日程管理、健康数据管理、账户认证工作流等。由于有大量的
    API，LLM 首先可以访问 API 搜索引擎找到正确的 API 调用，然后使用相应的文档进行调用。'
- en: '![](../Images/4d54c2f729573f060ba416830f2ac878.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4d54c2f729573f060ba416830f2ac878.png)'
- en: 'Fig. 12\. Pseudo code of how LLM makes an API call in API-Bank. (Image source:
    [Li et al. 2023](https://arxiv.org/abs/2304.08244))'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12\. LLM 在 API-Bank 中进行 API 调用的伪代码。（图片来源：[Li 等人 2023](https://arxiv.org/abs/2304.08244)）
- en: 'In the API-Bank workflow, LLMs need to make a couple of decisions and at each
    step we can evaluate how accurate that decision is. Decisions include:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在 API-Bank 工作流程中，LLM 需要做出一系列决策，每一步我们都可以评估该决策的准确性。决策包括：
- en: Whether an API call is needed.
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 是否需要 API 调用。
- en: 'Identify the right API to call: if not good enough, LLMs need to iteratively
    modify the API inputs (e.g. deciding search keywords for Search Engine API).'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确定要调用的正确 API：如果不够好，LLM 需要迭代修改 API 输入（例如为搜索引擎 API 决定搜索关键词）。
- en: 'Response based on the API results: the model can choose to refine and call
    again if results are not satisfied.'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于 API 结果的响应：如果结果不满意，模型可以选择优化并再次调用。
- en: 'This benchmark evaluates the agent’s tool use capabilities at three levels:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这个基准评估了代理工具在三个级别上的使用能力：
- en: Level-1 evaluates the ability to *call the API*. Given an API’s description,
    the model needs to determine whether to call a given API, call it correctly, and
    respond properly to API returns.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Level-1 评估了*调用 API*的能力。给定 API 的描述，模型需要确定是否调用给定的 API，正确调用它，并对 API 返回做出适当响应。
- en: Level-2 examines the ability to *retrieve the API*. The model needs to search
    for possible APIs that may solve the user’s requirement and learn how to use them
    by reading documentation.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Level-2 检查了*检索 API*的能力。模型需要搜索可能解决用户需求的 API，并通过阅读文档学习如何使用它们。
- en: Level-3 assesses the ability to *plan API beyond retrieve and call*. Given unclear
    user requests (e.g. schedule group meetings, book flight/hotel/restaurant for
    a trip), the model may have to conduct multiple API calls to solve it.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Level-3 评估了*计划 API 超越检索和调用*的能力。鉴于用户请求不明确（例如安排团体会议，为旅行预订航班/酒店/餐厅），模型可能需要进行多次
    API 调用来解决问题。
- en: Case Studies
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 案例研究
- en: Scientific Discovery Agent
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 科学发现代理
- en: '**ChemCrow** ([Bran et al. 2023](https://arxiv.org/abs/2304.05376)) is a domain-specific
    example in which LLM is augmented with 13 expert-designed tools to accomplish
    tasks across organic synthesis, drug discovery, and materials design. The workflow,
    implemented in [LangChain](https://github.com/hwchase17/langchain), reflects what
    was previously described in the [ReAct](#react) and [MRKLs](#mrkl) and combines
    CoT reasoning with tools relevant to the tasks:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '**ChemCrow**（[Bran等人，2023](https://arxiv.org/abs/2304.05376)）是一个领域特定的示例，其中LLM与13个专家设计的工具相结合，以完成有机合成，药物发现和材料设计等任务。
    在[LangChain](https://github.com/hwchase17/langchain)中实施的工作流程反映了先前在[ReAct](#react)和[MRKLs](#mrkl)中描述的内容，并将CoT推理与与任务相关的工具相结合：'
- en: The LLM is provided with a list of tool names, descriptions of their utility,
    and details about the expected input/output.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLM提供了一份工具名称列表，它们的实用描述以及有关预期输入/输出的详细信息。
- en: It is then instructed to answer a user-given prompt using the tools provided
    when necessary. The instruction suggests the model to follow the ReAct format
    - `Thought, Action, Action Input, Observation`.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后指示其使用必要的工具回答用户给定的提示。 指令建议模型遵循ReAct格式 - `思考，行动，行动输入，观察`。
- en: One interesting observation is that while the LLM-based evaluation concluded
    that GPT-4 and ChemCrow perform nearly equivalently, human evaluations with experts
    oriented towards the completion and chemical correctness of the solutions showed
    that ChemCrow outperforms GPT-4 by a large margin. This indicates a potential
    problem with using LLM to evaluate its own performance on domains that requires
    deep expertise. The lack of expertise may cause LLMs not knowing its flaws and
    thus cannot well judge the correctness of task results.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 一个有趣的观察是，虽然基于LLM的评估得出结论，GPT-4和ChemCrow的表现几乎相当，但与专家导向于解决方案的完成和化学正确性的人类评估表明，ChemCrow在很大程度上优于GPT-4。
    这表明在使用LLM评估其在需要深度专业知识的领域上的表现时可能存在问题。 缺乏专业知识可能导致LLM不了解其缺陷，因此无法很好地判断任务结果的正确性。
- en: '[Boiko et al. (2023)](https://arxiv.org/abs/2304.05332) also looked into LLM-empowered
    agents for scientific discovery, to handle autonomous design, planning, and performance
    of complex scientific experiments. This agent can use tools to browse the Internet,
    read documentation, execute code, call robotics experimentation APIs and leverage
    other LLMs.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '[Boiko等人（2023）](https://arxiv.org/abs/2304.05332)还研究了用于科学发现的LLM增强代理，以处理复杂科学实验的自主设计，规划和执行。
    该代理可以使用工具浏览互联网，阅读文档，执行代码，调用机器人实验API并利用其他LLM。'
- en: 'For example, when requested to `"develop a novel anticancer drug"`, the model
    came up with the following reasoning steps:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，当要求“开发一种新型抗癌药物”时，模型提出了以下推理步骤：
- en: inquired about current trends in anticancer drug discovery;
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 询问了当前抗癌药物发现的趋势；
- en: selected a target;
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择了一个目标；
- en: requested a scaffold targeting these compounds;
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请求了一个针对这些化合物的支架；
- en: Once the compound was identified, the model attempted its synthesis.
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦化合物被识别出来，模型就会尝试合成它。
- en: They also discussed the risks, especially with illicit drugs and bioweapons.
    They developed a test set containing a list of known chemical weapon agents and
    asked the agent to synthesize them. 4 out of 11 requests (36%) were accepted to
    obtain a synthesis solution and the agent attempted to consult documentation to
    execute the procedure. 7 out of 11 were rejected and among these 7 rejected cases,
    5 happened after a Web search while 2 were rejected based on prompt only.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 他们还讨论了风险，特别是非法药物和生物武器。 他们开发了一个测试集，其中包含一系列已知的化学武器剂，并要求代理人合成它们。 11个请求中有4个（36％）被接受以获得合成解决方案，并且代理人尝试查阅文档以执行该过程。
    11个中有7个被拒绝，而在这7个被拒绝的案例中，有5个发生在Web搜索之后，而有2个仅基于提示被拒绝。
- en: Generative Agents Simulation
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生成代理模拟
- en: '**Generative Agents** ([Park, et al. 2023](https://arxiv.org/abs/2304.03442))
    is super fun experiment where 25 virtual characters, each controlled by a LLM-powered
    agent, are living and interacting in a sandbox environment, inspired by The Sims.
    Generative agents create believable simulacra of human behavior for interactive
    applications.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '**生成代理**（[Park等人，2023](https://arxiv.org/abs/2304.03442)）是一个非常有趣的实验，其中有25个虚拟角色，每个角色由LLM驱动的代理控制，在一个受《模拟人生》启发的沙盒环境中生活和互动。
    生成代理为交互应用程序创建了可信的人类行为模拟。'
- en: The design of generative agents combines LLM with memory, planning and reflection
    mechanisms to enable agents to behave conditioned on past experience, as well
    as to interact with other agents.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 生成代理的设计结合了LLM、记忆、规划和反思机制，使代理能够根据过去的经验行为，以及与其他代理互动。
- en: '**Memory** stream: is a long-term memory module (external database) that records
    a comprehensive list of agents’ experience in natural language.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**记忆**流：是一个长期记忆模块（外部数据库），记录了自然语言中代理的全面经验列表。'
- en: Each element is an *observation*, an event directly provided by the agent. -
    Inter-agent communication can trigger new natural language statements.
  id: totrans-129
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个元素都是一个*观察*，是代理直接提供的事件。- 代理间通信可能触发新的自然语言陈述。
- en: '**Retrieval** model: surfaces the context to inform the agent’s behavior, according
    to relevance, recency and importance.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**检索**模型：根据相关性、最新性和重要性将上下文呈现给代理，以指导其行为。'
- en: 'Recency: recent events have higher scores'
  id: totrans-131
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最近性：最近的事件得分更高
- en: 'Importance: distinguish mundane from core memories. Ask LM directly.'
  id: totrans-132
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重要性：区分平凡的记忆和核心记忆。直接询问LM。
- en: 'Relevance: based on how related it is to the current situation / query.'
  id: totrans-133
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相关性：基于其与当前情况/查询的相关性。
- en: '**Reflection** mechanism: synthesizes memories into higher level inferences
    over time and guides the agent’s future behavior. They are *higher-level summaries
    of past events* (<- note that this is a bit different from [self-reflection](#self-reflection)
    above)'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**反思**机制：随着时间的推移，将记忆综合为更高层次的推断，并指导代理的未来行为。它们是*过去事件的高层摘要*（<-请注意，这与上面的[自我反思](#self-reflection)有所不同）'
- en: Prompt LM with 100 most recent observations and to generate 3 most salient high-level
    questions given a set of observations/statements. Then ask LM to answer those
    questions.
  id: totrans-135
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用最近的100个观察来提示LM，并在给定一组观察/陈述时生成3个最显著的高层问题。然后要求LM回答这些问题。
- en: '**Planning & Reacting**: translate the reflections and the environment information
    into actions'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**规划与反应**：将反思和环境信息转化为行动'
- en: Planning is essentially in order to optimize believability at the moment vs
    in time.
  id: totrans-137
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 规划主要是为了在当下和未来优化可信度。
- en: 'Prompt template: `{Intro of an agent X}. Here is X''s plan today in broad strokes:
    1)`'
  id: totrans-138
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提示模板：`{X代理的介绍}。这是X今天的整体计划：1)`
- en: Relationships between agents and observations of one agent by another are all
    taken into consideration for planning and reacting.
  id: totrans-139
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代理之间的关系以及一个代理对另一个代理的观察都被考虑在内，用于规划和反应。
- en: Environment information is present in a tree structure.
  id: totrans-140
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 环境信息以树结构呈现。
- en: '![](../Images/9b24b539d228abeecfe6e5353154308b.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9b24b539d228abeecfe6e5353154308b.png)'
- en: 'Fig. 13\. The generative agent architecture. (Image source: [Park et al. 2023](https://arxiv.org/abs/2304.03442))'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 图13. 生成代理架构。（图片来源：[Park等人2023](https://arxiv.org/abs/2304.03442)）
- en: This fun simulation results in emergent social behavior, such as information
    diffusion, relationship memory (e.g. two agents continuing the conversation topic)
    and coordination of social events (e.g. host a party and invite many others).
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这个有趣的模拟结果导致了新兴的社会行为，如信息扩散、关系记忆（例如两个代理继续谈论的话题）和社交事件的协调（例如举办派对并邀请许多其他人）。
- en: Proof-of-Concept Examples
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概念验证示例
- en: '[AutoGPT](https://github.com/Significant-Gravitas/Auto-GPT) has drawn a lot
    of attention into the possibility of setting up autonomous agents with LLM as
    the main controller. It has quite a lot of reliability issues given the natural
    language interface, but nevertheless a cool proof-of-concept demo. A lot of code
    in AutoGPT is about format parsing.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '[AutoGPT](https://github.com/Significant-Gravitas/Auto-GPT)引起了很多关注，因为它可能建立具有LLM作为主控制器的自主代理。鉴于自然语言界面，AutoGPT存在相当多的可靠性问题，但仍然是一个很酷的概念验证演示。AutoGPT中的许多代码都是关于格式解析的。'
- en: 'Here is the system message used by AutoGPT, where `{{...}}` are user inputs:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 这是AutoGPT使用的系统消息，其中`{{...}}`是用户输入：
- en: '[PRE1]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[GPT-Engineer](https://github.com/AntonOsika/gpt-engineer) is another project
    to create a whole repository of code given a task specified in natural language.
    The GPT-Engineer is instructed to think over a list of smaller components to build
    and ask for user input to clarify questions as needed.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '[GPT-Engineer](https://github.com/AntonOsika/gpt-engineer)是另一个项目，旨在根据自然语言中指定的任务创建一个完整的代码库。GPT-Engineer被指示思考一系列较小的组件构建，并根据需要要求用户澄清问题。'
- en: Here are a sample conversation for task clarification sent to OpenAI ChatCompletion
    endpoint used by GPT-Engineer. The user inputs are wrapped in `{{user input text}}`.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个发送给OpenAI ChatCompletion端点的任务澄清的样本对话，该端点由GPT-Engineer使用。用户输入被包裹在`{{用户输入文本}}`中。
- en: '[PRE2]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Then after these clarification, the agent moved into the code writing mode
    with a different system message. System message:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 然后在这些澄清之后，代理人以不同的系统消息进入了编写代码模式。系统消息：
- en: You will get instructions for code to write. You will write a very long answer.
    Make sure that every detail of the architecture is, in the end, implemented as
    code. Make sure that every detail of the architecture is, in the end, implemented
    as code.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 你将得到编写代码的指令。你将写一个非常长的答案。确保最终每个架构的细节都被实现为代码。确保最终每个架构的细节都被实现为代码。
- en: Think step by step and reason yourself to the right decisions to make sure we
    get it right. You will first lay out the names of the core classes, functions,
    methods that will be necessary, as well as a quick comment on their purpose.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 逐步思考并理性地做出正确的决定，以确保我们做对了。你将首先列出核心类、函数、方法的名称，以及对它们目的的快速注释。
- en: 'Then you will output the content of each file including ALL code. Each file
    must strictly follow a markdown code block format, where the following tokens
    must be replaced such that FILENAME is the lowercase file name including the file
    extension, LANG is the markup code block language for the code’s language, and
    CODE is the code:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你将输出每个文件的内容，包括所有代码。每个文件必须严格遵循markdown代码块格式，其中以下标记必须被替换，以便FILENAME是包括文件扩展名的小写文件名，LANG是代码语言的标记代码块语言，CODE是代码：
- en: FILENAME
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 文件名
- en: '[PRE3]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: You will start with the “entrypoint” file, then go to the ones that are imported
    by that file, and so on. Please note that the code should be fully functional.
    No placeholders.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 你将从“入口点”文件开始，然后转到被该文件导入的文件，依此类推。请注意，代码应该是完全可用的。没有占位符。
- en: Follow a language and framework appropriate best practice file naming convention.
    Make sure that files contain all imports, types etc. Make sure that code in different
    files are compatible with each other. Ensure to implement all code, if you are
    unsure, write a plausible implementation. Include module dependency or package
    manager dependency definition file. Before you finish, double check that all parts
    of the architecture is present in the files.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 遵循适当的最佳实践文件命名约定。确保文件包含所有导入、类型等。确保不同文件中的代码与彼此兼容。确保实现所有代码，如果不确定，写一个合理的实现。包括模块依赖或包管理器依赖定义文件。在完成之前，仔细检查所有架构的各个部分是否都在文件中。
- en: 'Useful to know: You almost always put different classes in different files.
    For Python, you always create an appropriate requirements.txt file. For NodeJS,
    you always create an appropriate package.json file. You always add a comment briefly
    describing the purpose of the function definition. You try to add comments explaining
    very complex bits of logic. You always follow the best practices for the requested
    languages in terms of describing the code written as a defined package/project.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 有用的知识：你几乎总是将不同的类放在不同的文件中。对于Python，你总是创建一个适当的requirements.txt文件。对于NodeJS，你总是创建一个适当的package.json文件。你总是添加一个简短描述函数定义目的的注释。你尝试添加解释非常复杂逻辑的注释。你总是遵循所请求语言的最佳实践，以描述编写的代码作为一个定义的包/项目。
- en: 'Python toolbelt preferences:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: Python工具包偏好：
- en: pytest
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: pytest
- en: dataclasses
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: dataclasses
- en: 'Conversatin samples:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 对话样本：
- en: '[PRE4]LANG\nCODE\n[PRE5]LANG\nCODE\n[PRE6]'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE4]LANG\nCODE\n[PRE5]LANG\nCODE\n[PRE6]'
- en: Challenges
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 挑战
- en: 'After going through key ideas and demos of building LLM-centered agents, I
    start to see a couple common limitations:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在经历了构建以LLM为中心的代理的关键思想和演示之后，我开始看到一些常见的限制：
- en: '**Finite context length**: The restricted context capacity limits the inclusion
    of historical information, detailed instructions, API call context, and responses.
    The design of the system has to work with this limited communication bandwidth,
    while mechanisms like self-reflection to learn from past mistakes would benefit
    a lot from long or infinite context windows. Although vector stores and retrieval
    can provide access to a larger knowledge pool, their representation power is not
    as powerful as full attention.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**有限的上下文长度**：受限的上下文容量限制了历史信息、详细说明、API调用上下文和响应的包含。系统的设计必须在这种有限的通信带宽下工作，而像自我反思这样的机制可以从过去的错误中学习，这将极大地受益于长或无限的上下文窗口。虽然向量存储和检索可以提供对更大知识库的访问，但它们的表示能力不如完全注意力强大。'
- en: '**Challenges in long-term planning and task decomposition**: Planning over
    a lengthy history and effectively exploring the solution space remain challenging.
    LLMs struggle to adjust plans when faced with unexpected errors, making them less
    robust compared to humans who learn from trial and error.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**长期规划和任务分解中的挑战**：在漫长的历史上进行规划并有效地探索解决方案空间仍然具有挑战性。 LLM在面对意外错误时很难调整计划，使它们比起能够通过试错学习的人类更不稳健。'
- en: '**Reliability of natural language interface**: Current agent system relies
    on natural language as an interface between LLMs and external components such
    as memory and tools. However, the reliability of model outputs is questionable,
    as LLMs may make formatting errors and occasionally exhibit rebellious behavior
    (e.g. refuse to follow an instruction). Consequently, much of the agent demo code
    focuses on parsing model output.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自然语言接口的可靠性**：当前代理系统依赖于自然语言作为LLM与记忆和工具等外部组件之间的接口。然而，模型输出的可靠性值得怀疑，因为LLM可能会出现格式错误，偶尔表现出叛逆行为（例如拒绝遵循指令）。因此，许多代理演示代码侧重于解析模型输出。'
- en: Citation
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引文
- en: 'Cited as:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 引用为：
- en: Weng, Lilian. (Jun 2023). LLM-powered Autonomous Agents". Lil’Log. https://lilianweng.github.io/posts/2023-06-23-agent/.
  id: totrans-172
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 翁，莉莉安。 (2023年6月). 由LLM驱动的自主代理。Lil’Log。[https://lilianweng.github.io/posts/2023-06-23-agent/](https://lilianweng.github.io/posts/2023-06-23-agent/).
- en: Or
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 或
- en: '[PRE7]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: References
  id: totrans-175
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考资料
- en: '[1] Wei et al. [“Chain of thought prompting elicits reasoning in large language
    models.”](https://arxiv.org/abs/2201.11903) NeurIPS 2022'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] 魏等人。[“思维链索引引发大型语言模型的推理。”](https://arxiv.org/abs/2201.11903) NeurIPS 2022'
- en: '[2] Yao et al. [“Tree of Thoughts: Dliberate Problem Solving with Large Language
    Models.”](https://arxiv.org/abs/2305.10601) arXiv preprint arXiv:2305.10601 (2023).'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] 姚等人。[“思维之树：与大型语言模型进行深思熟虑的问题解决。”](https://arxiv.org/abs/2305.10601) arXiv预印本
    arXiv:2305.10601 (2023).'
- en: '[3] Liu et al. [“Chain of Hindsight Aligns Language Models with Feedback “](https://arxiv.org/abs/2302.02676)
    arXiv preprint arXiv:2302.02676 (2023).'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] 刘等人。[“事后视角链将语言模型与反馈对齐”](https://arxiv.org/abs/2302.02676) arXiv预印本 arXiv:2302.02676
    (2023).'
- en: '[4] Liu et al. [“LLM+P: Empowering Large Language Models with Optimal Planning
    Proficiency”](https://arxiv.org/abs/2304.11477) arXiv preprint arXiv:2304.11477
    (2023).'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] 刘等人。[“LLM+P：赋予大型语言模型最佳规划能力”](https://arxiv.org/abs/2304.11477) arXiv预印本
    arXiv:2304.11477 (2023).'
- en: '[5] Yao et al. [“ReAct: Synergizing reasoning and acting in language models.”](https://arxiv.org/abs/2210.03629)
    ICLR 2023.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] 姚等人。[“ReAct：在语言模型中协同推理和行动。”](https://arxiv.org/abs/2210.03629) ICLR 2023.'
- en: '[6] Google Blog. [“Announcing ScaNN: Efficient Vector Similarity Search”](https://ai.googleblog.com/2020/07/announcing-scann-efficient-vector.html)
    July 28, 2020.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] 谷歌博客。[“宣布ScaNN：高效的向量相似性搜索”](https://ai.googleblog.com/2020/07/announcing-scann-efficient-vector.html)
    2020年7月28日。'
- en: '[7] [https://chat.openai.com/share/46ff149e-a4c7-4dd7-a800-fc4a642ea389](https://chat.openai.com/share/46ff149e-a4c7-4dd7-a800-fc4a642ea389)'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] [https://chat.openai.com/share/46ff149e-a4c7-4dd7-a800-fc4a642ea389](https://chat.openai.com/share/46ff149e-a4c7-4dd7-a800-fc4a642ea389)'
- en: '[8] Shinn & Labash. [“Reflexion: an autonomous agent with dynamic memory and
    self-reflection”](https://arxiv.org/abs/2303.11366) arXiv preprint arXiv:2303.11366
    (2023).'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] Shinn & Labash。[“反思：具有动态记忆和自我反思的自主代理”](https://arxiv.org/abs/2303.11366)
    arXiv预印本 arXiv:2303.11366 (2023).'
- en: '[9] Laskin et al. [“In-context Reinforcement Learning with Algorithm Distillation”](https://arxiv.org/abs/2210.14215)
    ICLR 2023.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] Laskin等人。[“具有算法蒸馏的上下文强化学习”](https://arxiv.org/abs/2210.14215) ICLR 2023.'
- en: '[10] Karpas et al. [“MRKL Systems A modular, neuro-symbolic architecture that
    combines large language models, external knowledge sources and discrete reasoning.”](https://arxiv.org/abs/2205.00445)
    arXiv preprint arXiv:2205.00445 (2022).'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] 卡帕斯等人。[“MRKL系统：将大型语言模型、外部知识源和离散推理相结合的模块化、神经符号架构。”](https://arxiv.org/abs/2205.00445)
    arXiv预印本 arXiv:2205.00445 (2022).'
- en: '[11] Weaviate Blog. [Why is Vector Search so fast?](https://weaviate.io/blog/why-is-vector-search-so-fast)
    Sep 13, 2022.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] Weaviate博客。[为什么向量搜索如此快？](https://weaviate.io/blog/why-is-vector-search-so-fast)
    2022年9月13日。'
- en: '[12] Li et al. [“API-Bank: A Benchmark for Tool-Augmented LLMs”](https://arxiv.org/abs/2304.08244)
    arXiv preprint arXiv:2304.08244 (2023).'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '[12] 李等人。[“API-Bank：用于工具增强LLM的基准”](https://arxiv.org/abs/2304.08244) arXiv预印本
    arXiv:2304.08244 (2023).'
- en: '[13] Shen et al. [“HuggingGPT: Solving AI Tasks with ChatGPT and its Friends
    in HuggingFace”](https://arxiv.org/abs/2303.17580) arXiv preprint arXiv:2303.17580
    (2023).'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '[13] 沈等人。[“HuggingGPT：使用ChatGPT及其HuggingFace朋友解决AI任务”](https://arxiv.org/abs/2303.17580)
    arXiv预印本 arXiv:2303.17580 (2023).'
- en: '[14] Bran et al. [“ChemCrow: Augmenting large-language models with chemistry
    tools.”](https://arxiv.org/abs/2304.05376) arXiv preprint arXiv:2304.05376 (2023).'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '[14] Bran等人。[“ChemCrow：用化学工具增强大型语言模型。”](https://arxiv.org/abs/2304.05376) arXiv预印本
    arXiv:2304.05376 (2023)。'
- en: '[15] Boiko et al. [“Emergent autonomous scientific research capabilities of
    large language models.”](https://arxiv.org/abs/2304.05332) arXiv preprint arXiv:2304.05332
    (2023).'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '[15] Boiko等人。[“大型语言模型的新兴自主科学研究能力。”](https://arxiv.org/abs/2304.05332) arXiv预印本
    arXiv:2304.05332 (2023)。'
- en: '[16] Joon Sung Park, et al. [“Generative Agents: Interactive Simulacra of Human
    Behavior.”](https://arxiv.org/abs/2304.03442) arXiv preprint arXiv:2304.03442
    (2023).'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '[16] Joon Sung Park等人。[“生成代理：人类行为的交互模拟。”](https://arxiv.org/abs/2304.03442)
    arXiv预印本 arXiv:2304.03442 (2023)。'
- en: '[17] AutoGPT. [https://github.com/Significant-Gravitas/Auto-GPT](https://github.com/Significant-Gravitas/Auto-GPT)'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '[17] AutoGPT。[https://github.com/Significant-Gravitas/Auto-GPT](https://github.com/Significant-Gravitas/Auto-GPT)'
- en: '[18] GPT-Engineer. [https://github.com/AntonOsika/gpt-engineer](https://github.com/AntonOsika/gpt-engineer)'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '[18] GPT-Engineer。[https://github.com/AntonOsika/gpt-engineer](https://github.com/AntonOsika/gpt-engineer)'
