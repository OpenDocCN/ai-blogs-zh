- en: Neural Architecture Search
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://lilianweng.github.io/posts/2020-08-06-nas/](https://lilianweng.github.io/posts/2020-08-06-nas/)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Although most popular and successful model architectures are designed by human
    experts, it doesn’t mean we have explored the entire network architecture space
    and settled down with the best option. We would have a better chance to find the
    optimal solution if we adopt a systematic and automatic way of learning high-performance
    model architectures.
  prefs: []
  type: TYPE_NORMAL
- en: Automatically learning and evolving network topologies is not a new idea ([Stanley
    & Miikkulainen, 2002](http://nn.cs.utexas.edu/downloads/papers/stanley.ec02.pdf)).
    In recent years, the pioneering work by [Zoph & Le 2017](https://arxiv.org/abs/1611.01578)
    and [Baker et al. 2017](https://arxiv.org/abs/1611.02167) has attracted a lot
    of attention into the field of Neural Architecture Search (NAS), leading to many
    interesting ideas for better, faster and more cost-efficient NAS methods.
  prefs: []
  type: TYPE_NORMAL
- en: As I started looking into NAS, I found this nice survey very helpful by [Elsken,
    et al 2019](https://arxiv.org/abs/1808.05377). They characterize NAS as a system
    with three major components, which is clean & concise, and also commonly adopted
    in other NAS papers.
  prefs: []
  type: TYPE_NORMAL
- en: '**Search space**: The NAS search space defines a set of operations (e.g. convolution,
    fully-connected, pooling) and how operations can be connected to form valid network
    architectures. The design of search space usually involves human expertise, as
    well as unavoidably human biases.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Search algorithm**: A NAS search algorithm samples a population of network
    architecture candidates. It receives the child model performance metrics as rewards
    (e.g. high accuracy, low latency) and optimizes to generate high-performance architecture
    candidates.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Evaluation strategy**: We need to measure, estimate, or predict the performance
    of a large number of proposed child models in order to obtain feedback for the
    search algorithm to learn. The process of candidate evaluation could be very expensive
    and many new methods have been proposed to save time or computation resources.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/f2ec97894b0a9be99ed47239193f62dc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 1\. Three main components of Neural Architecture Search (NAS) models.
    (Image source: [Elsken, et al. 2019](https://arxiv.org/abs/1808.05377) with customized
    annotation in red)'
  prefs: []
  type: TYPE_NORMAL
- en: Search Space
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The NAS search space defines a set of basic network operations and how operations
    can be connected to construct valid network architectures.
  prefs: []
  type: TYPE_NORMAL
- en: Sequential Layer-wise Operations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The most naive way to design the search space for neural network architectures
    is to depict network topologies, either CNN or RNN, with a list of *sequential
    layer-wise operations*, as seen in the early work of [Zoph & Le 2017](https://arxiv.org/abs/1611.01578)
    & [Baker et al. 2017](https://arxiv.org/abs/1611.02167). The serialization of
    network representation requires a decent amount of expert knowledge, since each
    operation is associated with different layer-specific parameters and such associations
    need to be hardcoded. For example, after predicting a `conv` op, the model should
    output kernel size, stride size, etc; or after predicting an `FC` op, we need
    to see the number of units as the next prediction.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/162cd5922b1fbf93bdca9c05fbaaf503.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 2\. (Top) A sequential representation of CNN. (Bottom) A sequential representation
    of the tree structure of a recurrent cell. (Image source: [Zoph & Le 2017](https://arxiv.org/abs/1611.01578))'
  prefs: []
  type: TYPE_NORMAL
- en: 'To make sure the generated architecture is valid, additional rules might be
    needed ([Zoph & Le 2017](https://arxiv.org/abs/1611.01578)):'
  prefs: []
  type: TYPE_NORMAL
- en: If a layer is not connected to any input layer then it is used as the input
    layer;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At the final layer, take all layer outputs that have not been connected and
    concatenate them;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If one layer has many input layers, then all input layers are concatenated in
    the depth dimension;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If input layers to be concatenated have different sizes, we pad the small layers
    with zeros so that the concatenated layers have the same sizes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The skip connection can be predicted as well, using an [attention](https://lilianweng.github.io/posts/2018-06-24-attention/)-style
    mechanism. At layer $i$ , an anchor point is added with $i−1$ content-based sigmoids
    to indicate which of the previous layers to be connected. Each sigmoid takes as
    input the hidden states of the current node $h_i$ and $i-1$ previous nodes $h_j,
    j=1, \dots, i-1$ .
  prefs: []
  type: TYPE_NORMAL
- en: $$ P(\text{Layer j is an input to layer i}) = \text{sigmoid}(v^\top \tanh(\mathbf{W}_\text{prev}
    h_j + \mathbf{W}_\text{curr} h_i)) $$
  prefs: []
  type: TYPE_NORMAL
- en: The sequential search space has a lot of representation power, but it is very
    large and consumes a ton of computation resources to exhaustively cover the search
    space. In the experiments by [Zoph & Le 2017](https://arxiv.org/abs/1611.01578),
    they were running 800 GPUs in parallel for 28 days and [Baker et al. 2017](https://arxiv.org/abs/1611.02167)
    restricted the search space to contain at most 2 `FC` layers.
  prefs: []
  type: TYPE_NORMAL
- en: Cell-based Representation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Inspired by the design of using repeated modules in successful vision model
    architectures (e.g. Inception, ResNet), the *NASNet search space* ([Zoph et al.
    2018](https://arxiv.org/abs/1707.07012)) defines the architecture of a conv net
    as the same cell getting repeated multiple times and each cell contains several
    operations predicted by the NAS algorithm. A well-designed cell module enables
    transferability between datasets. It is also easy to scale down or up the model
    size by adjusting the number of cell repeats.
  prefs: []
  type: TYPE_NORMAL
- en: 'Precisely, the NASNet search space learns two types of cells for network construction:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Normal Cell*: The input and output feature maps have the same dimension.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Reduction Cell*: The output feature map has its width and height reduced by
    half.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/430dd36492618672de78bd6322a3f3f7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 3\. The NASNet search space constrains the architecture as a repeated
    stack of cells. The cell architecture is optimized via NAS algorithms. (Image
    source: [Zoph et al. 2018](https://arxiv.org/abs/1707.07012))'
  prefs: []
  type: TYPE_NORMAL
- en: The predictions for each cell are grouped into $B$ blocks ($B=5$ in the NASNet
    paper), where each block has 5 prediction steps made by 5 distinct softmax classifiers
    corresponding to discrete choices of the elements of a block. Note that the NASNet
    search space does not have residual connections between cells and the model only
    learns skip connections on their own within blocks.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a7508c8b568570cd21a9ddc882fc8380.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig. 4\. (a) Each cell consists of $B$ blocks and each block is predicted by
    5 discrete decisions. (b) An concrete example of what operations can be chosen
    in each decision step.
  prefs: []
  type: TYPE_NORMAL
- en: During the experiments, they discovered that a modified version of [*DropPath*](https://arxiv.org/abs/1605.07648),
    named *ScheduledDropPath*, significantly improves the final performance of NASNet
    experiments. DropPath stochastically drops out paths (i.e. edges with operations
    attached in NASNet) with a fixed probability. ScheduledDropPath is DropPath with
    a linearly increasing probability of path dropping during training time.
  prefs: []
  type: TYPE_NORMAL
- en: '[Elsken, et al (2019)](https://arxiv.org/abs/1808.05377) point out three major
    advantages of the NASNet search space:'
  prefs: []
  type: TYPE_NORMAL
- en: The search space size is reduced drastically;
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The [motif](https://en.wikipedia.org/wiki/Network_motif)-based architecture
    can be more easily transferred to different datasets.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It demonstrates a strong proof of a useful design pattern of repeatedly stacking
    modules in architecture engineering. For example, we can build strong models by
    stacking residual blocks in CNN or stacking multi-headed attention blocks in Transformer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Hierarchical Structure
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To take advantage of already discovered well-designed network [motifs](https://en.wikipedia.org/wiki/Network_motif),
    the NAS search space can be constrained as a hierarchical structure, as in *Hierarchical
    NAS* (**HNAS**; ([Liu et al 2017](https://arxiv.org/abs/1711.00436))). It starts
    with a small set of primitives, including individual operations like convolution
    operation, pooling, identity, etc. Then small sub-graphs (or “motifs”) that consist
    of primitive operations are recursively used to form higher-level computation
    graphs.
  prefs: []
  type: TYPE_NORMAL
- en: 'A computation motif at level $\ell=1, \dots, L$ can be represented by $(G^{(\ell)},
    \mathcal{O}^{(\ell)})$, where:'
  prefs: []
  type: TYPE_NORMAL
- en: $\mathcal{O}^{(\ell)}$ is a set of operations, $\mathcal{O}^{(\ell)} = \{ o^{(\ell)}_1,
    o^{(\ell)}_2, \dots \}$
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $G^{(\ell)}$ is an adjacency matrix, where the entry $G_{ij}=k$ indicates that
    operation $o^{(\ell)}_k$ is placed between node $i$ and $j$. The node indices
    follow [topological ordering](https://en.wikipedia.org/wiki/Topological_sorting)
    in DAG, where the index $1$ is the source and the maximal index is the sink node.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/84eff081eea8da598c950e221697f152.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 5\. (Top) Three level-1 primitive operations are composed into a level-2
    motif. (Bottom) Three level-2 motifs are plugged into a base network structure
    and assembled into a level-3 motif. (Image source: [Liu et al 2017](https://arxiv.org/abs/1711.00436))'
  prefs: []
  type: TYPE_NORMAL
- en: To build a network according to the hierarchical structure, we start from the
    lowest level $\ell=1$ and recursively define the $m$-th motif operation at level
    $\ell$ as
  prefs: []
  type: TYPE_NORMAL
- en: $$ o^{(\ell)}_m = \text{assemble}\Big( G_m^{(\ell)}, \mathcal{O}^{(\ell-1)}
    \Big) $$
  prefs: []
  type: TYPE_NORMAL
- en: A hierarchical representation becomes $\Big( \big\{ \{ G_m^{(\ell)} \}_{m=1}^{M_\ell}
    \big\}_{\ell=2}^L, \mathcal{O}^{(1)} \Big), \forall \ell=2, \dots, L$, where $\mathcal{O}^{(1)}$
    contains a set of primitive operations.
  prefs: []
  type: TYPE_NORMAL
- en: 'The $\text{assemble}()$ process is equivalent to sequentially compute the feature
    map of node $i$ by aggregating all the feature maps of its predecessor node $j$
    following the topological ordering:'
  prefs: []
  type: TYPE_NORMAL
- en: $$ x_i = \text{merge} \big[ \{ o^{(\ell)}_{G^{(\ell)}_{ij}}(x_j) \}_{j < i}
    \big], i = 2, \dots, \vert G^{(\ell)} \vert $$
  prefs: []
  type: TYPE_NORMAL
- en: where $\text{merge}[]$ is implemented as depth-wise concatenation in the [paper](https://arxiv.org/abs/1711.00436).
  prefs: []
  type: TYPE_NORMAL
- en: Same as NASNet, experiments in [Liu et al (2017)](https://arxiv.org/abs/1711.00436)
    focused on discovering good cell architecture within a predefined “macro” structure
    with repeated modules. They showed that the power of simple search methods (e.g.
    random search or evolutionary algorithms) can be substantially enhanced using
    well-designed search spaces.
  prefs: []
  type: TYPE_NORMAL
- en: '[Cai et al (2018b)](https://arxiv.org/abs/1806.02639) propose a tree-structure
    search space using path-level network transformation. Each node in a tree structure
    defines an *allocation* scheme for splitting inputs for child nodes and a *merge*
    scheme for combining results from child nodes. The path-level network transformation
    allows replacing a single layer with a multi-branch motif if its corresponding
    merge scheme is add or concat.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d1f728ecbbe64329846e6ee01ebebeb4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 6\. An illustration of transforming a single layer to a tree-structured
    motif via path-level transformation operations. (Image source: [Cai et al. 2018b](https://arxiv.org/abs/1806.02639))'
  prefs: []
  type: TYPE_NORMAL
- en: Memory-bank Representation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A memory-bank representation of feed-forward networks is proposed by [Brock
    et al. (2017)](https://arxiv.org/abs/1708.05344) in [SMASH](#prediction-based).
    Instead of a graph of operations, they view a neural network as a system with
    multiple memory blocks which can read and write. Each layer operation is designed
    to: (1) read from a subset of memory blocks; (2) computes results; finally (3)
    write the results into another subset of blocks. For example, in a sequential
    model, a single memory block would get read and overwritten consistently.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/98ddaa10d53ff460523443f731fb1322.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 7\. Memory-bank representation of several popular network architecture
    blocks. (Image source: [Brock et al. 2017](https://arxiv.org/abs/1708.05344))'
  prefs: []
  type: TYPE_NORMAL
- en: Search Algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: NAS search algorithms sample a population of child networks. It receives the
    child models’ performance metrics as rewards and learns to generate high-performance
    architecture candidates. You may a lot in common with the field of hyperparameter
    search.
  prefs: []
  type: TYPE_NORMAL
- en: Random Search
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Random search is the most naive baseline. It samples a valid architecture candidate
    from the search space *at random* and no learning model is involved. Random search
    has proved to be quite useful in hyperparameter search ([Bergstra & Bengio 2012](http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf)).
    With a well-designed search space, random search could be a very challenging baseline
    to beat.
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The initial design of **NAS** ([Zoph & Le 2017](https://arxiv.org/abs/1611.01578))
    involves a RL-based controller for proposing child model architectures for evaluation.
    The controller is implemented as a RNN, outputting a variable-length sequence
    of tokens used for configuring a network architecture.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bd67d668ad813d590d090cc745133d79.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 8\. A high level overview of NAS, containing a RNN controller and a pipeline
    for evaluating child models. (Image source: [Zoph & Le 2017](https://arxiv.org/abs/1611.01578))'
  prefs: []
  type: TYPE_NORMAL
- en: The controller is trained as a *RL task* using [REINFORCE](https://lilianweng.github.io/posts/2018-04-08-policy-gradient/#reinforce).
  prefs: []
  type: TYPE_NORMAL
- en: '**Action space**: The action space is a list of tokens for defining a child
    network predicted by the controller (See more in the above [section](#sequential-layer-wise-operations)).
    The controller outputs *action*, $a_{1:T}$, where $T$ is the total number of tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reward**: The accuracy of a child network that can be achieved at convergence
    is the reward for training the controller, $R$.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Loss**: NAS optimizes the controller parameters $\theta$ with a REINFORCE
    loss. We want to maximize the expected reward (high accuracy) with the gradient
    as follows. The nice thing here with policy gradient is that it works even when
    the reward is non-differentiable.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $$ \nabla_{\theta} J(\theta) = \sum_{t=1}^T \mathbb{E}[\nabla_{\theta} \log
    P(a_t \vert a_{1:(t-1)}; \theta) R ] $$
  prefs: []
  type: TYPE_NORMAL
- en: '**MetaQNN** ([Baker et al. 2017](https://arxiv.org/abs/1611.02167)) trains
    an agent to sequentially choose CNN layers using [*Q-learning*](https://lilianweng.github.io/posts/2018-02-19-rl-overview/#q-learning-off-policy-td-control)
    with an [$\epsilon$-greedy](https://lilianweng.github.io/posts/2018-01-23-multi-armed-bandit/#%CE%B5-greedy-algorithm)
    exploration strategy and experience replay. The reward is the validation accuracy
    as well.'
  prefs: []
  type: TYPE_NORMAL
- en: $$ Q^{(t+1)}(s_t, a_t) = (1 - \alpha)Q^{(t)}(s_t, a_t) + \alpha (R_t + \gamma
    \max_{a \in \mathcal{A}} Q^{(t)}(s_{t+1}, a')) $$
  prefs: []
  type: TYPE_NORMAL
- en: where a state $s_t$ is a tuple of layer operation and related parameters. An
    action $a$ determines the connectivity between operations. The Q-value is proportional
    to how confident we are in two connected operations leading to high accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c509a9564129378d8df2cfbf47757f54.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 9\. Overview of MetaQNN - designing CNN models with Q-Learning. (Image
    source: [Baker et al. 2017](https://arxiv.org/abs/1611.02167))'
  prefs: []
  type: TYPE_NORMAL
- en: Evolutionary Algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**NEAT** (short for *NeuroEvolution of Augmenting Topologies*) is an approach
    for evolving neural network topologies with [genetic algorithm (GA)](https://en.wikipedia.org/wiki/Genetic_algorithm),
    proposed by [Stanley & Miikkulainen](http://nn.cs.utexas.edu/downloads/papers/stanley.ec02.pdf)
    in 2002\. NEAT evolves both connection weights and network topology together.
    Each gene encodes the full information for configuring a network, including node
    weights and edges. The population grows by applying mutation of both weights and
    connections, as well as crossover between two parent genes. For more in neuroevolution,
    please refer to the in-depth [survey](https://www.nature.com/articles/s42256-018-0006-z)
    by Stanley et al. (2019).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a34060a04a4d1ebf71a7dff595671ac4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 10\. Mutations in the NEAT algorithm. (Image source: Fig 3 & 4 in [Stanley
    & Miikkulainen, 2002](http://nn.cs.utexas.edu/downloads/papers/stanley.ec02.pdf))'
  prefs: []
  type: TYPE_NORMAL
- en: '[Real et al. (2018)](https://arxiv.org/abs/1802.01548) adopt the evolutionary
    algorithms (EA) as a way to search for high-performance network architectures,
    named **AmoebaNet**. They apply the [tournament selection](https://en.wikipedia.org/wiki/Tournament_selection)
    method, which at each iteration picks a best candidate out of a random set of
    samples and places its mutated offspring back into the population. When the tournament
    size is $1$, it is equivalent to random selection.'
  prefs: []
  type: TYPE_NORMAL
- en: AmoebaNet modified the tournament selection to favor *younger* genotypes and
    always discard the oldest models within each cycle. Such an approach, named *aging
    evolution*, allows AmoebaNet to cover and explore more search space, rather than
    to narrow down on good performance models too early.
  prefs: []
  type: TYPE_NORMAL
- en: 'Precisely, in every cycle of the tournament selection with aging regularization
    (See Figure 11):'
  prefs: []
  type: TYPE_NORMAL
- en: Sample $S$ models from the population and the one with highest accuracy is chosen
    as *parent*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A *child* model is produced by mutating *parent*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then the child model is trained, evaluated and added back into the population.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The oldest model is removed from the population.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/1bb4b9b74a716bf7e004900a0458d1c8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 11\. The algorithm of aging evolution. (Image source: [Real et al. 2018](https://arxiv.org/abs/1802.01548))'
  prefs: []
  type: TYPE_NORMAL
- en: 'Two types of mutations are applied:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Hidden state mutation*: randomly chooses a pairwise combination and rewires
    a random end such that there is no loop in the graph.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Operation mutation*: randomly replaces an existing operation with a random
    one.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/f7af6d8e48d4be6444e8c880b361cac3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 12\. Two types of mutations in AmoebaNet. (Image source: [Real et al.
    2018](https://arxiv.org/abs/1802.01548))'
  prefs: []
  type: TYPE_NORMAL
- en: In their experiments, EA and RL work equally well in terms of the final validation
    accuracy, but EA has better anytime performance and is able to find smaller models.
    Here using EA in NAS is still expensive in terms of computation, as each experiment
    took 7 days with 450 GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: '**HNAS** ([Liu et al 2017](https://arxiv.org/abs/1711.00436)) also employs
    the evolutionary algorithms (the original tournament selection) as their search
    strategy. In the [hierarchical structure](#hierarchical-structure) search space,
    each edge is an operation. Thus genotype mutation in their experiments is applied
    by replacing a random edge with a different operation. The replacement set includes
    an `none` op, so it can alter, remove and add an edge. The initial set of genotypes
    is created by applying a large number of random mutations on “trivial” motifs
    (all identity mappings).'
  prefs: []
  type: TYPE_NORMAL
- en: Progressive Decision Process
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Constructing a model architecture is a sequential process. Every additional
    operator or layer brings extra complexity. If we guide the search model to start
    the investigation from simple models and gradually evolve to more complex architectures,
    it is like to introduce [“curriculum”](https://lilianweng.github.io/posts/2020-01-29-curriculum-rl/)
    into the search model’s learning process.
  prefs: []
  type: TYPE_NORMAL
- en: '*Progressive NAS* (**PNAS**; [Liu, et al 2018](https://arxiv.org/abs/1712.00559))
    frames the problem of NAS as a progressive procedure for searching models of increasing
    complexity. Instead of RL or EA, PNAS adopts a Sequential Model-based Bayesian
    Optimization (SMBO) as the search strategy. PNAS works similar to A* search, as
    it searches for models from simple to hard while simultaneously learning a surrogate
    function to guide the search.'
  prefs: []
  type: TYPE_NORMAL
- en: '[A* search algorithm](https://en.wikipedia.org/wiki/A*_search_algorithm) (“best-first
    search”) is a popular algorithm for path finding. The problem is framed as finding
    a path of smallest cost from a specific starting node to a given target node in
    a weighted graph. At each iteration, A* finds a path to extend by minimizing:
    $f(n)=g(n)+h(n)$, where $n$ is the next node, $g(n)$ is the cost from start to
    $n$, and $h(n)$ is the heuristic function that estimates the minimum cost of going
    from node $n$ to the goal.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: PNAS uses the [NASNet](#cell-based-representation) search space. Each block
    is specified as a 5-element tuple and PNAS only considers the element-wise addition
    as the step 5 combination operator, no concatenation. Differently, instead of
    setting the number of blocks $B$ at a fixed number, PNAS starts with $B=1$, a
    model with only one block in a cell, and gradually increases $B$.
  prefs: []
  type: TYPE_NORMAL
- en: The performance on a validation set is used as feedback to train a *surrogate*
    model for *predicting* the performance of novel architectures. With this predictor,
    we can thus decide which models should be prioritized to be evaluated next. Since
    the performance predictor should be able to handle various-sized inputs, accuracy,
    and sample-efficient, they ended up using an RNN model.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c0c7168b03a6a9a7626b7465201e2223.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 13\. The algorithm of Progressive NAS. (Image source: [Liu, et al 2018](https://arxiv.org/abs/1712.00559))'
  prefs: []
  type: TYPE_NORMAL
- en: Gradient descent
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Using gradient descent to update the architecture search model requires an effort
    to make the process of choosing discrete operations differentiable. These approaches
    usually combine the learning of both architecture parameters and network weights
    together into one model. See more in the [section](#one-shot-approach-search--evaluation)
    on the *“one-shot”* approach.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation Strategy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We need to measure, estimate or predict the performance of every child model
    in order to obtain feedback for optimizing the search algorithm. The process of
    candidate evaluation could be very expensive and many new evaluation methods have
    been proposed to save time or computation. When evaluating a child model, we mostly
    care about its performance measured as accuracy on a validation set. Recent work
    has started looking into other factors of a model, such as model size and latency,
    as certain devices may have limitations on memory or demand fast response time.
  prefs: []
  type: TYPE_NORMAL
- en: Training from Scratch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The most naive approach is to train every child network independently from scratch
    until *convergence* and then measure its accuracy on a validation set ([Zoph &
    Le 2017](https://arxiv.org/abs/1611.01578)). It provides solid performance numbers,
    but one complete train-converge-evaluate loop only generates a single data sample
    for training the RL controller (let alone RL is known to be sample-inefficient
    in general). Thus it is very expensive in terms of computation consumption.
  prefs: []
  type: TYPE_NORMAL
- en: Proxy Task Performance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are several approaches for using a proxy task performance as the performance
    estimator of a child network, which is generally cheaper and faster to calculate:'
  prefs: []
  type: TYPE_NORMAL
- en: Train on a smaller dataset.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Train for fewer epochs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Train and evaluate a down-scaled model in the search stage. For example, once
    a cell structure is learned, we can play with the number of cell repeats or scale
    up the number of filters ([Zoph et al. 2018](https://arxiv.org/abs/1707.07012)).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predict the learning curve. [Baker et al (2018)](https://arxiv.org/abs/1705.10823)
    model the prediction of validation accuracies as a time-series regression problem.
    The features for the regression model ($\nu$-support vector machine regressions;
    $\nu$-SVR) include the early sequences of accuracy per epoch, architecture parameters,
    and hyperparameters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parameter Sharing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Instead of training every child model independently from scratch. You may ask,
    ok, what if we fabricate dependency between them and find a way to reuse weights?
    Some researchers succeeded to make such approaches work.
  prefs: []
  type: TYPE_NORMAL
- en: Inspired by [Net2net](https://arxiv.org/abs/1511.05641) transformation, [Cai
    et al (2017)](https://arxiv.org/abs/1707.04873) proposed *Efficient Architecture
    Search* (**EAS**). EAS sets up an RL agent, known as a meta-controller, to predict
    function-preserving network transformation so as to grow the network depth or
    layer width. Because the network is growing incrementally, the weights of previously
    validated networks can be *reused* for further exploration. With inherited weights,
    newly constructed networks only need some light-weighted training.
  prefs: []
  type: TYPE_NORMAL
- en: 'A meta-controller learns to generate *network transformation actions* given
    the current network architecture, which is specified with a variable-length string.
    In order to handle architecture configuration of a variable length, the meta-controller
    is implemented as a bi-directional recurrent network. Multiple actor networks
    output different transformation decisions:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Net2WiderNet* operation allows to replace a layer with a wider layer, meaning
    more units for fully-connected layers, or more filters for convolutional layers,
    while preserving the functionality.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Net2DeeperNet* operation allows to insert a new layer that is initialized
    as adding an identity mapping between two layers so as to preserve the functionality.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/85ead62e0e2376b10b2a6c718a680ce7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 14\. Overview of the RL based meta-controller in Efficient Architecture
    Search (NAS). After encoding the architecture configuration, it outputs net2net
    transformation actions through two separate actor networks. (Image source: [Cai
    et al 2017](https://arxiv.org/abs/1707.04873))'
  prefs: []
  type: TYPE_NORMAL
- en: With similar motivation, *Efficient NAS* (**ENAS**; [Pham et al. 2018](https://arxiv.org/abs/1802.03268))
    speeds up NAS (i.e. 1000x less) by aggressively sharing parameters among child
    models. The core motivation behind ENAS is the observation that all of the sampled
    architecture graphs can be viewed as *sub-graphs* of a larger *supergraph*. All
    the child networks are sharing weights of this supergraph.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/13a3b34d0e78744fb9f92287eaf8bca6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 15\. (Left) The graph represents the entire search space for a 4-node
    recurrent cell, but only connections in red are active. (Middle) An example of
    how the left active sub-graph can be translated into a child model architecture.
    (Right) The network parameters produced by an RNN controller for the architecture
    in the middle. (Image source: [Pham et al. 2018](https://arxiv.org/abs/1802.03268))'
  prefs: []
  type: TYPE_NORMAL
- en: 'ENAS alternates between training the shared model weights $\omega$ and training
    the controller $\theta$:'
  prefs: []
  type: TYPE_NORMAL
- en: The parameters of the controller LSTM $\theta$ are trained with [REINFORCE](https://lilianweng.github.io/posts/2018-04-08-policy-gradient/#reinforce),
    where the reward $R(\mathbf{m}, \omega)$ is computed on the validation set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The shared parameters of the child models $\omega$ are trained with standard
    supervised learning loss. Note that different operators associated with the same
    node in the supergraph would have their own distinct parameters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Prediction-Based
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A routine child model evaluation loop is to update model weights via standard
    gradient descent. SMASH ([Brock et al. 2017](https://arxiv.org/abs/1708.05344))
    proposes a different and interesting idea: *Can we predict the model weights directly
    based on the network architecture parameters?*'
  prefs: []
  type: TYPE_NORMAL
- en: They employ a [HyperNet](https://blog.otoro.net/2016/09/28/hyper-networks/)
    ([Ha et al 2016](https://arxiv.org/abs/1609.09106)) to directly generate the weights
    of a model conditioned on an encoding of its architecture configuration. Then
    the model with HyperNet-generated weights is validated directly. Note that we
    don’t need extra training for every child model but we do need to train the HyperNet.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2cd317f43f55af6c55121bde877b265b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 16\. The algorithm of SMASH. (Image source: [Brock et al. 2017](https://arxiv.org/abs/1708.05344))'
  prefs: []
  type: TYPE_NORMAL
- en: The correlation between model performance with SMASH-generated weights and true
    validation errors suggests that predicted weights can be used for model comparison,
    to some extent. We do need a HyperNet of large enough capacity, as the correlation
    would be corrupted if the HyperNet model is too small compared to the child model
    size.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6b06bd725a8bce3c26f8b4f186e193d1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 17\. The algorithm of SMASH. (Image source: [Brock et al. 2017](https://arxiv.org/abs/1708.05344))'
  prefs: []
  type: TYPE_NORMAL
- en: 'SMASH can be viewed as another way to implement the idea of [parameter sharing](#parameter-sharing).
    One problem of SMASH as pointed out by [Pham et al. (2018)](https://arxiv.org/abs/1802.03268)
    is: The usage of HyperNet restricts the weights of SMASH child models to a *low-rank
    space*, because weights are generated via tensor products. In comparison, [ENAS](#ENAS)
    has no such restrictions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'One-Shot Approach: Search + Evaluation'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Running search & evaluation independently for a large population of child models
    is expensive. We have seen promising approaches like [Brock et al. (2017)](https://arxiv.org/abs/1708.05344)
    or [Pham et al. (2018)](https://arxiv.org/abs/1802.03268), where training a single
    model is enough for emulating any child model in the search space.
  prefs: []
  type: TYPE_NORMAL
- en: The **one-shot** architecture search extends the idea of weight sharing and
    further combines the learning of architecture generation together with weight
    parameters. The following approaches all treat child architectures as different
    sub-graphs of a supergraph with shared weights between common edges in the supergraph.
  prefs: []
  type: TYPE_NORMAL
- en: '[Bender et al (2018)](http://proceedings.mlr.press/v80/bender18a/bender18a.pdf)
    construct a single large over-parameterized network, known as the **One-Shot model**,
    such that it contains every possible operation in the search space. With [ScheduledDropPath](#ScheduledDropPath)
    (the dropout rate is increased over time, which is $r^{1/k}$ at the end of training,
    where $0 < r < 1$ is a hyperparam and $k$ is the number of incoming paths) and
    some carefully designed tricks (e.g. ghost batch normalization, L2 regularization
    only on the active architecture), the training of such a giant model can be stabilized
    enough and used for evaluating any child model sampled from the supergraph.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ae065a97797659d226b262c04694d171.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 18\. The architecture of the One-Shot model in [Bender et al 2018](http://proceedings.mlr.press/v80/bender18a/bender18a.pdf).
    Each cell has $N$ choice blocks and each choice block can select up to 2 operations.
    Solid edges are used in every architecture, where dash lines are optional. (Image
    source: [Bender et al 2018](http://proceedings.mlr.press/v80/bender18a/bender18a.pdf))'
  prefs: []
  type: TYPE_NORMAL
- en: Once the one-shot model is trained, it is used for evaluating the performance
    of many different architectures sampled at random by zeroing out or removing some
    operations. This sampling process can be replaced by RL or evolution.
  prefs: []
  type: TYPE_NORMAL
- en: They observed that the difference between the accuracy measured with the one-shot
    model and the accuracy of the same architecture after a small fine-tuning could
    be very large. Their hypothesis is that the one-shot model automatically learns
    to focus on the *most useful* operations in the network and comes to *rely on*
    these operations when they are available. Thus zeroing out useful operations lead
    to big reduction in model accuracy, while removing less important components only
    causes a small impact — Therefore, we see a larger variance in scores when using
    the one-shot model for evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2b1a1fd1a7d09fc48145904f613d7872.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 19\. A stratified sample of models with different one-shot model accuracy
    versus their true validation accuracy as stand-alone models. (Image source: [Bender
    et al 2018](http://proceedings.mlr.press/v80/bender18a/bender18a.pdf))'
  prefs: []
  type: TYPE_NORMAL
- en: Clearly designing such a search graph is not a trivial task, but it demonstrates
    a strong potential with the one-shot approach. It works well with only gradient
    descent and no additional algorithm like RL or EA is a must.
  prefs: []
  type: TYPE_NORMAL
- en: Some believe that one main cause for inefficiency in NAS is to treat the architecture
    search as a *black-box optimization* and thus we fall into methods like RL, evolution,
    SMBO, etc. If we shift to rely on standard gradient descent, we could potentially
    make the search process more effectively. As a result, [Liu et al (2019)](https://arxiv.org/abs/1806.09055)
    propose *Differentiable Architecture Search* (**DARTS**). DARTS introduces a continuous
    relaxation on each path in the search supergraph, making it possible to jointly
    train architecture parameters and weights via gradient descent.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s use the directed acyclic graph (DAG) representation here. A cell is a
    DAG consisting of a topologically ordered sequence of $N$ nodes. Each node has
    a latent representation $x_i$ to be learned. Each edge $(i, j)$ is tied to some
    operation $o^{(i,j)} \in \mathcal{O}$ that transforms $x_j$ to compose $x_i$:'
  prefs: []
  type: TYPE_NORMAL
- en: $$ x_i = \sum_{j < i} o^{(i,j)}(x_j) $$
  prefs: []
  type: TYPE_NORMAL
- en: To make the search space continuous, DARTS relaxes the categorical choice of
    a particular operation as a softmax over all the operations and the task of architecture
    search is reduced to learn a set of mixing probabilities $\alpha = \{ \alpha^{(i,j)}
    \}$.
  prefs: []
  type: TYPE_NORMAL
- en: $$ \bar{o}^{(i,j)}(x) = \sum_{o\in\mathcal{O}} \frac{\exp(\alpha_{ij}^o)}{\sum_{o'\in\mathcal{O}}
    \exp(\alpha^{o'}_{ij})} o(x) $$
  prefs: []
  type: TYPE_NORMAL
- en: where $\alpha_{ij}$ is a vector of dimension $\vert \mathcal{O} \vert$, containing
    weights between nodes $i$ and $j$ over different operations.
  prefs: []
  type: TYPE_NORMAL
- en: 'The bilevel optimization exists as we want to optimize both the network weights
    $w$ and the architecture representation $\alpha$:'
  prefs: []
  type: TYPE_NORMAL
- en: $$ \begin{aligned} \min_\alpha & \mathcal{L}_\text{validate} (w^*(\alpha), \alpha)
    \\ \text{s.t.} & w^*(\alpha) = \arg\min_w \mathcal{L}_\text{train} (w, \alpha)
    \end{aligned} $$
  prefs: []
  type: TYPE_NORMAL
- en: 'At step $k$, given the current architecture parameters $\alpha_{k−1}$, we first
    optimize weights $w_k$ by moving $w_{k−1}$ in the direction of minimizing the
    training loss $\mathcal{L}_\text{train}(w_{k−1}, \alpha_{k−1})$ with a learning
    rate $\xi$. Next, while keeping the newly updated weights $w_k$ fixed, we update
    the mixing probabilities so as to minimize the validation loss *after a single
    step of gradient descent w.r.t. the weights*:'
  prefs: []
  type: TYPE_NORMAL
- en: $$ J_\alpha = \mathcal{L}_\text{val}(w_k - \xi \nabla_w \mathcal{L}_\text{train}(w_k,
    \alpha_{k-1}), \alpha_{k-1}) $$
  prefs: []
  type: TYPE_NORMAL
- en: The motivation here is that we want to find an architecture with a low validation
    loss when its weights are optimized by gradient descent and the one-step unrolled
    weights serve as the *surrogate* for $w^∗(\alpha)$.
  prefs: []
  type: TYPE_NORMAL
- en: 'Side note: Earlier we have seen similar formulation in [MAML](https://lilianweng.github.io/posts/2018-11-30-meta-learning/#maml)
    where the two-step optimization happens between task losses and the meta-learner
    update, as well as framing [Domain Randomization](https://lilianweng.github.io/posts/2019-05-05-domain-randomization/#dr-as-optimization)
    as a bilevel optimization for better transfer in the real environment.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/6a6bbab7b9da6028ab2d892fb76347f6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 20\. An illustration of how DARTS applies continuous relaxation on edges
    in DAG supergraph and identifies the final model. (Image source: [Liu et al 2019](https://arxiv.org/abs/1806.09055))'
  prefs: []
  type: TYPE_NORMAL
- en: $$ \begin{aligned} \text{Let }w'_k &= w_k - \xi \nabla_w \mathcal{L}_\text{train}(w_k,
    \alpha_{k-1}) & \\ J_\alpha &= \mathcal{L}_\text{val}(w_k - \xi \nabla_w \mathcal{L}_\text{train}(w_k,
    \alpha_{k-1}), \alpha_{k-1}) = \mathcal{L}_\text{val}(w'_k, \alpha_{k-1}) & \\
    \nabla_\alpha J_\alpha &= \nabla_{\alpha_{k-1}} \mathcal{L}_\text{val}(w'_k, \alpha_{k-1})
    \nabla_\alpha \alpha_{k-1} + \nabla_{w'_k} \mathcal{L}_\text{val}(w'_k, \alpha_{k-1})\nabla_\alpha
    w'_k & \\& \text{; multivariable chain rule}\\ &= \nabla_{\alpha_{k-1}} \mathcal{L}_\text{val}(w'_k,
    \alpha_{k-1}) + \nabla_{w'_k} \mathcal{L}_\text{val}(w'_k, \alpha_{k-1}) \big(
    - \xi \color{red}{\nabla^2_{\alpha, w} \mathcal{L}_\text{train}(w_k, \alpha_{k-1})}
    \big) & \\ &\approx \nabla_{\alpha_{k-1}} \mathcal{L}_\text{val}(w'_k, \alpha_{k-1})
    - \xi \nabla_{w'_k} \mathcal{L}_\text{val}(w'_k, \alpha_{k-1}) \color{red}{\frac{\nabla_\alpha
    \mathcal{L}_\text{train}(w_k^+, \alpha_{k-1}) - \nabla_\alpha \mathcal{L}_\text{train}(w_k^-,
    \alpha_{k-1}) }{2\epsilon}} & \\ & \text{; apply numerical differentiation approximation}
    \end{aligned} $$
  prefs: []
  type: TYPE_NORMAL
- en: where the red part is using numerical differentiation approximation where $w_k^+
    = w_k + \epsilon \nabla_{w’_k} \mathcal{L}_\text{val}(w’_k, \alpha_{k-1})$ and
    $w_k^- = w_k - \epsilon \nabla_{w’_k} \mathcal{L}_\text{val}(w’_k, \alpha_{k-1})$.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5f93c962518c89f572619ac9356b32b9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 21\. The algorithm overview of DARTS. (Image source: [Liu et al 2019](https://arxiv.org/abs/1806.09055))'
  prefs: []
  type: TYPE_NORMAL
- en: As another idea similar to DARTS, Stochastic NAS ([Xie et al., 2019](https://arxiv.org/abs/1812.09926))
    applies a continuous relaxation by employing the concrete distribution (CONCRETE
    = CONtinuous relaxations of disCRETE random variables; [Maddison et al 2017](https://arxiv.org/abs/1611.00712))
    and reparametrization tricks. The goal is same as DARTS, to make the discrete
    distribution differentiable and thus enable optimization by gradient descent.
  prefs: []
  type: TYPE_NORMAL
- en: DARTS is able to greatly reduce the cost of GPU hours. Their experiments for
    searching for CNN cells have $N=7$ and only took 1.5 days with a single GPU. However,
    it suffers from the high GPU memory consumption issue due to its continuous representation
    of network architecture. In order to fit the model into the memory of a single
    GPU, they picked a small $N$.
  prefs: []
  type: TYPE_NORMAL
- en: To constrain the GPU memory consumption, **ProxylessNAS** ([Cai et al., 2019](https://arxiv.org/abs/1812.00332))
    views NAS as a path-level pruning process in DAG and binarizes the architecture
    parameters to force only one path to be active between two nodes at a time. The
    probabilities for an edge being either masked out or not are then learned by sampling
    a few binarized architectures and using *BinaryConnect* ([Courbariaux et al.,
    2015](https://arxiv.org/abs/1511.00363)) to update the corresponding probabilities.
    ProxylessNAS demonstrates a strong connection between NAS and model compression.
    By using path-level compression, it is able to save memory consumption by one
    order of magnitude.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s continue with the graph representation. In a DAG adjacency matrix $G$
    where $G_{ij}$ represents an edge between node $i$ and $j$ and its value can be
    chosen from the set of $\vert \mathcal{O} \vert$ candidate primitive operations,
    $\mathcal{O} = \{ o_1, \dots \}$. The One-Shot model, DARTS and ProxylessNAS all
    consider each edge as a mixture of operations, $m_\mathcal{O}$, but with different
    tweaks.
  prefs: []
  type: TYPE_NORMAL
- en: In One-Shot, $m_\mathcal{O}(x)$ is the sum of all the operations. In DARTS,
    it is a weighted sum where weights are softmax over a real-valued architecture
    weighting vector $\alpha$ of length $\vert \mathcal{O} \vert$. ProxylessNAS transforms
    the softmax probabilities of $\alpha$ into a binary gate and uses the binary gate
    to keep only one operation active at a time.
  prefs: []
  type: TYPE_NORMAL
- en: $$ \begin{aligned} m^\text{one-shot}_\mathcal{O}(x) &= \sum_{i=1}^{\vert \mathcal{O}
    \vert} o_i(x) \\ m^\text{DARTS}_\mathcal{O}(x) &= \sum_{i=1}^{\vert \mathcal{O}
    \vert} p_i o_i(x) = \sum_{i=1}^{\vert \mathcal{O} \vert} \frac{\exp(\alpha_i)}{\sum_j
    \exp(\alpha_j)} o_i(x) \\ m^\text{binary}_\mathcal{O}(x) &= \sum_{i=1}^{\vert
    \mathcal{O} \vert} g_i o_i(x) = \begin{cases} o_1(x) & \text{with probability
    }p_1, \\ \dots &\\ o_{\vert \mathcal{O} \vert}(x) & \text{with probability }p_{\vert
    \mathcal{O} \vert} \end{cases} \\ \text{ where } g &= \text{binarize}(p_1, \dots,
    p_N) = \begin{cases} [1, 0, \dots, 0] & \text{with probability }p_1, \\ \dots
    & \\ [0, 0, \dots, 1] & \text{with probability }p_N. \\ \end{cases} \end{aligned}
    $$![](../Images/f7b2de1bd5c2b3fc14ab6dfd4e6515d0.png)
  prefs: []
  type: TYPE_NORMAL
- en: 'Fig. 22\. ProxylessNAS has two training steps running alternatively. (Image
    source: [Cai et al., 2019](https://arxiv.org/abs/1812.00332))'
  prefs: []
  type: TYPE_NORMAL
- en: 'ProxylessNAS runs two training steps alternatively:'
  prefs: []
  type: TYPE_NORMAL
- en: When training weight parameters $w$, it freezes the architecture parameters
    $\alpha$ and stochastically samples binary gates $g$ according to the above $m^\text{binary}_\mathcal{O}(x)$.
    The weight parameters can be updated with standard gradient descent.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'When training architecture parameters $\alpha$, it freezes $w$, resets the
    binary gates and then updates $\alpha$ on the validation set. Following the idea
    of *BinaryConnect*, the gradient w.r.t. architecture parameters can be approximately
    estimated using $\partial \mathcal{L} / \partial g_i$ in replacement for $\partial
    \mathcal{L} / \partial p_i$:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: $$ \begin{aligned} \frac{\partial \mathcal{L}}{\partial \alpha_i} &= \sum_{j=1}^{\vert
    \mathcal{O} \vert} \frac{\partial \mathcal{L}}{\partial p_j} \frac{\partial p_j}{\partial
    \alpha_i} \approx \sum_{j=1}^{\vert \mathcal{O} \vert} \frac{\partial \mathcal{L}}{\partial
    g_j} \frac{\partial p_j}{\partial \alpha_i} = \sum_{j=1}^{\vert \mathcal{O} \vert}
    \frac{\partial \mathcal{L}}{\partial g_j} \frac{\partial \frac{e^{\alpha_j}}{\sum_k
    e^{\alpha_k}}}{\partial \alpha_i} \\ &= \sum_{j=1}^{\vert \mathcal{O} \vert} \frac{\partial
    \mathcal{L}}{\partial g_j} \frac{\sum_k e^{\alpha_k} (\mathbf{1}_{i=j} e^{\alpha_j})
    - e^{\alpha_j} e^{\alpha_i} }{(\sum_k e^{\alpha_k})^2} = \sum_{j=1}^{\vert \mathcal{O}
    \vert} \frac{\partial \mathcal{L}}{\partial g_j} p_j (\mathbf{1}_{i=j} -p_i) \end{aligned}
    $$
  prefs: []
  type: TYPE_NORMAL
- en: Instead of BinaryConnect, REINFORCE can also be used for parameter updates with
    the goal for maximizing the reward, while no RNN meta-controller is involved.
  prefs: []
  type: TYPE_NORMAL
- en: 'Computing $\partial \mathcal{L} / \partial g_i$ needs to calculate and store
    $o_i(x)$, which requires $\vert \mathcal{O} \vert$ times GPU memory. To resolve
    this issue, they factorize the task of choosing one path out of $N$ into multiple
    binary selection tasks (Intuition: “if a path is the best choice, it should be
    better than any other path”). At every update step, only two paths are sampled
    while others are masked. These two selected paths are updated according to the
    above equation and then scaled properly so that other path weights are unchanged.
    After this process, one of the sampled paths is enhanced (path weight increases)
    and the other is attenuated (path weight decreases), while all other paths stay
    unaltered.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Besides accuracy, ProxylessNAS also considers *latency* as an important metric
    to optimize, as different devices might have very different requirements on inference
    time latency (e.g. GPU, CPU, mobile). To make latency differentiable, they model
    latency as a continuous function of the network dimensions. The expected latency
    of a mixed operation can be written as $\mathbb{E}[\text{latency}] = \sum_j p_j
    F(o_j)$, where $F(.)$ is a latency prediction model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a655071e726bab3619152647bd51474d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 23\. Add a differentiable latency loss into the training of ProxylessNAS.
    (Image source: [Cai et al., 2019](https://arxiv.org/abs/1812.00332))'
  prefs: []
  type: TYPE_NORMAL
- en: What’s the Future?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far we have seen many interesting new ideas on automating the network architecture
    engineering through neural architecture search and many have achieved very impressive
    performance. However, it is a bit hard to do inference on *why* some architecture
    work well and how we can develop modules generalizable across tasks rather than
    being very dataset-specific.
  prefs: []
  type: TYPE_NORMAL
- en: 'As also noted in [Elsken, et al (2019)](https://arxiv.org/abs/1808.05377):'
  prefs: []
  type: TYPE_NORMAL
- en: “…, so far it provides little insights into why specific architectures work
    well and how similar the architectures derived in independent runs would be. Identifying
    common motifs, providing an understanding why those motifs are important for high
    performance, and investigating if these motifs generalize over different problems
    would be desirable.”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In the meantime, purely focusing on improvement over validation accuracy might
    not be enough ([Cai et al., 2019](https://arxiv.org/abs/1812.00332)). Devices
    like mobile phones for daily usage in general have limited memory and computation
    power. While AI applications are on the way to affect our daily life, it is unavoidable
    to be more *device-specific*.
  prefs: []
  type: TYPE_NORMAL
- en: Another interesting investigation is to consider *unlabelled dataset* and [self-supervised
    learning](https://lilianweng.github.io/posts/2019-11-10-self-supervised/) for
    NAS. The size of labelled dataset is always limited and it is not easy to tell
    whether such a dataset has biases or big deviation from the real world data distribution.
  prefs: []
  type: TYPE_NORMAL
- en: '[Liu et al (2020)](https://arxiv.org/abs/2003.12056) delve into the question
    *“Can we find high-quality neural architecture without human-annotated labels?”*
    and proposed a new setup called *Unsupervised Neural Architecture Search* (**UnNAS**).
    The quality of the architecture needs to be estimated in an unsupervised fashion
    during the search phase. The paper experimented with three unsupervised [pretext
    tasks](https://lilianweng.github.io/posts/2019-11-10-self-supervised/#images-based):
    image rotation prediction, colorization, and solving the jigsaw puzzle.'
  prefs: []
  type: TYPE_NORMAL
- en: 'They observed in a set of UnNAS experiments that:'
  prefs: []
  type: TYPE_NORMAL
- en: High rank correlation between supervised accuracy and pretext accuracy *on the
    same dataset*. Typically the rank correlation is higher than 0.8, regardless of
    the dataset, the search space, and the pretext task.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: High rank correlation between supervised accuracy and pretext accuracy *across
    datasets*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Better pretext accuracy translates to better supervised accuracy.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Performance of UnNAS architecture is comparable to supervised counterparts,
    though not better yet.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: One hypothesis is that the architecture quality is correlated with image statistics.
    Because CIFAR-10 and ImageNet are all on the natural images, they are comparable
    and the results are transferable. UnNAS could potentially enable a much larger
    amount of unlabelled data into the search phase which captures image statistics
    better.
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameter search is a long-standing topic in the ML community. And NAS
    automates architecture engineering. Gradually we are trying to automate processes
    in ML which usually demand a lot of human efforts. Taking even one more step further,
    is it possible to automatically discover ML algorithms? **AutoML-Zero** ([Real
    et al 2020](https://arxiv.org/abs/2003.03384)) investigates this idea. Using [aging
    evolutionary algorithms](#aging-evolutionary-algorithms), AutoML-Zero automatically
    searches for whole ML algorithms using little restriction on the form with only
    simple mathematical operations as building blocks.
  prefs: []
  type: TYPE_NORMAL
- en: It learns three component functions. Each function only adopts very basic operations.
  prefs: []
  type: TYPE_NORMAL
- en: '`Setup`: initialize memory variables (weights).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Learn`: modify memory variables'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Predict`: make a prediction from an input $x$.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/91e35e425d80cefdc533367d79b67b16.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 24\. Algorithm evaluation on one task (Image source: [Real et al 2020](https://arxiv.org/abs/2003.03384))'
  prefs: []
  type: TYPE_NORMAL
- en: 'Three types of operations are considered when mutating a parent genotype:'
  prefs: []
  type: TYPE_NORMAL
- en: Insert a random instruction or remove an instruction at a random location in
    a component function;
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Randomize all the instructions in a component function;
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Modify one of the arguments of an instruction by replacing it with a random
    choice (e.g. “swap the output address” or “change the value of a constant”)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/05f4e105c9f2dd76faa6928646062b59.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 25\. An illustration of evolutionary progress on projected binary CIFAR-10
    with example code. (Image source: [Real et al 2020](https://arxiv.org/abs/2003.03384))'
  prefs: []
  type: TYPE_NORMAL
- en: 'Appendix: Summary of NAS Papers'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '| Model name | Search space | Search algorithms | Child model evaluation |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| [NEAT (2002)](http://nn.cs.utexas.edu/downloads/papers/stanley.ec02.pdf)
    | - | Evolution (Genetic algorithm) | - |'
  prefs: []
  type: TYPE_TB
- en: '| [NAS (2017)](https://arxiv.org/abs/1611.01578) | Sequential layer-wise ops
    | RL (REINFORCE) | Train from scratch until convergence |'
  prefs: []
  type: TYPE_TB
- en: '| [MetaQNN (2017)](https://arxiv.org/abs/1611.02167) | Sequential layer-wise
    ops | RL (Q-learning with $\epsilon$-greedy) | Train for 20 epochs |'
  prefs: []
  type: TYPE_TB
- en: '| [HNAS (2017)](https://arxiv.org/abs/1711.00436) | Hierarchical structure
    | Evolution (Tournament selection) | Train for a fixed number of iterations |'
  prefs: []
  type: TYPE_TB
- en: '| [NASNet (2018)](https://arxiv.org/abs/1707.07012) | Cell-based | RL (PPO)
    | Train for 20 epochs |'
  prefs: []
  type: TYPE_TB
- en: '| [AmoebaNet (2018)](https://arxiv.org/abs/1802.01548) | NASNet search space
    | Evolution (Tournament selection with aging regularization) | Train for 25 epochs
    |'
  prefs: []
  type: TYPE_TB
- en: '| [EAS (2018a)](https://arxiv.org/abs/1707.04873) | Network transformation
    | RL (REINFORCE) | 2-stage training |'
  prefs: []
  type: TYPE_TB
- en: '| [PNAS (2018)](https://arxiv.org/abs/1712.00559) | Reduced version of NASNet
    search space | SMBO; Progressive search for architectures of increasing complexity
    | Train for 20 epochs |'
  prefs: []
  type: TYPE_TB
- en: '| [ENAS (2018)](https://arxiv.org/abs/1802.03268) | Both sequential and cell-based
    search space | RL (REINFORCE) | Train one model with shared weights |'
  prefs: []
  type: TYPE_TB
- en: '| [SMASH (2017)](https://arxiv.org/abs/1708.05344) | Memory-bank representation
    | Random search | HyperNet predicts weights of evaluated architectures. |'
  prefs: []
  type: TYPE_TB
- en: '| [One-Shot (2018)](http://proceedings.mlr.press/v80/bender18a.html) | An over-parameterized
    one-shot model | Random search (zero out some paths at random) | Train the one-shot
    model |'
  prefs: []
  type: TYPE_TB
- en: '| [DARTS (2019)](https://arxiv.org/abs/1806.09055) | NASNet search space |
    Gradient descent (Softmax weights over operations) |'
  prefs: []
  type: TYPE_TB
- en: '| [ProxylessNAS (2019)](https://arxiv.org/abs/1812.00332) | Tree structure
    architecture | Gradient descent (BinaryConnect) or REINFORCE |'
  prefs: []
  type: TYPE_TB
- en: '| [SNAS (2019)](https://arxiv.org/abs/1812.09926) | NASNet search space | Gradient
    descent (concrete distribution) |'
  prefs: []
  type: TYPE_TB
- en: Citation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Cited as:'
  prefs: []
  type: TYPE_NORMAL
- en: Weng, Lilian. (Aug 2020). Neural architecture search. Lil’Log. https://lilianweng.github.io/posts/2020-08-06-nas/.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Or
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Reference
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] Thomas Elsken, Jan Hendrik Metzen, Frank Hutter. [“Neural Architecture
    Search: A Survey”](https://arxiv.org/abs/1808.05377) JMLR 20 (2019) 1-21.'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] Kenneth O. Stanley, et al. [“Designing neural networks through neuroevolution”](https://www.nature.com/articles/s42256-018-0006-z)
    Nature Machine Intelligence volume 1, pages 24–35 (2019).'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] Kenneth O. Stanley & Risto Miikkulainen. [“Evolving Neural Networks through
    Augmenting Topologies”](http://nn.cs.utexas.edu/downloads/papers/stanley.ec02.pdf)
    Evolutionary Computation 10(2): 99-127 (2002).'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] Barret Zoph, Quoc V. Le. [“Neural architecture search with reinforcement
    learning”](https://arxiv.org/abs/1611.01578) ICLR 2017.'
  prefs: []
  type: TYPE_NORMAL
- en: '[5] Bowen Baker, et al. [“Designing Neural Network Architectures using Reinforcement
    Learning”](https://arxiv.org/abs/1611.02167) ICLR 2017.'
  prefs: []
  type: TYPE_NORMAL
- en: '[6] Bowen Baker, et al. [“Accelerating neural architecture search using performance
    prediction”](https://arxiv.org/abs/1705.10823) ICLR Workshop 2018.'
  prefs: []
  type: TYPE_NORMAL
- en: '[7] Barret Zoph, et al. [“Learning transferable architectures for scalable
    image recognition”](https://arxiv.org/abs/1707.07012) CVPR 2018.'
  prefs: []
  type: TYPE_NORMAL
- en: '[8] Hanxiao Liu, et al. [“Hierarchical representations for efficient architecture
    search.”](https://arxiv.org/abs/1711.00436) ICLR 2018.'
  prefs: []
  type: TYPE_NORMAL
- en: '[9] Esteban Real, et al. [“Regularized Evolution for Image Classifier Architecture
    Search”](https://arxiv.org/abs/1802.01548) arXiv:1802.01548 (2018).'
  prefs: []
  type: TYPE_NORMAL
- en: '[10] Han Cai, et al. [“Efficient architecture search by network transformation”]
    AAAI 2018a.'
  prefs: []
  type: TYPE_NORMAL
- en: '[11] Han Cai, et al. [“Path-Level Network Transformation for Efficient Architecture
    Search”](https://arxiv.org/abs/1806.02639) ICML 2018b.'
  prefs: []
  type: TYPE_NORMAL
- en: '[12] Han Cai, Ligeng Zhu & Song Han. [“ProxylessNAS: Direct Neural Architecture
    Search on Target Task and Hardware”](https://arxiv.org/abs/1812.00332) ICLR 2019.'
  prefs: []
  type: TYPE_NORMAL
- en: '[13] Chenxi Liu, et al. [“Progressive neural architecture search”](https://arxiv.org/abs/1712.00559)
    ECCV 2018.'
  prefs: []
  type: TYPE_NORMAL
- en: '[14] Hieu Pham, et al. [“Efficient neural architecture search via parameter
    sharing”](https://arxiv.org/abs/1802.03268) ICML 2018.'
  prefs: []
  type: TYPE_NORMAL
- en: '[15] Andrew Brock, et al. [“SMASH: One-shot model architecture search through
    hypernetworks.”](https://arxiv.org/abs/1708.05344) ICLR 2018.'
  prefs: []
  type: TYPE_NORMAL
- en: '[16] Gabriel Bender, et al. [“Understanding and simplifying one-shot architecture
    search.”](http://proceedings.mlr.press/v80/bender18a.html) ICML 2018.'
  prefs: []
  type: TYPE_NORMAL
- en: '[17] Hanxiao Liu, Karen Simonyan, Yiming Yang. [“DARTS: Differentiable Architecture
    Search”](https://arxiv.org/abs/1806.09055) ICLR 2019.'
  prefs: []
  type: TYPE_NORMAL
- en: '[18] Sirui Xie, Hehui Zheng, Chunxiao Liu, Liang Lin. [“SNAS: Stochastic Neural
    Architecture Search”](https://arxiv.org/abs/1812.09926) ICLR 2019.'
  prefs: []
  type: TYPE_NORMAL
- en: '[19] Chenxi Liu et al. [“Are Labels Necessary for Neural Architecture Search?”](https://arxiv.org/abs/2003.12056)
    ECCV 2020.'
  prefs: []
  type: TYPE_NORMAL
- en: '[20] Esteban Real, et al. [“AutoML-Zero: Evolving Machine Learning Algorithms
    From Scratch”](https://arxiv.org/abs/2003.03384) ICML 2020.'
  prefs: []
  type: TYPE_NORMAL
