- en: Generalized Language Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通用语言模型
- en: 原文：[https://lilianweng.github.io/posts/2019-01-31-lm/](https://lilianweng.github.io/posts/2019-01-31-lm/)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[https://lilianweng.github.io/posts/2019-01-31-lm/](https://lilianweng.github.io/posts/2019-01-31-lm/)'
- en: '[Updated on 2019-02-14: add [ULMFiT](#ulmfit) and [GPT-2](#gpt-2).]'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[2019-02-14更新：添加[ULMFiT](#ulmfit)和[GPT-2](#gpt-2)。]'
- en: '[Updated on 2020-02-29: add [ALBERT](#albert).]'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[2020-02-29更新：添加[ALBERT](#albert)。]'
- en: '[Updated on 2020-10-25: add [RoBERTa](#roberta).]'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[2020-10-25更新：添加[RoBERTa](#roberta)。]'
- en: '[Updated on 2020-12-13: add [T5](#t5).]'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[2020-12-13更新：添加[T5](#t5)。]'
- en: '[Updated on 2020-12-30: add [GPT-3](#gpt-3).]'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[2020-12-30更新：添加[GPT-3](#gpt-3)。]'
- en: '[Updated on 2021-11-13: add [XLNet](#xlnet), [BART](#bart) and [ELECTRA](#electra);
    Also updated the [Summary](#summary) section.]'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[2021-11-13更新：添加[XLNet](#xlnet)、[BART](#bart)和[ELECTRA](#electra)；同时更新[Summary](#summary)部分。]'
- en: '![](../Images/b295e956fc053ad892d37a2ed1912cf6.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b295e956fc053ad892d37a2ed1912cf6.png)'
- en: 'Fig. 0\. I guess they are Elmo & Bert? (Image source: [here](https://www.youtube.com/watch?v=l5einDQ-Ttc))'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 图0\. 我猜它们是Elmo和Bert？（图片来源：[这里](https://www.youtube.com/watch?v=l5einDQ-Ttc)）
- en: We have seen amazing progress in NLP in 2018\. Large-scale pre-trained language
    modes like [OpenAI GPT](https://blog.openai.com/language-unsupervised/) and [BERT](https://arxiv.org/abs/1810.04805)
    have achieved great performance on a variety of language tasks using generic model
    architectures. The idea is similar to how ImageNet classification pre-training
    helps many vision tasks (*). Even better than vision classification pre-training,
    this simple and powerful approach in NLP does not require labeled data for pre-training,
    allowing us to experiment with increased training scale, up to our very limit.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在2018年看到了NLP方面的惊人进展。像[OpenAI GPT](https://blog.openai.com/language-unsupervised/)和[BERT](https://arxiv.org/abs/1810.04805)这样的大规模预训练语言模型使用通用模型架构在各种语言任务上取得了出色的表现。这个想法类似于ImageNet分类预训练如何帮助许多视觉任务（*）。甚至比视觉分类预训练更好，这种在NLP中简单而强大的方法不需要标记的预训练数据，使我们能够尝试增加训练规模，直至极限。
- en: '*(*) He et al. (2018) [found](https://arxiv.org/abs/1811.08883) that pre-training
    might not be necessary for image segmentation task.*'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '*（*）He等人（2018）[发现](https://arxiv.org/abs/1811.08883)，对于图像分割任务，预训练可能并非必要。*'
- en: In my previous NLP [post on word embedding](https://lilianweng.github.io/posts/2017-10-15-word-embedding/),
    the introduced embeddings are not context-specific — they are learned based on
    word concurrency but not sequential context. So in two sentences, “*I am eating
    an apple*” and “*I have an Apple phone*”, two “apple” words refer to very different
    things but they would still share the same word embedding vector.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在我之前关于词嵌入的NLP [文章](https://lilianweng.github.io/posts/2017-10-15-word-embedding/)中，介绍的嵌入不是特定上下文的
    — 它们是基于词的并发性而不是顺序上下文学习的。因此，在两个句子中，“*我在吃一个苹果*”和“*我有一个苹果手机*”中，两个“苹果”词指的是非常不同的东西，但它们仍然共享相同的词嵌入向量。
- en: Despite this, early adoption of word embeddings in problem-solving is to use
    them as additional features for an existing task-specific model and in a way the
    improvement is bounded.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，早期在问题解决中采用词嵌入的方式是将它们用作现有任务特定模型的附加特征，而且改进是有限的。
- en: In this post, we will discuss how various approaches were proposed to make embeddings
    dependent on context, and to make them easier and cheaper to be applied to downstream
    tasks in general form.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将讨论各种方法是如何提出的，使嵌入依赖于上下文，并使它们更容易、更便宜地应用于一般形式的下游任务。
- en: CoVe
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CoVe
- en: '**CoVe** ([McCann et al. 2017](https://arxiv.org/abs/1708.00107)), short for
    **Contextual Word Vectors**, is a type of word embeddings learned by an encoder
    in an [attentional seq-to-seq](https://lilianweng.github.io/posts/2018-06-24-attention/#born-for-translation)
    machine translation model. Different from traditional word embeddings introduced
    [here](https://lilianweng.github.io/posts/2017-10-15-word-embedding/), CoVe word
    representations are functions of the entire input sentence.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**CoVe**（[McCann等人，2017](https://arxiv.org/abs/1708.00107)），简称**上下文词向量**，是由注意力seq-to-seq机器翻译模型中的编码器学习的一种词嵌入类型。与此处介绍的传统词嵌入不同，CoVe词表示是整个输入句子的函数。'
- en: NMT Recap
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: NMT Recap
- en: Here the Neural Machine Translation ([NMT](https://github.com/THUNLP-MT/MT-Reading-List))
    model is composed of a standard, two-layer, bidirectional LSTM encoder and an
    attentional two-layer unidirectional LSTM decoder. It is pre-trained on the English-German
    translation task. The encoder learns and optimizes the embedding vectors of English
    words in order to translate them to German. With the intuition that the encoder
    should capture high-level semantic and syntactic meanings before transforming
    words into another language, the encoder output is used to provide contextualized
    word embeddings for various downstream language tasks.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的神经机器翻译([NMT](https://github.com/THUNLP-MT/MT-Reading-List))模型由标准的双层双向LSTM编码器和具有注意力机制的双层单向LSTM解码器组成。它在英德翻译任务上进行了预训练。编码器学习并优化英语单词的嵌入向量，以便将它们翻译成德语。基于这样的直觉，即编码器应该在将单词转换为另一种语言之前捕捉高级语义和句法含义，编码器输出用于为各种下游语言任务提供上下文化的单词嵌入。
- en: '![](../Images/6c8839f955a10f40612e99c1615e8e70.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6c8839f955a10f40612e99c1615e8e70.png)'
- en: Fig. 1\. The NMT base model used in CoVe.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图1\. CoVe中使用的NMT基础模型。
- en: 'A sequence of $n$ words in source language (English): $x = [x_1, \dots, x_n]$.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 源语言（英语）中的$n$个单词序列：$x = [x_1, \dots, x_n]$。
- en: 'A sequence of $m$ words in target language (German): $y = [y_1, \dots, y_m]$.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标语言（德语）中的$m$个单词序列：$y = [y_1, \dots, y_m]$。
- en: 'The [GloVe](https://lilianweng.github.io/posts/2017-10-15-word-embedding/#glove-global-vectors)
    vectors of source words: $\text{GloVe}(x)$.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 源单词的[GloVe](https://lilianweng.github.io/posts/2017-10-15-word-embedding/#glove-global-vectors)向量：$\text{GloVe}(x)$。
- en: 'Randomly initialized embedding vectors of target words: $z = [z_1, \dots, z_m]$.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标单词的随机初始化嵌入向量：$z = [z_1, \dots, z_m]$。
- en: 'The biLSTM encoder outputs a sequence of hidden states: $h = [h_1, \dots, h_n]
    = \text{biLSTM}(\text{GloVe}(x))$ and $h_t = [\overrightarrow{h}_t; \overleftarrow{h}_t]$
    where the forward LSTM computes $\overrightarrow{h}_t = \text{LSTM}(x_t, \overrightarrow{h}_{t-1})$
    and the backward computation gives us $\overleftarrow{h}_t = \text{LSTM}(x_t,
    \overleftarrow{h}_{t-1})$.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: biLSTM编码器输出一系列隐藏状态：$h = [h_1, \dots, h_n] = \text{biLSTM}(\text{GloVe}(x))$，$h_t
    = [\overrightarrow{h}_t; \overleftarrow{h}_t]$，其中前向LSTM计算$\overrightarrow{h}_t
    = \text{LSTM}(x_t, \overrightarrow{h}_{t-1})$，后向计算给出$\overleftarrow{h}_t = \text{LSTM}(x_t,
    \overleftarrow{h}_{t-1})$。
- en: 'The attentional decoder outputs a distribution over words: $p(y_t \mid H, y_1,
    \dots, y_{t-1})$ where $H$ is a stack of hidden states $\{h\}$ along the time
    dimension:'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意力解码器输出一个关于单词的分布：$p(y_t \mid H, y_1, \dots, y_{t-1})$，其中$H$是沿着时间维度的隐藏状态堆栈$\{h\}$：
- en: '$$ \begin{aligned} \text{decoder hidden state: } s_t &= \text{LSTM}([z_{t-1};
    \tilde{h}_{t-1}], s_{t-1}) \\ \text{attention weights: } \alpha_t &= \text{softmax}(H(W_1
    s_t + b_1)) \\ \text{context-adjusted hidden state: } \tilde{h}_t &= \tanh(W_2[H^\top\alpha_t;s_t]
    + b_2) \\ \text{decoder output: } p(y_t\mid H, y_1, \dots, y_{t-1}) &= \text{softmax}(W_\text{out}
    \tilde{h}_t + b_\text{out}) \end{aligned} $$'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \begin{aligned} \text{解码器隐藏状态：} s_t &= \text{LSTM}([z_{t-1}; \tilde{h}_{t-1}],
    s_{t-1}) \\ \text{注意力权重：} \alpha_t &= \text{softmax}(H(W_1 s_t + b_1)) \\ \text{调整后的上下文隐藏状态：}
    \tilde{h}_t &= \tanh(W_2[H^\top\alpha_t;s_t] + b_2) \\ \text{解码器输出：} p(y_t\mid
    H, y_1, \dots, y_{t-1}) &= \text{softmax}(W_\text{out} \tilde{h}_t + b_\text{out})
    \end{aligned} $$
- en: Use CoVe in Downstream Tasks
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在下游任务中使用CoVe
- en: 'The hidden states of NMT encoder are defined as **context vectors** for other
    language tasks:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: NMT编码器的隐藏状态被定义为其他语言任务的**上下文向量**：
- en: $$ \text{CoVe}(x) = \text{biLSTM}(\text{GloVe}(x)) $$
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \text{CoVe}(x) = \text{biLSTM}(\text{GloVe}(x)) $$
- en: The paper proposed to use the concatenation of GloVe and CoVe for question-answering
    and classification tasks. GloVe learns from the ratios of global word co-occurrences,
    so it has no sentence context, while CoVe is generated by processing text sequences
    is able to capture the contextual information.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 该论文建议在问答和分类任务中使用GloVe和CoVe的串联。GloVe从全局单词共现比例中学习，因此没有句子上下文，而CoVe是通过处理文本序列生成的，能够捕捉上下文信息。
- en: $$ v = [\text{GloVe}(x); \text{CoVe}(x)] $$
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: $$ v = [\text{GloVe}(x); \text{CoVe}(x)] $$
- en: Given a downstream task, we first generate the concatenation of GloVe + CoVe
    vectors of input words and then feed them into the task-specific models as additional
    features.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个下游任务，我们首先生成输入单词的GloVe + CoVe向量的串联，然后将它们作为额外特征输入到任务特定模型中。
- en: '![](../Images/7885bd5c60581360a3b1a0ef989e3f50.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7885bd5c60581360a3b1a0ef989e3f50.png)'
- en: 'Fig. 2\. The CoVe embeddings are generated by an encoder trained for machine
    translation task. The encoder can be plugged into any downstream task-specific
    model. (Image source: [original paper](https://arxiv.org/abs/1708.00107))'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图2\. CoVe嵌入是由为机器翻译任务训练的编码器生成的。编码器可以插入到任何下游任务特定模型中。（图片来源：[原始论文](https://arxiv.org/abs/1708.00107)）
- en: '**Summary**: The limitation of CoVe is obvious: (1) pre-training is bounded
    by available datasets on the supervised translation task; (2) the contribution
    of CoVe to the final performance is constrained by the task-specific model architecture.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**总结**：CoVe的局限性是显而易见的：（1）预训练受监督翻译任务可用数据集的限制；（2）CoVe对最终性能的贡献受任务特定模型架构的限制。'
- en: In the following sections, we will see that ELMo overcomes issue (1) by unsupervised
    pre-training and OpenAI GPT & BERT further overcome both problems by unsupervised
    pre-training + using generative model architecture for different downstream tasks.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将看到ELMo通过无监督预训练和OpenAI GPT & BERT进一步通过无监督预训练+使用生成模型架构来克服这两个问题。
- en: ELMo
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ELMo
- en: '**ELMo**, short for **Embeddings from Language Model** ([Peters, et al, 2018](https://arxiv.org/abs/1802.05365))
    learns contextualized word representation by pre-training a language model in
    an *unsupervised* way.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**ELMo**，即**来自语言模型的嵌入**（[Peters, et al, 2018](https://arxiv.org/abs/1802.05365)）通过以*无监督*方式预训练语言模型来学习上下文化的词表示。'
- en: Bidirectional Language Model
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 双向语言模型
- en: The bidirectional Language Model (**biLM**) is the foundation for ELMo. While
    the input is a sequence of $n$ tokens, $(x_1, \dots, x_n)$, the language model
    learns to predict the probability of next token given the history.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 双向语言模型（**biLM**）是ELMo的基础。输入是一个长度为$n$的标记序列，$(x_1, \dots, x_n)$，语言模型学习预测给定历史的下一个标记的概率。
- en: In the forward pass, the history contains words before the target token,
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在前向传播中，历史包含目标标记之前的单词，
- en: $$ p(x_1, \dots, x_n) = \prod_{i=1}^n p(x_i \mid x_1, \dots, x_{i-1}) $$
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: $$ p(x_1, \dots, x_n) = \prod_{i=1}^n p(x_i \mid x_1, \dots, x_{i-1}) $$
- en: In the backward pass, the history contains words after the target token,
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在反向传播中，历史包含目标标记之后的单词，
- en: $$ p(x_1, \dots, x_n) = \prod_{i=1}^n p(x_i \mid x_{i+1}, \dots, x_n) $$
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: $$ p(x_1, \dots, x_n) = \prod_{i=1}^n p(x_i \mid x_{i+1}, \dots, x_n) $$
- en: The predictions in both directions are modeled by multi-layer LSTMs with hidden
    states $\overrightarrow{\mathbf{h}}_{i,\ell}$ and $\overleftarrow{\mathbf{h}}_{i,\ell}$
    for input token $x_i$ at the layer level $\ell=1,\dots,L$. The final layer’s hidden
    state $\mathbf{h}_{i,L} = [\overrightarrow{\mathbf{h}}_{i,L}; \overleftarrow{\mathbf{h}}_{i,L}]$
    is used to output the probabilities over tokens after softmax normalization. They
    share the embedding layer and the softmax layer, parameterized by $\Theta_e$ and
    $\Theta_s$ respectively.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 两个方向的预测由具有隐藏状态$\overrightarrow{\mathbf{h}}_{i,\ell}$和$\overleftarrow{\mathbf{h}}_{i,\ell}$的多层LSTM模型建模，用于输入标记$x_i$的层级$\ell=1,\dots,L$。最终层的隐藏状态$\mathbf{h}_{i,L}
    = [\overrightarrow{\mathbf{h}}_{i,L}; \overleftarrow{\mathbf{h}}_{i,L}]$用于在softmax归一化后输出标记的概率。它们共享嵌入层和softmax层，分别由参数$\Theta_e$和$\Theta_s$参数化。
- en: '![](../Images/8894e79aa27e2cd7a4dd5923cf770327.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8894e79aa27e2cd7a4dd5923cf770327.png)'
- en: 'Fig. 3\. The biLSTM base model of ELMo. (Image source: recreated based on the
    figure in ["Neural Networks, Types, and Functional Programming"](http://colah.github.io/posts/2015-09-NN-Types-FP/)
    by Christopher Olah.)'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图3\. ELMo的双向LSTM基础模型。（图片来源：基于["神经网络、类型和函数式编程"](http://colah.github.io/posts/2015-09-NN-Types-FP/)中的图重新创建，作者Christopher
    Olah。）
- en: 'The model is trained to minimize the negative log likelihood (= maximize the
    log likelihood for true words) in both directions:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型经过训练，以最小化负对数似然（=最大化真实单词的对数似然）的方式进行双向训练：
- en: $$ \begin{aligned} \mathcal{L} = - \sum_{i=1}^n \Big( \log p(x_i \mid x_1, \dots,
    x_{i-1}; \Theta_e, \overrightarrow{\Theta}_\text{LSTM}, \Theta_s) + \\ \log p(x_i
    \mid x_{i+1}, \dots, x_n; \Theta_e, \overleftarrow{\Theta}_\text{LSTM}, \Theta_s)
    \Big) \end{aligned} $$
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \begin{aligned} \mathcal{L} = - \sum_{i=1}^n \Big( \log p(x_i \mid x_1, \dots,
    x_{i-1}; \Theta_e, \overrightarrow{\Theta}_\text{LSTM}, \Theta_s) + \\ \log p(x_i
    \mid x_{i+1}, \dots, x_n; \Theta_e, \overleftarrow{\Theta}_\text{LSTM}, \Theta_s)
    \Big) \end{aligned} $$
- en: ELMo Representations
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ELMo表示
- en: 'On top of a $L$-layer biLM, ELMo stacks all the hidden states across layers
    together by learning a task-specific linear combination. The hidden state representation
    for the token $x_i$ contains $2L+1$ vectors:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个$L$层的双向语言模型（biLM）之上，ELMo通过学习任务特定的线性组合将所有层的隐藏状态堆叠在一起。对于标记$x_i$的隐藏状态表示包含$2L+1$个向量：
- en: $$ R_i = \{ \mathbf{h}_{i,\ell} \mid \ell = 0, \dots, L \} $$
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: $$ R_i = \{ \mathbf{h}_{i,\ell} \mid \ell = 0, \dots, L \} $$
- en: where $\mathbf{h}_{0, \ell}$ is the embedding layer output and $\mathbf{h}_{i,
    \ell} = [\overrightarrow{\mathbf{h}}_{i,\ell}; \overleftarrow{\mathbf{h}}_{i,\ell}]$.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{h}_{0, \ell}$ 是嵌入层输出，$\mathbf{h}_{i, \ell} = [\overrightarrow{\mathbf{h}}_{i,\ell};
    \overleftarrow{\mathbf{h}}_{i,\ell}]$。
- en: The weights, $\mathbf{s}^\text{task}$, in the linear combination are learned
    for each end task and normalized by softmax. The scaling factor $\gamma^\text{task}$
    is used to correct the misalignment between the distribution of biLM hidden states
    and the distribution of task specific representations.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 线性组合中的权重 $\mathbf{s}^\text{task}$ 是为每个最终任务学习的，并通过 softmax 进行归一化。缩放因子 $\gamma^\text{task}$
    用于纠正 biLM 隐藏状态分布与任务特定表示分布之间的不对齐。
- en: $$ v_i = f(R_i; \Theta^\text{task}) = \gamma^\text{task} \sum_{\ell=0}^L s^\text{task}_i
    \mathbf{h}_{i,\ell} $$
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: $$ v_i = f(R_i; \Theta^\text{task}) = \gamma^\text{task} \sum_{\ell=0}^L s^\text{task}_i
    \mathbf{h}_{i,\ell} $$
- en: 'To evaluate what kind of information is captured by hidden states across different
    layers, ELMo is applied on semantic-intensive and syntax-intensive tasks respectively
    using representations in different layers of biLM:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估隐藏状态在不同层中捕获的信息类型，ELMo 分别应用于语义密集和语法密集任务，使用 biLM 不同层中的表示：
- en: '**Semantic task**: The *word sense disambiguation (WSD)* task emphasizes the
    meaning of a word given a context. The biLM top layer is better at this task than
    the first layer.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语义任务**：*词义消歧（WSD）* 任务强调在给定上下文的情况下单词的含义。顶层 biLM 在这个任务上比第一层更好。'
- en: '**Syntax task**: The *[part-of-speech](https://en.wikipedia.org/wiki/Part-of-speech_tagging)
    (POS) tagging* task aims to infer the grammatical role of a word in one sentence.
    A higher accuracy can be achieved by using the biLM first layer than the top layer.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语法任务**：*[词性标注](https://en.wikipedia.org/wiki/Part-of-speech_tagging)*（POS）标注任务旨在推断一句话中单词的语法角色。使用
    biLM 的第一层比顶层可以获得更高的准确性。'
- en: The comparison study indicates that syntactic information is better represented
    at lower layers while semantic information is captured by higher layers. Because
    different layers tend to carry different type of information, *stacking them together
    helps*.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 比较研究表明，句法信息在较低层更好地表示，而语义信息则由较高层捕获。因为不同层往往携带不同类型的信息，*将它们堆叠在一起有助于*。
- en: Use ELMo in Downstream Tasks
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在下游任务中使用 ELMo
- en: Similar to how [CoVe](#use-cove-in-downstream-tasks) can help different downstream
    tasks, ELMo embedding vectors are included in the input or lower levels of task-specific
    models. Moreover, for some tasks (i.e., [SNLI](#nli) and [SQuAD](#qa), but not
    [SRL](#srl)), adding them into the output level helps too.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于 [CoVe](#use-cove-in-downstream-tasks) 如何帮助不同的下游任务，ELMo 嵌入向量被包含在输入或任务特定模型的较低层。此外，对于一些任务（即
    [SNLI](#nli) 和 [SQuAD](#qa)，但不包括 [SRL](#srl)），将它们添加到输出层也有帮助。
- en: The improvements brought up by ELMo are largest for tasks with a small supervised
    dataset. With ELMo, we can also achieve similar performance with much less labeled
    data.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ELMo 带来的改进对于具有小型监督数据集的任务影响最大。使用 ELMo，我们也可以用更少的标记数据达到类似的性能。
- en: '**Summary**: The language model pre-training is unsupervised and theoretically
    the pre-training can be scaled up as much as possible since the unlabeled text
    corpora are abundant. However, it still has the dependency on task-customized
    models and thus the improvement is only incremental, while searching for a good
    model architecture for every task remains non-trivial.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '**总结**：语言模型的预训练是无监督的，理论上预训练可以尽可能扩展，因为未标记的文本语料库丰富。然而，它仍然依赖于任务定制模型，因此改进仅是渐进的，而为每个任务寻找一个良好的模型架构仍然是非平凡的。'
- en: Cross-View Training
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 跨视图训练
- en: In ELMo the unsupervised pre-training and task-specific learning happen for
    two independent models in two separate training stages. **Cross-View Training**
    (abbr. **CVT**; [Clark et al., 2018](https://arxiv.org/abs/1809.08370)) combines
    them into one unified semi-supervised learning procedure where the representation
    of a biLSTM encoder is improved by both supervised learning with labeled data
    and unsupervised learning with unlabeled data on auxiliary tasks.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在 ELMo 中，无监督预训练和任务特定学习发生在两个独立模型中的两个单独训练阶段。**跨视图训练**（简称 **CVT**；[Clark et al.,
    2018](https://arxiv.org/abs/1809.08370)）将它们合并为一个统一的半监督学习过程，其中 biLSTM 编码器的表示通过有标记数据的监督学习和无标记数据的无监督学习在辅助任务上得到改进。
- en: Model Architecture
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型架构
- en: The model consists of a two-layer bidirectional LSTM encoder and a primary prediction
    module. During training, the model is fed with labeled and unlabeled data batches
    alternatively.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型由两层双向 LSTM 编码器和一个主要预测模块组成。在训练过程中，模型交替接收带标签和未标记的数据批次。
- en: On *labeled examples*, all the model parameters are updated by standard supervised
    learning. The loss is the standard cross entropy.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在*带标签的示例*上，所有模型参数都通过标准监督学习进行更新。损失是标准的交叉熵。
- en: On *unlabeled examples*, the primary prediction module still can produce a “soft”
    target, even though we cannot know exactly how accurate they are. In a couple
    of auxiliary tasks, the predictor only sees and processes a restricted view of
    the input, such as only using encoder hidden state representation in one direction.
    The auxiliary task outputs are expected to match the primary prediction target
    for a full view of input.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在*未标记的示例*上，主要预测模块仍然可以生成“软”目标，尽管我们无法准确知道它们有多准确。在一些辅助任务中，预测器只看到并处理输入的受限视图，例如只使用编码器隐藏状态表示中的一个方向。期望辅助任务输出与完整输入的主要预测目标匹配。
- en: In this way, the encoder is forced to distill the knowledge of the full context
    into partial representation. At this stage, the biLSTM encoder is backpropagated
    but the primary prediction module is *fixed*. The loss is to minimize the distance
    between auxiliary and primary predictions.
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这样，编码器被迫将完整上下文的知识提炼成部分表示。在这个阶段，biLSTM 编码器被反向传播，但主要预测模块是*固定的*。损失在于最小化辅助和主要预测之间的距离。
- en: '![](../Images/8154e60ba2d899fa5219289be73c8ba6.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8154e60ba2d899fa5219289be73c8ba6.png)'
- en: 'Fig. 4\. The overview of semi-supervised language model cross-view training.
    (Image source: [original paper](https://arxiv.org/abs/1809.08370))'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图4. 半监督语言模型跨视图训练的概述。（图片来源：[原始论文](https://arxiv.org/abs/1809.08370)）
- en: Multi-Task Learning
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多任务学习
- en: When training for multiple tasks simultaneously, CVT adds several extra primary
    prediction models for additional tasks. They all share the same sentence representation
    encoder. During supervised training, once one task is randomly selected, parameters
    in its corresponding predictor and the representation encoder are updated. With
    unlabeled data samples, the encoder is optimized jointly across all the tasks
    by minimizing the differences between auxiliary outputs and primary prediction
    for every task.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 当同时训练多个任务时，CVT 为额外任务添加了几个额外的主要预测模型。它们都共享相同的句子表示编码器。在监督训练期间，一旦随机选择一个任务，就会更新其对应预测器和表示编码器中的参数。通过未标记的数据样本，编码器通过最小化辅助输出和每个任务的主要预测之间的差异来跨所有任务联合优化。
- en: 'The multi-task learning encourages better generality of representation and
    in the meantime produces a nice side-product: all-tasks-labeled examples from
    unlabeled data. They are precious data labels considering that cross-task labels
    are useful but fairly rare.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 多任务学习鼓励更好的表示泛化性，同时产生一个不错的副产品：来自未标记数据的所有任务标记示例。考虑到跨任务标签很有用但相当稀有，它们是宝贵的数据标签。
- en: Use CVT in Downstream Tasks
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在下游任务中使用 CVT
- en: Theoretically the primary prediction module can take any form, generic or task-specific
    design. The examples presented in the CVT paper include both cases.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 理论上，主要预测模块可以采用任何形式，通用或特定于任务的设计。CVT 论文中提供的示例包括这两种情况。
- en: 'In sequential tagging tasks (classification for every token) like [NER](#ner)
    or [POS](#pos) tagging, the predictor module contains two fully connected layers
    and a softmax layer on the output to produce a probability distribution over class
    labels. For each token $\mathbf{x}_i$, we take the corresponding hidden states
    in two layers, $\mathbf{h}_1^{(i)}$ and $\mathbf{h}_2^{(i)}$:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在顺序标记任务（对每个标记进行分类）中，如[NER](#ner)或[POS](#pos)标记，预测器模块包含两个全连接层和一个 softmax 层，用于生成类标签的概率分布。对于每个标记$\mathbf{x}_i$，我们取两层中的相应隐藏状态，$\mathbf{h}_1^{(i)}$
    和 $\mathbf{h}_2^{(i)}$：
- en: $$ \begin{aligned} p_\theta(y_i \mid \mathbf{x}_i) &= \text{NN}(\mathbf{h}^{(i)})
    \\ &= \text{NN}([\mathbf{h}_1^{(i)}; \mathbf{h}_2^{(i)}]) \\ &= \text{softmax}
    \big( \mathbf{W}\cdot\text{ReLU}(\mathbf{W'}\cdot[\mathbf{h}_1^{(i)}; \mathbf{h}_2^{(i)}])
    + \mathbf{b} \big) \end{aligned} $$
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \begin{aligned} p_\theta(y_i \mid \mathbf{x}_i) &= \text{NN}(\mathbf{h}^{(i)})
    \\ &= \text{NN}([\mathbf{h}_1^{(i)}; \mathbf{h}_2^{(i)}]) \\ &= \text{softmax}
    \big( \mathbf{W}\cdot\text{ReLU}(\mathbf{W'}\cdot[\mathbf{h}_1^{(i)}; \mathbf{h}_2^{(i)}])
    + \mathbf{b} \big) \end{aligned} $$
- en: The auxiliary tasks are only fed with forward or backward LSTM state in the
    first layer. Because they only observe partial context, either on the left or
    right, they have to learn like a language model, trying to predict the next token
    given the context. The `fwd` and `bwd` auxiliary tasks only take one direction.
    The `future` and `past` tasks take one step further in forward and backward direction,
    respectively.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 辅助任务只接收第一层的前向或后向LSTM状态。因为它们只观察到部分上下文，要么在左侧要么在右侧，它们必须像语言模型一样学习，尝试根据上下文预测下一个标记。`fwd`和`bwd`辅助任务只采用一个方向。`future`和`past`任务在前向和后向方向上进一步进行一步。
- en: $$ \begin{aligned} p_\theta^\text{fwd}(y_i \mid \mathbf{x}_i) &= \text{NN}^\text{fwd}(\overrightarrow{\mathbf{h}}^{(i)})
    \\ p_\theta^\text{bwd}(y_i \mid \mathbf{x}_i) &= \text{NN}^\text{bwd}(\overleftarrow{\mathbf{h}}^{(i)})
    \\ p_\theta^\text{future}(y_i \mid \mathbf{x}_i) &= \text{NN}^\text{future}(\overrightarrow{\mathbf{h}}^{(i-1)})
    \\ p_\theta^\text{past}(y_i \mid \mathbf{x}_i) &= \text{NN}^\text{past}(\overleftarrow{\mathbf{h}}^{(i+1)})
    \end{aligned} $$![](../Images/a53253d738a76634f4155d83123fa9c2.png)
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \begin{aligned} p_\theta^\text{fwd}(y_i \mid \mathbf{x}_i) &= \text{NN}^\text{fwd}(\overrightarrow{\mathbf{h}}^{(i)})
    \\ p_\theta^\text{bwd}(y_i \mid \mathbf{x}_i) &= \text{NN}^\text{bwd}(\overleftarrow{\mathbf{h}}^{(i)})
    \\ p_\theta^\text{future}(y_i \mid \mathbf{x}_i) &= \text{NN}^\text{future}(\overrightarrow{\mathbf{h}}^{(i-1)})
    \\ p_\theta^\text{past}(y_i \mid \mathbf{x}_i) &= \text{NN}^\text{past}(\overleftarrow{\mathbf{h}}^{(i+1)})
    \end{aligned} $$![](../Images/a53253d738a76634f4155d83123fa9c2.png)
- en: 'Fig. 5\. The sequential tagging task depends on four auxiliary prediction models,
    their inputs only involving hidden states in one direction: forward, backward,
    future and past. (Image source: [original paper](https://arxiv.org/abs/1809.08370))'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图5。顺序标记任务依赖于四个辅助预测模型，它们的输入只涉及单向隐藏状态：前向、后向、未来和过去。（图片来源：[原论文](https://arxiv.org/abs/1809.08370)）
- en: Note that if the primary prediction module has dropout, the dropout layer works
    as usual when training with labeled data, but it is not applied when generating
    “soft” target for auxiliary tasks during training with unlabeled data.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，如果主要的预测模块使用了dropout，在使用标记数据进行训练时，dropout层会像平常一样工作，但在使用无标签数据进行训练时，不会应用于生成辅助任务的“软”目标。
- en: 'In the machine translation task, the primary prediction module is replaced
    with a standard unidirectional LSTM decoder with attention. There are two auxiliary
    tasks: (1) apply dropout on the attention weight vector by randomly zeroing out
    some values; (2) predict the future word in the target sequence. The primary prediction
    for auxiliary tasks to match is the best predicted target sequence produced by
    running the fixed primary decoder on the input sequence with [beam search](https://en.wikipedia.org/wiki/Beam_search).'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器翻译任务中，主要的预测模块被标准的单向LSTM解码器与注意力所取代。有两个辅助任务：（1）通过随机将一些值归零来在注意力权重向量上应用dropout；（2）预测目标序列中的未来单词。辅助任务的主要预测是通过在输入序列上运行固定的主要解码器并使用[束搜索](https://en.wikipedia.org/wiki/Beam_search)产生的最佳预测目标序列来匹配的。
- en: ULMFiT
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ULMFiT
- en: The idea of using generative pretrained LM + task-specific fine-tuning was first
    explored in ULMFiT ([Howard & Ruder, 2018](https://arxiv.org/abs/1801.06146)),
    directly motivated by the success of using ImageNet pre-training for computer
    vision tasks. The base model is [AWD-LSTM](https://arxiv.org/abs/1708.02182).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 使用生成式预训练LM + 任务特定微调的想法首次在ULMFiT中探索（[Howard & Ruder, 2018](https://arxiv.org/abs/1801.06146)），直接受到使用ImageNet预训练进行计算机视觉任务成功的启发。基础模型是[AWD-LSTM](https://arxiv.org/abs/1708.02182)。
- en: 'ULMFiT follows three steps to achieve good transfer learning results on downstream
    language classification tasks:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: ULMFiT遵循三个步骤来在下游语言分类任务上实现良好的迁移学习结果：
- en: '*General LM pre-training*: on Wikipedia text.'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*通用LM预训练*：在维基百科文本上。'
- en: '*Target task LM fine-tuning*: ULMFiT proposed two training techniques for stabilizing
    the fine-tuning process. See below.'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*目标任务LM微调*：ULMFiT提出了两种训练技术来稳定微调过程。见下文。'
- en: '**Discriminative fine-tuning** is motivated by the fact that different layers
    of LM capture different types of information (see [discussion](#elmo-representations)
    above). ULMFiT proposed to tune each layer with different learning rates, $\{\eta^1,
    \dots, \eta^\ell, \dots, \eta^L\}$, where $\eta$ is the base learning rate for
    the first layer, $\eta^\ell$ is for the $\ell$-th layer and there are $L$ layers
    in total.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**区分性微调**的动机是LM的不同层捕获不同类型的信息（见上面的[讨论](#elmo-representations)）。ULMFiT建议使用不同的学习率$\{\eta^1,
    \dots, \eta^\ell, \dots, \eta^L\}$微调每一层，其中$\eta$是第一层的基础学习率，$\eta^\ell$是第$\ell$层的学习率，总共有$L$层。'
- en: '**Slanted triangular learning rates (STLR)** refer to a special learning rate
    scheduling that first linearly increases the learning rate and then linearly decays
    it. The increase stage is short so that the model can converge to a parameter
    space suitable for the task fast, while the decay period is long allowing for
    better fine-tuning.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**倾斜三角形学习率（STLR）** 是指一种特殊的学习率调度，首先线性增加学习率，然后线性衰减。增加阶段很短，使模型能够快速收敛到适合任务的参数空间，而衰减期较长，可以更好地进行微调。'
- en: '*Target task classifier fine-tuning*: The pretrained LM is augmented with two
    standard feed-forward layers and a softmax normalization at the end to predict
    a target label distribution.'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*目标任务分类器微调*：预训练的LM通过两个标准前馈层和最后的softmax归一化来预测目标标签分布。'
- en: '**Concat pooling** extracts max-polling and mean-pooling over the history of
    hidden states and concatenates them with the final hidden state.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**连接池** 从隐藏状态的历史中提取最大池化和平均池化，并将它们与最终隐藏状态连接起来。'
- en: '**Gradual unfreezing** helps to avoid catastrophic forgetting by gradually
    unfreezing the model layers starting from the last one. First the last layer is
    unfrozen and fine-tuned for one epoch. Then the next lower layer is unfrozen.
    This process is repeated until all the layers are tuned.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**渐进解冻** 通过逐渐解冻模型层来避免灾难性遗忘，从最后一层开始逐渐解冻。首先解冻最后一层，并进行一轮微调。然后解冻下一层。重复此过程，直到所有层都被微调。'
- en: '![](../Images/32bdf1c8e90e836e9925564a53f7fe7b.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/32bdf1c8e90e836e9925564a53f7fe7b.png)'
- en: 'Fig. 6\. Three training stages of ULMFiT. (Image source: [original paper](https://arxiv.org/abs/1801.06146))'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图6. ULMFiT的三个训练阶段。（图片来源：[原始论文](https://arxiv.org/abs/1801.06146)）
- en: GPT
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GPT
- en: Following the similar idea of ELMo, OpenAI **GPT**, short for **Generative Pre-training
    Transformer** ([Radford et al., 2018](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)),
    expands the unsupervised language model to a much larger scale by training on
    a giant collection of free text corpora. Despite of the similarity, GPT has two
    major differences from ELMo.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 遵循ELMo的类似思想，OpenAI **GPT**，即**生成式预训练Transformer**（[Radford等，2018](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)），通过在巨大的自由文本语料库上进行训练，将无监督语言模型扩展到更大规模。尽管有相似之处，但GPT与ELMo有两个主要区别。
- en: 'The model architectures are different: ELMo uses a shallow concatenation of
    independently trained left-to-right and right-to-left multi-layer LSTMs, while
    GPT is a multi-layer transformer decoder.'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型架构不同：ELMo使用独立训练的从左到右和从右到左的多层LSTM浅层连接，而GPT是多层Transformer解码器。
- en: 'The use of contextualized embeddings in downstream tasks are different: ELMo
    feeds embeddings into models customized for specific tasks as additional features,
    while GPT fine-tunes the same base model for all end tasks.'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在下游任务中使用上下文化嵌入不同：ELMo将嵌入馈送到为特定任务定制的模型中作为附加特征，而GPT为所有最终任务微调相同的基础模型。
- en: Transformer Decoder as Language Model
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Transformer解码器作为语言模型
- en: Compared to the [original transformer](https://arxiv.org/abs/1706.03762) architecture,
    the [transformer decoder](https://arxiv.org/abs/1801.10198) model discards the
    encoder part, so there is only one single input sentence rather than two separate
    source and target sequences.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 与[原始Transformer](https://arxiv.org/abs/1706.03762)架构相比，[Transformer解码器](https://arxiv.org/abs/1801.10198)模型丢弃了编码器部分，因此只有一个单独的输入句子，而不是两个独立的源序列和目标序列。
- en: This model applies multiple transformer blocks over the embeddings of input
    sequences. Each block contains a masked *multi-headed self-attention* layer and
    a *pointwise feed-forward* layer. The final output produces a distribution over
    target tokens after softmax normalization.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型在输入序列的嵌入上应用多个Transformer块。每个块包含一个掩码的*多头自注意力*层和一个*逐点前馈*层。在softmax归一化后，最终输出产生目标标记的分布。
- en: '![](../Images/c56af6fb2bb49ce420474bb91bb09e19.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c56af6fb2bb49ce420474bb91bb09e19.png)'
- en: Fig. 7\. The transformer decoder model architecture in OpenAI GPT.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图7. OpenAI GPT中的Transformer解码器模型架构。
- en: 'The loss is the negative log-likelihood, same as [ELMo](#elmo), but without
    backward computation. Let’s say, the context window of the size $k$ is located
    before the target word and the loss would look like:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 损失是负对数似然，与[ELMo](#elmo)相同，但没有反向计算。假设大小为$k$的上下文窗口位于目标词之前，损失将如下所示：
- en: $$ \mathcal{L}_\text{LM} = -\sum_{i} \log p(x_i\mid x_{i-k}, \dots, x_{i-1})
    $$
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \mathcal{L}_\text{LM} = -\sum_{i} \log p(x_i\mid x_{i-k}, \dots, x_{i-1})
    $$
- en: Byte Pair Encoding
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 字节对编码
- en: '**Byte Pair Encoding** ([**BPE**](https://arxiv.org/abs/1508.07909)) is used
    to encode the input sequences. BPE was originally proposed as a data compression
    algorithm in 1990s and then was adopted to solve the open-vocabulary issue in
    machine translation, as we can easily run into rare and unknown words when translating
    into a new language. Motivated by the intuition that rare and unknown words can
    often be decomposed into multiple subwords, BPE finds the best word segmentation
    by iteratively and greedily merging frequent pairs of characters.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '**字节对编码**（[**BPE**](https://arxiv.org/abs/1508.07909)）用于编码输入序列。BPE最初是在1990年代作为数据压缩算法提出的，后来被采用来解决机器翻译中的开放词汇问题，因为在翻译成新语言时很容易遇到罕见和未知的单词。受到罕见和未知单词通常可以分解为多个子词的直觉启发，BPE通过迭代和贪婪地合并频繁的字符对找到最佳的单词分割。'
- en: Supervised Fine-Tuning
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 监督微调
- en: The most substantial upgrade that OpenAI GPT proposed is to get rid of the task-specific
    model and use the pre-trained language model directly!
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI GPT提出的最重要的升级是摆脱特定任务模型，直接使用预训练的语言模型！
- en: Let’s take classification as an example. Say, in the labeled dataset, each input
    has $n$ tokens, $\mathbf{x} = (x_1, \dots, x_n)$, and one label $y$. GPT first
    processes the input sequence $\mathbf{x}$ through the pre-trained transformer
    decoder and the last layer output for the last token $x_n$ is $\mathbf{h}_L^{(n)}$.
    Then with only one new trainable weight matrix $\mathbf{W}_y$, it can predict
    a distribution over class labels.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 以分类为例。假设在标记的数据集中，每个输入有$n$个标记，$\mathbf{x} = (x_1, \dots, x_n)$，一个标签$y$。GPT首先通过预训练的变压器解码器处理输入序列$\mathbf{x}$，最后一个标记$x_n$的最后一层输出为$\mathbf{h}_L^{(n)}$。然后，只需一个新的可训练权重矩阵$\mathbf{W}_y$，就可以预测类标签的分布。
- en: '![](../Images/875401fc21f1e27510b35d8e9743ff71.png)$$ P(y\mid x_1, \dots, x_n)
    = \text{softmax}(\mathbf{h}_L^{(n)}\mathbf{W}_y) $$'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/875401fc21f1e27510b35d8e9743ff71.png)$$ P(y\mid x_1, \dots, x_n)
    = \text{softmax}(\mathbf{h}_L^{(n)}\mathbf{W}_y) $$'
- en: 'The loss is to minimize the negative log-likelihood for true labels. In addition,
    adding the LM loss as an auxiliary loss is found to be beneficial, because:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数是为了最小化真实标签的负对数似然。此外，将LM损失作为辅助损失添加是有益的，因为：
- en: (1) it helps accelerate convergence during training and
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (1) 它有助于加速训练过程的收敛
- en: (2) it is expected to improve the generalization of the supervised model.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (2) 预计将改善监督模型的泛化能力。
- en: $$ \begin{aligned} \mathcal{L}_\text{cls} &= \sum_{(\mathbf{x}, y) \in \mathcal{D}}
    \log P(y\mid x_1, \dots, x_n) = \sum_{(\mathbf{x}, y) \in \mathcal{D}} \log \text{softmax}(\mathbf{h}_L^{(n)}(\mathbf{x})\mathbf{W}_y)
    \\ \mathcal{L}_\text{LM} &= -\sum_{i} \log p(x_i\mid x_{i-k}, \dots, x_{i-1})
    \\ \mathcal{L} &= \mathcal{L}_\text{cls} + \lambda \mathcal{L}_\text{LM} \end{aligned}
    $$
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \begin{aligned} \mathcal{L}_\text{cls} &= \sum_{(\mathbf{x}, y) \in \mathcal{D}}
    \log P(y\mid x_1, \dots, x_n) = \sum_{(\mathbf{x}, y) \in \mathcal{D}} \log \text{softmax}(\mathbf{h}_L^{(n)}(\mathbf{x})\mathbf{W}_y)
    \\ \mathcal{L}_\text{LM} &= -\sum_{i} \log p(x_i\mid x_{i-k}, \dots, x_{i-1})
    \\ \mathcal{L} &= \mathcal{L}_\text{cls} + \lambda \mathcal{L}_\text{LM} \end{aligned}
    $$
- en: With similar designs, no customized model structure is needed for other end
    tasks (see Fig. 7). If the task input contains multiple sentences, a special delimiter
    token (`$`) is added between each pair of sentences. The embedding for this delimiter
    token is a new parameter we need to learn, but it should be pretty minimal.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 采用类似的设计，其他端任务不需要定制的模型结构（参见图7）。如果任务输入包含多个句子，那么在每对句子之间添加一个特殊的分隔符标记（`$`）。这个分隔符标记的嵌入是我们需要学习的新参数，但应该非常小。
- en: For the sentence similarity task, because the ordering does not matter, both
    orderings are included. For the multiple choice task, the context is paired with
    every answer candidate.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 对于句子相似性任务，因为顺序不重要，两种顺序都包含在内。对于多项选择任务，上下文与每个答案候选项配对。
- en: '![](../Images/8df4c9a97a91221efbc0d9a54dc72654.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8df4c9a97a91221efbc0d9a54dc72654.png)'
- en: 'Fig. 8\. Training objects in slightly modified GPT transformer models for downstream
    tasks. (Image source: [original paper](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf))'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图8. 对下游任务进行轻微修改的GPT变压器模型的训练对象。（图片来源：[原始论文](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)）
- en: '**Summary**: It is super neat and encouraging to see that such a general framework
    is capable to beat SOTA on most language tasks at that time (June 2018). At the
    first stage, generative pre-training of a language model can absorb as much free
    text as possible. Then at the second stage, the model is fine-tuned on specific
    tasks with a small labeled dataset and a minimal set of new parameters to learn.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '**摘要**：看到这样一个通用框架能够在当时（2018年6月）击败大多数语言任务的SOTA是非常整洁和鼓舞人心的。在第一阶段，语言模型的生成式预训练可以吸收尽可能多的自由文本。然后在第二阶段，模型在具有少量标记数据集和最小一组新参数的特定任务上进行微调学习。'
- en: One limitation of GPT is its uni-directional nature — the model is only trained
    to predict the future left-to-right context.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: GPT的一个局限性是其单向性 —— 该模型只被训练来预测未来的从左到右的上下文。
- en: BERT
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: BERT
- en: '**BERT**, short for **Bidirectional Encoder Representations from Transformers**
    ([Devlin, et al., 2019](https://arxiv.org/abs/1810.04805)) is a direct descendant
    to [GPT](#gpt): train a large language model on free text and then fine-tune on
    specific tasks without customized network architectures.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '**BERT**，全称**双向编码器表示来自变压器**（[Devlin, et al., 2019](https://arxiv.org/abs/1810.04805)）是[GPT](#gpt)的直接后裔：在自由文本上训练一个大型语言模型，然后在不使用定制网络架构的情况下对特定任务进行微调。'
- en: 'Compared to GPT, the largest difference and improvement of BERT is to make
    training **bi-directional**. The model learns to predict both context on the left
    and right. The paper according to the ablation study claimed that:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 与GPT相比，BERT最大的区别和改进在于使训练**双向**。该模型学会预测左侧和右侧的上下文。根据消融研究，论文声称：
- en: “bidirectional nature of our model is the single most important new contribution”
  id: totrans-128
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “我们模型的双向性质是最重要的新贡献”
- en: Pre-training Tasks
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预训练任务
- en: The model architecture of BERT is a multi-layer bidirectional Transformer encoder.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: BERT的模型架构是一个多层双向变压器编码器。
- en: '![](../Images/21d172e04ad3af78ccd1b25c91f6e051.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/21d172e04ad3af78ccd1b25c91f6e051.png)'
- en: 'Fig. 9\. Recap of Transformer Encoder model architecture. (Image source: [Transformer
    paper](https://arxiv.org/abs/1706.03762))'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 图9. 变压器编码器模型架构回顾。（图片来源：[变压器论文](https://arxiv.org/abs/1706.03762)）
- en: To encourage the bi-directional prediction and sentence-level understanding,
    BERT is trained with two tasks instead of the basic language task (that is, to
    predict the next token given context).
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 为了鼓励双向预测和句子级理解，BERT训练了两个任务，而不是基本的语言任务（即根据上下文预测下一个标记）。
- en: '***Task 1: Mask language model (MLM)**'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '***任务1：掩码语言模型（MLM）**'
- en: 'From [Wikipedia](https://en.wikipedia.org/wiki/Cloze_test): “A cloze test (also
    cloze deletion test) is an exercise, test, or assessment consisting of a portion
    of language with certain items, words, or signs removed (cloze text), where the
    participant is asked to replace the missing language item. … The exercise was
    first described by W.L. Taylor in 1953.”'
  id: totrans-135
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来自[Wikipedia](https://en.wikipedia.org/wiki/Cloze_test)：“闭式测试（也称为闭式删除测试）是一种练习、测试或评估，由一部分语言组成，其中某些项目、单词或符号被删除（闭式文本），参与者被要求替换缺失的语言项目。…该练习最早由W.L.泰勒在1953年描述。”
- en: 'It is unsurprising to believe that a representation that learns the context
    around a word rather than just after the word is able to better capture its meaning,
    both syntactically and semantically. BERT encourages the model to do so by training
    on the *“mask language model” task*:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 通过训练*“掩码语言模型”任务*，可以毫不奇怪地认为，学习单词周围而不仅仅是单词后面的上下文的表示能够更好地捕捉其含义，无论是在句法上还是语义上。BERT通过这种方式鼓励模型：
- en: 'Randomly mask 15% of tokens in each sequence. Because if we only replace masked
    tokens with a special placeholder `[MASK]`, the special token would never be encountered
    during fine-tuning. Hence, BERT employed several heuristic tricks:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机掩码每个序列中的15%的标记。因为如果我们只用特殊占位符`[MASK]`替换掩码标记，那么在微调过程中永远不会遇到这个特殊标记。因此，BERT采用了几种启发式技巧：
- en: (a) with 80% probability, replace the chosen words with `[MASK]`;
  id: totrans-138
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: (a) 以80%的概率，用`[MASK]`替换选定的单词；
- en: (b) with 10% probability, replace with a random word;
  id: totrans-139
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: (b) 以10%的概率，随机替换为一个单词；
- en: (c) with 10% probability, keep it the same.
  id: totrans-140
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: (c) 以10%的概率，保持不变。
- en: The model only predicts the missing words, but it has no information on which
    words have been replaced or which words should be predicted. The output size is
    only 15% of the input size.
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 该模型只预测缺失的单词，但它没有信息表明哪些单词已被替换或应该被预测。输出大小仅为输入大小的15%。
- en: '**Task 2: Next sentence prediction**'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '**任务2：下一个句子预测**'
- en: 'Motivated by the fact that many downstream tasks involve the understanding
    of relationships between sentences (i.e., [QA](#qa), [NLI](#nli)), BERT added
    another auxiliary task on training a *binary classifier* for telling whether one
    sentence is the next sentence of the other:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 受到许多下游任务涉及句子之间关系理解的事实的启发（即，[QA](#qa)、[NLI](#nli)），BERT 在训练中增加了另一个辅助任务，即训练一个用于告知一个句子是否是另一个句子的下一个句子的
    *二元分类器*：
- en: 'Sample sentence pairs (A, B) so that:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 样本句对 (A, B) 使得：
- en: (a) 50% of the time, B follows A;
  id: totrans-145
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: (a) 50% 的时间，B 跟随 A；
- en: (b) 50% of the time, B does not follow A.
  id: totrans-146
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: (b) 50% 的时间，B 不跟随 A。
- en: The model processes both sentences and output a binary label indicating whether
    B is the next sentence of A.
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 该模型处理两个句子并输出一个二进制标签，指示 B 是否是 A 的下一个句子。
- en: The training data for both auxiliary tasks above can be trivially generated
    from any monolingual corpus. Hence the scale of training is unbounded. The training
    loss is the sum of the mean masked LM likelihood and mean next sentence prediction
    likelihood.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 上述两个辅助任务的训练数据可以轻松地从任何单语语料库中生成。因此，训练规模是不受限制的。训练损失是平均掩码语言模型似然和平均下一个句子预测似然的总和。
- en: '![](../Images/04602b91571e9fcfa55944ab22493b96.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/04602b91571e9fcfa55944ab22493b96.png)'
- en: 'Fig. 10\. Comparison of BERT, OpenAI GPT and ELMo model architectures. (Image
    source: [original paper](https://arxiv.org/abs/1810.04805))'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10\. BERT、OpenAI GPT 和 ELMo 模型架构的比较。 (图片来源：[原始论文](https://arxiv.org/abs/1810.04805))
- en: Input Embedding
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 输入嵌入
- en: 'The input embedding is the sum of three parts:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 输入嵌入是三个部分的总和：
- en: '*WordPiece tokenization embeddings*: The [WordPiece](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/37842.pdf)
    [model](https://arxiv.org/pdf/1609.08144.pdf) was originally proposed for Japanese
    or Korean segmentation problem. Instead of using naturally split English word,
    they can be further divided into smaller sub-word units so that it is more effective
    to handle rare or unknown words. Please read [linked](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/37842.pdf)
    [papers](https://arxiv.org/pdf/1609.08144.pdf) for the optimal way to split words
    if interested.'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*WordPiece 分词嵌入*：[WordPiece](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/37842.pdf)
    模型最初是为日语或韩语分词问题提出的。它们可以将英语单词进一步分割为更小的子词单元，以便更有效地处理罕见或未知单词。如果感兴趣，请阅读 [链接的](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/37842.pdf)
    [论文](https://arxiv.org/pdf/1609.08144.pdf) 以了解拆分单词的最佳方式。'
- en: '*Segment embeddings*: If the input contains two sentences, they have sentence
    A embeddings and sentence B embeddings respectively and they are separated by
    a special character `[SEP]`; Only sentence A embeddings are used if the input
    only contains one sentence.'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*段落嵌入*：如果输入包含两个句子，则它们分别具有句子 A 嵌入和句子 B 嵌入，并且它们由特殊字符 `[SEP]` 分隔；如果输入只包含一个句子，则仅使用句子
    A 嵌入。'
- en: '*Position embeddings*: Positional embeddings are learned rather than hard-coded.'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*位置嵌入*：位置嵌入是学习而不是硬编码的。'
- en: '![](../Images/a51025c2642a16e94382a84078f418ed.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a51025c2642a16e94382a84078f418ed.png)'
- en: 'Fig. 11\. BERT input representation. (Image source: [original paper](https://arxiv.org/abs/1810.04805))'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11\. BERT 输入表示。 (图片来源：[原始论文](https://arxiv.org/abs/1810.04805))
- en: Note that the first token is always forced to be `[CLS]` — a placeholder that
    will be used later for prediction in downstream tasks.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，第一个标记始终被强制为 `[CLS]` —— 一个稍后将用于下游任务中预测的占位符。
- en: Use BERT in Downstream Tasks
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 BERT 在下游任务中
- en: BERT fine-tuning requires only a few new parameters added, just like OpenAI
    GPT.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 微调只需要添加一些新参数，就像 OpenAI GPT 一样。
- en: For classification tasks, we get the prediction by taking the final hidden state
    of the special first token `[CLS]`, $\mathbf{h}^\text{[CLS]}_L$, and multiplying
    it with a small weight matrix, $\text{softmax}(\mathbf{h}^\text{[CLS]}_L \mathbf{W}_\text{cls})$.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分类任务，我们通过取特殊的第一个标记 `[CLS]` 的最终隐藏状态 $\mathbf{h}^\text{[CLS]}_L$，并将其与一个小的权重矩阵相乘，$\text{softmax}(\mathbf{h}^\text{[CLS]}_L
    \mathbf{W}_\text{cls})$ 来得到预测。
- en: For [QA](#qa) tasks like SQuAD, we need to predict the text span in the given
    paragraph for an given question. BERT predicts two probability distributions of
    every token, being the start and the end of the text span. Only two new small
    matrices, $\mathbf{W}_\text{s}$ and $\mathbf{W}_\text{e}$, are newly learned during
    fine-tuning and $\text{softmax}(\mathbf{h}^\text{(i)}_L \mathbf{W}_\text{s})$
    and $\text{softmax}(\mathbf{h}^\text{(i)}_L \mathbf{W}_\text{e})$ define two probability
    distributions.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 对于像SQuAD这样的[QA](#qa)任务，我们需要预测给定问题的给定段落中的文本跨度。BERT预测每个标记的两个概率分布，即文本跨度的开始和结束。只有两个新的小矩阵，$\mathbf{W}_\text{s}$和$\mathbf{W}_\text{e}$，在微调期间新学习，$\text{softmax}(\mathbf{h}^\text{(i)}_L
    \mathbf{W}_\text{s})$和$\text{softmax}(\mathbf{h}^\text{(i)}_L \mathbf{W}_\text{e})$定义了两个概率分布。
- en: Overall the add-on part for end task fine-tuning is very minimal — one or two
    weight matrices to convert the Transform hidden states to an interpretable format.
    Check the paper for implementation details for other cases.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 总体上，用于最终任务微调的附加部分非常少——只有一个或两个权重矩阵将Transform隐藏状态转换为可解释的格式。查看其他情况的实现细节，请参考论文。
- en: '![](../Images/2c687a403bfe93c0c5cacfde08a426ff.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2c687a403bfe93c0c5cacfde08a426ff.png)'
- en: 'Fig. 12\. Training objects in slightly modified BERT models for downstream
    tasks. (Image source: [original paper](https://arxiv.org/abs/1810.04805))'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 图12. 修改后的BERT模型在下游任务中的训练对象。（图片来源：[原论文](https://arxiv.org/abs/1810.04805)）
- en: A summary table compares differences between fine-tuning of [OpenAI GPT](#gpt)
    and BERT.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 一张总结表比较了[OpenAI GPT](#gpt)和BERT的微调之间的差异。
- en: '| | **OpenAI GPT** | **BERT** | | Special char | `[SEP]` and `[CLS]` are only
    introduced at fine-tuning stage. | `[SEP]` and `[CLS]` and sentence A/B embeddings
    are learned at the pre-training stage. | | Training process | 1M steps, batch
    size 32k words. | 1M steps, batch size 128k words. | | Fine-tuning | lr = 5e-5
    for all fine-tuning tasks. | Use task-specific lr for fine-tuning. |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| | **OpenAI GPT** | **BERT** | | 特殊字符 | `[SEP]`和`[CLS]`仅在微调阶段引入。 | `[SEP]`和`[CLS]`以及句子A/B嵌入在预训练阶段学习。
    | | 训练过程 | 100万步，批量大小32k个单词。 | 100万步，批量大小128k个单词。 | | 微调 | 所有微调任务的学习率为5e-5。 |
    使用任务特定的学习率进行微调。 |'
- en: ALBERT
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ALBERT
- en: '**ALBERT** ([Lan, et al. 2019](https://arxiv.org/abs/1909.11942)), short for
    **A Lite BERT**, is a light-weighted version of [BERT](#BERT) model. An ALBERT
    model can be trained 1.7x faster with 18x fewer parameters, compared to a BERT
    model of similar configuration. ALBERT incorporates three changes as follows:
    the first two help reduce parameters and memory consumption and hence speed up
    the training speed, while the third one proposes a more chanllenging training
    task to replace the next sentence prediction (NSP) objective.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '**ALBERT**（[Lan等人，2019](https://arxiv.org/abs/1909.11942)），简称**A Lite BERT**，是[BERT](#BERT)模型的轻量级版本。与类似配置的BERT模型相比，ALBERT模型的训练速度可以提高1.7倍，参数数量减少18倍。ALBERT包含三个改变：前两个有助于减少参数和内存消耗，从而加快训练速度，而第三个提出了一个更具挑战性的训练任务，以取代下一个句子预测（NSP）目标。'
- en: Factorized Embedding Parameterization
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 因子化嵌入参数化
- en: In BERT, the WordPiece tokenization embedding size $E$ is configured to be the
    same as the hidden state size $H$. That is saying, if we want to increase the
    model size (larger $H$), we need to learn a larger tokenization embedding too,
    which is expensive because it depends on the vocabulary size ($V$).
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在BERT中，WordPiece标记化嵌入大小$E$被配置为与隐藏状态大小$H$相同。也就是说，如果我们想增加模型大小（更大的$H$），我们需要学习更大的标记化嵌入，这是昂贵的，因为它取决于词汇量（$V$）。
- en: Conceptually, because the tokenization embedding is expected to learn *context-independent*
    representation and the hidden states are *context-dependent*, it makes sense to
    separate the size of the hidden layers from the size of vocabulary embedding.
    Using factorized embedding parameterization, the large vocabulary embedding matrix
    of size $V \times H$ is decomposed into two small matrices of size $V \times E$
    and $E \times H$. Given $H \gt E$ or even $H \gg E$, factorization can result
    in significant parameter reduction.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 从概念上讲，由于标记化嵌入预计学习*与上下文无关*的表示，而隐藏状态是*与上下文相关*的，因此将隐藏层的大小与词汇表嵌入的大小分开是有道理的。使用因子化嵌入参数化，大小为$V
    \times H$的大词汇表嵌入矩阵被分解为大小为$V \times E$和$E \times H$的两个小矩阵。给定$H \gt E$甚至$H \gg E$，因子化可以显著减少参数。
- en: Cross-layer Parameter Sharing
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 跨层参数共享
- en: 'Parameter sharing across layers can happen in many ways: (a) only share feed-forward
    part; (b) only share attention parameters; or (c) share all the parameters. This
    technique reduces the number of parameters by a ton and does not damage the performance
    too much.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 跨层参数共享可以通过多种方式实现：(a) 仅共享前向传播部分；(b) 仅共享注意力参数；或者(c) 共享所有参数。这种技术大大减少了参数数量，而且并不会对性能造成太大损害。
- en: Sentence-Order Prediction (SOP)
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 句子顺序预测（SOP）
- en: Interestingly, the [next sentence prediction (NSP)](#NSP) task of BERT turned
    out to be too easy. ALBERT instead adopted a sentence-order prediction (SOP) [self-supervised](https://lilianweng.github.io/posts/2019-11-10-self-supervised/)
    loss,
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，BERT的[下一句预测（NSP）](#NSP)任务结果太容易了。相反，ALBERT采用了句子顺序预测（SOP）[自监督](https://lilianweng.github.io/posts/2019-11-10-self-supervised/)
    损失，
- en: 'Positive sample: two consecutive segments from the same document.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正样本：同一文档中的两个连续片段。
- en: 'Negative sample: same as above, but the segment order is switched.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 负样本：与上述相同，但是段落顺序被交换。
- en: For the NSP task, the model can make reasonable predictions if it is able to
    detect topics when A and B are from different contexts. In comparison, SOP is
    harder as it requires the model to fully understand the coherence and ordering
    between segments.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 对于NSP任务，如果模型能够检测到A和B来自不同上下文时的主题，那么模型可以做出合理的预测。相比之下，SOP更难，因为它要求模型完全理解段落之间的连贯性和顺序。
- en: GPT-2
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GPT-2
- en: The [OpenAI](https://blog.openai.com/better-language-models/) [GPT-2](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
    language model is a direct successor to [GPT](#gpt). GPT-2 has 1.5B parameters,
    10x more than the original GPT, and it achieves SOTA results on 7 out of 8 tested
    language modeling datasets in a *zero-shot transfer setting* without any task-specific
    fine-tuning. The pre-training dataset contains 8 million Web pages collected by
    crawling qualified outbound links from [Reddit](https://www.reddit.com/). Large
    improvements by OpenAI GPT-2 are specially noticeable on small datasets and datasets
    used for measuring *long-term dependency*.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '[OpenAI](https://blog.openai.com/better-language-models/)的[GPT-2](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)语言模型是[GPT](#gpt)的直接继承者。GPT-2有15亿参数，比原始GPT多10倍，它在零-shot转移设置下在8个测试语言建模数据集中的7个上取得了SOTA结果，而且没有进行任何特定任务的微调。预训练数据集包含通过从[Reddit](https://www.reddit.com/)爬取合格的外链收集的800万个网页。OpenAI
    GPT-2在小数据集和用于衡量*长期依赖性*的数据集上的大幅改进特别明显。'
- en: Zero-Shot Transfer
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 零-shot转移
- en: The pre-training task for GPT-2 is solely language modeling. All the downstream
    language tasks are framed as predicting conditional probabilities and there is
    no task-specific fine-tuning.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-2的预训练任务仅仅是语言建模。所有下游语言任务都被构建为预测条件概率，没有进行任何特定任务的微调。
- en: Text generation is straightforward using LM.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用LM进行文本生成很简单。
- en: Machine translation task, for example, English to Chinese, is induced by conditioning
    LM on pairs of “English sentence = Chinese sentence” and “the target English sentence
    =” at the end.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器翻译任务，例如从英语到中文，是通过将语言模型与“英语句子 = 中文句子”和“目标英语句子 =”配对来诱导的。
- en: 'For example, the conditional probability to predict might look like: `P(? |
    I like green apples. = 我喜欢绿苹果。 A cat meows at him. = 一只猫对他喵。It is raining cats
    and dogs. =")`'
  id: totrans-186
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 例如，用于预测的条件概率可能如下所示：`P(? | I like green apples. = 我喜欢绿苹果。 A cat meows at him.
    = 一只猫对他喵。It is raining cats and dogs. =")`
- en: QA task is formatted similar to translation with pairs of questions and answers
    in the context.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: QA任务的格式与在上下文中的问题和答案的翻译类似。
- en: Summarization task is induced by adding `TL;DR:` after the articles in the context.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 摘要任务是通过在上下文中的文章后添加`TL;DR:`来诱导的。
- en: BPE on Byte Sequences
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BPE对字节序列进行编码
- en: Same as the original GPT, GPT-2 uses [BPE](#byte-pair-encoding) but on [UTF-8](https://en.wikipedia.org/wiki/UTF-8)
    byte sequences. Each byte can represent 256 different values in 8 bits, while
    UTF-8 can use up to 4 bytes for one character, supporting up to $2^{31}$ characters
    in total. Therefore, with byte sequence representation we only need a vocabulary
    of size 256 and do not need to worry about pre-processing, tokenization, etc.
    Despite of the benefit, current byte-level LMs still have non-negligible performance
    gap with the SOTA word-level LMs.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 与原始GPT一样，GPT-2使用[BPE](#byte-pair-encoding)但在[UTF-8](https://en.wikipedia.org/wiki/UTF-8)字节序列上。每个字节可以用8位表示256个不同的值，而UTF-8可以使用最多4个字节表示一个字符，支持总共$2^{31}$个字符。因此，使用字节序列表示，我们只需要一个大小为256的词汇表，不需要担心预处理、标记化等问题。尽管有这些好处，当前的字节级语言模型仍然与SOTA单词级语言模型存在着不可忽视的性能差距。
- en: BPE merges frequently co-occurred byte pairs in a greedy manner. To prevent
    it from generating multiple versions of common words (i.e. `dog.`, `dog!` and
    `dog?` for the word `dog`), GPT-2 prevents BPE from merging characters across
    categories (thus `dog` would not be merged with punctuations like `.`, `!` and
    `?`). This tricks help increase the quality of the final byte segmentation.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: BPE以贪婪的方式合并频繁共现的字节对。为了防止生成常见单词的多个版本（例如`dog.`, `dog!`和`dog?`对于单词`dog`），GPT-2阻止BPE跨类别合并字符（因此`dog`不会与标点符号如`.`、`!`和`?`合并）。这些技巧有助于提高最终字节分割的质量。
- en: Using the byte sequence representation, GPT-2 is able to assign a probability
    to any Unicode string, regardless of any pre-processing steps.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 使用字节序列表示，GPT-2能够为任何Unicode字符串分配概率，而不受任何预处理步骤的影响。
- en: Model Modifications
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型修改
- en: 'Compared to GPT, other than having many more transformer layers and parameters,
    GPT-2 incorporates only a few architecture modifications:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 与GPT相比，除了拥有更多的变压器层和参数外，GPT-2仅包含少量架构修改：
- en: '[Layer normalization](https://arxiv.org/abs/1607.06450) was moved to the input
    of each sub-block, similar to a residual unit of type [“building block”](https://arxiv.org/abs/1603.05027)
    (differently from the original type [“bottleneck”](https://arxiv.org/abs/1512.03385),
    it has batch normalization applied before weight layers).'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[层归一化](https://arxiv.org/abs/1607.06450)被移动到每个子块的输入端，类似于类型为[“building block”](https://arxiv.org/abs/1603.05027)的残差单元（与原始类型[“bottleneck”](https://arxiv.org/abs/1512.03385)不同，它在权重层之前应用了批量归一化）。'
- en: An additional layer normalization was added after the final self-attention block.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在最终的自注意力块之后添加了额外的层归一化。
- en: A modified initialization was constructed as a function of the model depth.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个修改后的初始化函数是根据模型深度构建的。
- en: The weights of residual layers were initially scaled by a factor of $1/ \sqrt{N}$
    where N is the number of residual layers.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 残差层的权重最初按照$1/ \sqrt{N}$的因子进行缩放，其中N是残差层的数量。
- en: Use larger vocabulary size and context size.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用更大的词汇量和上下文大小。
- en: RoBERTa
  id: totrans-200
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RoBERTa
- en: '**RoBERTa** (short for **R**obustly **o**ptimized **BERT** **a**pproach; [Liu,
    et al. 2019](https://arxiv.org/abs/1907.11692)) refers to a new receipt for training
    BERT to achieve better results, as they found that the original BERT model is
    significantly undertrained. The receipt contains the following learnings:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '**RoBERTa**（**R**obustly **o**ptimized **BERT** **a**pproach的缩写；[Liu, et al.
    2019](https://arxiv.org/abs/1907.11692)）指的是一种新的训练BERT以获得更好结果的方法，因为他们发现原始的BERT模型训练不足。该方法包括以下学习内容：'
- en: Train for longer with bigger batch size.
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用更大的批量大小进行更长时间的训练。
- en: Remove the [next sentence prediction (NSP)](#nsp) task.
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 移除[下一句预测（NSP）](#nsp)任务。
- en: Use longer sequences in training data format. The paper found that using individual
    sentences as inputs hurts downstream performance. Instead we should use multiple
    sentences sampled contiguously to form longer segments.
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练数据格式中使用更长的序列。研究发现，将单独的句子作为输入会损害下游性能。相反，我们应该使用连续采样的多个句子来形成更长的段落。
- en: Change the masking pattern dynamically. The original BERT applies masking once
    during the data preprocessing stage, resulting in a static mask across training
    epochs. RoBERTa applies masks in 10 different ways across 40 epochs.
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 动态更改掩码模式。原始的BERT在数据预处理阶段只应用一次掩码，导致在训练周期中掩码保持静态。RoBERTa在40个周期中以10种不同的方式应用掩码。
- en: RoBERTa also added a new dataset [CommonCrawl News](https://commoncrawl.org/2016/10/news-dataset-available/)
    and further confirmed that pretraining with *more data helps* improve the performance
    on downstream tasks. It was trained with the [BPE on byte sequences](#bpe-on-byte-sequences),
    same as in [GPT-2](#gpt-2). They also found that choices of hyperparameters have
    a big impact on the model performance.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: RoBERTa还添加了一个新的数据集[CommonCrawl News](https://commoncrawl.org/2016/10/news-dataset-available/)，并进一步证实，预训练使用*更多数据*有助于提高在下游任务上的性能。它是使用[BPE
    on byte sequences](#bpe-on-byte-sequences)进行训练的，与[GPT-2](#gpt-2)相同。他们还发现，超参数的选择对模型性能有很大影响。
- en: T5
  id: totrans-207
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: T5
- en: 'The language model **T5** is short for **“Text-to-Text Transfer Transformer”**
    ([Raffel et al., 2020](https://arxiv.org/abs/1910.10683)). The encoder-decoder
    implementation follows the [original Transformer](https://arxiv.org/abs/1706.03762)
    architecture: tokens → embedding → encoder → decoder → output. T5 adopts the framework
    “Natural Language Decathlon” ([McCann et al., 2018](https://arxiv.org/abs/1806.08730)),
    where many common NLP tasks are translated into question-answering over a context.
    Instead of an explicit QA format, T5 uses short task prefixes to distinguish task
    intentions and separately fine-tunes the model on every individual task. The text-to-text
    framework enables easier transfer learning evaluation with the same model on a
    diverse set of tasks.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型**T5**简称**“文本到文本转换变压器”**([Raffel等人，2020](https://arxiv.org/abs/1910.10683))。编码器-解码器实现遵循[原始Transformer](https://arxiv.org/abs/1706.03762)架构：tokens
    → embedding → encoder → decoder → output。T5采用了“自然语言十项全能”框架([McCann等人，2018](https://arxiv.org/abs/1806.08730))，其中许多常见的NLP任务被转化为在上下文中进行问答。T5使用简短的任务前缀来区分任务意图，并分别在每个单独的任务上微调模型。文本到文本框架使得在多样化任务集上使用相同模型进行更容易的迁移学习评估。
- en: '![](../Images/b46018486da8da6afc2f688082d8ac7f.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b46018486da8da6afc2f688082d8ac7f.png)'
- en: 'Fig. 13\. A diagram of T5 task evaluation. The text-to-text framework casts
    every task into a generic form: feeding input text to predict some target text.
    (Image source: [Raffel et al., 2020](https://arxiv.org/abs/1910.10683))'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 图13. T5任务评估图。文本到文本框架将每个任务转化为通用形式：输入文本以预测某些目标文本。（图片来源：[Raffel等人，2020](https://arxiv.org/abs/1910.10683))
- en: The model is trained on Web corpus extracted from Apr 2019 with various filters
    applied. The model is fine-tuned for each downstream task separately via “adapter
    layers” (add an extra layer for training) or “gradual unfreezing” (see [ULMFiT](#ulmfit)).
    Both fine-tuning approaches only update partial parameters while keeping the majority
    of the model parameters unchanged. T5-11B achieved SOTA results on many NLP tasks.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型是在2019年4月从Web语料库中提取并应用各种过滤器进行训练的。该模型通过“适配器层”（添加额外的层进行训练）或“逐步解冻”（参见[ULMFiT](#ulmfit)）分别为每个下游任务进行微调。这两种微调方法仅更新部分参数，同时保持大部分模型参数不变。T5-11B在许多NLP任务上取得了SOTA结果。
- en: As the authors mentioned in the paper “…our goal is not to propose new methods
    but instead to provide a comprehensive perspective on where the field stands”,
    the T5 long paper described a lot of training setup and evaluation processes in
    detail, a good read for people who are interested in training a LM from scratch.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 正如作者在论文中提到的“…我们的目标不是提出新方法，而是提供一个全面的视角，了解该领域的现状”，T5长篇论文详细描述了许多训练设置和评估过程，对于那些有兴趣从零开始训练LM的人来说是一篇不错的文章。
- en: GPT-3
  id: totrans-213
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GPT-3
- en: '**GPT-3** ([Brown et al., 2020](https://arxiv.org/abs/2005.14165)) has the
    same architecture as [GPT-2](#gpt-2) but contains 175B parameters, 10x larger
    than GPT-2 (1.5B). In addition, GPT-3 uses alternating dense and locally banded
    sparse attention patterns, same as in [sparse transformer](https://lilianweng.github.io/posts/2020-04-07-the-transformer-family/#sparse-attention-matrix-factorization-sparse-transformers).
    In order to fit such a huge model across multiple GPUs, GPT-3 is trained with
    partitions along both width and depth dimension. The training data is a filtered
    version of Common Crawl mixed with a few other high-quality curated datasets.
    To avoid the contamination that downstream tasks might appear in the training
    data, the authors attempted to remove all the overlaps with all the studied benchmark
    dataset from the training dataset. Unfortunately the filtering process is not
    perfect due to a bug.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '**GPT-3**（[Brown et al., 2020](https://arxiv.org/abs/2005.14165)）与[GPT-2](#gpt-2)具有相同的架构，但包含
    175B 参数，比 GPT-2（1.5B）大 10 倍。此外，GPT-3 使用交替的密集和局部带状稀疏注意力模式，与[sparse transformer](https://lilianweng.github.io/posts/2020-04-07-the-transformer-family/#sparse-attention-matrix-factorization-sparse-transformers)中的相同。为了使这样一个庞大的模型跨多个
    GPU 进行适配，GPT-3 在宽度和深度维度上进行了分区训练。训练数据是 Common Crawl 的一个经过筛选的版本，混合了其他几个高质量的策划数据集。为了避免下游任务可能出现在训练数据中的污染，作者试图从训练数据中删除与所有研究的基准数据集的所有重叠。不幸的是，由于一个错误，过滤过程并不完美。'
- en: '![](../Images/1ffc7843a4c8d03d87aa78cb5ccae80b.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1ffc7843a4c8d03d87aa78cb5ccae80b.png)'
- en: 'Fig. 14\. Training datasets for GPT-3\. Note that the occurrence of each dataset
    during training is not proportional to the dataset size. (Table source: [Brown
    et al., 2020](https://arxiv.org/abs/2005.14165))'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14\. GPT-3 的训练数据集。请注意，每个数据集在训练期间出现的次数与数据集大小不成比例。（表来源：[Brown et al., 2020](https://arxiv.org/abs/2005.14165)）
- en: For all the downstream evaluation, GPT-3 is tested in the few-shot setting without
    any gradient-based fine-tuning. Here the few-shot examples are provided as part
    of the prompt. GPT-3 achieves strong performance on many NLP datasets, comparable
    with fine-tuned BERT models.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所有下游评估，GPT-3 在少样本设置下进行测试，没有进行基于梯度的微调。这里的少样本示例作为提示的一部分提供。GPT-3 在许多自然语言处理数据集上表现出色，与微调的
    BERT 模型相媲美。
- en: '![](../Images/69548e287ddc9f879f2592db08695b5b.png)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/69548e287ddc9f879f2592db08695b5b.png)'
- en: 'Fig. 15\. The evaluation performance increases with the model size and the
    number of examples. (Image source: [Brown et al., 2020](https://papers.nips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html))'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15\. 随着模型大小和示例数量的增加，评估性能也在提高。（图片来源：[Brown et al., 2020](https://papers.nips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html)）
- en: XLNet
  id: totrans-220
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: XLNet
- en: 'The *Autoregressive (AR)* model such as GPT and *autoencoder (AE)* model such
    as BERT are two most common ways for language modeling. However, each has their
    own disadvantages: AR does not learn the bidirectional context, which is needed
    by downstream tasks like reading comprehension and AE assumes masked positions
    are independent given all other unmasked tokens which oversimplifies the long
    context dependency.'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '*自回归（AR）*模型如 GPT 和*自编码器（AE）*模型如 BERT 是语言建模的两种最常见方式。然而，它们各自都有缺点：AR 没有学习双向上下文，这是下游任务如阅读理解所需的，而
    AE 假设掩码位置在给定所有其他未掩码标记的情况下是独立的，这过于简化了长上下文依赖性。'
- en: '**XLNet** ([Yang et al. 2019](https://arxiv.org/abs/1906.08237)) generalizes
    the AE method to incorporate the benefits of AR. XLNet proposed the **permutation
    language modeling** objective. For a text sequence, it samples a factorization
    order $\mathbf{z}$ and decomposes the likelihood $p_\theta(\mathbf{x})$ according
    to this factorization order,'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '**XLNet**（[Yang et al. 2019](https://arxiv.org/abs/1906.08237)）将 AE 方法推广到包含
    AR 的优点。XLNet 提出了**置换语言建模**目标。对于一个文本序列，它会对一个因子化顺序 $\mathbf{z}$ 进行采样，并根据这个因子化顺序分解似然
    $p_\theta(\mathbf{x})$，'
- en: $$ \begin{aligned} \mathcal{L}_\text{XLNet} &= - \mathbb{E}_{\mathbf{z} \sim
    \mathcal{Z}_T} \Big[ \sum_{t=1}^T \log p_\theta (X_{z_t} = x \mid \mathbf{x}_{\mathbf{z}_{
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \begin{aligned} \mathcal{L}_\text{XLNet} &= - \mathbb{E}_{\mathbf{z} \sim
    \mathcal{Z}_T} \Big[ \sum_{t=1}^T \log p_\theta (X_{z_t} = x \mid \mathbf{x}_{\mathbf{z}_{</t}}
    \mid \mathbf{z}_{<t}) \Big] \end{aligned} $$
- en: where $\mathcal{Z}_T$ is a set of all possible permutation of length $T$; $z_t$
    and $\mathbf{z}_{<t}$ denote the $t$-th element and the first $t-1$ elements of
    a permutation $\mathbf{z} \in \mathcal{Z}_T$.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathcal{Z}_T$ 是长度为 $T$ 的所有可能排列的集合；$z_t$ 和 $\mathbf{z}_{<t}$ 分别表示排列 $\mathbf{z}
    \in \mathcal{Z}_T$ 的第 $t$ 个元素和前 $t-1$ 个元素。
- en: Note that the naive representation of the hidden state of the context, $h_\theta
    (\mathbf{x}_{\mathbf{z}_{<t}})$ in red, does not depend on which position the
    model tries to predict, as the permutation breaks the default ordering. Therefore,
    XLNet re-parameterized it to a function of the target position too, $g_\theta
    (\mathbf{x}_{\mathbf{z}_{<t}}, z_t)$ in blue.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，上下文的隐藏状态的朴素表示，$h_\theta (\mathbf{x}_{\mathbf{z}_{<t}})$ 中的红色，不依赖于模型尝试预测的位置，因为排列打破了默认顺序。因此，XLNet
    将其重新参数化为目标位置的函数，$g_\theta (\mathbf{x}_{\mathbf{z}_{<t}}, z_t)$ 中的蓝色。
- en: 'However, two different requirements on $g_\theta (\mathbf{x}_{\mathbf{z}_{<t}},
    z_t)$ lead to a two-stream self-attention design to accommodate:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，对 $g_\theta (\mathbf{x}_{\mathbf{z}_{<t}}, z_t)$ 的两种不同要求导致了两流自注意力设计以适应：
- en: When predicting $x_{z_t}$, it should only encode the position $z_t$ but not
    the content $x_{z_t}$; otherwise it is trivial. This is wrapped into the “query
    representation” $g_{z_t} = g_\theta (\mathbf{x}_{\mathbf{z}_{<t}}, z_t)$ does
    not encode $x_{z_t}$.
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当预测 $x_{z_t}$ 时，应该只编码位置 $z_t$ 而不是内容 $x_{z_t}$；否则就太简单了。这被包含在“查询表示” $g_{z_t} =
    g_\theta (\mathbf{x}_{\mathbf{z}_{<t}}, z_t)$ 中，不编码 $x_{z_t}$。
- en: When predicting $x_j$ where $j > t$, it should encode the content $x_{z_t}$
    as well to provide the full context. This is the “content representation” $h_{z_t}
    = h_\theta(\mathbf{x}_{\leq t})$.
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当预测 $x_j$，其中 $j > t$ 时，应该编码内容 $x_{z_t}$ 以提供完整的上下文。这就是“内容表示” $h_{z_t} = h_\theta(\mathbf{x}_{\leq
    t})$。
- en: '![](../Images/892f41fe7fba16cf7d7ff7c461cc925a.png)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/892f41fe7fba16cf7d7ff7c461cc925a.png)'
- en: 'Fig. 16\. The illustration of two-stream self-attention mechanism in XLNet.
    (Image source: [Yang et al. 2019](https://arxiv.org/abs/1906.08237))'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16\. XLNet 中两流自注意力机制的示意图。（图片来源：[Yang et al. 2019](https://arxiv.org/abs/1906.08237)）
- en: Conceptually, the two streams of representations are updated as follows,
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 在概念上，两个表示流的更新如下，
- en: $$ \begin{aligned} g_{z_t}^{(m)} &\gets \text{Attention}(Q = g^{(m-1)}_{z_t},
    KV=\mathbf{h}^{(m-1)}_{\color{red}{\mathbf{z}_{
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \begin{aligned} g_{z_t}^{(m)} &\gets \text{Attention}(Q = g^{(m-1)}_{z_t},
    KV=\mathbf{h}^{(m-1)}_{\color{red}{\mathbf{z}_{
- en: Given the difficulty of optimization in permutation language modeling, XLNet
    is set to only predict the last chunk of tokens in a factorization order.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于排列语言建模的优化困难，XLNet 被设置为仅预测因子化顺序中的最后一块标记。
- en: The name in XLNet actually comes from [Transformer-XL](https://lilianweng.github.io/posts/2020-04-07-the-transformer-family/#longer-attention-span-transformer-xl).
    It incorporates the design of Transformer-XL to extend the attention span by reusing
    hidden states from previous segments.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: XLNet 的名称实际上来自于 [Transformer-XL](https://lilianweng.github.io/posts/2020-04-07-the-transformer-family/#longer-attention-span-transformer-xl)。它结合了
    Transformer-XL 的设计，通过重用先前段落的隐藏状态来扩展注意力跨度。
- en: '![](../Images/a79a7223aef1053bb1991b0e5912ffa9.png)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a79a7223aef1053bb1991b0e5912ffa9.png)'
- en: 'Fig. 17\. Comparison of model performance of XLNet with a couple other language
    models on GLUE, all single-task, no ensembles. (Image source: [Yang et al. 2019](https://arxiv.org/abs/1906.08237))'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17\. XLNet 在 GLUE 上与其他几种语言模型的模型性能比较，都是单任务，没有集成。（图片来源：[Yang et al. 2019](https://arxiv.org/abs/1906.08237)）
- en: BART
  id: totrans-237
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: BART
- en: '**BART** ([Lewis et al., 2019](https://arxiv.org/abs/1910.13461)) is a denoising
    autoencoder to recover the original text from a randomly corrupted version. It
    combines **B**idirectional and **A**uto**R**egressive **T**ransformer: precisely,
    jointly training BERT-like bidirectional encoder and GPT-like autoregressive decoder
    together. The loss is simply just to minimize the negative log-likelihood.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '**BART**（[Lewis et al., 2019](https://arxiv.org/abs/1910.13461)）是一个去噪自编码器，用于从随机损坏的版本中恢复原始文本。它结合了**B**idirectional
    和 **A**uto**R**egressive **T**ransformer：准确地说，同时训练类似 BERT 的双向编码器和类似 GPT 的自回归解码器。损失仅仅是最小化负对数似然。'
- en: '![](../Images/3366a233257fc3ad2b3ca2b0a394261c.png)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3366a233257fc3ad2b3ca2b0a394261c.png)'
- en: 'Fig. 18\. A schematic comparison of BART with BERT and GPT. (Image source:
    [Lewis et al., 2019](https://arxiv.org/abs/1910.13461))'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18\. BART 与 BERT 和 GPT 的示意图比较。（图片来源：[Lewis et al., 2019](https://arxiv.org/abs/1910.13461)）
- en: They experimented with a variety of noising transformations, including token
    masking, token deletion, text infilling (i.e. A randomly sampled text span, which
    may contain multiple tokens, is replaced with a `[MASK]` token), sentence permutation,
    documentation rotation (i.e. A document is rotated to begin with a random token.).
    The best noising approach they discovered is text infilling and sentence shuffling.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 他们尝试了各种噪声转换，包括标记遮蔽、标记删除、文本填充（即随机抽样的文本段，可能包含多个标记，被一个`[MASK]`标记替换）、句子排列、文档旋转（即将文档旋转到以一个随机标记开头）。他们发现的最佳噪声方法是文本填充和句子混洗。
- en: '![](../Images/13b2e600fbf7511dd2131c7c31662dea.png)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/13b2e600fbf7511dd2131c7c31662dea.png)'
- en: 'Fig. 19\. Comparison of different language modeling pre-training objectives.
    (Image source: [Lewis et al., 2019](https://arxiv.org/abs/1910.13461))'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 图19。不同语言建模预训练目标的比较。（图片来源：[Lewis et al., 2019](https://arxiv.org/abs/1910.13461)）
- en: 'Learnings from their experiments:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 从他们的实验中学到的：
- en: The performance of pre-training methods varies significantly across downstream
    tasks.
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预训练方法在下游任务中的性能差异显著。
- en: Token masking is crucial, as the performance is poor when only sentence permutation
    or documentation rotation is applied.
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标记遮蔽是至关重要的，因为仅应用句子排列或文档旋转时性能较差。
- en: Left-to-right pre-training improves generation.
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从左到右的预训练改善了生成。
- en: Bidirectional encoders are crucial for SQuAD.
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 双向编码器对SQuAD至关重要。
- en: The pre-training objective is not the only important factor. Architectural improvements
    such as relative-position embeddings or segment-level recurrence matter too.
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预训练目标并不是唯一重要的因素。架构改进，如相对位置嵌入或段级循环也很重要。
- en: Autoregressive language models perform best on ELI5.
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自回归语言模型在ELI5上表现最佳。
- en: BART achieves the most consistently strong performance.
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BART在表现上最为稳健。
- en: ELECTRA
  id: totrans-252
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ELECTRA
- en: Most current pre-training large language models demand a lot of computation
    resources, raising concerns about their cost and accessibility. **ELECTRA** (“Efficiently
    Learning an Encoder that Classifies Token Replacements Accurately”; [Clark et
    al. 2020](https://arxiv.org/abs/2003.10555)) aims to improve the *pre-training
    efficiency*, which frames the language modeling as a discrimination task instead
    of generation task.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 当前大多数预训练大型语言模型需要大量计算资源，引发了对其成本和可访问性的担忧。**ELECTRA**（“Efficiently Learning an
    Encoder that Classifies Token Replacements Accurately”; [Clark et al. 2020](https://arxiv.org/abs/2003.10555)）旨在提高*预训练效率*，将语言建模框架定位为一个区分任务而非生成任务。
- en: '![](../Images/4030428bda894cc7bb7ad335bb189190.png)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4030428bda894cc7bb7ad335bb189190.png)'
- en: 'Fig. 20\. Illustration of ELECTRA model architecture. (Image source: [Clark
    et al. 2020](https://arxiv.org/abs/2003.10555))'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 图20。ELECTRA模型架构示意图。（图片来源：[Clark et al. 2020](https://arxiv.org/abs/2003.10555)）
- en: ELECTRA proposes a new pretraining task, called “Replaced Token Detection” (RTD).
    Let’s randomly sample $k$ positions to be masked. Each selected token in the original
    text is replaced by a plausible alternative predicted by a small language model,
    known as the generator $G$. The discriminator $D$ predicts whether each token
    is original or replaced.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: ELECTRA提出了一个新的预训练任务，称为“替换标记检测”（RTD）。让我们随机抽样$k$个位置进行标记。原始文本中每个选定的标记都将被一个小语言模型预测的合理替代品替换，这个模型被称为生成器$G$。鉴别器$D$预测每个标记是原始的还是替换的。
- en: $$ \begin{aligned} \boldsymbol{m} &= [m_1, \dots, m_k] \text{ where } m_i \sim
    \text{unif}\{1, n\}\text{ for } i=1, \dots, k \\ \boldsymbol{x}^\text{masked}
    &= \text{REPLACE}(\boldsymbol{x}, \boldsymbol{m}, \texttt{[MASK]}) \\ \boldsymbol{x}^\text{corrupt}
    &= \text{REPLACE}(\boldsymbol{x}, \boldsymbol{m}, \tilde{\boldsymbol{x}}) \text{
    where } \tilde{x}_t \sim p_G(x_i \mid \boldsymbol{x}^\text{masked}) \text{ for
    } i \in \boldsymbol{m} \\ \end{aligned} $$
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \begin{aligned} \boldsymbol{m} &= [m_1, \dots, m_k] \text{ where } m_i \sim
    \text{unif}\{1, n\}\text{ for } i=1, \dots, k \\ \boldsymbol{x}^\text{masked}
    &= \text{REPLACE}(\boldsymbol{x}, \boldsymbol{m}, \texttt{[MASK]}) \\ \boldsymbol{x}^\text{corrupt}
    &= \text{REPLACE}(\boldsymbol{x}, \boldsymbol{m}, \tilde{\boldsymbol{x}}) \text{
    where } \tilde{x}_t \sim p_G(x_i \mid \boldsymbol{x}^\text{masked}) \text{ for
    } i \in \boldsymbol{m} \\ \end{aligned} $$
- en: The loss for the generator is the negative log-likelihood just as in other language
    models. The loss for the discriminator is the cross-entropy. Note that the generator
    is not adversarially trained to fool the discriminator but simply to optimize
    the NLL, since their experiments show negative results.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器的损失与其他语言模型一样是负对数似然。鉴别器的损失是交叉熵。请注意，生成器并不是通过对抗训练来愚弄鉴别器，而只是为了优化NLL，因为他们的实验显示了负面结果。
- en: $$ \begin{aligned} \mathcal{L}_\text{MLM}(\mathbf{x}, \theta_G) &= \mathbb{E}\Big(\sum_{i
    \in \boldsymbol{m}} -\log p_G (x_i \mid \boldsymbol{x}^\text{masked} )\Big) \\
    \mathcal{L}_\text{Disc}(\mathbf{x}, \theta_D) &= \mathbb{E}\Big( - \mathbb{1}[x^\text{corrupt}_t
    = x_t] \log D(\boldsymbol{x}^\text{corrupt}, t) - \mathbb{1}[x^\text{corrupt}_t
    \neq x_t] \log (1 - \log D(\boldsymbol{x}^\text{corrupt}, t)) \Big) \end{aligned}
    $$
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \begin{aligned} \mathcal{L}_\text{MLM}(\mathbf{x}, \theta_G) &= \mathbb{E}\Big(\sum_{i
    \in \boldsymbol{m}} -\log p_G (x_i \mid \boldsymbol{x}^\text{masked} )\Big) \\
    \mathcal{L}_\text{Disc}(\mathbf{x}, \theta_D) &= \mathbb{E}\Big( - \mathbb{1}[x^\text{corrupt}_t
    = x_t] \log D(\boldsymbol{x}^\text{corrupt}, t) - \mathbb{1}[x^\text{corrupt}_t
    \neq x_t] \log (1 - \log D(\boldsymbol{x}^\text{corrupt}, t)) \Big) \end{aligned}
    $$
- en: They found it more beneficial to only share the embeddings between generator
    & discriminator while using a small generator (1/4 to 1/2 the discriminator size),
    rather than sharing all the weights (i.e. two models have to be the same size
    then). In addition, joint training of the generator and discriminator works better
    than two-stage training of each alternatively.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 他们发现，只在生成器和鉴别器之间共享嵌入更有益，同时使用较小的生成器（鉴别器大小的1/4到1/2），而不是共享所有权重（即两个模型必须是相同大小）。此外，生成器和鉴别器的联合训练比交替两阶段训练效果更好。
- en: After pretraining the generator is discarded and only the ELECTRA discriminator
    is fine-tuned further for downstream tasks. The following table shows ELECTRA’s
    performance on the GLUE dev set.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练后，生成器被丢弃，只有 ELECTRA 鉴别器进一步进行下游任务的微调。下表显示了 ELECTRA 在 GLUE 开发集上的表现。
- en: '![](../Images/d275ea694a92c86cd6c82e941e3211dc.png)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d275ea694a92c86cd6c82e941e3211dc.png)'
- en: 'Fig. 21\. Comparison of ELECTRA with other language models on the GLUE dev
    set. (Image source: [Clark et al. 2020](https://arxiv.org/abs/2003.10555))'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 图 21\. ELECTRA 与 GLUE 开发集上其他语言模型的比较。（图片来源：[Clark et al. 2020](https://arxiv.org/abs/2003.10555)）
- en: Summary
  id: totrans-264
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: '|  | Base model | Pretraining Tasks |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '|  | 基础模型 | 预训练任务。'
- en: '| --- | --- | --- |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| CoVe | seq2seq NMT model | supervised learning using translation dataset.
    |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| CoVe | seq2seq NMT 模型 | 使用翻译数据集进行监督学习。'
- en: '| ELMo | two-layer biLSTM | next token prediction |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| ELMo | 两层双向 LSTM | 下一个标记预测。'
- en: '| CVT | two-layer biLSTM | semi-supervised learning using both labeled and
    unlabeled datasets |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| CVT | 两层双向 LSTM | 使用标记和未标记数据集进行半监督学习。'
- en: '| ULMFiT | AWD-LSTM | autoregressive pretraining on Wikitext-103 |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| ULMFiT | AWD-LSTM | 在 Wikitext-103 上进行自回归预训练。'
- en: '| GPT | Transformer decoder | next token prediction |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| GPT | Transformer 解码器 | 下一个标记预测。'
- en: '| BERT | Transformer encoder | mask language model + next sentence prediction
    |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| BERT | Transformer 编码器 | 掩码语言模型 + 下一个句子预测 |'
- en: '| ALBERT | same as BERT but light-weighted | mask language model + sentence
    order prediction |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| ALBERT | 与 BERT 相同但轻量级 | 掩码语言模型 + 句子顺序预测。'
- en: '| GPT-2 | Transformer decoder | next token prediction |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| GPT-2 | Transformer 解码器 | 下一个标记预测。'
- en: '| RoBERTa | same as BERT | mask language model (dynamic masking) |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| RoBERTa | 与 BERT 相同 | 掩码语言模型（动态掩码）。'
- en: '| T5 | Transformer encoder + decoder | pre-trained on a multi-task mixture
    of unsupervised and supervised tasks and for which each task is converted into
    a text-to-text format. |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| T5 | Transformer 编码器 + 解码器 | 在无监督和监督任务的多任务混合上进行预训练，每个任务都转换为文本到文本的格式。'
- en: '| GPT-3 | Transformer decoder | next token prediction |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3 | Transformer 解码器 | 下一个标记预测。'
- en: '| XLNet | same as BERT | permutation language modeling |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| XLNet | 与 BERT 相同 | 排列语言建模。'
- en: '| BART | BERT encoder + GPT decoder | reconstruct text from a noised version
    |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| BART | BERT 编码器 + GPT 解码器 | 从噪声版本重构文本。'
- en: '| ELECTRA | same as BERT | replace token detection |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| ELECTRA | 与 BERT 相同 | 替换标记检测。'
- en: 'Metric: Perplexity'
  id: totrans-281
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 指标：困惑度
- en: Perplexity is often used as an intrinsic evaluation metric for gauging how well
    a language model can capture the real word distribution conditioned on the context.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 困惑度通常用作内在评估指标，用于衡量语言模型在给定上下文条件下能够多好地捕捉真实词分布。
- en: 'A [perplexity](https://en.wikipedia.org/wiki/Perplexity) of a discrete proability
    distribution $p$ is defined as the exponentiation of the entropy:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 离散概率分布 $p$ 的困惑度定义为熵的幂运算：
- en: $$ 2^{H(p)} = 2^{-\sum_x p(x) \log_2 p(x)} $$
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: $$ 2^{H(p)} = 2^{-\sum_x p(x) \log_2 p(x)} $$
- en: 'Given a sentence with $N$ words, $s = (w_1, \dots, w_N)$, the entropy looks
    as follows, simply assuming that each word has the same frequency, $\frac{1}{N}$:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个具有 $N$ 个单词的句子，$s = (w_1, \dots, w_N)$，熵如下所示，简单地假设每个单词具有相同的频率，$\frac{1}{N}$：
- en: $$ H(s) = -\sum_{i=1}^N P(w_i) \log_2 p(w_i) = -\sum_{i=1}^N \frac{1}{N} \log_2
    p(w_i) $$
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: $$ H(s) = -\sum_{i=1}^N P(w_i) \log_2 p(w_i) = -\sum_{i=1}^N \frac{1}{N} \log_2
    p(w_i) $$
- en: 'The perplexity for the sentence becomes:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 该句子的困惑度变为：
- en: $$ \begin{aligned} 2^{H(s)} &= 2^{-\frac{1}{N} \sum_{i=1}^N \log_2 p(w_i)} =
    (2^{\sum_{i=1}^N \log_2 p(w_i)})^{-\frac{1}{N}} = (p(w_1) \dots p(w_N))^{-\frac{1}{N}}
    \end{aligned} $$
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \begin{aligned} 2^{H(s)} &= 2^{-\frac{1}{N} \sum_{i=1}^N \log_2 p(w_i)} =
    (2^{\sum_{i=1}^N \log_2 p(w_i)})^{-\frac{1}{N}} = (p(w_1) \dots p(w_N))^{-\frac{1}{N}}
    \end{aligned} $$
- en: A good language model should predict high word probabilities. Therefore, the
    smaller perplexity the better.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 一个优秀的语言模型应该能够预测高概率的词。因此，困惑度越小越好。
- en: Common Tasks and Datasets
  id: totrans-290
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 常见任务和数据集
- en: '**Question-Answering**'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: '**问答**'
- en: '[SQuAD](https://rajpurkar.github.io/SQuAD-explorer/) (Stanford Question Answering
    Dataset): A reading comprehension dataset, consisting of questions posed on a
    set of Wikipedia articles, where the answer to every question is a span of text.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[SQuAD](https://rajpurkar.github.io/SQuAD-explorer/)（斯坦福问答数据集）：一个阅读理解数据集，由一组维基百科文章上提出的问题组成，每个问题的答案都是一段文本。'
- en: '[RACE](http://www.qizhexie.com/data/RACE_leaderboard) (ReAding Comprehension
    from Examinations): A large-scale reading comprehension dataset with more than
    28,000 passages and nearly 100,000 questions. The dataset is collected from English
    examinations in China, which are designed for middle school and high school students.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[RACE](http://www.qizhexie.com/data/RACE_leaderboard)（来自考试的阅读理解）：一个大规模的阅读理解数据集，包含超过28,000个段落和近100,000个问题。该数据集收集自中国的英语考试，面向中学和高中学生。'
- en: See [more QA datasets in a later post](https://lilianweng.github.io/posts/2020-10-29-odqa/#appendix-qa-datasets).
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查看[更多问答数据集](https://lilianweng.github.io/posts/2020-10-29-odqa/#appendix-qa-datasets)。
- en: '**Commonsense Reasoning**'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: '**常识推理**'
- en: '[Story Cloze Test](http://cs.rochester.edu/nlp/rocstories/): A commonsense
    reasoning framework for evaluating story understanding and generation. The test
    requires a system to choose the correct ending to multi-sentence stories from
    two options.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Story Cloze Test](http://cs.rochester.edu/nlp/rocstories/)：用于评估故事理解和生成的常识推理框架。该测试要求系统从两个选项中选择多句故事的正确结局。'
- en: '[SWAG](https://rowanzellers.com/swag/) (Situations With Adversarial Generations):
    multiple choices; contains 113k sentence-pair completion examples that evaluate
    grounded common-sense inference'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[SWAG](https://rowanzellers.com/swag/)（具有对抗生成的情境）：多项选择；包含113k个句子对完成示例，评估基于常识的推理。'
- en: '**Natural Language Inference (NLI)**: also known as **Text Entailment**, an
    exercise to discern in logic whether one sentence can be inferred from another.'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: '**自然语言推理（NLI）**：也称为**文本蕴涵**，是一种逻辑推理的练习，用于判断一个句子是否可以从另一个句子中推断出来。'
- en: '[RTE](https://aclweb.org/aclwiki/Textual_Entailment_Resource_Pool) (Recognizing
    Textual Entailment): A set of datasets initiated by text entailment challenges.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[RTE](https://aclweb.org/aclwiki/Textual_Entailment_Resource_Pool)（文本蕴涵资源池）：由文本蕴涵挑战发起的一组数据集。'
- en: '[SNLI](https://nlp.stanford.edu/projects/snli/) (Stanford Natural Language
    Inference): A collection of 570k human-written English sentence pairs manually
    labeled for balanced classification with the labels `entailment`, `contradiction`,
    and `neutral`.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[SNLI](https://nlp.stanford.edu/projects/snli/)（斯坦福自然语言推理）：由人工标记的570k个英语句子对集合，用于平衡分类的标签有`蕴涵`、`矛盾`和`中立`。'
- en: '[MNLI](https://www.nyu.edu/projects/bowman/multinli/) (Multi-Genre NLI): Similar
    to SNLI, but with a more diverse variety of text styles and topics, collected
    from transcribed speech, popular fiction, and government reports.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[MNLI](https://www.nyu.edu/projects/bowman/multinli/)（多样化文体自然语言推理）：类似于SNLI，但涵盖更多样化的文体和主题，包括转录的演讲、流行小说和政府报告。'
- en: '[QNLI](https://gluebenchmark.com/tasks) (Question NLI): Converted from SQuAD
    dataset to be a binary classification task over pairs of (question, sentence).'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[QNLI](https://gluebenchmark.com/tasks)（问题NLI）：从SQuAD数据集转换而来，用于对（问题，句子）对进行二元分类任务。'
- en: '[SciTail](http://data.allenai.org/scitail/): An entailment dataset created
    from multiple-choice science exams and web sentences.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[SciTail](http://data.allenai.org/scitail/)：从多项选择科学考试和网络句子中创建的蕴涵数据集。'
- en: '**Named Entity Recognition (NER)**: labels sequences of words in a text which
    are the names of things, such as person and company names, or gene and protein
    names'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: '**命名实体识别（NER）**：标记文本中的词序列，这些词是事物的名称，如人名、公司名，或基因和蛋白质名称。'
- en: '[CoNLL 2003 NER task](https://www.clips.uantwerpen.be/conll2003/): consists
    of newswire from the Reuters, concentrating on four types of named entities: persons,
    locations, organizations and names of miscellaneous entities.'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[CoNLL 2003 NER task](https://www.clips.uantwerpen.be/conll2003/)：包括路透社的新闻稿，集中在四种命名实体上：人物、地点、组织和其他实体的名称。'
- en: '[OntoNotes 5.0](https://catalog.ldc.upenn.edu/LDC2013T19): This corpus contains
    text in English, Arabic and Chinese, tagged with four different entity types (PER,
    LOC, ORG, MISC).'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[OntoNotes 5.0](https://catalog.ldc.upenn.edu/LDC2013T19)：该语料库包含英语、阿拉伯语和中文文本，标记有四种不同的实体类型（PER、LOC、ORG、MISC）。'
- en: '[Reuters Corpus](https://trec.nist.gov/data/reuters/reuters.html): A large
    collection of Reuters News stories.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Reuters Corpus](https://trec.nist.gov/data/reuters/reuters.html)：路透社新闻故事的大量收集。'
- en: Fine-Grained NER (FGN)
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 精细化命名实体识别（FGN）
- en: '**Sentiment Analysis**'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: '**情感分析**'
- en: '[SST](https://nlp.stanford.edu/sentiment/index.html) (Stanford Sentiment Treebank)'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[SST](https://nlp.stanford.edu/sentiment/index.html)（斯坦福情感树库）'
- en: '[IMDb](http://ai.stanford.edu/~amaas/data/sentiment/): A large dataset of movie
    reviews with binary sentiment classification labels.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[IMDb](http://ai.stanford.edu/~amaas/data/sentiment/)：一个包含电影评论的大型数据集，带有二元情感分类标签。'
- en: '**Semantic Role Labeling (SRL)**: models the predicate-argument structure of
    a sentence, and is often described as answering “Who did what to whom”.'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: '**语义角色标注（SRL）**：模拟句子的谓语-论元结构，通常描述为回答“谁对谁做了什么”。'
- en: '[CoNLL-2004 & CoNLL-2005](http://www.lsi.upc.edu/~srlconll/)'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[CoNLL-2004 & CoNLL-2005](http://www.lsi.upc.edu/~srlconll/)'
- en: '**Sentence similarity**: also known as *paraphrase detection*'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '**句子相似度**：也称为*释义检测*'
- en: '[MRPC](https://www.microsoft.com/en-us/download/details.aspx?id=52398) (MicRosoft
    Paraphrase Corpus): It contains pairs of sentences extracted from news sources
    on the web, with annotations indicating whether each pair is semantically equivalent.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[MRPC](https://www.microsoft.com/en-us/download/details.aspx?id=52398)（微软释义语料库）：它包含从网络新闻来源中提取的句子对，带有指示每对是否语义等价的注释。'
- en: '[QQP](https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs) (Quora
    Question Pairs) STS Benchmark: Semantic Textual Similarity'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[QQP](https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs)（Quora
    问题对）STS 基准：语义文本相似性'
- en: '**Sentence Acceptability**: a task to annotate sentences for grammatical acceptability.'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: '**句子可接受性**：一个用于标注句子语法可接受性的任务。'
- en: '[CoLA](https://nyu-mll.github.io/CoLA/) (Corpus of Linguistic Acceptability):
    a binary single-sentence classification task.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[CoLA](https://nyu-mll.github.io/CoLA/)（语言可接受性语料库）：一个二元单句分类任务。'
- en: '**Text Chunking**: To divide a text in syntactically correlated parts of words.'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: '**文本分块**：将文本分成句法相关的词块。'
- en: '[CoNLL-2000](https://www.clips.uantwerpen.be/conll2000/chunking/)'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[CoNLL-2000](https://www.clips.uantwerpen.be/conll2000/chunking/)'
- en: '**Part-of-Speech (POS) Tagging**: tag parts of speech to each token, such as
    noun, verb, adjective, etc. the Wall Street Journal portion of the Penn Treebank
    (Marcus et al., 1993).'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: '**词性标注（POS）**：为每个标记标记词性，如名词、动词、形容词等。宾夕法尼亚树库的华尔街日报部分（Marcus等，1993）。'
- en: '**Machine Translation**: See [Standard NLP](https://nlp.stanford.edu/projects/nmt/)
    page.'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: '**机器翻译**：参见[标准自然语言处理](https://nlp.stanford.edu/projects/nmt/)页面。'
- en: WMT 2015 English-Czech data (Large)
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: WMT 2015 英语-捷克语数据（大型）
- en: WMT 2014 English-German data (Medium)
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: WMT 2014 英语-德语数据（中型）
- en: IWSLT 2015 English-Vietnamese data (Small)
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: IWSLT 2015 英语-越南语数据（小型）
- en: '**Coreference Resolution**: cluster mentions in text that refer to the same
    underlying real world entities.'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: '**指代消解**：对指代同一底层实体的文本进行聚类。'
- en: '[CoNLL-2012](http://conll.cemantix.org/2012/data.html)'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[CoNLL-2012](http://conll.cemantix.org/2012/data.html)'
- en: '**Long-range Dependency**'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: '**长距离依赖**'
- en: '[LAMBADA](http://clic.cimec.unitn.it/lambada/) (LAnguage Modeling Broadened
    to Account for Discourse Aspects): A collection of narrative passages extracted
    from the BookCorpus and the task is to predict the last word, which require at
    least 50 tokens of context for a human to successfully predict.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[LAMBADA](http://clic.cimec.unitn.it/lambada/)（语言建模扩展以考虑话语方面）：从BookCorpus中提取的叙述段落的集合，任务是预测最后一个单词，这需要至少50个上下文标记才能成功预测。'
- en: '[Children’s Book Test](https://research.fb.com/downloads/babi/): is built from
    books that are freely available in [Project Gutenberg](https://www.gutenberg.org/).
    The task is to predict the missing word among 10 candidates.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[儿童图书测试](https://research.fb.com/downloads/babi/)：由[古腾堡计划](https://www.gutenberg.org/)中免费提供的书籍构建。任务是在10个候选词中预测缺失的单词。'
- en: '**Multi-task benchmark**'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: '**多任务基准**'
- en: 'GLUE multi-task benchmark: [https://gluebenchmark.com](https://gluebenchmark.com/)'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GLUE 多任务基准：[https://gluebenchmark.com](https://gluebenchmark.com/)
- en: 'decaNLP benmark: [https://decanlp.com](https://decanlp.com/)'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: decaNLP 基准：[https://decanlp.com](https://decanlp.com/)
- en: '**Unsupervised pretraining dataset**'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: '**无监督预训练数据集**'
- en: '[Books corpus](https://googlebooks.byu.edu/): The corpus contains “over 7,000
    unique unpublished books from a variety of genres including Adventure, Fantasy,
    and Romance.”'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[图书语料库](https://googlebooks.byu.edu/): 该语料库包含“来自各种流派的7000多本独特未发表的书籍，包括冒险、奇幻和浪漫等。”'
- en: '[1B Word Language Model Benchmark](http://www.statmt.org/lm-benchmark/)'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10亿词语语言模型基准](http://www.statmt.org/lm-benchmark/)'
- en: '[English Wikipedia](https://en.wikipedia.org/wiki/Wikipedia:Database_download#English-language_Wikipedia):
    ~2500M words'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[英文维基百科](https://en.wikipedia.org/wiki/Wikipedia:Database_download#English-language_Wikipedia):
    约2500M字'
- en: '* * *'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 'Cited as:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 'Cited as:'
- en: '[PRE0]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Reference
  id: totrans-341
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Reference
- en: '[1] Bryan McCann, et al. [“Learned in translation: Contextualized word vectors.”](https://arxiv.org/abs/1708.00107)
    NIPS. 2017.'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Bryan McCann 等人. [“在翻译中学到的：上下文化的词向量。”](https://arxiv.org/abs/1708.00107)
    NIPS. 2017年.'
- en: '[2] Kevin Clark et al. [“Semi-Supervised Sequence Modeling with Cross-View
    Training.”](https://arxiv.org/abs/1809.08370) EMNLP 2018.'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Kevin Clark 等人. [“使用交叉视图训练的半监督序列建模。”](https://arxiv.org/abs/1809.08370)
    EMNLP 2018.'
- en: '[3] Matthew E. Peters, et al. [“Deep contextualized word representations.”](https://arxiv.org/abs/1802.05365)
    NAACL-HLT 2017.'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Matthew E. Peters 等人. [“深度上下文化的词表示。”](https://arxiv.org/abs/1802.05365)
    NAACL-HLT 2017.'
- en: '[4] OpenAI Blog [“Improving Language Understanding with Unsupervised Learning”](https://blog.openai.com/language-unsupervised/),
    June 11, 2018.'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] OpenAI Blog [“通过无监督学习改进语言理解”](https://blog.openai.com/language-unsupervised/),
    2018年6月11日.'
- en: '[5] OpenAI Blog [“Better Language Models and Their Implications.”](https://blog.openai.com/better-language-models/)
    Feb 14, 2019.'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] OpenAI Blog [“更好的语言模型及其影响。”](https://blog.openai.com/better-language-models/)
    2019年2月14日.'
- en: '[6] Jeremy Howard and Sebastian Ruder. [“Universal language model fine-tuning
    for text classification.”](https://arxiv.org/abs/1801.06146) ACL 2018.'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Jeremy Howard 和 Sebastian Ruder. [“文本分类的通用语言模型微调。”](https://arxiv.org/abs/1801.06146)
    ACL 2018.'
- en: '[7] Alec Radford et al. [“Improving Language Understanding by Generative Pre-Training”](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf).
    OpenAI Blog, June 11, 2018.'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] Alec Radford 等人. [“通过生成式预训练改进语言理解”](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf).
    OpenAI Blog, 2018年6月11日.'
- en: '[8] Jacob Devlin, et al. [“BERT: Pre-training of deep bidirectional transformers
    for language understanding.”](https://arxiv.org/abs/1810.04805) arXiv:1810.04805
    (2018).'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] Jacob Devlin 等人. [“BERT: 深度双向转换器的预训练用于语言理解。”](https://arxiv.org/abs/1810.04805)
    arXiv:1810.04805 (2018).'
- en: '[9] Mike Schuster, and Kaisuke Nakajima. [“Japanese and Korean voice search.”](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/37842.pdf)
    ICASSP. 2012.'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] Mike Schuster 和 Kaisuke Nakajima. [“日语和韩语语音搜索。”](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/37842.pdf)
    ICASSP. 2012.'
- en: '[10] Google’s Neural Machine Translation System: Bridging the Gap between Human
    and Machine Translation'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] 谷歌的神经机器翻译系统：弥合人类和机器翻译之间的差距'
- en: '[11] Ashish Vaswani, et al. [“Attention is all you need.”](https://arxiv.org/abs/1706.03762)
    NIPS 2017.'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] Ashish Vaswani 等人. [“注意力就是你所需要的。”](https://arxiv.org/abs/1706.03762) NIPS
    2017年.'
- en: '[12] Peter J. Liu, et al. [“Generating wikipedia by summarizing long sequences.”](https://arxiv.org/abs/1801.10198)
    ICLR 2018.'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: '[12] Peter J. Liu 等人. [“通过总结长序列生成维基百科。”](https://arxiv.org/abs/1801.10198)
    ICLR 2018年.'
- en: '[13] Sebastian Ruder. [“10 Exciting Ideas of 2018 in NLP”](http://ruder.io/10-exciting-ideas-of-2018-in-nlp/)
    Dec 2018.'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: '[13] Sebastian Ruder. [“2018年NLP领域的10个激动人心的想法”](http://ruder.io/10-exciting-ideas-of-2018-in-nlp/)
    2018年12月。'
- en: '[14] Alec Radford, et al. [“Language Models are Unsupervised Multitask Learners.”](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf).
    2019.'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: '[14] Alec Radford 等人. [“语言模型是无监督多任务学习者。”](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf).
    2019年.'
- en: '[15] Rico Sennrich, et al. [“Neural machine translation of rare words with
    subword units.”](https://arxiv.org/abs/1508.07909) arXiv preprint arXiv:1508.07909\.
    2015.'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: '[15] Rico Sennrich 等人. [“使用子词单元进行稀有词的神经机器翻译。”](https://arxiv.org/abs/1508.07909)
    arXiv preprint arXiv:1508.07909\. 2015年.'
- en: '[16] Zhenzhong Lan, et al. [“ALBERT: A Lite BERT for Self-supervised Learning
    of Language Representations.”](https://arxiv.org/abs/1909.11942) arXiv Preprint
    arXiv:1909.11942 (2019).'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: '[16] Zhenzhong Lan 等人. [“ALBERT: 用于自监督学习语言表示的轻量级BERT。”](https://arxiv.org/abs/1909.11942)
    arXiv Preprint arXiv:1909.11942 (2019).'
- en: '[17] Yinhan Liu, et al. [“RoBERTa: A Robustly Optimized BERT Pretraining Approach.”](https://arxiv.org/abs/1907.11692)
    arXiv Preprint arXiv:1907.11692 (2019).'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: '[17] Yinhan Liu 等人. [“RoBERTa: 一种稳健优化的BERT预训练方法。”](https://arxiv.org/abs/1907.11692)
    arXiv Preprint arXiv:1907.11692 (2019).'
- en: '[18] Tom B Brown, et al. [“Language Models are Few-Shot Learners”](https://arxiv.org/abs/2005.14165)
    NeuriPS 2020.'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: '[18] Tom B Brown等人 [“语言模型是少样本学习器”](https://arxiv.org/abs/2005.14165) NeuriPS
    2020.'
- en: '[19] Zhilin Yang et al. [“XLNet: Generalized Autoregressive Pretraining for
    Language Understanding.”](https://arxiv.org/abs/1906.08237) NeuriPS 2019.'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: '[19] Zhilin Yang等人 [“XLNet: 通用自回归预训练用于语言理解。”](https://arxiv.org/abs/1906.08237)
    NeuriPS 2019.'
- en: '[20] Mike Lewis et al. [“BART: Denoising Sequence-to-Sequence Pre-training
    for Natural Language Generation, Translation, and Comprehension.”](https://arxiv.org/abs/1910.13461)
    ACL 2020.'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: '[20] Mike Lewis等人 [“BART: 去噪序列到序列预训练用于自然语言生成、翻译和理解。”](https://arxiv.org/abs/1910.13461)
    ACL 2020.'
- en: '[21] Kevin Clark et al. [“ELECTRA: Pre-training Text Encoders as Discriminators
    Rather Than Generators.”](https://arxiv.org/abs/2003.10555) ICLR 2020.'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: '[21] Kevin Clark等人 [“ELECTRA: 将文本编码器预训练为判别器而不是生成器。”](https://arxiv.org/abs/2003.10555)
    ICLR 2020.'
- en: '[22] Colin Raffel, et al. [“Exploring the Limits of Transfer Learning with
    a Unified Text-to-Text Transformer”](https://arxiv.org/abs/1910.10683) JMLR 2020.'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: '[22] Colin Raffel等人 [“探索统一文本到文本变压器的迁移学习极限”](https://arxiv.org/abs/1910.10683)
    JMLR 2020.'
- en: '[architecture](https://lilianweng.github.io/tags/architecture/)'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[架构](https://lilianweng.github.io/tags/architecture/)'
- en: '[nlp](https://lilianweng.github.io/tags/nlp/)'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[自然语言处理](https://lilianweng.github.io/tags/nlp/)'
- en: '[long-read](https://lilianweng.github.io/tags/long-read/)'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[长文本](https://lilianweng.github.io/tags/long-read/)'
- en: '[transformer](https://lilianweng.github.io/tags/transformer/)'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[变压器](https://lilianweng.github.io/tags/transformer/)'
- en: '[attention](https://lilianweng.github.io/tags/attention/)'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[注意力机制](https://lilianweng.github.io/tags/attention/)'
- en: '[language-model](https://lilianweng.github.io/tags/language-model/)'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[语言模型](https://lilianweng.github.io/tags/language-model/)'
- en: '[«'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: '[«'
- en: Are Deep Neural Networks Dramatically Overfitted?](https://lilianweng.github.io/posts/2019-03-14-overfit/)
    [»
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 深度神经网络是否严重过拟合？](https://lilianweng.github.io/posts/2019-03-14-overfit/) [»
- en: 'Object Detection Part 4: Fast Detection Models](https://lilianweng.github.io/posts/2018-12-27-object-recognition-part-4/)[](https://twitter.com/intent/tweet/?text=Generalized%20Language%20Models&url=https%3a%2f%2flilianweng.github.io%2fposts%2f2019-01-31-lm%2f&hashtags=architecture%2cnlp%2clong-read%2ctransformer%2cattention%2clanguage-model)[](https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2flilianweng.github.io%2fposts%2f2019-01-31-lm%2f&title=Generalized%20Language%20Models&summary=Generalized%20Language%20Models&source=https%3a%2f%2flilianweng.github.io%2fposts%2f2019-01-31-lm%2f)[](https://reddit.com/submit?url=https%3a%2f%2flilianweng.github.io%2fposts%2f2019-01-31-lm%2f&title=Generalized%20Language%20Models)[](https://facebook.com/sharer/sharer.php?u=https%3a%2f%2flilianweng.github.io%2fposts%2f2019-01-31-lm%2f)[](https://api.whatsapp.com/send?text=Generalized%20Language%20Models%20-%20https%3a%2f%2flilianweng.github.io%2fposts%2f2019-01-31-lm%2f)[](https://telegram.me/share/url?text=Generalized%20Language%20Models&url=https%3a%2f%2flilianweng.github.io%2fposts%2f2019-01-31-lm%2f)©
    2024 [Lil''Log](https://lilianweng.github.io/) Powered by [Hugo](https://gohugo.io/)
    & [PaperMod](https://git.io/hugopapermod)[](#top "Go to Top (Alt + G)")'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: '目标检测第四部分: 快速检测模型](https://lilianweng.github.io/posts/2018-12-27-object-recognition-part-4/)[](https://twitter.com/intent/tweet/?text=通用语言模型&url=https%3a%2f%2flilianweng.github.io%2fposts%2f2019-01-31-lm%2f&hashtags=architecture%2cnlp%2clong-read%2ctransformer%2cattention%2clanguage-model)[](https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2flilianweng.github.io%2fposts%2f2019-01-31-lm%2f&title=通用语言模型&summary=通用语言模型&source=https%3a%2f%2flilianweng.github.io%2fposts%2f2019-01-31-lm%2f)[](https://reddit.com/submit?url=https%3a%2f%2flilianweng.github.io%2fposts%2f2019-01-31-lm%2f&title=通用语言模型)[](https://facebook.com/sharer/sharer.php?u=https%3a%2f%2flilianweng.github.io%2fposts%2f2019-01-31-lm%2f)[](https://api.whatsapp.com/send?text=通用语言模型%20-%20https%3a%2f%2flilianweng.github.io%2fposts%2f2019-01-31-lm%2f)[](https://telegram.me/share/url?text=通用语言模型&url=https%3a%2f%2flilianweng.github.io%2fposts%2f2019-01-31-lm%2f)©
    2024 [Lil''Log](https://lilianweng.github.io/) Powered by [Hugo](https://gohugo.io/)
    & [PaperMod](https://git.io/hugopapermod)[](#top "返回顶部 (Alt + G)")'
