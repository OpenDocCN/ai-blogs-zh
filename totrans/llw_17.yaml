- en: How to Build an Open-Domain Question Answering System?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何构建一个开放领域问答系统？
- en: 原文：[https://lilianweng.github.io/posts/2020-10-29-odqa/](https://lilianweng.github.io/posts/2020-10-29-odqa/)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://lilianweng.github.io/posts/2020-10-29-odqa/](https://lilianweng.github.io/posts/2020-10-29-odqa/)
- en: '[Updated on 2020-11-12: add [an example](#openai-api-example) on closed-book
    factual QA using OpenAI API (beta).'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[2020-11-12更新：添加[一个示例](#openai-api-example)，展示使用OpenAI API（beta）进行闭合书事实问答的示例。'
- en: A model that can answer any question with regard to factual knowledge can lead
    to many useful and practical applications, such as working as a chatbot or an
    AI assistant🤖. In this post, we will review several common approaches for building
    such an open-domain question answering system.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 一个能够回答关于事实知识的任何问题的模型可以导致许多有用和实用的应用，例如作为聊天机器人或AI助手🤖。在本文中，我们将回顾构建这样一个开放领域问答系统的几种常见方法。
- en: 'Disclaimers given so many papers in the wild:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于野外有如此多的论文：
- en: Assume we have access to a powerful pretrained [language model](https://lilianweng.github.io/posts/2019-01-31-lm/).
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假设我们可以访问一个强大的预训练[语言模型](https://lilianweng.github.io/posts/2019-01-31-lm/)。
- en: We do not cover how to use structured knowledge base (e.g. Freebase, WikiData)
    here.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们不在这里讨论如何使用结构化知识库（例如Freebase，WikiData）。
- en: We only focus on a single-turn QA instead of a multi-turn conversation style
    QA.
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们只关注单轮问答，而不是多轮对话式问答。
- en: We mostly focus on QA models that contain neural networks, specially Transformer-based
    language models.
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们主要关注包含神经网络的问答模型，特别是基于Transformer的语言模型。
- en: I admit that I missed a lot of papers with architectures designed specifically
    for QA tasks between 2017-2019😔
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我承认我错过了很多在2017-2019年间专门设计用于问答任务的架构的论文😔
- en: What is Open-Domain Question Answering?
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是开放领域问答？
- en: '**Open-domain Question Answering (ODQA)** is a type of language tasks, asking
    a model to produce answers to factoid questions in natural language. The true
    answer is objective, so it is simple to evaluate model performance.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**开放领域问答（ODQA）**是一种语言任务类型，要求模型以自然语言产生对事实性问题的答案。真实答案是客观的，因此评估模型性能很简单。'
- en: For example,
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The “open-domain” part refers to the lack of the relevant context for any arbitrarily
    asked factual question. In the above case, the model only takes as the input the
    question but no article about “why Einstein didn’t win a Nobel Prize for the theory
    of relativity” is provided, where the term “the law of the photoelectric effect”
    is likely mentioned. In the case when both the question and the context are provided,
    the task is known as **Reading comprehension (RC)**.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: “开放领域”部分指的是对于任意提出的事实性问题缺乏相关背景信息。在上述情况下，模型只接受问题作为输入，但没有提供关于“为什么爱因斯坦因相对论未获诺贝尔奖”的文章，其中可能提到“光电效应定律”的术语。当提供问题和上下文时，任务被称为**阅读理解（RC）**。
- en: An ODQA model may work with or without *access to an external source of knowledge*
    (e.g. Wikipedia) and these two conditions are referred to as *open-book* or *closed-book*
    question answering, respectively.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 一个ODQA模型可以使用或不使用*外部知识源*（例如维基百科），这两种条件分别称为*开放书*或*闭合书*问答。
- en: 'When considering different types of open-domain questions, I like the classification
    by [Lewis, et al., 2020](https://arxiv.org/abs/2008.02637), in increasing order
    of difficulty:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在考虑不同类型的开放领域问题时，我喜欢按照[Lewis等人，2020](https://arxiv.org/abs/2008.02637)的分类，按照难度递增的顺序：
- en: A model is able to correctly memorize and respond with the answer to a question
    that has been seen at training time.
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个模型能够正确地记忆并回答在训练时见过的问题的答案。
- en: A model is able to answer novel questions at test time and choose an answer
    from the set of answers it has seen during training.
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个模型能够在测试时回答新颖的问题，并从训练时见过的答案集中选择一个答案。
- en: A model is able to answer novel questions which have answers not contained in
    the training dataset.
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个模型能够回答训练数据集中没有包含答案的新颖问题。
- en: '![](../Images/ef5af34b63b5c143825c6fbb409dea14.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ef5af34b63b5c143825c6fbb409dea14.png)'
- en: Fig. 1\. Overview of three frameworks discussed in this post.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图1。本文讨论的三种框架概述。
- en: Notation
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 符号
- en: Given a question $x$ and a ground truth answer span $y$, the context passage
    containing the true answer is labelled as $z \in \mathcal{Z}$, where $\mathcal{Z}$
    is an external knowledge corpus. Wikipedia is a common choice for such an external
    knowledge source.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个问题$x$和一个地面真实答案跨度$y$，包含真实答案的上下文段落被标记为$z \in \mathcal{Z}$，其中$\mathcal{Z}$是一个外部知识语料库。维基百科是这样一个外部知识来源的常见选择。
- en: Concerns of QA data fine-tuning
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: QA数据微调的问题
- en: Before we dive into the details of many models below. I would like to point
    out one concern of fine-tuning a model with common QA datasets, which appears
    as one fine-tuning step in several ODQA models. It could be concerning, because
    there is a significant overlap between questions in the train and test sets in
    several public QA datasets.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入讨论下面许多模型的细节之前，我想指出一个关于使用常见QA数据集对模型进行微调的问题，这在几个ODQA模型中作为一个微调步骤出现。这可能令人担忧，因为在几个公共QA数据集中，训练集和测试集中的问题存在显著重叠。
- en: '[Lewis, et al., (2020)](https://arxiv.org/abs/2008.02637) ([code](https://github.com/facebookresearch/QA-Overlap))
    found that 58-71% of test-time answers are also present somewhere in the training
    sets and 28-34% of test-set questions have a near-duplicate paraphrase in their
    corresponding training sets. In their experiments, several models performed notably
    worse when duplicated or paraphrased questions were removed from the training
    set.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '[Lewis等人（2020）](https://arxiv.org/abs/2008.02637)（[代码](https://github.com/facebookresearch/QA-Overlap)）发现58-71%的测试时间答案也出现在某处的训练集中，28-34%的测试集问题在其对应的训练集中有近似重复的释义。在他们的实验中，当从训练集中删除重复或释义问题时，几个模型的表现明显较差。'
- en: 'Open-book QA: Retriever-Reader'
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开放式问答：检索器-阅读器
- en: Given a factoid question, if a language model has no context or is not big enough
    to memorize the context which exists in the training dataset, it is unlikely to
    guess the correct answer. In an open-book exam, students are allowed to refer
    to external resources like notes and books while answering test questions. Similarly,
    a ODQA system can be paired with a rich knowledge base to identify relevant documents
    as evidence of answers.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个事实性问题，如果语言模型没有上下文或不足够大以记住训练数据集中存在的上下文，那么猜测正确答案的可能性很小。在开卷考试中，学生可以在回答测试问题时参考外部资源，如笔记和书籍。类似地，ODQA系统可以与丰富的知识库配对，以识别相关文档作为答案的证据。
- en: We can decompose the process of finding answers to given questions into two
    stages,
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将回答给定问题的过程分解为两个阶段，
- en: Find the related context in an external repository of knowledge;
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在外部知识库中找到相关内容；
- en: Process the retrieved context to *extract* an answer.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 处理检索到的内容以*提取*答案。
- en: '![](../Images/31e9dac723bf29e2d59178871b39a297.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/31e9dac723bf29e2d59178871b39a297.png)'
- en: Fig. 2\. The retriever-reader QA framework combines information retrieval with
    machine reading comprehension.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图2. 检索器-阅读器问答框架将信息检索与机器阅读理解结合起来。
- en: Such a retriever + reader framework was first proposed in **DrQA** (“Document
    retriever Question-Answering” by [Chen et al., 2017](https://arxiv.org/abs/1704.00051);
    [code](https://github.com/facebookresearch/DrQA)). The retriever and the reader
    components can be set up and trained independently, or jointly trained [end-to-end](#end-to-end-joint-training).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的检索器+阅读器框架最初是由**DrQA**（由[Chen等人，2017](https://arxiv.org/abs/1704.00051)提出；[代码](https://github.com/facebookresearch/DrQA)）提出的。检索器和阅读器组件可以独立设置和训练，或者进行联合训练[端到端](#end-to-end-joint-training)。
- en: Retriever Model
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检索器模型
- en: Two popular approaches for implementing the retriever is to use the information
    retrieval (IR) system that depends on (1) the classic non-learning-based [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)
    features (“classic IR”) or (2) dense embedding vectors of text produced by neural
    networks (“neural IR”).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 实现检索器的两种流行方法是使用依赖于（1）经典非学习型[TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)特征的信息检索（“经典IR”）或（2）由神经网络生成的文本的密集嵌入向量的密集嵌入向量（“神经IR”）。
- en: Classic IR
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 经典IR
- en: '**DrQA** ([Chen et al., 2017](https://arxiv.org/abs/1704.00051)) adopts an
    efficient non-learning-based search engine based on the [vector space model](https://en.wikipedia.org/wiki/Vector_space_model).
    Every query and document is modelled as a bag-of-word vector, where each term
    is weighted by TF-IDF (term frequency $\times$ inverse document frequency).'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**DrQA**（[Chen等人，2017](https://arxiv.org/abs/1704.00051)）采用了基于[向量空间模型](https://en.wikipedia.org/wiki/Vector_space_model)的高效非学习型搜索引擎。每个查询和文档都被建模为词袋向量，其中每个术语都由TF-IDF（词频$\times$逆文档频率）加权。'
- en: '$$ \begin{aligned} \text{tf-idf}(t, d, \mathcal{D}) &= \text{tf}(t, d) \times
    \text{idf}(t, \mathcal{D}) \\ \text{tf}(t, d) &= \log(1 + \text{freq}(t, d)) \\
    \text{idf}(t, \mathcal{D}) &= \log \Big( \frac{\vert\mathcal{D}\vert}{\vert d\in\mathcal{D}:
    t\in d\vert} \Big) \end{aligned} $$'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '$$ \begin{aligned} \text{tf-idf}(t, d, \mathcal{D}) &= \text{tf}(t, d) \times
    \text{idf}(t, \mathcal{D}) \\ \text{tf}(t, d) &= \log(1 + \text{freq}(t, d)) \\
    \text{idf}(t, \mathcal{D}) &= \log \Big( \frac{\vert\mathcal{D}\vert}{\vert d\in\mathcal{D}:
    t\in d\vert} \Big) \end{aligned} $$'
- en: where $t$ is a unigram or bigram term in a document $d$ from a collection of
    documents $\mathcal{D}$ . $\text{freq}(t, d)$ measures how many times a term $t$
    appears in $d$. Note that the term-frequency here includes bigram counts too,
    which is found to be very helpful because the local word order is taken into consideration
    via bigrams. As part of the implementation, DrQA maps the bigrams of $2^{24}$
    bins using unsigned murmur3 hash.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$t$是来自文档$d$的单个词或双词术语，来自文档集合$\mathcal{D}$。$\text{freq}(t, d)$衡量术语$t$在$d$中出现的次数。请注意，这里的词频也包括双词计数，这是非常有帮助的，因为通过双词考虑了局部词序。作为实现的一部分，DrQA使用无符号murmur3哈希将$2^{24}$个bin的双词映射。
- en: Precisely, DrQA implemented Wikipedia as its knowledge source and this choice
    has became a default setting for many ODQA studies since then. The non-ML document
    retriever returns the top $k=5$ most relevant Wikipedia articles given a question.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，DrQA将维基百科作为其知识来源，这个选择自那时以来已成为许多ODQA研究的默认设置。非机器学习文档检索器在给定问题的情况下返回前$k=5$个最相关的维基百科文章。
- en: '**BERTserini** ([Yang et al., 2019](https://arxiv.org/abs/1902.01718)) pairs
    the open-source [*Anserini*](https://github.com/castorini/anserini) IR toolkit
    as the retriever with a fine-tuned pre-trained BERT model as the reader. The top
    $k$ documents ($k=10$) are retrieved via the `post-v3.0` branch of Anserini with
    the query treated as a bag of words. The retrieved text segments are ranked by
    [BM25](https://en.wikipedia.org/wiki/Okapi_BM25), a classic TF-IDF-based retrieval
    scoring function. In terms of the effect of text granularity on performance, they
    found that paragraph retrieval > sentence retrieval > article retrieval.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**BERTserini**（[Yang等，2019](https://arxiv.org/abs/1902.01718)）将开源[*Anserini*](https://github.com/castorini/anserini)信息检索工具包与经过微调的预训练BERT模型配对作为阅读器。通过`post-v3.0`分支的Anserini检索前$k$个文档（$k=10$），将查询视为词袋。检索到的文本段通过[BM25](https://en.wikipedia.org/wiki/Okapi_BM25)进行排名，这是一种经典的基于TF-IDF的检索评分函数。在文本粒度对性能的影响方面，他们发现段落检索
    > 句子检索 > 文章检索。'
- en: '![](../Images/029b4c5dba542aa068e30d1ad6a2b08e.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/029b4c5dba542aa068e30d1ad6a2b08e.png)'
- en: 'Fig. 3\. An illustration of BERTserini architecture. (Image source: [Yang et
    al., 2019](https://arxiv.org/abs/1902.01718))'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图3\. BERTserini架构示意图。（图片来源：[Yang等，2019](https://arxiv.org/abs/1902.01718)）
- en: '*ElasticSearch + BM25* is used by the **Multi-passage BERT** QA model ([Wang
    et al., 2019](https://arxiv.org/abs/1908.08167)). They found that splitting articles
    into passages with the length of 100 words by *sliding window* brings 4% improvements,
    since splitting documents into passages without overlap may cause some near-boundary
    evidence to lose useful contexts.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '*ElasticSearch + BM25*被**多段BERT**问答模型所使用（[Wang等，2019](https://arxiv.org/abs/1908.08167)）。他们发现，通过*滑动窗口*将文章分割成长度为100个单词的段落可以带来4%的改进，因为将文档分割成没有重叠的段落可能导致一些接近边界的证据失去有用的上下文。'
- en: Neural IR
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 神经信息检索
- en: There is a long history in learning a low-dimensional representation of text,
    denser than raw term-based vectors ([Deerwester et al., 1990](http://lsa.colorado.edu/papers/JASIS.lsi.90.pdf);
    [Yih, et al., 2011](https://www.aclweb.org/anthology/W11-0329/)). Dense representations
    can be learned through matrix decomposition or some neural network architectures
    (e.g. MLP, LSTM, bidirectional LSTM, etc). When involving neural networks, such
    approaches are referred to as “Neural IR”, Neural IR is a new category of methods
    for retrieval problems, but it is not necessary to perform better/superior than
    classic IR ([Lim, 2018](https://sigir.org/wp-content/uploads/2019/01/p040.pdf)).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 学习文本的低维表示已有很长的历史，比原始基于术语的向量更密集（[Deerwester等，1990](http://lsa.colorado.edu/papers/JASIS.lsi.90.pdf);
    [Yih等，2011](https://www.aclweb.org/anthology/W11-0329/)）。密集表示可以通过矩阵分解或一些神经网络架构（例如MLP，LSTM，双向LSTM等）学习。当涉及神经网络时，这些方法被称为“神经信息检索”，神经信息检索是检索问题的新类别方法，但不一定比经典信息检索表现更好/更优越（[Lim，2018](https://sigir.org/wp-content/uploads/2019/01/p040.pdf)）。
- en: 'After the success of many large-scale [general language models](https://lilianweng.github.io/posts/2019-01-31-lm/),
    many QA models embrace the following approach:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多大规模[通用语言模型](https://lilianweng.github.io/posts/2019-01-31-lm/)取得成功之后，许多问答模型采用以下方法：
- en: $$ h_x = E_x(x)\quad h_z = E_z(z)\quad \text{score}(x, z) = h_x^\top h_z $$
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: $$ h_x = E_x(x)\quad h_z = E_z(z)\quad \text{score}(x, z) = h_x^\top h_z $$
- en: Extract the dense representations of a question $x$ and a context passage $z$
    by feeding them into a language model;
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过将问题$x$和上下文段落$z$的密集表示馈送到语言模型中来提取；
- en: Use the dot-product of these two representations as the retrieval score to rank
    and select most relevant passages.
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用这两个表示的点积作为检索分数来排名和选择最相关的段落。
- en: ORQA, REALM and DPR all use such a scoring function for context retrieval, which
    will be described in detail in a [later section](#end-to-end-joint-training) on
    the end-to-end QA model.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: ORQA、REALM 和 DPR 都使用这样一个用于上下文检索的评分函数，将在[后续部分](#end-to-end-joint-training)中详细描述端到端
    QA 模型。
- en: An extreme approach, investigated by **DenSPI** (“Dense-Sparse Phrase Index”;
    [Seo et al., 2019](https://arxiv.org/abs/1906.05807)), is to encode all the text
    in the knowledge corpus at the *phrase* level and then only rely on the retriever
    to identify the most relevant phrase as the predicted answer. In this way, the
    retriever+reader pipeline is reduced to only retriever. Of course, the index would
    be much larger and the retrieval problem is more challenging.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 一种极端的方法，由**DenSPI**（“Dense-Sparse Phrase Index”; [Seo et al., 2019](https://arxiv.org/abs/1906.05807)）调查，是在知识语料库中以*短语*级别编码所有文本，然后仅依赖检索器识别最相关的短语作为预测答案。这样，检索器+阅读器流水线仅减少到检索器。当然，索引会更大，检索问题更具挑战性。
- en: DenSPI introduces a *query-agnostic* indexable representation of document phrases.
    Precisely it encodes query-agnostic representations of text spans in Wikipedia
    offline and looks for the answer at inference time by performing nearest neighbor
    search. It can drastically speed up the inference time, because there is no need
    to re-encode documents for every new query, which is often required by a reader
    model.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: DenSPI 引入了一个文档短语的*查询无关*可索引表示。准确地说，它在维基百科离线中编码文本跨度的查询无关表示，并在推理时通过执行最近邻搜索来查找答案。这可以极大地加快推理时间，因为不需要为每个新查询重新编码文档，这通常是读者模型所要求的。
- en: 'Given a question $x$ and a fixed set of (Wikipedia) documents, $z_1, \dots,
    z_K$ and each document $z_k$ contains $N_k$ words, $z_k = \langle z_k^{(1)}, \dots,
    z_k^{(N_k)}\rangle$. An ODQA model is a scoring function $F$ for each candidate
    phrase span $z_k^{(i:j)}, 1 \leq i \leq j \leq N_k$, such that the truth answer
    is the phrase with maximum score: $y = {\arg\max}_{k,i,j} F(x, z_k^{(i:j)})$.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个问题 $x$ 和一组固定的（维基百科）文档，$z_1, \dots, z_K$，每个文档 $z_k$ 包含 $N_k$ 个词，$z_k = \langle
    z_k^{(1)}, \dots, z_k^{(N_k)}\rangle$。一个 ODQA 模型是一个为每个候选短语跨度 $z_k^{(i:j)}, 1 \leq
    i \leq j \leq N_k$ 计算得分的函数 $F$，使得真实答案是得分最高的短语：$y = {\arg\max}_{k,i,j} F(x, z_k^{(i:j)})$。
- en: 'The phrase representation $z_k^{(i:j)}$ combines both dense and sparse vectors,
    $z_k^{(i:j)} = [d_k^{(i:j)}, s_k^{(i:j)}] \in \mathbb{R}^{d^d + d^s}$ (note that
    $d^d \ll d^s$):'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 短语表示 $z_k^{(i:j)}$ 结合了密集向量和稀疏向量，$z_k^{(i:j)} = [d_k^{(i:j)}, s_k^{(i:j)}] \in
    \mathbb{R}^{d^d + d^s}$（注意 $d^d \ll d^s$）：
- en: The dense vector $d_k^{(i:j)}$ is effective for encoding local *syntactic* and
    *semantic* cues, as what can be learned by a pretrained language model.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 密集向量 $d_k^{(i:j)}$ 有效地编码了本地*句法*和*语义*线索，就像预训练语言模型所学到的那样。
- en: The sparse vector $s_k^{(i:j)}$ is superior at encoding precise *lexical* information.
    The sparse vector is term-frequency-based encoding. DenSPI uses 2-gram term-frequency
    same as DrQA, resulting a highly sparse representation ($d^s \approx 16$M)
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 稀疏向量 $s_k^{(i:j)}$ 在编码精确的*词汇*信息方面表现优异。这个稀疏向量是基于词频的编码。DenSPI 使用与 DrQA 相同的 2-gram
    词频，导致高度稀疏的表示（$d^s \approx 16$M）。
- en: The dense vector $d^{(i:j)}$ is further decomposed into three parts, $d^{(i:j)}
    = [a_i, b_j, c_{ij}] \in \mathbb{R}^{2d^b + 1}$ where $2d^b + 1 = d^d$. All three
    components are learned based on different columns of the fine-tuned BERT representations.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 密集向量 $d^{(i:j)}$ 进一步分解为三部分，$d^{(i:j)} = [a_i, b_j, c_{ij}] \in \mathbb{R}^{2d^b
    + 1}$，其中 $2d^b + 1 = d^d$。所有三个组件都是基于微调后的 BERT 表示学习的。
- en: A vector $a_i$ encodes the *start* position for the $i$-th word of the document;
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向量 $a_i$ 编码文档中第 $i$ 个词的*起始*位置；
- en: A vector $b_j$ encodes the *end* position for the $j$-th word of the document;
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向量 $b_j$ 编码文档中第 $j$ 个词的*结束*位置；
- en: A scalar $c_{ij}$ measures the *coherency* between the start and the end vectors,
    helping avoid non-constituent phrases during inference.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标量 $c_{ij}$ 衡量了起始向量和结束向量之间的*连贯性*，有助于在推理过程中避免非成分短语。
- en: For all possible $(i,j,k)$ tuples where $j-i < J$, the text span embeddings
    are precomputed and stored as a *phrase index*. The maximum span length $J$ is
    a predefined scalar constant.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所有可能的 $(i,j,k)$ 元组，其中 $j-i < J$，文本跨度嵌入被预先计算并存储为*短语索引*。最大跨度长度 $J$ 是一个预定义的标量常数。
- en: '![](../Images/ac4d275deeb39bc73a001ce0bdc33c29.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ac4d275deeb39bc73a001ce0bdc33c29.png)'
- en: 'Fig. 4\. An illustration of Dense-Sparse Phrase Index (DenSPI) architecture.
    (Image source: [Seo et al., 2019](https://arxiv.org/abs/1906.05807))'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '图4\. Dense-Sparse Phrase Index（DenSPI）架构示意图。 (图片来源: [Seo et al., 2019](https://arxiv.org/abs/1906.05807))'
- en: At the inference time, the question is mapped into the same vector space $x=[d’,
    s’] \in \mathbb{R}^{d^d + d^s}$, where the dense vector $d’$ is extracted from
    the BERT embedding of the special `[CLS]` symbol. The same BERT model is shared
    for encoding both questions and phrases. The final answer is predicted by $k^*,
    i^*, j^* = \arg\max x^\top z_k^{(i:j)}$.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 推理时，问题被映射到相同的向量空间 $x=[d’, s’] \in \mathbb{R}^{d^d + d^s}$，其中密集向量 $d’$ 是从特殊的
    `[CLS]` 符号的BERT嵌入中提取的。相同的BERT模型用于编码问题和短语。最终答案由 $k^*, i^*, j^* = \arg\max x^\top
    z_k^{(i:j)}$ 预测。
- en: Reader Model
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 读者模型
- en: The reader model learns to solve the reading comprehension task — extract an
    answer for a given question from a given context document. Here we only discuss
    approaches for machine comprehension using neural networks.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 读者模型学习解决阅读理解任务 — 从给定上下文文档中提取问题的答案。这里我们只讨论使用神经网络进行机器理解的方法。
- en: Bi-directional LSTM
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 双向 LSTM
- en: 'The reader model for answer detection of **DrQA** ([Chen et al., 2017](https://arxiv.org/abs/1704.00051))
    is a 3-layer bidirectional LSTM with hidden size 128\. Every relevant paragraph
    of retrieved Wikipedia articles is encoded by a sequence of feature vector, $\{\tilde{\mathbf{z}}_1,
    \dots, \tilde{\mathbf{z}}_m \}$. Each feature vector $\hat{\mathbf{z}}_i \in \mathbb{R}^{d_z}$
    is expected to capture useful contextual information around one token $z_i$. The
    feature consists of several categories of features:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '**DrQA**的答案检测读者模型（[Chen et al., 2017](https://arxiv.org/abs/1704.00051)）是一个具有隐藏大小为128的3层双向LSTM。检索到的维基百科文章的每个相关段落都被编码为特征向量序列，$\{\tilde{\mathbf{z}}_1,
    \dots, \tilde{\mathbf{z}}_m \}$。每个特征向量 $\hat{\mathbf{z}}_i \in \mathbb{R}^{d_z}$
    预计能够捕获围绕一个标记 $z_i$ 的有用上下文信息。该特征包括几类特征：'
- en: 'Word embeddings: A 300d [Glove](https://lilianweng.github.io/posts/2017-10-15-word-embedding/#glove-global-vectors)
    word embedding trained from 800B Web crawl data, $f_\text{embed} = E_g(z_i)$.'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 词嵌入：从800B Web爬取数据训练的300d [Glove](https://lilianweng.github.io/posts/2017-10-15-word-embedding/#glove-global-vectors)词嵌入，$f_\text{embed}
    = E_g(z_i)$。
- en: 'Exact match: Whether a word $z_i$ appears in the question $x$, $f_\text{match}
    = \mathbb{I}(z_i \in x)$.'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 精确匹配：一个词 $z_i$ 是否出现在问题 $x$ 中，$f_\text{match} = \mathbb{I}(z_i \in x)$。
- en: 'Token features: This includes POS (part-of-speech) tagging, NER (named entity
    recognition), and TF (term-frequency), $f_\text{token}(z_i) = (\text{POS}(z_i),
    \text{NER}(z_i), \text{TF}(z_i))$.'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 标记特征：包括POS（词性标注）、NER（命名实体识别）和TF（词频），$f_\text{token}(z_i) = (\text{POS}(z_i),
    \text{NER}(z_i), \text{TF}(z_i))$。
- en: 'Aligned question embedding: The attention score $y_{ij}$ is designed to capture
    inter-sentence matching and similarity between the paragraph token $z_i$ and the
    question word $x_j$. This feature adds soft alignments between similar but non-identical
    words.'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对齐的问题嵌入：注意力分数 $y_{ij}$ 设计用于捕获句子间匹配和段落标记 $z_i$ 与问题词 $x_j$ 之间的相似性。该特征增加了类似但非相同单词之间的软对齐。
- en: $$ \begin{aligned} f_\text{align}(z_i) &= \sum_j y_{i,j} E_g(x_j) \\ y_{i,j}
    &= \frac{\exp(\alpha(E_g(z_i))^\top \alpha(E_g(x_j)) )}{\sum_{j'} \exp(\alpha(E_g(z_i))^\top
    \alpha(E_g(x_{j'})) ) } \end{aligned} $$
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \begin{aligned} f_\text{align}(z_i) &= \sum_j y_{i,j} E_g(x_j) \\ y_{i,j}
    &= \frac{\exp(\alpha(E_g(z_i))^\top \alpha(E_g(x_j)) )}{\sum_{j'} \exp(\alpha(E_g(z_i))^\top
    \alpha(E_g(x_{j'})) ) } \end{aligned} $$
- en: where $\alpha$ is a single dense layer with ReLU and $E_g(.)$ is the glove word
    embedding.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\alpha$ 是一个具有ReLU的单个密集层，$E_g(.)$ 是glove词嵌入。
- en: 'The feature vector of a paragraph of $m$ tokens is fed into LSTM to obtain
    the final paragraph vectors:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 一个包含$m$个标记的段落的特征向量被馈送到LSTM中以获得最终段落向量：
- en: $$ \begin{aligned} \mathbf{z} = \{\mathbf{z}_1, \dots, \mathbf{z}_m\} &= \text{LSTM}(\{\tilde{\mathbf{z}}_1,
    \dots, \tilde{\mathbf{z}}_m\}) \\ \text{where } \tilde{\mathbf{z}}_i &= \{f_\text{embed},
    f_\text{match}, f_\text{token}, f_\text{align}\} \end{aligned} $$
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \begin{aligned} \mathbf{z} = \{\mathbf{z}_1, \dots, \mathbf{z}_m\} &= \text{LSTM}(\{\tilde{\mathbf{z}}_1,
    \dots, \tilde{\mathbf{z}}_m\}) \\ \text{where } \tilde{\mathbf{z}}_i &= \{f_\text{embed},
    f_\text{match}, f_\text{token}, f_\text{align}\} \end{aligned} $$
- en: 'The question is encoded as a weighted sum of the embeddings of every word in
    the question:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 问题被编码为问题中每个单词的嵌入的加权和：
- en: $$ \mathbf{x} = \sum_j b_j E(x_j) \quad b_j = \text{softmax}(\mathbf{w}^\top
    E(x_j)) $$
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \mathbf{x} = \sum_j b_j E(x_j) \quad b_j = \text{softmax}(\mathbf{w}^\top
    E(x_j)) $$
- en: where $\mathbf{w}$ is a weight vector to learn.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{w}$ 是一个学习的权重向量。
- en: Once the feature vectors are constructed for the question and all the related
    paragraphs, the reader needs to predict the probabilities of each position in
    a paragraph to be the start and the end of an answer span, $p_\text{start}(i_s)$
    and $p_\text{end}(i_s)$, respectively. Across all the paragraphs, the optimal
    span is returned as the final answer with maximum $p_\text{start}(i_s) \times
    p_\text{end}(i_e) $.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦为问题和所有相关段落构建了特征向量，阅读器需要预测段落中每个位置成为答案范围起始和结束的概率，分别为 $p_\text{start}(i_s)$ 和
    $p_\text{end}(i_s)$。在所有段落中，以最大的 $p_\text{start}(i_s) \times p_\text{end}(i_e)
    $ 返回最佳范围作为最终答案。
- en: $$ \begin{aligned} p_\text{start}(i_s) \propto \exp(\mathbf{z}_{i_s} \mathbf{W}_s
    \mathbf{x}) \\ p_\text{end}(i_e) \propto \exp(\mathbf{z}_{i_e} \mathbf{W}_e \mathbf{x})
    \\ \text{ s.t. } i_s \leq i_e \leq i_s + 15 \end{aligned} $$
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \begin{aligned} p_\text{start}(i_s) \propto \exp(\mathbf{z}_{i_s} \mathbf{W}_s
    \mathbf{x}) \\ p_\text{end}(i_e) \propto \exp(\mathbf{z}_{i_e} \mathbf{W}_e \mathbf{x})
    \\ \text{ s.t. } i_s \leq i_e \leq i_s + 15 \end{aligned} $$
- en: where $\mathbf{W}_s$ and $\mathbf{W}_e$ are learned parameters.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{W}_s$ 和 $\mathbf{W}_e$ 是学习的参数。
- en: BERT-universe
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: BERT-universe
- en: 'Following the success of [BERT](https://lilianweng.github.io/posts/2019-01-31-lm/#bert)
    ([Devlin et al., 2018](https://arxiv.org/abs/1810.04805)), many QA models develop
    the machine comprehension component based on BERT. Let’s define the BERT model
    as a function that can take one or multiple strings (concatenated by `[SEP]`)
    as input and outputs a set of BERT encoding vectors for the special `[CLS]` token
    and every input token:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在[BERT](https://lilianweng.github.io/posts/2019-01-31-lm/#bert)（[Devlin et al.,
    2018](https://arxiv.org/abs/1810.04805)）取得成功之后，许多问答模型基于BERT开发了机器理解组件。让我们将BERT模型定义为一个函数，可以接受一个或多个字符串（由
    `[SEP]` 连接）作为输入，并输出特殊 `[CLS]` 标记和每个输入标记的一组BERT编码向量：
- en: $$ \text{BERT}(s_1, s_2, \dots) = [\mathbf{h}^\texttt{[CLS]}, \mathbf{h}^{(1)},
    \mathbf{h}^{(2)}, \dots] $$
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \text{BERT}(s_1, s_2, \dots) = [\mathbf{h}^\texttt{[CLS]}, \mathbf{h}^{(1)},
    \mathbf{h}^{(2)}, \dots] $$
- en: where $\mathbf{h}^\texttt{[CLS]}$ is the embedding vector for the special `[CLS]`
    token and $\mathbf{h}^{(i)}$ is the embedding vector for the $i$-th token.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{h}^\texttt{[CLS]}$ 是特殊 `[CLS]` 标记的嵌入向量，$\mathbf{h}^{(i)}$ 是第 $i$
    个标记的嵌入向量。
- en: To use BERT for reading comprehension, it learns two additional weights, $\mathbf{W}_s$
    and $\mathbf{W}_e$, and $\text{softmax}(\mathbf{h}^{(i)}\mathbf{W}_s)$ and $\text{softmax}(\mathbf{h}^{(i)}\mathbf{W}_e)$
    define two probability distributions of start and end position of the predicted
    span per token.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用BERT进行阅读理解，它学习两个额外的权重，$\mathbf{W}_s$ 和 $\mathbf{W}_e$，并且 $\text{softmax}(\mathbf{h}^{(i)}\mathbf{W}_s)$
    和 $\text{softmax}(\mathbf{h}^{(i)}\mathbf{W}_e)$ 定义了每个标记预测范围的起始和结束位置的两个概率分布。
- en: '**BERTserini** ([Yang et al., 2019](https://arxiv.org/abs/1902.01718)) utilizes
    a pre-trained BERT model to work as the reader. Their experiments showed that
    *fine-tuning* pretrained BERT with SQuAD is sufficient to achieve high accuracy
    in identifying answer spans.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '**BERTserini**（[Yang et al., 2019](https://arxiv.org/abs/1902.01718)）利用预训练的BERT模型作为阅读器。他们的实验表明，用SQuAD对预训练的BERT进行微调就足以在识别答案范围方面取得高准确度。'
- en: '![](../Images/dd711961782416f2d66312e8efcce9b1.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dd711961782416f2d66312e8efcce9b1.png)'
- en: 'Fig. 5\. How BERT is used to solve question-answering tasks. (Image source:
    [Devlin et al., 2018](https://arxiv.org/abs/1810.04805))'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图5\. BERT如何用于解决问答任务。（图片来源：[Devlin et al., 2018](https://arxiv.org/abs/1810.04805)）
- en: 'The key difference of the BERTserini reader from the original BERT is: to allow
    comparison and aggregation of results from different segments, the final softmax
    layer over different answer spans is removed. The pre-trained BERT model is fine-tuned
    on the training set of SQuAD, where all inputs to the reader are padded to 384
    tokens with the learning rate 3e-5.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: BERTserini阅读器与原始BERT的关键区别在于：为了允许对不同段落的结果进行比较和聚合，移除了不同答案范围上的最终softmax层。预训练的BERT模型在SQuAD的训练集上进行微调，其中所有输入都被填充到384个标记，并且学习率为3e-5。
- en: When ranking all the extracted answer spans, the retriever score (BM25) and
    the reader score (probability of token being the start position $\times$ probability
    of the same token being the end position ) are combined via linear interpolation.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在对所有提取的答案范围进行排名时，检索器分数（BM25）和阅读器分数（token是起始位置的概率 $\times$ 相同token是结束位置的概率）通过线性插值进行组合。
- en: The original BERT normalizes the probability distributions of start and end
    position per token for every passage independently. Differently, the **Multi-passage
    BERT** ([Wang et al., 2019](https://arxiv.org/abs/1908.08167)) normalizes answer
    scores across all the retrieved passages of one question [globally](https://arxiv.org/abs/1710.10723).
    Precisely, multi-passage BERT removes the final normalization layer per passage
    in BERT for QA (same as in BERTserini) and then adds a global `softmax` over all
    the word positions of all the passages. Global normalization makes the reader
    model more stable while pin-pointing answers from a large number of passages.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 原始的BERT独立地为每个段落的每个标记规范化起始和结束位置的概率分布。与之不同的是，**多段BERT**（[Wang等，2019](https://arxiv.org/abs/1908.08167)）在一个问题的所有检索到的段落中全局地规范化答案分数。准确地说，多段BERT去除了BERT中用于QA的每个段落的最终规范化层（与BERTserini中相同），然后在所有段落的所有单词位置上添加一个全局的`softmax`。全局规范化使阅读器模型在从大量段落中找到答案时更加稳定。
- en: In addition, multi-passage BERT implemented an independent *passage ranker*
    model via another BERT model and the rank score for $(x, z)$ is generated by a
    `softmax` over the representation vectors of the first `[CLS]` token. The passage
    ranker brings in extra 2% improvements. Similar idea of re-ranking passages with
    BERT was discussed in [Nogueira & Cho, 2019](https://arxiv.org/abs/1901.04085),
    too.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，多段BERT通过另一个BERT模型实现了独立的*段落排名器*模型，并且$(x, z)$的排名分数是通过对第一个`[CLS]`标记的表示向量进行`softmax`生成的。段落排名器带来额外的2%改进。在[Nogueira
    & Cho，2019](https://arxiv.org/abs/1901.04085)中也讨论了使用BERT重新排列段落的类似想法。
- en: Interestingly, [Wang et al., 2019](https://arxiv.org/abs/1908.08167) found that
    *explicit inter-sentence matching* does not seem to be critical for RC tasks with
    BERT; check the original paper for how the experiments were designed. One possible
    reason is that the multi-head self-attention layers in BERT has already embedded
    the inter-sentence matching.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，[Wang等，2019](https://arxiv.org/abs/1908.08167)发现，在使用BERT进行RC任务时，*显式的句间匹配*似乎并不是关键因素；请查看原始论文以了解实验是如何设计的。一个可能的原因是BERT中的多头自注意力层已经嵌入了句间匹配。
- en: End-to-end Joint Training
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 端到端联合训练
- en: The retriever and reader components can be jointly trained. This section covers
    R^3, ORQA, REALM and DPR. There are a lot of common designs, such as BERT-based
    dense vectors for retrieval and the loss function on maximizing the marginal likelihood
    of obtaining true answers.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 检索器和阅读器组件可以联合训练。本节涵盖了R^3、ORQA、REALM和DPR。有许多共同的设计，例如基于BERT的密集向量用于检索和最大化获得真实答案的边际似然的损失函数。
- en: The retriever and reader models in the **R^3** (“Reinforced Ranker-Reader”;
    [Wang, et al., 2017](https://arxiv.org/abs/1709.00023)) QA system are jointly
    trained via [reinforcement learning](https://lilianweng.github.io/posts/2018-02-19-rl-overview/).
    (Note that to keep the term consistent between papers in this section, the “ranker”
    model in the original R^3 paper is referred to as the “retriever” model here.)
    Both components are variants of [Match-LSTM](https://arxiv.org/abs/1512.08849),
    which relies on an attention mechanism to compute word similarities between the
    passage and question sequences.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '**R^3**（“强化排名-阅读器”；[Wang等，2017](https://arxiv.org/abs/1709.00023)）QA系统中的检索器和阅读器模型通过[强化学习](https://lilianweng.github.io/posts/2018-02-19-rl-overview/)联合训练。（请注意，在本节中保持术语一致，原始R^3论文中的“排名器”模型在这里被称为“检索器”模型。）这两个组件都是[Match-LSTM](https://arxiv.org/abs/1512.08849)的变体，依赖于注意机制来计算段落和问题序列之间的单词相似性。'
- en: '**How does the Match-LSTM module work?** Given a question $\mathbf{X}$ of $d_x$
    words and a passage $\mathbf{Z}$ of $d_z$ words, both representations use fixed
    [Glove](https://lilianweng.github.io/posts/2017-10-15-word-embedding/#glove-global-vectors)
    word embeddings,'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '**Match-LSTM模块是如何工作的？** 给定一个包含$d_x$个单词的问题$\mathbf{X}$和一个包含$d_z$个单词的段落$\mathbf{Z}$，两个表示都使用固定的[Glove](https://lilianweng.github.io/posts/2017-10-15-word-embedding/#glove-global-vectors)词嵌入，'
- en: $$ \begin{aligned} \mathbf{H}^x &= \text{BiLSTM}(\mathbf{X}) \in \mathbb{R}^{l
    \times d_x} \\ \mathbf{H}^z &= \text{BiLSTM}(\mathbf{Z}) \in \mathbb{R}^{l \times
    d_z} \\ \mathbf{G} &= \text{softmax}((\mathbf{W}^g \mathbf{H}^x + \mathbf{b}^g
    \otimes \mathbf{e}_{d_x})^\top \mathbf{H}^z) \in \mathbb{R}^{d_x \times d_z} &
    \text{; an attention matrix}\\ \bar{\mathbf{H}}^x &= \mathbf{H}^x \mathbf{G} \in
    \mathbb{R}^{l \times d_z} \\ \mathbf{M} &= \text{ReLU} \Big( \mathbf{W}^m \begin{bmatrix}
    \mathbf{H}^z \\ \bar{\mathbf{H}}^x \\ \mathbf{H}^z \odot \bar{\mathbf{H}}^x \\
    \mathbf{H}^z - \bar{\mathbf{H}}^x \end{bmatrix} \Big) \in \mathbb{R}^{2l \times
    d_z} \\ \mathbf{H}^m &= \text{BiLSTM}(M) \in \mathbb{R}^{l \times d_z} \end{aligned}
    $$
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \begin{aligned} \mathbf{H}^x &= \text{BiLSTM}(\mathbf{X}) \in \mathbb{R}^{l
    \times d_x} \\ \mathbf{H}^z &= \text{BiLSTM}(\mathbf{Z}) \in \mathbb{R}^{l \times
    d_z} \\ \mathbf{G} &= \text{softmax}((\mathbf{W}^g \mathbf{H}^x + \mathbf{b}^g
    \otimes \mathbf{e}_{d_x})^\top \mathbf{H}^z) \in \mathbb{R}^{d_x \times d_z} &
    \text{; 一个注意力矩阵}\\ \bar{\mathbf{H}}^x &= \mathbf{H}^x \mathbf{G} \in \mathbb{R}^{l
    \times d_z} \\ \mathbf{M} &= \text{ReLU} \Big( \mathbf{W}^m \begin{bmatrix} \mathbf{H}^z
    \\ \bar{\mathbf{H}}^x \\ \mathbf{H}^z \odot \bar{\mathbf{H}}^x \\ \mathbf{H}^z
    - \bar{\mathbf{H}}^x \end{bmatrix} \Big) \in \mathbb{R}^{2l \times d_z} \\ \mathbf{H}^m
    &= \text{BiLSTM}(M) \in \mathbb{R}^{l \times d_z} \end{aligned} $$
- en: where $l$ is the hidden dimension of the bidirectional LSTM module. $\mathbf{W}^g
    \in \mathbb{R}^{l\times l}$, $\mathbf{b}^g \in \mathbb{R}^l$, and $\mathbf{W}^m
    \in \mathbb{R}^{2l \times 4l}$ are parameters to learn. The operator $\otimes
    \mathbf{e}_{d_x}$ is the outer product to repeat the column vector $\mathbf{b}^g$
    $d_x$ times.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$l$是双向LSTM模块的隐藏维度。$\mathbf{W}^g \in \mathbb{R}^{l\times l}$，$\mathbf{b}^g
    \in \mathbb{R}^l$，$\mathbf{W}^m \in \mathbb{R}^{2l \times 4l}$是要学习的参数。运算符$\otimes
    \mathbf{e}_{d_x}$是外积，重复列向量$\mathbf{b}^g$ $d_x$次。
- en: The ranker and reader components share the same Match-LSTM module with two separate
    prediction heads in the last layer, resulting in $\mathbf{H}^\text{rank}$ and
    $\mathbf{H}^\text{reader}$.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 排序器和阅读器组件在最后一层中共享相同的Match-LSTM模块，导致$\mathbf{H}^\text{rank}$和$\mathbf{H}^\text{reader}$。
- en: '![](../Images/77a18c1ac97832793216d3fe33ae3eae.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/77a18c1ac97832793216d3fe33ae3eae.png)'
- en: 'Fig. 6\. The overview of R^3 (reinforced ranker-reader) architecture. Both
    components share the same Match-LSTM module. (Image source: [Wang, et al., 2017](https://arxiv.org/abs/1709.00023))'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图6。R^3（强化排序器-阅读器）架构概述。两个组件共享相同的Match-LSTM模块。（图片来源：[Wang, et al., 2017](https://arxiv.org/abs/1709.00023)）
- en: The retriever runs a max-pooling operation per passage and then aggregates to
    output a probability of each passage entailing the answer.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 信息检索器对每个段落进行最大池化操作，然后聚合输出每个段落包含答案的概率。
- en: $$ \begin{aligned} \mathbf{u}_i &= \text{max-pooling}(\mathbf{H}^\text{rank}_i)
    \in \mathbb{R}^l \\ \mathbf{C} &= \text{tanh}(\mathbf{W}^c[\mathbf{u}_1;\dots;\mathbf{u}_N]
    + \mathbf{b}^c \otimes \mathbf{e}_N) \in \mathbb{R}^{l \times n} \\ \gamma &=
    \text{softmax}(\mathbf{w}^c \mathbf{C}) \in \mathbb{R}^n \end{aligned} $$
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \begin{aligned} \mathbf{u}_i &= \text{max-pooling}(\mathbf{H}^\text{rank}_i)
    \in \mathbb{R}^l \\ \mathbf{C} &= \text{tanh}(\mathbf{W}^c[\mathbf{u}_1;\dots;\mathbf{u}_N]
    + \mathbf{b}^c \otimes \mathbf{e}_N) \in \mathbb{R}^{l \times n} \\ \gamma &=
    \text{softmax}(\mathbf{w}^c \mathbf{C}) \in \mathbb{R}^n \end{aligned} $$
- en: Finally, the retriever is viewed as a *policy* to output action to sample a
    passage according to predicted $\gamma$,
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，信息检索器被视为一个*策略*，输出动作以根据预测的$\gamma$抽取段落，
- en: $$ \pi(z \vert x; \theta^\gamma) = \gamma_z $$
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \pi(z \vert x; \theta^\gamma) = \gamma_z $$
- en: The reader predicts the start position $\beta^s$ and the end position $\beta^e$
    of the answer span. Two positions are computed in the same way, with independent
    parameters to learn. There are $V$ words in all the passages involved.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读器预测答案跨度的起始位置$\beta^s$和结束位置$\beta^e$。两个位置以相同方式计算，具有独立的学习参数。所有涉及的段落中共有$V$个单词。
- en: $$ \begin{aligned} \mathbf{H}^\text{read} &= [\mathbf{H}^\text{read}_\tau; \mathbf{H}^\text{read}_{\text{neg}_1};
    \dots; \mathbf{H}^\text{read}_{\text{neg}_n}] \\ \mathbf{F}^s &= \text{tanh}(\mathbf{W}^s
    \mathbf{H}^\text{read} + \mathbf{b}^s \otimes \mathbf{e}_V) \quad \beta^s = \text{softmax}(\mathbf{w}^s
    \mathbf{F}^s) \in \mathbb{R}^V \\ \mathbf{F}^e &= \text{tanh}(\mathbf{W}^e \mathbf{H}^\text{read}
    + \mathbf{b}^e \otimes \mathbf{e}_V) \quad \beta^e = \text{softmax}(\mathbf{w}^e
    \mathbf{F}^e) \in \mathbb{R}^V \\ L(y \vert z, x) &= -\log(\beta^s_{y_z^s})-\log(\beta^e_{y_z^e})
    \end{aligned} $$
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \begin{aligned} \mathbf{H}^\text{read} &= [\mathbf{H}^\text{read}_\tau; \mathbf{H}^\text{read}_{\text{neg}_1};
    \dots; \mathbf{H}^\text{read}_{\text{neg}_n}] \\ \mathbf{F}^s &= \text{tanh}(\mathbf{W}^s
    \mathbf{H}^\text{read} + \mathbf{b}^s \otimes \mathbf{e}_V) \quad \beta^s = \text{softmax}(\mathbf{w}^s
    \mathbf{F}^s) \in \mathbb{R}^V \\ \mathbf{F}^e &= \text{tanh}(\mathbf{W}^e \mathbf{H}^\text{read}
    + \mathbf{b}^e \otimes \mathbf{e}_V) \quad \beta^e = \text{softmax}(\mathbf{w}^e
    \mathbf{F}^e) \in \mathbb{R}^V \\ L(y \vert z, x) &= -\log(\beta^s_{y_z^s})-\log(\beta^e_{y_z^e})
    \end{aligned} $$
- en: where $y$ is the ground-truth answer and the passage $z$ is sampled by the retriever.
    $\beta^s_{y_z^s}$ and $\beta^s_{y_z^e}$ represent the probabilities of the start
    and end positions of $y$ in passage $z$.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$y$是地面真相答案，段落$z$由检索器采样。$\beta^s_{y_z^s}$和$\beta^s_{y_z^e}$表示$y$在段落$z$中起始和结束位置的概率。
- en: The training objective for the end-to-end R^3 QA system is to minimize the negative
    log-likelihood of obtaining the correct answer $y$ given a question $x$,
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 端到端R^3 QA系统的训练目标是最小化给定问题$x$的正确答案$y$的负对数似然，
- en: $$ \begin{aligned} \mathcal{J}(\theta) &= -\mathbb{E}_{z\sim\pi(.\vert x)} [L(y
    \vert z, x)] \\ \nabla \mathcal{J}(\theta) &= - \nabla_\theta \sum_z \pi(z \vert
    x) L(y \vert z, x) \\ &= - \sum_z \big( L(y \vert z, x) \nabla_\theta\pi(z \vert
    x) + \pi(z \vert x) \nabla_\theta L(y \vert z, x) \big) \\ &= - \mathbb{E}_{z\sim\pi(.\vert
    x)} \big( \color{red}{L(y \vert z, x)\nabla_\theta\log\pi(z \vert x)} + \nabla_\theta
    L(y \vert z, x) \big) \\ &\approx - \mathbb{E}_{z\sim\pi(.\vert x)} \big( \underbrace{\color{red}{R(y
    \vert z, x)\nabla_\theta\log\pi(z \vert x)}}_\text{REINFORCE} + \nabla_\theta
    L(y \vert z, x) \big) \end{aligned} $$
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \begin{aligned} \mathcal{J}(\theta) &= -\mathbb{E}_{z\sim\pi(.\vert x)} [L(y
    \vert z, x)] \\ \nabla \mathcal{J}(\theta) &= - \nabla_\theta \sum_z \pi(z \vert
    x) L(y \vert z, x) \\ &= - \sum_z \big( L(y \vert z, x) \nabla_\theta\pi(z \vert
    x) + \pi(z \vert x) \nabla_\theta L(y \vert z, x) \big) \\ &= - \mathbb{E}_{z\sim\pi(.\vert
    x)} \big( \color{red}{L(y \vert z, x)\nabla_\theta\log\pi(z \vert x)} + \nabla_\theta
    L(y \vert z, x) \big) \\ &\approx - \mathbb{E}_{z\sim\pi(.\vert x)} \big( \underbrace{\color{red}{R(y
    \vert z, x)\nabla_\theta\log\pi(z \vert x)}}_\text{REINFORCE} + \nabla_\theta
    L(y \vert z, x) \big) \end{aligned} $$
- en: 'Essentially in training, given a passage $z$ sampled by the retriever, the
    reader is trained by gradient descent while the retriever is trained by [REINFORCE](https://lilianweng.github.io/posts/2018-04-08-policy-gradient/#reinforce)
    using $L(y \vert z, x)$ as the reward function. However, $L(y \vert z, x)$ is
    not bounded and may introduce a lot of variance. The paper replaces the reward
    with a customized scoring function by comparing the ground truth $y$ and the answer
    extracted by the reader $\hat{y}$:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 训练过程中，给定由检索器采样的段落$z$，阅读器通过梯度下降进行训练，而检索器则通过[REINFORCE](https://lilianweng.github.io/posts/2018-04-08-policy-gradient/#reinforce)进行训练，使用$L(y
    \vert z, x)$作为奖励函数。然而，$L(y \vert z, x)$并非有界，可能引入很多方差。该论文通过比较地面真相$y$和阅读器提取的答案$\hat{y}$，用自定义评分函数替换奖励：
- en: $$ R(y, \hat{y} \vert z) = \begin{cases} 2 & \text{if } y = \hat{y}\\ f1(y,
    \hat{y}) & \text{if } y \cap \hat{y} = \varnothing \\ -1 & \text{otherwise} \end{cases}
    $$![](../Images/8266be557a80072acd8a9ba12c582d15.png)
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: $$ R(y, \hat{y} \vert z) = \begin{cases} 2 & \text{if } y = \hat{y}\\ f1(y,
    \hat{y}) & \text{if } y \cap \hat{y} = \varnothing \\ -1 & \text{otherwise} \end{cases}
    $$![](../Images/8266be557a80072acd8a9ba12c582d15.png)
- en: 'Fig. 7\. The workflow of R^3 training process. (Image source: [acl2020-openqa-tutorial/slides/part4](https://github.com/danqi/acl2020-openqa-tutorial/blob/master/slides/part4-retriever-reader.pdf))'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图7\. R^3训练过程的工作流程。（图片来源：[acl2020-openqa-tutorial/slides/part4](https://github.com/danqi/acl2020-openqa-tutorial/blob/master/slides/part4-retriever-reader.pdf)）
- en: '**ORQA** (“Open-Retrieval Question-Answering”; [Lee et al., 2019](https://arxiv.org/abs/1906.00300))
    jointly learns a retriever + reader QA model to optimize marginal log-likelihood
    of obtaining correct answers in a supervised manner. No explicit “black-box” IR
    system is involved. Instead, it is capable of retrieving any text in an open corpus.
    During training, ORQA does not need ground-truth context passages (i.e. reading
    comprehension datasets) but only needs (question, answer) string pairs. Both retriever
    and reader components are based on BERT, but not shared.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '**ORQA**（“开放式检索问答”；[Lee等，2019](https://arxiv.org/abs/1906.00300)）共同学习检索器+阅读器QA模型，以监督方式优化获得正确答案的边际对数似然。没有明确的“黑盒”IR系统参与。相反，它能够检索开放语料库中的任何文本。在训练过程中，ORQA不需要地面真相上下文段落（即阅读理解数据集），而只需要（问题，答案）字符串对。检索器和阅读器组件都基于BERT，但不共享。'
- en: '![](../Images/d1d494451a1b61d82bbbf05f30b1b6ff.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d1d494451a1b61d82bbbf05f30b1b6ff.png)'
- en: 'Fig. 8\. An illustration of the retriever component in ORQA. (Image source:
    replotted based on one slide in [acl2020-openqa-tutorial/slides/part5](https://github.com/danqi/acl2020-openqa-tutorial/blob/master/slides/part5-dense-retriever-e2e-training.pdf))'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图8\. ORQA中检索器组件的示意图。（图片来源：重新绘制，基于[acl2020-openqa-tutorial/slides/part5](https://github.com/danqi/acl2020-openqa-tutorial/blob/master/slides/part5-dense-retriever-e2e-training.pdf)中的一张幻灯片）
- en: All the evidence blocks are ranked by a retrieval score, defined as the inner
    product of BERT embedding vectors of the `[CLS]` token of the question $x$ and
    the evidence block $z$. Note that the encoders for questions and context are independent.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 所有证据块都按检索分数排名，定义为问题$x$的`[CLS]`标记和证据块$z$的BERT嵌入向量的内积。请注意，问题和上下文的编码器是独立的。
- en: $$ \begin{aligned} h_x &= \mathbf{W}_x \text{BERT}_x(x)^{\mathtt{[CLS]}} \\
    h_z &= \mathbf{W}_z \text{BERT}_z(z)^{\mathtt{[CLS]}} \\ S_\text{retr}(z, x) &=
    h_x^\top h_z \end{aligned} $$
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \begin{aligned} h_x &= \mathbf{W}_x \text{BERT}_x(x)^{\mathtt{[CLS]}} \\
    h_z &= \mathbf{W}_z \text{BERT}_z(z)^{\mathtt{[CLS]}} \\ S_\text{retr}(z, x) &=
    h_x^\top h_z \end{aligned} $$
- en: 'The retriever module is pretrained with *Inverse Cloze Task (ICT)*, which is
    to predict the context given a sentence, opposite to the standard [Cloze Task](https://en.wikipedia.org/wiki/Cloze_test).
    The ICT objective is to maximize the retrieval score of the correct context $z$
    given a random sentence $x$:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 检索器模块使用*Inverse Cloze Task (ICT)*进行预训练，即给定一个句子预测上下文，与标准的[Cloze Task](https://en.wikipedia.org/wiki/Cloze_test)相反。ICT目标是最大化给定随机句子$x$的正确上下文$z$的检索分数：
- en: $$ L_\text{ICT} = p_\text{early}(z \vert x) = \frac{\exp(S_\text{retr}(z, x))}{\sum_{z'\in\text{BATCH}(\mathcal{Z})}
    \exp(S_\text{retr}(z', x))} $$
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: $$ L_\text{ICT} = p_\text{early}(z \vert x) = \frac{\exp(S_\text{retr}(z, x))}{\sum_{z'\in\text{BATCH}(\mathcal{Z})}
    \exp(S_\text{retr}(z', x))} $$
- en: where $\text{BATCH}(\mathcal{Z})$ is the set of evidence blocks in the same
    batch used as sampled negatives.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\text{BATCH}(\mathcal{Z})$是同一批次中用作负样本的证据块集合。
- en: After such pretraining, the BERT retriever is expected to have representations
    good enough for evidence retrieval. Only the question encoder needs to be fine-tuned
    for answer extraction. In other words, the evidence block encoder (i.e., $\mathbf{W}_z$
    and $\text{BERT}_z$) is fixed and thus all the evidence block encodings can be
    pre-computed with support for [fast Maximum Inner Product Search (MIPS)](#fast-maximum-inner-product-search-mips).
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 经过这样的预训练，预期BERT检索器的表示足够好以进行证据检索。只需对问题编码器进行微调以进行答案提取。换句话说，证据块编码器（即$\mathbf{W}_z$和$\text{BERT}_z$）是固定的，因此所有证据块编码都可以预先计算，并支持[快速最大内积搜索（MIPS）](#fast-maximum-inner-product-search-mips)。
- en: '![](../Images/07848864f28b51f46371ee5e738d6a60.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/07848864f28b51f46371ee5e738d6a60.png)'
- en: 'Fig. 9\. An illustration of the reader component in ORQA. (Image source: [acl2020-openqa-tutorial/slides/part5](https://github.com/danqi/acl2020-openqa-tutorial/blob/master/slides/part5-dense-retriever-e2e-training.pdf))'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 图9\. ORQA中阅读器组件的示意图（图片来源：[acl2020-openqa-tutorial/slides/part5](https://github.com/danqi/acl2020-openqa-tutorial/blob/master/slides/part5-dense-retriever-e2e-training.pdf)）
- en: 'The reader follows the same design as in the original [BERT RC](https://lilianweng.github.io/posts/2019-01-31-lm/#use-bert-in-downstream-tasks)
    experiments. It learns in a supervised manner, while the parameters of the evidence
    block encoder are fixed and all other parameters are fine-tuned. Given a question
    $x$ and a gold answer string $y$, the reader loss contains two parts:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读器遵循原始[BERT RC](https://lilianweng.github.io/posts/2019-01-31-lm/#use-bert-in-downstream-tasks)实验中的相同设计。它以监督方式学习，而证据块编码器的参数是固定的，所有其他参数都进行微调。给定问题$x$和黄金答案字符串$y$，阅读器损失包含两部分：
- en: $$ \mathcal{L}(x, y) = \mathcal{L}_\text{early}(x, y) + \mathcal{L}_\text{full}(x,
    y) $$
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \mathcal{L}(x, y) = \mathcal{L}_\text{early}(x, y) + \mathcal{L}_\text{full}(x,
    y) $$
- en: '(1) Find all correct text spans within top $k$ evidence blocks and optimize
    for the marginal likelihood of a text span $s$ that matches the true answer $y$:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: (1) 在前$k$个证据块中找到所有正确的文本跨度，并优化匹配真实答案$y$的文本跨度$s$的边际似然：
- en: $$ \begin{aligned} h_s &= \text{BERT}_R(x, y)^{(\text{START}(s))} \\ h_e &=
    \text{BERT}_R(x, y)^{(\text{END}(s))} \\ S_\text{read}(z, s, x) &= \text{MLP}([h_s;
    h_e]) \\ p(z, s \vert x) &= \frac{\exp(S_\text{read}(z, s, x))}{\sum_{z'\in\text{TOP}(k)}
    \sum_{s'\in z'} \exp(S_\text{read}(z', s', x))} \\ L_\text{full}(x, y) &= - \log
    \sum_{\substack{z \in \text{TOP}(k)\\ s \in z}} \sum_{y=\text{TEXT}(s)} p(z, s
    \vert x) \end{aligned} $$
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \begin{aligned} h_s &= \text{BERT}_R(x, y)^{(\text{START}(s))} \\ h_e &=
    \text{BERT}_R(x, y)^{(\text{END}(s))} \\ S_\text{read}(z, s, x) &= \text{MLP}([h_s;
    h_e]) \\ p(z, s \vert x) &= \frac{\exp(S_\text{read}(z, s, x))}{\sum_{z'\in\text{TOP}(k)}
    \sum_{s'\in z'} \exp(S_\text{read}(z', s', x))} \\ L_\text{full}(x, y) &= - \log
    \sum_{\substack{z \in \text{TOP}(k)\\ s \in z}} \sum_{y=\text{TEXT}(s)} p(z, s
    \vert x) \end{aligned} $$
- en: where $y=\text{TEXT}(s)$ indicates whether the answer $y$ matches the text span
    $s$. $\text{TOP}(k)$ is the top $k$ retrieved blocks according to $S_\text{retr}(z,
    x)$. The paper sets $k=5$.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$y=\text{TEXT}(s)$表示答案$y$是否与文本跨度$s$匹配。$\text{TOP}(k)$是根据$S_\text{retr}(z,
    x)$检索到的前$k$个块。该论文设置$k=5$。
- en: (2) At the early stage of learning, when the retriever is not strong enough,
    it is possible none of the top $k$ blocks contains the answer. To avoid such sparse
    learning signals, ORQA considers a larger set of $c$ evidence blocks for more
    aggressive learning. The paper has $c=5000$.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: (2) 在学习的早期阶段，当检索器还不够强大时，可能没有一个顶部的$k$块包含答案。为了避免这种稀疏的学习信号，ORQA考虑了更大的$c$证据块集合，以进行更积极的学习。论文中$c=5000$。
- en: $$ L_\text{early}(x, y) = -\log \sum_{\substack{z\in \text{TOP}(c)\\y\in\text{TEXT}(z)}}
    p_\text{early}(z\vert x) = -\log \sum_{\substack{z\in \text{TOP}(c)\\y\in\text{TEXT}(z)}}
    \frac{\exp(S_\text{retr}(z, x)}{\sum_{z'\in\text{TOP}(c)} \exp(S_\text{retr}(z',
    x)} $$
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: $$ L_\text{early}(x, y) = -\log \sum_{\substack{z\in \text{TOP}(c)\\y\in\text{TEXT}(z)}}
    p_\text{early}(z\vert x) = -\log \sum_{\substack{z\in \text{TOP}(c)\\y\in\text{TEXT}(z)}}
    \frac{\exp(S_\text{retr}(z, x)}{\sum_{z'\in\text{TOP}(c)} \exp(S_\text{retr}(z',
    x)} $$
- en: 'Some issues in SQuAD dataset were discussed in the ORQA paper:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: ORQA论文中讨论了SQuAD数据集中的一些问题：
- en: '" The notable drop between development and test accuracy for SQuAD is a reflection
    of an artifact in the dataset—its 100k questions are derived from only 536 documents.
    Therefore, good retrieval targets are highly correlated between training examples,
    violating the IID assumption, and making it unsuitable for learned retrieval.
    We strongly suggest that those who are interested in end-to-end open-domain QA
    models no longer train and evaluate with SQuAD for this reason."'
  id: totrans-138
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “SQuAD的开发和测试准确率之间的显着下降反映了数据集中的一个问题——其10万个问题仅来自536个文档。因此，训练示例之间的好的检索目标高度相关，违反了IID假设，使其不适用于学习检索。出于这个原因，我们强烈建议那些对端到端开放域QA模型感兴趣的人不再使用SQuAD进行训练和评估。”
- en: '**REALM** (“Retrieval-Augmented Language Model pre-training”; [Guu et al.,
    2020](https://arxiv.org/abs/2002.08909)) also jointly trains retriever + reader
    by optimizing the marginal likelihood of obtaining the true answer:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '**REALM**（“检索增强语言模型预训练”；[Guu等人，2020年](https://arxiv.org/abs/2002.08909)）还通过优化获得真实答案的边际似然来联合训练检索器+读者：'
- en: $$ p(y \vert x) = \sum_{z \in \mathcal{Z}} \underbrace{p(y \vert x, z)}_\text{reader}
    \underbrace{p(z \vert x)}_\text{retriever} \approx \sum_{z \in \text{TOP}_k(\mathcal{Z})}
    p(y \vert x, z) p(z \vert x) $$![](../Images/09a2aca893d76d02388dc2e33a0aad7d.png)
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: $$ p(y \vert x) = \sum_{z \in \mathcal{Z}} \underbrace{p(y \vert x, z)}_\text{读者}
    \underbrace{p(z \vert x)}_\text{检索器} \approx \sum_{z \in \text{TOP}_k(\mathcal{Z})}
    p(y \vert x, z) p(z \vert x) $$![](../Images/09a2aca893d76d02388dc2e33a0aad7d.png)
- en: 'Fig. 10\. REALM is first unsupervised pre-trained with salient spans masking
    and then fine-tuned with QA data. (Image source: [Guu et al., 2020](https://arxiv.org/abs/2002.08909)).'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 图10。REALM首先使用显著跨度屏蔽进行无监督预训练，然后再用QA数据进行微调。（图片来源：[Guu等人，2020年](https://arxiv.org/abs/2002.08909)）。
- en: REALM computes two probabilities, $p(z \vert x)$ and $p(y \vert x, z)$, same
    as ORQA. However, different from ICT in ORQA, REALM upgrades the unsupervised
    pre-training step with several new design decisions, leading towards better retrievals.
    REALM pre-trains the model with Wikipedia or CC-News corpus.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: REALM计算两个概率，$p(z \vert x)$ 和 $p(y \vert x, z)$，与ORQA相同。然而，与ORQA中的ICT不同，REALM通过几个新的设计决策升级了无监督预训练步骤，从而实现更好的检索。REALM使用维基百科或CC-News语料库对模型进行预训练。
- en: Use *salient span masking*. Named entities and dates are identified. Then one
    of these “salient spans” is selected and masked. Salient span masking is a special
    case of MLM and works out well for QA tasks.
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用*显著跨度屏蔽*。命名实体和日期被识别。然后选择并屏蔽其中一个“显著跨度”。显著跨度屏蔽是MLM的一个特例，对QA任务效果很好。
- en: Add an *empty null document*. Because not every question demands a context document.
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加一个*空的空文档*。因为并非每个问题都需要一个上下文文档。
- en: No trivial retrieval. The context document should not be same as the selected
    sentence with a masked span.
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 不要进行平凡的检索。上下文文档不应与选择的带有屏蔽跨度的句子相同。
- en: Apply the same ICT loss as in ORQA to encourage learning when the retrieval
    quality is still poor at the early stage of training.
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练的早期阶段，当检索质量仍然较差时，应用与ORQA中相同的ICT损失以鼓励学习。
- en: “Among all systems, the most direct comparison with REALM is ORQA (Lee et al.,
    2019), where the fine-tuning setup, hyperparameters and training data are identical.
    The improvement of REALM over ORQA is purely due to better pre-training methods.”
    — from REALM paper.
  id: totrans-147
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “在所有系统中，与REALM最直接的比较是ORQA（Lee等人，2019年），其中微调设置、超参数和训练数据是相同的。REALM相对于ORQA的改进纯粹是由于更好的预训练方法。”
    ——来自REALM论文。
- en: Both unsupervised pre-training and supervised fine-tuning optimize the same
    log-likelihood $\log p(y \vert x)$. Because the parameters of the retriever encoder
    for evidence documents are also updated in the process, the index for MIPS is
    changing. REALM asynchronously refreshes the index with the updated encoder parameters
    every several hundred training steps.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督预训练和监督微调都优化相同的对数似然$\log p(y \vert x)$。由于证据文档的检索器编码器的参数也在此过程中更新，MIPS的索引正在变化。REALM每隔几百个训练步骤异步使用更新的编码器参数刷新索引。
- en: '[Balachandran, et al. (2021)](https://arxiv.org/abs/2104.08710) found that
    REALM is significantly undertrained and REALM++ achieves great EM accuracy improvement
    (3-5%) by scaling up the model training with larger batch size and more retrieved
    documents for the reader to process.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '[Balachandran等人（2021）](https://arxiv.org/abs/2104.08710)发现REALM明显训练不足，而REALM++通过增加模型训练的批量大小和更多检索文档，使读者处理的EM准确性得到了很大的提高（3-5%）。'
- en: '**DPR** (“Dense Passage Retriever”; [Karpukhin et al., 2020](https://arxiv.org/abs/2004.04906),
    [code](https://github.com/facebookresearch/DPR)) argues that ICT pre-training
    could be too computationally expensive and the ORQA’s context encoder might be
    sub-optimal because it is not fine-tuned with question-answer pairs. DPR aims
    to resolve these two issues by only training a dense dual-encoder architecture
    for retrieval only from a small number of Q/A pairs, without any pre-training.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '**DPR**（“密集段落检索器”；[Karpukhin等人，2020](https://arxiv.org/abs/2004.04906)，[code](https://github.com/facebookresearch/DPR)）认为ICT预训练可能过于计算昂贵，而ORQA的上下文编码器可能不够优化，因为它没有与问题-答案对进行微调。DPR旨在通过仅训练一个密集双编码器架构，仅从少量Q/A对中检索，而无需任何预训练来解决这两个问题。'
- en: Same as previous work, DPR uses the dot-product (L2 distance or cosine similarity
    also works) of BERT representations as retrieval score. The loss function for
    training the dual-encoder is the NLL of the positive passage, which essentially
    takes the same formulation as [ICT loss](#ICT-loss) of ORQA. Note that both of
    them consider other passages in the same batch as the negative samples, named
    *in-batch negative sampling*. The main difference is that DPR relies on supervised
    QA data, while ORQA trains with ICT on unsupervised corpus. At the inference time,
    DPR uses [FAISS](https://github.com/facebookresearch/faiss) to run fast MIPS.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 与先前的工作相同，DPR使用BERT表示的点积（L2距离或余弦相似度也适用）作为检索分数。用于训练双编码器的损失函数是正面段落的NLL，其本质上与[ICT损失](#ICT-loss)的ORQA相同。请注意，它们都将同一批次中的其他段落视为负样本，称为*批内负采样*。主要区别在于DPR依赖于监督QA数据，而ORQA在无监督语料库上进行ICT训练。在推断时，DPR使用[FAISS](https://github.com/facebookresearch/faiss)来快速运行MIPS。
- en: 'DPR did a set of comparison experiments involving several different types of
    negatives:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: DPR进行了一系列比较实验，涉及几种不同类型的负例：
- en: 'Random: any random passage from the corpus;'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机：来自语料库的任意随机段落；
- en: 'BM25: top passages returned by BM25 which don’t contain the answer but match
    most question tokens;'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: BM25：由BM25返回的顶级段落，不包含答案但匹配大多数问题标记；
- en: 'In-batch negative sampling (“gold”): positive passages paired with other questions
    which appear in the training set.'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 批内负采样（“金标”）：与训练集中出现的其他问题配对的正面段落。
- en: DPR found that using gold passages from the same mini-batch and one negative
    passage with high BM25 score works the best. To further improve the retrieval
    results, DPR also explored a setting where a BM25 score and a dense embedding
    retrieval score are linearly combined to serve as a new ranking function.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: DPR发现，使用来自同一小批次的金标段落和一个具有较高BM25分数的负面段落效果最好。为了进一步改善检索结果，DPR还探索了一种设置，其中BM25分数和密集嵌入检索分数被线性组合以作为新的排名函数。
- en: 'Open-book QA: Retriever-Generator'
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开放书问答：检索器-生成器
- en: Compared to the retriever-reader approach, the retriever-generator also has
    2 stages but the second stage is to generate free text directly to answer the
    question rather than to extract start/end position in a retrieved passage. Some
    paper also refer to this as *Generative question answering*.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 与检索器-阅读器方法相比，检索器-生成器也有2个阶段，但第二阶段是直接生成自由文本来回答问题，而不是在检索到的段落中提取起始/结束位置。一些论文也将此称为*生成式问答*。
- en: '![](../Images/b4b9253390c26d5cc09e1668d48bb9a8.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b4b9253390c26d5cc09e1668d48bb9a8.png)'
- en: Fig. 11\. The retriever + generator QA framework combines a document retrieval
    system with a general language model.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 图11. 检索器+生成器QA框架将文档检索系统与通用语言模型结合在一起。
- en: A pretrained LM has a great capacity of memorizing knowledge in its parameters,
    as shown above. However, they cannot easily modify or expand their memory, cannot
    straightforwardly provide insights into their predictions, and may produce non-existent
    illusion.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练的LM在其参数中具有记忆知识的巨大能力，如上所示。然而，它们不能轻松修改或扩展其记忆，不能直接提供对其预测的见解，并可能产生不存在的幻觉。
- en: '[Petroni et al. (2020)](https://arxiv.org/abs/2005.04611) studied how the retrieved
    relevant context can help a generative language model produce better answers.
    They found:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '[Petroni等人（2020）](https://arxiv.org/abs/2005.04611)研究了检索到的相关上下文如何帮助生成式语言模型产生更好的答案。他们发现：'
- en: Augmenting queries with relevant contexts dramatically improves the pretrained
    LM on unsupervised machine reading capabilities.
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过相关上下文增强查询，显著提高了预训练的LM在无监督机器阅读能力上的表现。
- en: An off-the-shelf IR system is sufficient for BERT to match the performance of
    a supervised ODQA baseline;
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个现成的IR系统足以使BERT达到监督ODQA基线的性能；
- en: BERT’s [NSP](https://lilianweng.github.io/posts/2019-01-31-lm/#pre-training-tasks)
    pre-training strategy is a highly effective unsupervised mechanism in dealing
    with noisy and irrelevant contexts.
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: BERT的[NSP](https://lilianweng.github.io/posts/2019-01-31-lm/#pre-training-tasks)预训练策略是一种高效的无监督机制，用于处理嘈杂和无关的上下文。
- en: 'They pair the BERT model with different types of context, including adversarial
    (unrelated context), retrieved (by BM25), and generative (by an autoregressive
    language model of 1.4N parameters, trained on CC-NEWS). The model is found to
    be robust to adversarial context, but only when the question and the context are
    provided as two segments (e.g. separated by `[SEP]`). One hypothesis is related
    to NSP task: “BERT might learn to not condition across segments for masked token
    prediction if the NSP score is low, thereby implicitly detecting irrelevant and
    noisy contexts.”'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 将BERT模型与不同类型的上下文配对，包括对抗性（无关上下文）、检索（通过BM25检索）和生成性（通过一个包含1.4N参数的自回归语言模型，在CC-NEWS上训练）。发现该模型对对抗性上下文具有鲁棒性，但仅当问题和上下文作为两个片段提供时（例如，由`[SEP]`分隔）。一个假设与NSP任务有关：“如果NSP得分低，BERT可能会学习不跨片段进行掩码标记预测，从而隐式检测到无关和嘈杂的上下文。”
- en: '**RAG** (“Retrieval-Augmented Generation”; [Lewis et al., 2020](https://arxiv.org/abs/2005.11401))
    combines pre-trained parametric (language model) and non-parametric memory (external
    knowledge index) together for language generation. RAG can be fine-tuned on any
    seq2seq task, whereby both the retriever and the sequence generator are jointly
    learned. They found that unconstrained generation outperforms previous extractive
    approaches.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '**RAG**（“检索增强生成”；[Lewis等人，2020](https://arxiv.org/abs/2005.11401)）将预训练的参数化（语言模型）和非参数化记忆（外部知识索引）结合在一起进行语言生成。RAG可以在任何seq2seq任务上进行微调，从而同时学习检索器和序列生成器。他们发现，无约束的生成优于先前的抽取式方法。'
- en: 'RAG consists of a retriever model $p_\eta(z \vert x)$ and a generator model
    $p_\theta(y_i \vert x, z, y_{1:i-1})$:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: RAG由一个检索器模型$p_\eta(z \vert x)$和一个生成器模型$p_\theta(y_i \vert x, z, y_{1:i-1})$组成：
- en: The retriever uses the input sequence $x$ to retrieve text passages $z$, implemented
    as a [DPR](#DPR) retriever. $\log p_\eta(z \vert x) \propto E_z(z)^\top E_x(x)$.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检索器使用输入序列$x$检索文本段落$z$，实现为[DPR](#DPR)检索器。$\log p_\eta(z \vert x) \propto E_z(z)^\top
    E_x(x)$。
- en: The generator uses $z$ as additional context when generating the target sequence
    $y$, where the context and the question are simply concatenated.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成器在生成目标序列$y$时使用$z$作为额外上下文，其中上下文和问题简单地连接在一起。
- en: 'Depending on whether using the same or different retrieved documents for each
    token generation, there are two versions of RAG:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 根据每个标记生成时是否使用相同或不同的检索文档，RAG有两个版本：
- en: $$ \begin{aligned} p_\text{RAG-seq}(y \vert x) &= \sum_{z \in \text{TOP}_k(p_\eta(.\vert
    x))} p_\eta(z \vert x) \prod_i^N p_\theta(y_i \vert x, z, y_{1:i-1}) \\ p_\text{RAG-token}(y
    \vert x) &= \prod_i^N \sum_{z \in \text{TOP}_k(p_\eta(.\vert x))} p_\eta(z_i\vert
    x) p_\theta(y_i \vert x, z_i, y_{1:i-1}) \end{aligned} $$
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \begin{aligned} p_\text{RAG-seq}(y \vert x) &= \sum_{z \in \text{TOP}_k(p_\eta(.\vert
    x))} p_\eta(z \vert x) \prod_i^N p_\theta(y_i \vert x, z, y_{1:i-1}) \\ p_\text{RAG-token}(y
    \vert x) &= \prod_i^N \sum_{z \in \text{TOP}_k(p_\eta(.\vert x))} p_\eta(z_i\vert
    x) p_\theta(y_i \vert x, z_i, y_{1:i-1}) \end{aligned} $$
- en: The retriever + generator in RAG is jointly trained to minimize the NLL loss,
    $\mathcal{L}_\text{RAG} = \sum_j -\log p(y_j \vert x_j)$. Updating the passage
    encoder $E_z(.)$ is expensive as it requires the model to re-index the documents
    for fast MIPS. RAG does not find fine-tuning $E_z(.)$ necessary (like in [ORQA](#ORQA))
    and only updates the query encoder + generator.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: RAG中的检索器+生成器是联合训练的，以最小化NLL损失，$\mathcal{L}_\text{RAG} = \sum_j -\log p(y_j \vert
    x_j)$。更新段落编码器$E_z(.)$是昂贵的，因为它要求模型重新索引文档以进行快速MIPS。RAG认为不需要对$E_z(.)$进行微调（就像在[ORQA](#ORQA)中一样），只更新查询编码器+生成器。
- en: '![](../Images/7c8fb0446c4e7648be17acea83e33c8f.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7c8fb0446c4e7648be17acea83e33c8f.png)'
- en: 'Fig. 12\. An illustration of retrieval-augmented generation (RAG) architecture.
    (Image source: [Lewis et al., 2020](https://arxiv.org/abs/2005.11401))'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 图12。检索增强生成（RAG）架构的示意图。（图片来源：[Lewis等人，2020](https://arxiv.org/abs/2005.11401)）
- en: At decoding/test time, RAG-token can be evaluated via a [beam search](https://d2l.ai/chapter_recurrent-modern/beam-search.html#id1).
    RAG-seq cannot be broken down into a set of per-token likelihood, so it runs beam
    search for each candidate document $z$ and picks the one with optimal $p_\theta(y_i
    \vert x, z, y_{1:i-1})$.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在解码/测试时，RAG-token可以通过[束搜索](https://d2l.ai/chapter_recurrent-modern/beam-search.html#id1)进行评估。RAG-seq无法分解为一组每个标记的可能性，因此它为每个候选文档$z$运行束搜索，并选择具有最佳$p_\theta(y_i
    \vert x, z, y_{1:i-1})$的文档。
- en: The *Fusion-in-Decoder* approach, proposed by [Izacard & Grave (2020)](https://arxiv.org/abs/2007.01282)
    is also based on a pre-trained T5\. It works similar to RAG but differently for
    how the context is integrated into the decoder.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '*Fusion-in-Decoder* 方法，由[Izacard & Grave (2020)](https://arxiv.org/abs/2007.01282)提出，也基于预训练的T5。它的工作方式类似于RAG，但在上下文如何整合到解码器中方面有所不同。'
- en: Retrieve top $k$ related passage of 100 words each, using BM25 or DPR.
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检索前$k$个相关段落，每个段落100个词，使用BM25或DPR。
- en: Each retrieved passage and its title are concatenated with the question using
    special tokens like `question:`, `title:` and `context:` to indicate the content
    differences.
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个检索到的段落及其标题都使用特殊标记（如`question:`、`title:`和`context:`）与问题连接，以指示内容差异。
- en: Each retrieved passage is processed independently and later combined in the
    decoder. Processing passages independently in the encoder allows us to parallelize
    the computation. OTOH, processing them jointly encourages better aggregation of
    multiple pieces of evidence. The aggregation part is missing in extractive approaches.
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个检索到的段落都会被独立处理，然后在解码器中合并。在编码器中独立处理段落允许我们并行计算。另一方面，在联合处理它们时鼓励更好地聚合多个证据片段。在抽取式方法中缺少聚合部分。
- en: Note that they did fine-tune the pretrained LM independently for each dataset.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，他们对预训练的语言模型独立进行了微调，针对每个数据集。
- en: 'Closed-book QA: Generative Language Model'
  id: totrans-182
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 闭卷问答：生成式语言模型
- en: Big language models have been pre-trained on a large collection of unsupervised
    textual corpus. Given enough parameters, these models are able to memorize some
    factual knowledge within parameter weights. Therefore, we can use these models
    to do question-answering without explicit context, just like in a closed-book
    exam. The pre-trained language models produce *free text* to respond to questions,
    no explicit reading comprehension.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型已在大量无监督文本语料库上进行了预训练。在有足够参数的情况下，这些模型能够记忆一些事实知识在参数权重中。因此，我们可以使用这些模型进行无需明确上下文的问答，就像在闭卷考试中一样。预训练语言模型生成*自由文本*以回答问题，没有明确的阅读理解。
- en: '![](../Images/d4c3b32d9caf8584475b3c5117ad2c4f.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d4c3b32d9caf8584475b3c5117ad2c4f.png)'
- en: 'Fig. 13\. The amount of computation used for training big language models of
    different sizes is getting big. (Image source: [Brown et al., 2020](https://arxiv.org/abs/2005.14165)).'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 图13。用于训练不同规模的大型语言模型所使用的计算量正在增加。（图片来源：[Brown等人，2020](https://arxiv.org/abs/2005.14165)）。
- en: '[Roberts et al. (2020)](https://arxiv.org/abs/2002.08910) measured the practical
    utility of a language model by fine-tuning a pre-trained model to answer questions
    without access to any external context or knowledge. They fine-tuned the [T5](https://arxiv.org/abs/1910.10683)
    language model (same architecture as the original Transformer) to answer questions
    without inputting any additional information or context. Such setup enforces the
    language model to answer questions based on “knowledge” that it internalized during
    pre-training.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '[Roberts等人（2020）](https://arxiv.org/abs/2002.08910)通过微调预训练模型来衡量语言模型的实用性，以回答问题而无需访问任何外部上下文或知识。他们将[T5](https://arxiv.org/abs/1910.10683)语言模型（与原始Transformer相同的架构）微调为回答问题，而无需输入任何额外信息或上下文。这种设置迫使语言模型根据在预训练期间内化的“知识”来回答问题。'
- en: '![](../Images/8018e92fe405f6335004919ac5f4671d.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8018e92fe405f6335004919ac5f4671d.png)'
- en: 'Fig. 14\. T5 is first pre-trained with salient span masking and then fine-tuned
    for each QA dataset to produce answers in free text. (Image source: [Roberts et
    al. 2020](https://arxiv.org/abs/2002.08910))'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 图14. T5首先通过显著的遮蔽预训练，然后针对每个QA数据集进行微调，以生成自由文本中的答案。（图片来源：[Roberts等人，2020](https://arxiv.org/abs/2002.08910)）
- en: The original T5 models were pre-trained on a multi-task mixture including an
    unsupervised [“masked language modeling”](https://lilianweng.github.io/posts/2019-01-31-lm/#use-bert-in-downstream-tasks)
    (MLM) tasks on the C4 (“Colossal Clean Crawled Corpus”) dataset as well as fine-tuned
    altogether with supervised translation, summarization, classification, and reading
    comprehension tasks. [Roberts, et al. (2020)](https://arxiv.org/abs/2002.08910)
    took a pre-trained T5 model and continued pre-training with [salient span masking](#ssm)
    over Wikipedia corpus, which has been found to substantially boost the performance
    for ODQA. Then they fine-tuned the model for each QA datasets independently.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 原始的T5模型在多任务混合上进行了预训练，包括在C4（“巨大干净爬取语料库”）数据集上的无监督[“遮蔽语言建模”](https://lilianweng.github.io/posts/2019-01-31-lm/#use-bert-in-downstream-tasks)（MLM）任务，以及与监督翻译、摘要、分类和阅读理解任务一起进行了微调。[Roberts等人（2020）](https://arxiv.org/abs/2002.08910)采用了一个预训练的T5模型，并继续使用[显著遮蔽](#ssm)在维基百科语料库上进行预训练，这已被发现可以大幅提升开放领域问答的性能。然后他们为每个QA数据集独立地微调模型。
- en: With a pre-trained T5 language model + continue pre-training with salient spans
    masking + fine-tuning for each QA dataset,
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 使用预训练的T5语言模型+继续使用显著遮蔽进行预训练+针对每个QA数据集进行微调，
- en: It can attain competitive results in open-domain question answering without
    access to external knowledge.
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可以在没有访问外部知识的情况下获得开放领域问答的竞争性结果。
- en: A larger model can obtain better performance. For example, a T5 with 11B parameters
    is able to match the performance with [DPR](#DPR) with 3 BERT-base models, each
    with 330M parameters.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更大的模型可以获得更好的性能。例如，具有11B参数的T5能够与[DPR](#DPR)的3个BERT-base模型（每个模型有330M参数）的性能匹敌。
- en: 'Interestingly, fine-tuning is not strictly necessary. GPT3 ([Brown et al.,
    2020](https://arxiv.org/abs/2005.14165)) has been evaluated on the closed book
    question answering task *without any gradient updates or fine-tuning*. During
    evaluation, the few-shot, one-shot and zero-shot settings here only refer to how
    many demonstrations are provided as context in the text input:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，微调并非绝对必要。GPT3（[Brown等人，2020](https://arxiv.org/abs/2005.14165)）在闭卷问答任务上进行了评估，*没有进行任何梯度更新或微调*。在评估过程中，这里的少次学习、一次学习和零次学习设置仅指提供了多少演示作为文本输入中的上下文：
- en: '“few-shot learning”: GPT3 is allowed to take as many demonstrations as what
    can fit into the model’s context window (typically 10 to 100).'
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: “少次学习”：GPT3可以获取尽可能多的演示，以适应模型的上下文窗口（通常为10到100）。
- en: '“one-shot learning”: only one demonstration is provided.'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: “一次性学习”：只提供一次演示。
- en: '“zero-shot learning”: no demonstrations are allowed and only an instruction
    in natural language is given to the model.'
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: “零次学习”：不允许演示，只向模型提供自然语言的指令。
- en: The performance grows with the model size. On the TriviaQA dataset, GPT3 evaluation
    with demonstrations can match or exceed the performance of SOTA baseline with
    fine-tuning.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 随着模型规模的增长，性能也在提升。在TriviaQA数据集上，GPT3在演示的情况下可以达到或超过通过微调得到的SOTA基线的性能。
- en: '![](../Images/492b7ccdac1e5a920e93db076cd87437.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/492b7ccdac1e5a920e93db076cd87437.png)'
- en: 'Fig. 15\. GPT3''s performance on TriviaQA grows smoothly with the model size.
    More demonstrations lead to better performance. (Image source: [Brown et al.,
    2020](https://arxiv.org/abs/2005.14165)).'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 图15\. GPT3在TriviaQA上的表现随着模型规模的增长而平稳增长。更多的演示会导致更好的表现。（图片来源：[Brown等人，2020](https://arxiv.org/abs/2005.14165)）。
- en: Check out this cool example in OpenAI API [playground viewer](https://beta.openai.com/playground/p/HMoho4552EHXrPLbmOIxpX4X).
    The model is able to answer factal questions in short answer and not to make up
    things when the model does not know the answer. I added the last two questions
    and asked the model to respond with `A:`. The API is still in beta version, so
    you might need to [apply](https://beta.openai.com/) to get on the wait list.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 查看OpenAI API的这个酷例子[playground viewer](https://beta.openai.com/playground/p/HMoho4552EHXrPLbmOIxpX4X)。该模型能够以简短回答形式回答事实性问题，并且在模型不知道答案时不会凭空捏造。我添加了最后两个问题，并要求模型以`A:`回答。该API仍处于测试版，因此您可能需要[申请](https://beta.openai.com/)加入等待列表。
- en: '[PRE1]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Related Techniques
  id: totrans-202
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 相关技术
- en: Fast Maximum Inner Product Search (MIPS)
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 快速最大内积搜索（MIPS）
- en: MIPS (maximum inner product search) is a crucial component in many open-domain
    question answering models. In retriever + reader/generator framework, a large
    number of passages from the knowledge source are encoded and stored in a memory.
    A retrieval model is able to query the memory to identify the top relevant passages
    which have the maximum inner product with the question’s embedding.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: MIPS（最大内积搜索）是许多开放领域问答模型中的关键组件。在检索器 + 读者/生成器框架中，来自知识源的大量段落被编码并存储在内存中。检索模型能够查询内存以识别与问题嵌入具有最大内积的顶级相关段落。
- en: We need fast MIPS because the number of precomputed passage representations
    can be gigantic. There are several ways to achieve fast MIPS at run time, such
    as [asymmetric LSH](https://papers.nips.cc/paper/5329-asymmetric-lsh-alsh-for-sublinear-time-maximum-inner-product-search-mips.pdf),
    [data-dependent hashing](https://arxiv.org/abs/1501.01062), and [FAISS](https://github.com/facebookresearch/faiss).
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要快速的MIPS，因为预先计算的段落表示数量可能是巨大的。在运行时实现快速MIPS有几种方法，例如[非对称LSH](https://papers.nips.cc/paper/5329-asymmetric-lsh-alsh-for-sublinear-time-maximum-inner-product-search-mips.pdf)、[数据相关哈希](https://arxiv.org/abs/1501.01062)和[FAISS](https://github.com/facebookresearch/faiss)。
- en: Language Model Pre-training
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 语言模型预训练
- en: Two pre-training tasks are especially helpful for QA tasks, as we have discussed
    above.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 两个预训练任务对QA任务特别有帮助，正如我们上面所讨论的。
- en: '**Inverse Cloze Task** (proposed by [ORQA](#ORQA)): The goal of [Cloze Task](https://en.wikipedia.org/wiki/Cloze_test)
    is to predict masked-out text based on its context. The prediction of Inverse
    Cloze Task (ICT) is in the reverse direction, aiming to predict the context given
    a sentence. In the context of QA tasks, a random sentence can be treated as a
    pseudo-question, and its context can be treated as pseudo-evidence.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**逆向填空任务**（由[ORQA](#ORQA)提出）：[填空任务](https://en.wikipedia.org/wiki/Cloze_test)的目标是根据上下文预测被屏蔽的文本。逆向填空任务（ICT）的预测方向相反，旨在根据一个句子预测上下文。在QA任务的背景下，一个随机句子可以被视为伪问题，其上下文可以被视为伪证据。'
- en: '**Salient Spans Masking** (proposed by [REALM](#REALM)): Salient span masking
    is a special case for MLM task in language model training. First, we find *salient
    spans* by using a tagger to identify named entities and a regular expression to
    identify dates. Then one of the detected salient spans is selected and masked.
    The task is to predict this masked salient span.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**显著跨度屏蔽**（由[REALM](#REALM)提出）：显著跨度屏蔽是语言模型训练中MLM任务的一个特殊情况。首先，我们通过使用标记器识别命名实体和正则表达式识别日期来找到*显著跨度*。然后选择并屏蔽检测到的一个显著跨度。任务是预测这个被屏蔽的显著跨度。'
- en: Summary
  id: totrans-210
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: '| Model | Retriever | Reader / Generator | Pre-training / Fine-tuning | End2end
    |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 检索器 | 读者 / 生成器 | 预训练 / 微调 | 端到端 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| DrQA | TF-IDF | Bi-directional LSTM | – | No |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| DrQA | TF-IDF | 双向LSTM | – | 否 |'
- en: '| BERTserini | Aserini + BM25 | BERT without softmax layer | Fine-tune with
    SQuAD | No |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| BERTserini | Aserini + BM25 | 没有softmax层的BERT | 与SQuAD微调 | 否 |'
- en: '| Multi-passage BERT | ElasticSearch + BM25 | Multi-passage BERT + Passage
    ranker |  | No |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| 多段BERT | ElasticSearch + BM25 | 多段BERT + 段落排序器 |  | 否 |'
- en: '| R^3 | Classic IR + Match-LSTM | Match-LSTM |  | Yes |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| R^3 | 经典IR + Match-LSTM | Match-LSTM |  | 是 |'
- en: '| ORQA | Dot product of BERT embeddings | BERT-RC | Inverse cloze task | Yes
    |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| ORQA | BERT嵌入的点积 | BERT-RC | 逆向填空任务 | 是 |'
- en: '| REALM | Dot product of BERT embeddings | BERT-RC | Salient span masking |
    Yes |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| REALM | BERT嵌入的点积 | BERT-RC | 显著跨度屏蔽 | 是 |'
- en: '| DPR | Dot product of BERT embeddings | BERT-RC | supervised training with
    QA pairs | Yes |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| DPR | BERT嵌入的点积 | BERT-RC | 通过QA对进行监督训练 | 是 |'
- en: '| DenSPI | Classic + Neural IR | – |  | Yes |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| DenSPI | 经典 + 神经信息检索 | – |  | 是 |'
- en: '| T5 + SSM | – | T5 | SSM on [CommonCrawl](https://commoncrawl.org/the-data/get-started/)
    data + Fine-tuning on QA data | Yes |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| T5 + SSM | – | T5 | SSM在[CommonCrawl](https://commoncrawl.org/the-data/get-started/)数据上
    + 在QA数据上进行微调 | 是 |'
- en: '| GPT3 | – | GPT3 | NSP on [CommonCrawl](https://commoncrawl.org/the-data/get-started/)
    data | Yes |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| GPT3 | – | GPT3 | NSP在[CommonCrawl](https://commoncrawl.org/the-data/get-started/)数据上
    | 是 |'
- en: '| RAG | DPR retriever | [BART](https://arxiv.org/abs/1910.13461) |  | Yes |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| RAG | DPR检索器 | [BART](https://arxiv.org/abs/1910.13461) |  | 是 |'
- en: '| Fusion-in-Decoder | BM25 / DPR retriever | Tranformer |  | No |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| Fusion-in-Decoder | BM25 / DPR检索器 | Tranformer |  | 否 |'
- en: '![](../Images/c272a31e9b28bd10d951b6e49a223503.png)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c272a31e9b28bd10d951b6e49a223503.png)'
- en: 'Fig. 16\. A comparison of performance of several QA models on common QA datasets.
    On TriviaQA, two columns of results are reported, on the open domain test set
    (left) and on the hidden test set (right). (Image source: [Izacard & Grave, 2020](https://arxiv.org/abs/2007.01282)).'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 图16. 几个问答模型在常见问答数据集上的性能比较。在TriviaQA上，报告了两列结果，分别是开放领域测试集（左）和隐藏测试集（右）。 (图片来源：[Izacard
    & Grave, 2020](https://arxiv.org/abs/2007.01282))。
- en: Citation
  id: totrans-227
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引用
- en: 'Cited as:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 被引用为：
- en: Weng, Lilian. (Oct 2020). How to build an open-domain question answering system?
    Lil’Log. https://lilianweng.github.io/posts/2020-10-29-odqa/.
  id: totrans-229
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Weng, Lilian. (2020年10月). 如何构建一个开放领域的问答系统？Lil’Log. https://lilianweng.github.io/posts/2020-10-29-odqa/.
- en: Or
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 或
- en: '[PRE2]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Appendix: QA Datasets'
  id: totrans-232
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 附录：问答数据集
- en: '[SQuAD 2.0](https://rajpurkar.github.io/SQuAD-explorer/): the Stanford QA dataset.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[SQuAD 2.0](https://rajpurkar.github.io/SQuAD-explorer/)：斯坦福问答数据集。'
- en: '[RACE](http://www.qizhexie.com/data/RACE_leaderboard): a reading comprehension
    dataset collected from English Examinations that are created for middle school
    and high school students.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[RACE](http://www.qizhexie.com/data/RACE_leaderboard)：从为中学和高中学生创建的英语考试中收集的阅读理解数据集。'
- en: '[TREC QA](https://trec.nist.gov/data/qa.html): the TREC QA collections.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[TREC问答](https://trec.nist.gov/data/qa.html)：TREC问答集合。'
- en: '[MS MARCO](https://microsoft.github.io/msmarco/): a QA dataset featuring 100,000
    real Bing questions and a human generated answer.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[MS MARCO](https://microsoft.github.io/msmarco/)：一个包含10万个真实必应问题和人工生成答案的问答数据集。'
- en: '[CuratedTREC](https://github.com/brmson/dataset-factoid-curated): based on
    the benchmarks from the TREC QA tasks that have been curated by [Baudis & Sedivy
    (2015)](https://link.springer.com/chapter/10.1007%2F978-3-319-24027-5_20).'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[CuratedTREC](https://github.com/brmson/dataset-factoid-curated)：基于TREC问答任务的基准，由[Baudis
    & Sedivy (2015)](https://link.springer.com/chapter/10.1007%2F978-3-319-24027-5_20)策划。'
- en: '[Google Natural Questions](https://ai.google.com/research/NaturalQuestions/dataset):
    contains real user questions issued to Google search, and answers found from Wikipedia
    by annotators.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Google自然问题](https://ai.google.com/research/NaturalQuestions/dataset)：包含向Google搜索发出的真实用户问题，以及由注释者从维基百科找到的答案。'
- en: '[WebQuestions](https://github.com/brmson/dataset-factoid-webquestions): designed
    for knowledge-base QA with answers restricted to Freebase entities.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[WebQuestions](https://github.com/brmson/dataset-factoid-webquestions)：设计用于基于知识库的问答，答案限制在Freebase实体上。'
- en: '[WikiQA](https://www.microsoft.com/en-us/research/publication/wikiqa-a-challenge-dataset-for-open-domain-question-answering/):
    Bing query logs were used as the source of questions. Each question is then linked
    to a Wikipedia page that potentially contains the answer.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[WikiQA](https://www.microsoft.com/en-us/research/publication/wikiqa-a-challenge-dataset-for-open-domain-question-answering/)：必应查询日志被用作问题的来源。然后将每个问题链接到潜在包含答案的维基百科页面。'
- en: '[WikiMovies](https://research.fb.com/downloads/babi/): contains movie-related
    questions from the OMDb and MovieLens databases and where the questions can be
    answered using Wikipedia pages.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[WikiMovies](https://research.fb.com/downloads/babi/)：包含来自OMDb和MovieLens数据库的与电影相关的问题，这些问题可以使用维基百科页面回答。'
- en: '[WikiReading](https://github.com/google-research-datasets/wiki-reading): to
    predict textual values from the structured knowledge base Wikidata by reading
    the text of the corresponding Wikipedia articles.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[WikiReading](https://github.com/google-research-datasets/wiki-reading)：通过阅读相应维基百科文章的文本来预测结构化知识库Wikidata中的文本值。'
- en: '[TriviaQA](https://nlp.cs.washington.edu/triviaqa/): a reading comprehension
    dataset containing 95K question-answer pairs authored by trivia enthusiasts and
    independently gathered multiple evidence documents per question.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[TriviaQA](https://nlp.cs.washington.edu/triviaqa/)：一个包含95K问题-答案对的阅读理解数据集，由爱好者编写，并独立收集每个问题的多个证据文档。'
- en: '[Jeopardy! Questions](https://www.kaggle.com/tunguz/200000-jeopardy-questions):
    contains 200,000+ [Jeopardy!](https://en.wikipedia.org/wiki/Jeopardy!) questions.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Jeopardy! 问题](https://www.kaggle.com/tunguz/200000-jeopardy-questions)：包含
    200,000+ [Jeopardy!](https://en.wikipedia.org/wiki/Jeopardy!) 问题。'
- en: '[DeepMind Q&A Dataset](https://cs.nyu.edu/~kcho/DMQA/): question/answer pairs
    from CNN and Daily Mail articles.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[DeepMind Q&A 数据集](https://cs.nyu.edu/~kcho/DMQA/)：来自 CNN 和 Daily Mail 文章的问题/答案对。'
- en: '[bAbi](https://research.fb.com/downloads/babi/): a rich collection of datasets
    for text understanding by Facebook.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[bAbi](https://research.fb.com/downloads/babi/)：Facebook 用于文本理解的丰富数据集集合。'
- en: '[FEVER](https://fever.ai/data.html): for fact extraction and verification.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[FEVER](https://fever.ai/data.html)：用于事实提取和验证。'
- en: '[SearchQA](https://github.com/nyu-dl/dl4ir-searchQA): question-answer pairs
    were crawled from from [J! Archive](https://j-archive.com/), and then augmented
    with text snippets from Google.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[SearchQA](https://github.com/nyu-dl/dl4ir-searchQA)：问题-答案对从 [J! Archive](https://j-archive.com/)
    爬取，然后用 Google 的文本片段进行增强。'
- en: '[Quasar-T](https://github.com/bdhingra/quasar): a collection of open-domain
    trivia questions and their answers obtained from various internet sources.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Quasar-T](https://github.com/bdhingra/quasar)：从各种互联网来源获取的开放领域琐事问题及其答案的集合。'
- en: '[Quiz bowl](https://people.cs.umass.edu/~miyyer/qblearn/index.html): contains
    data from a trivia competition called quiz bowl.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Quiz bowl](https://people.cs.umass.edu/~miyyer/qblearn/index.html)：包含来自名为
    quiz bowl 的琐事竞赛的数据。'
- en: '[AmbigNQ](https://nlp.cs.washington.edu/ambigqa/): ambiguous questions selected
    from NQ-OPEN dataset.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[AmbigNQ](https://nlp.cs.washington.edu/ambigqa/)：从 NQ-OPEN 数据集中选出的模糊问题。'
- en: '[QA-Overlap](https://github.com/facebookresearch/QA-Overlap): a collections
    of overlapped answers/questions between train and test set for Natural Questions,
    TriviaQA, and WebQuestions.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[QA-Overlap](https://github.com/facebookresearch/QA-Overlap)：自然问题、TriviaQA和WebQuestions训练集和测试集之间重叠答案/问题的集合。'
- en: References
  id: totrans-253
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Danqi Chen & Scott Yih. [“ACL2020 Tutorial: Open-Domain Question Answering”](https://github.com/danqi/acl2020-openqa-tutorial)
    July 2020.'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] 陈丹琦和斯科特·伊。[“ACL2020 教程：开放领域问答”](https://github.com/danqi/acl2020-openqa-tutorial)
    2020年7月。'
- en: '[2] Danqi Chen, et al. [“Reading Wikipedia to Answer Open-Domain Questions”](https://arxiv.org/abs/1704.00051)
    ACL 2017\. | [code](https://github.com/facebookresearch/DrQA)'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] 陈丹琦等人。[“阅读维基百科以回答开放领域问题”](https://arxiv.org/abs/1704.00051) ACL 2017。|
    [code](https://github.com/facebookresearch/DrQA)'
- en: '[3] Shuohang Wang, et al. [“R^3: Reinforced Ranker-Reader for Open-Domain Question
    Answering”](https://arxiv.org/abs/1709.00023) AAAI 2018.'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] 王硕航等人。[“R^3：强化排序-阅读器用于开放领域问答”](https://arxiv.org/abs/1709.00023) AAAI 2018。'
- en: '[4] Jimmy Lin. [“The neural hype and comparisons against weak baselines.”](https://sigir.org/wp-content/uploads/2019/01/p040.pdf)
    ACM SIGIR Forum. Vol. 52\. No. 2\. 2019.'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] 吉米·林。[“神经炒作和与弱基线的比较。”](https://sigir.org/wp-content/uploads/2019/01/p040.pdf)
    ACM SIGIR 论坛。Vol. 52。No. 2。2019年。'
- en: '[5] Wei Yang, et al. [“End-to-End Open-Domain Question Answering with BERTserini”](https://arxiv.org/abs/1902.01718)
    NAACL 2019.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] 杨伟等人。[“使用 BERTserini 进行端到端开放领域问答”](https://arxiv.org/abs/1902.01718) NAACL
    2019。'
- en: '[6] Christopher Clark & Matt Gardner. [“Simple and Effective Multi-Paragraph
    Reading Comprehension.”](https://arxiv.org/abs/1710.10723) arXiv:1710.10723 (2017).'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] 克里斯托弗·克拉克和马特·加德纳。[“简单而有效的多段阅读理解。”](https://arxiv.org/abs/1710.10723) arXiv:1710.10723
    (2017)。'
- en: '[7] Rodrigo Nogueira & Kyunghyun Cho. [“Passage Re-ranking with BERT.”](https://arxiv.org/abs/1901.04085)
    arXiv preprint arXiv:1901.04085 (2019). | [code](https://github.com/nyu-dl/dl4marco-bert)'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] Rodrigo Nogueira 和 Kyunghyun Cho。[“使用 BERT 进行段落重新排序。”](https://arxiv.org/abs/1901.04085)
    arXiv 预印本 arXiv:1901.04085 (2019)。| [code](https://github.com/nyu-dl/dl4marco-bert)'
- en: '[8] Zhiguo Wang, et al. [“Multi-passage BERT: A globally normalized BERT model
    for open-domain question answering.”](https://arxiv.org/abs/1908.08167) EMNLP
    2019.'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] 王志国等人。[“多段 BERT：用于开放领域问答的全局归一化 BERT 模型。”](https://arxiv.org/abs/1908.08167)
    EMNLP 2019。'
- en: '[9] Minjoon Seo et al. [“Real-time open-domain question answering with dense-sparse
    phrase index.”](https://arxiv.org/abs/1906.05807) ACL 2019.'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] 徐敏俊等人。[“具有稠密-稀疏短语索引的实时开放领域问答。”](https://arxiv.org/abs/1906.05807) ACL 2019。'
- en: '[10] Kenton Lee, et al. [“Latent Retrieval for Weakly Supervised Open Domain
    Question Answering”](https://arxiv.org/abs/1906.00300) ACL 2019.'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] Kenton Lee 等人。[“弱监督开放领域问答的潜在检索”](https://arxiv.org/abs/1906.00300) ACL
    2019。'
- en: '[11] Kelvin Guu, et al. [“REALM: Retrieval-Augmented Language Model Pre-Training”](https://arxiv.org/abs/2002.08909)
    arXiv:2002.08909 (2020).'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] Kelvin Guu 等人。[“REALM：检索增强语言模型预训练”](https://arxiv.org/abs/2002.08909)
    arXiv:2002.08909 (2020)。'
- en: '[12] Vladimir Karpukhin et al. [“Dense passage retrieval for open-domain question
    answering.”](https://arxiv.org/abs/2004.04906). EMNLP 2020\. | [code](https://github.com/facebookresearch/DPR)'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '[12] Vladimir Karpukhin等人。[“用于开放领域问答的密集段落检索。”](https://arxiv.org/abs/2004.04906)
    EMNLP 2020。 | [代码](https://github.com/facebookresearch/DPR)'
- en: '[13] Patrick Lewis et al. [“Retrieval-Augmented Generation for Knowledge-Intensive
    NLP Tasks”](https://arxiv.org/abs/2005.11401) arXiv:2005.11401 (2020).'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '[13] Patrick Lewis等人。[“用于知识密集型自然语言处理任务的检索增强生成”](https://arxiv.org/abs/2005.11401)
    arXiv:2005.11401 (2020)。'
- en: '[14] Adam Roberts, et al. [“How Much Knowledge Can You Pack Into the Parameters
    of a Language Model?”](https://arxiv.org/abs/2002.08910) EMNLP 2020.'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '[14] Adam Roberts等人。[“语言模型参数中可以包含多少知识？”](https://arxiv.org/abs/2002.08910)
    EMNLP 2020。'
- en: '[15] Tom Brown, et al. [“Language models are few-shot learners.”](https://arxiv.org/abs/2005.14165)
    arXiv:2005.14165 (2020).'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '[15] Tom Brown等人。[“语言模型是少样本学习者。”](https://arxiv.org/abs/2005.14165) arXiv:2005.14165
    (2020)。'
- en: '[16] Fabio Petroni, et al. [“How Context Affects Language Models’ Factual Predictions”](https://arxiv.org/abs/2005.04611)
    AKBC 2020.'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '[16] Fabio Petroni等人。[“上下文如何影响语言模型的事实预测”](https://arxiv.org/abs/2005.04611)
    AKBC 2020。'
- en: '[17] Gautier Izacard & Edouard Grave. [“Leveraging passage retrieval with generative
    models for open domain question answering.”](https://arxiv.org/abs/2007.01282)
    arXiv:2007.01282 (2020).'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '[17] Gautier Izacard和Edouard Grave。[“利用生成模型的段落检索进行开放领域问答。”](https://arxiv.org/abs/2007.01282)
    arXiv:2007.01282 (2020)。'
- en: '[18] [“Dive into deep learning: Beam search”](https://d2l.ai/chapter_recurrent-modern/beam-search.html)'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '[18] [“深入深度学习：波束搜索”](https://d2l.ai/chapter_recurrent-modern/beam-search.html)'
- en: '[19] Patrick Lewis, et al. [“Question and Answer Test-Train Overlap in Open-Domain
    Question Answering Datasets”](https://arxiv.org/abs/2008.02637) arXiv:2008.02637
    (2020). | [data](https://github.com/facebookresearch/QA-Overlap)'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '[19] Patrick Lewis等人。[“开放领域问答数据集中的问题和答案测试训练重叠”](https://arxiv.org/abs/2008.02637)
    arXiv:2008.02637 (2020)。 | [数据](https://github.com/facebookresearch/QA-Overlap)'
- en: '[20] Hervé Jegou, et al. [“Faiss: A library for efficient similarity search”](https://engineering.fb.com/2017/03/29/data-infrastructure/faiss-a-library-for-efficient-similarity-search/)
    Mar 2017.'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '[20] Hervé Jegou等人。[“Faiss：高效相似性搜索的库”](https://engineering.fb.com/2017/03/29/data-infrastructure/faiss-a-library-for-efficient-similarity-search/)
    2017年3月。'
- en: '[21] Vidhisha Balachandran, et al. [“Simple and Efficient ways to Improve REALM.”](https://arxiv.org/abs/2104.08710)
    arXiv:2104.08710 (2021).'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '[21] Vidhisha Balachandran等人。[“改进REALM的简单有效方法。”](https://arxiv.org/abs/2104.08710)
    arXiv:2104.08710 (2021)。'
