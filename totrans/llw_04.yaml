- en: Prompt Engineering
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提示工程
- en: 原文：[https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/](https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/](https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/)
- en: '**Prompt Engineering**, also known as **In-Context Prompting**, refers to methods
    for how to communicate with LLM to steer its behavior for desired outcomes *without*
    updating the model weights. It is an empirical science and the effect of prompt
    engineering methods can vary a lot among models, thus requiring heavy experimentation
    and heuristics.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**提示工程**，也称为**上下文提示**，指的是如何与LLM进行通信以引导其行为以实现期望的结果，*而不更新模型权重*。这是一门经验科学，提示工程方法的效果在模型之间可能有很大差异，因此需要大量的实验和启发式方法。'
- en: This post only focuses on prompt engineering for autoregressive language models,
    so nothing with Cloze tests, image generation or multimodality models. At its
    core, the goal of prompt engineering is about alignment and model steerability.
    Check my [previous post](https://lilianweng.github.io/posts/2021-01-02-controllable-text-generation/)
    on controllable text generation.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本文仅关注自回归语言模型的提示工程，因此与填空测试、图像生成或多模态模型无关。在本质上，提示工程的目标是关于对齐和模型可操纵性。查看我的[先前文章](https://lilianweng.github.io/posts/2021-01-02-controllable-text-generation/)关于可控文本生成。
- en: '[My personal spicy take] In my opinion, some prompt engineering papers are
    not worthy 8 pages long, since those tricks can be explained in one or a few sentences
    and the rest is all about benchmarking. An easy-to-use and shared benchmark infrastructure
    should be more beneficial to the community. Iterative prompting or external tool
    use would not be trivial to set up. Also non-trivial to align the whole research
    community to adopt it.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[我个人的独特看法] 在我看来，一些提示工程论文并不值得写成8页长，因为这些技巧可以用一两句话解释清楚，其余都是关于基准测试。一个易于使用和共享的基准测试基础设施对社区更有益。迭代提示或外部工具使用并不容易设置。将整个研究社区对齐以采用它也不容易。'
- en: Basic Prompting
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基本提示
- en: Zero-shot and few-shot learning are two most basic approaches for prompting
    the model, pioneered by many LLM papers and commonly used for benchmarking LLM
    performance.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 零样本和少样本学习是提示模型的两种最基本方法，由许多LLM论文开创，并常用于LLM性能基准测试。
- en: Zero-Shot
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 零样本
- en: '**Zero-shot learning** is to simply feed the task text to the model and ask
    for results.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '**零样本学习**就是简单地将任务文本提供给模型并要求结果。'
- en: (All the sentiment analysis examples are from SST-2)
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: （所有情感分析示例均来自SST-2）
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Few-shot
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 少样本
- en: '**Few-shot learning** presents a set of high-quality demonstrations, each consisting
    of both input and desired output, on the target task. As the model first sees
    good examples, it can better understand human intention and criteria for what
    kinds of answers are wanted. Therefore, few-shot learning often leads to better
    performance than zero-shot. However, it comes at the cost of more token consumption
    and may hit the context length limit when input and output text are long.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**少样本学习**提供了一组高质量的演示，每个演示都包括输入和期望的输出，在目标任务上。由于模型首先看到了好的示例，它可以更好地理解人类意图和对所需答案种类的标准。因此，少样本学习通常比零样本表现更好。然而，这是以更多的标记消耗为代价的，并且当输入和输出文本很长时可能会达到上下文长度限制。'
- en: '[PRE1]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Many studies looked into how to construct in-context examples to maximize the
    performance and observed that **choice of prompt format, training examples, and
    the order of the examples can lead to dramatically different performance**, from
    near random guess to near SoTA.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 许多研究探讨了如何构建上下文示例以最大化性能，并观察到**提示格式的选择、训练示例以及示例的顺序可能导致截然不同的性能**，从几乎随机猜测到接近最先进技术。
- en: '[Zhao et al. (2021)](https://arxiv.org/abs/2102.09690) investigated the case
    of few-shot classification and proposed that several biases with LLM (they use
    GPT-3 in the experiments) contribute to such high variance: (1) *Majority label
    bias* exists if distribution of labels among the examples is unbalanced; (2) *Recency
    bias* refers to the tendency where the model may repeat the label at the end;
    (3) *Common token bias* indicates that LLM tends to produce common tokens more
    often than rare tokens. To conquer such bias, they proposed a method to calibrate
    the label probabilities output by the model to be uniform when the input string
    is `N/A`.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '[赵等人（2021）](https://arxiv.org/abs/2102.09690)调查了少样本分类的情况，并提出LLM存在几种偏差（他们在实验中使用GPT-3），导致高方差：(1)
    *多数标签偏差* 存在于如果示例中的标签分布不平衡; (2) *最近性偏差* 指的是模型可能在最后重复标签; (3) *常见标记偏差* 表明LLM倾向于更频繁地生成常见标记而不是罕见标记。为了克服这种偏差，他们提出了一种方法，即在输入字符串为`N/A`时，校准模型输出的标签概率为均匀分布。'
- en: Tips for Example Selection
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 示例选择提示
- en: Choose examples that are semantically similar to the test example using $k$-NN
    clustering in the embedding space ([Liu et al., 2021](https://arxiv.org/abs/2101.06804))
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用嵌入空间中的$k$-NN聚类选择与测试示例在语义上相似的示例（[刘等人，2021](https://arxiv.org/abs/2101.06804)）
- en: 'To select a diverse and representative set of examples, [Su et al. (2022)](https://arxiv.org/abs/2209.01975)
    proposed to use a graph-based approach: (1) First, construct a directed graph
    $G=(V, E)$ based on the embedding (e.g. by [SBERT](https://arxiv.org/abs/1908.10084)
    or [other](https://arxiv.org/abs/2201.10005) [embedding](https://platform.openai.com/docs/guides/embeddings)
    [models](https://openai.com/blog/new-and-improved-embedding-model)) cosine similarity
    between samples, where each node points to its $k$ nearest neighbors; (2) Start
    with a set of selected samples $\mathcal{L}=\emptyset$ and a set of remaining
    samples $\mathcal{U}$. Each sample $u \in \mathcal{U}$ is scored by $$ \text{score}(u)
    = \sum_{v \in \{v \mid (u, v) \in E, v\in \mathcal{U}\}} s(v)\quad\text{where
    }s(v)=\rho^{- \vert \{\ell \in \mathcal{L} \vert (v, \ell)\in E \}\vert},\quad\rho
    > 1 $$ such that $s(v)$ is low if many of $v$’s neighbors are selected and thus
    the scoring encourages to pick diverse samples.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了选择多样且具代表性的示例集，[苏等人（2022）](https://arxiv.org/abs/2209.01975)提出使用基于图的方法：(1)
    首先，基于样本之间的嵌入（例如通过[SBERT](https://arxiv.org/abs/1908.10084)或[其他](https://arxiv.org/abs/2201.10005)[嵌入](https://platform.openai.com/docs/guides/embeddings)[模型](https://openai.com/blog/new-and-improved-embedding-model))余弦相似度构建一个有向图$G=(V,
    E)$，其中每个节点指向其$k$个最近邻居; (2) 从一组已选择的样本$\mathcal{L}=\emptyset$和一组剩余样本$\mathcal{U}$开始。每个样本$u
    \in \mathcal{U}$的得分为$$ \text{score}(u) = \sum_{v \in \{v \mid (u, v) \in E, v\in
    \mathcal{U}\}} s(v)\quad\text{其中}s(v)=\rho^{- \vert \{\ell \in \mathcal{L} \vert
    (v, \ell)\in E \}\vert},\quad\rho > 1 $$，这样如果$v$的邻居中有很多被选择，则$s(v)$较低，因此评分鼓励选择多样化的样本。
- en: '[Rubin et al. (2022)](https://arxiv.org/abs/2112.08633) proposed to train embeddings
    via [contrastive learning](https://lilianweng.github.io/posts/2021-05-31-contrastive/)
    specific to one training dataset for in-context learning sample selection. Given
    each training pair $(x, y)$, the quality of one example $e_i$ (formatted input-output
    pair) can be measured by a conditioned probability assigned by LM: $\text{score}(e_i)
    = P_\text{LM}(y \mid e_i, x)$. We can identify other examples with top-$k$ and
    bottom-$k$ scores as positive and negative sets of candidates for every training
    pair and use that for contrastive learning.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[鲁宾等人（2022）](https://arxiv.org/abs/2112.08633)提出通过对比学习专门针对一个训练数据集进行嵌入训练，以进行上下文学习样本选择。对于每个训练对$(x,
    y)$，一个示例$e_i$（格式化的输入-输出对）的质量可以通过LM分配的条件概率来衡量：$\text{score}(e_i) = P_\text{LM}(y
    \mid e_i, x)$。我们可以将具有前$k$和后$k$分数的其他示例识别为每个训练对的正面和负面候选集，并将其用于对比学习。'
- en: Some researchers tried [Q-Learning](https://lilianweng.github.io/posts/2018-02-19-rl-overview/#q-learning-off-policy-td-control)
    to do sample selection. ([Zhang et al. 2022](https://arxiv.org/abs/2211.04486))
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些研究人员尝试使用[Q学习](https://lilianweng.github.io/posts/2018-02-19-rl-overview/#q-learning-off-policy-td-control)进行样本选择。([张等人，2022](https://arxiv.org/abs/2211.04486))
- en: Motivated by uncertainty-based [active learning](https://lilianweng.github.io/posts/2022-02-20-active-learning/),
    [Diao et al. (2023)](https://arxiv.org/abs/2302.12246) suggested to identify examples
    with high disagreement or entropy among multiple sampling trials. Then annotate
    these examples to be used in few-shot prompts.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 受不确定性驱动的[主动学习](https://lilianweng.github.io/posts/2022-02-20-active-learning/)的启发，[Diao等人（2023）](https://arxiv.org/abs/2302.12246)建议识别在多次抽样试验中存在高争议或熵的示例。然后注释这些示例以在少样本提示中使用。
- en: Tips for Example Ordering
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 示例排序的提示
- en: A general suggestion is to keep the selection of examples diverse, relevant
    to the test sample and in random order to avoid majority label bias and recency
    bias.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个一般建议是保持示例选择多样化，与测试样本相关，并以随机顺序排列，以避免多数标签偏见和最新偏见。
- en: Increasing model sizes or including more training examples does not reduce variance
    among different permutations of in-context examples. Same order may work well
    for one model but badly for another. When the validation set is limited, consider
    choosing the order such that the model does not produce extremely unbalanced predictions
    or being overconfident about its predictions. ([Lu et al. 2022](https://arxiv.org/abs/2104.08786))
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增加模型大小或包含更多训练示例并不能减少上下文示例中不同排列之间的方差。同样的顺序可能对一个模型效果很好，但对另一个模型效果很差。当验证集有限时，考虑选择顺序，使得模型不会产生极端不平衡的预测或对其预测过于自信。([Lu等人，2022](https://arxiv.org/abs/2104.08786))
- en: Instruction Prompting
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 指导提示
- en: The purpose of presenting few-shot examples in the prompt is to explain our
    intent to the model; in other words, describe the task instruction to the model
    in the form of demonstrations. However, few-shot can be expensive in terms of
    token usage and restricts the input length due to limited context length. So,
    why not just give the instruction directly?
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在提示中呈现少样本示例的目的是向模型解释我们的意图；换句话说，以示范的形式向模型描述任务指导。然而，少样本可能在标记使用方面昂贵，并且由于有限的上下文长度限制了输入长度。那么，为什么不直接给出指导呢？
- en: '*Instructed LM* (e.g. [InstructGPT](https://openai.com/research/instruction-following),
    [natural instruction](https://github.com/allenai/natural-instructions)) finetunes
    a pretrained model with high-quality tuples of (task instruction, input, ground
    truth output) to make LM better understand user intention and follow instruction.
    [RLHF](https://lilianweng.github.io/posts/2021-01-02-controllable-text-generation/#rl-fine-tuning-with-human-preferences)
    (Reinforcement Learning from Human Feedback) is a common method to do so. The
    benefit of instruction following style fine-tuning improves the model to be more
    aligned with human intention and greatly reduces the cost of communication.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '*指导式语言模型*（例如[InstructGPT](https://openai.com/research/instruction-following)，[自然指导](https://github.com/allenai/natural-instructions)）通过高质量的（任务指导，输入，期望输出）元组对预训练模型进行微调，以使语言模型更好地理解用户意图并遵循指导。[RLHF](https://lilianweng.github.io/posts/2021-01-02-controllable-text-generation/#rl-fine-tuning-with-human-preferences)（从人类反馈中进行强化学习）是一种常见的方法。指导式微调的好处在于改进模型与人类意图更加一致，并大大降低了沟通成本。'
- en: When interacting with instruction models, we should describe the task requirement
    in details, trying to be *specific* and *precise* and avoiding say “not do something”
    but rather specify what to do.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 与指导模型交互时，我们应该详细描述任务要求，尽量*具体*和*精确*，避免说“不要做某事”，而是明确指定要做什么。
- en: '[PRE2]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Explaining the desired audience is another smart way to give instructions
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 解释所需的受众是另一种明智的给出指导的方式
- en: For example to produce education materials for kids,
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 例如，为儿童制作教育材料，
- en: '[PRE3]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: And safe content,
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以及安全内容，
- en: '[PRE4]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '*In-context instruction learning* ([Ye et al. 2023](https://arxiv.org/abs/2302.14691))
    combines few-shot learning with instruction prompting. It incorporates multiple
    demonstration examples across different tasks in the prompt, each demonstration
    consisting of instruction, task input and output. Note that their experiments
    were only on classification tasks and the instruction prompt contains all label
    options.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '*上下文指导学习*（[Ye等人，2023](https://arxiv.org/abs/2302.14691)）将少样本学习与指导提示相结合。它在提示中整合了不同任务中的多个示例，每个示例包括指导、任务输入和输出。请注意，他们的实验仅针对分类任务，指导提示包含所有标签选项。'
- en: '[PRE5]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Self-Consistency Sampling
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自一致性抽样
- en: '**Self-consistency sampling** ([Wang et al. 2022a](https://arxiv.org/abs/2203.11171))
    is to sample multiple outputs with temperature > 0 and then selecting the best
    one out of these candidates. The criteria for selecting the best candidate can
    vary from task to task. A general solution is to pick **majority vote**. For tasks
    that are easy to validate such as a programming question with unit tests, we can
    simply run through the interpreter and verify the correctness with unit tests.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**自一致性抽样**（[Wang等人，2022a](https://arxiv.org/abs/2203.11171)）是通过温度大于0的多个输出进行抽样，然后从这些候选项中选择最佳答案。选择最佳候选项的标准可能因任务而异。一个通用的解决方案是选择**多数投票**。对于易于验证的任务，比如带有单元测试的编程问题，我们可以简单地通过解释器运行并使用单元测试验证正确性。'
- en: Chain-of-Thought (CoT)
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 思维链（CoT）
- en: '**Chain-of-thought (CoT) prompting** ([Wei et al. 2022](https://arxiv.org/abs/2201.11903))
    generates a sequence of short sentences to describe reasoning logics step by step,
    known as *reasoning chains* or *rationales*, to eventually lead to the final answer.
    The benefit of CoT is more pronounced for **complicated reasoning tasks**, while
    using **large models** (e.g. with more than 50B parameters). Simple tasks only
    benefit slightly from CoT prompting.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**思维链（CoT）提示**（[魏等人，2022](https://arxiv.org/abs/2201.11903)）生成一系列简短的句子来逐步描述推理逻辑，称为*推理链*或*理由*，最终导致最终答案。
    CoT对于**复杂的推理任务**效果更为显著，而使用**大型模型**（例如具有超过50B参数的模型）。简单任务只能从CoT提示中稍微受益。'
- en: Types of CoT prompts
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CoT提示的类型
- en: 'Two main types of CoT prompting:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: CoT提示的两种主要类型：
- en: '**Few-shot CoT**. It is to prompt the model with a few demonstrations, each
    containing manually written (or model-generated) high-quality reasoning chains.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**少样本CoT**。这是通过几个示例提示模型，每个示例包含手动编写（或模型生成的）高质量推理链。'
- en: (All the math reasoning examples are from [GSM8k](https://github.com/openai/grade-school-math))
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: （所有数学推理示例均来自[GSM8k](https://github.com/openai/grade-school-math)）
- en: '[PRE6]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '**Zero-shot CoT**. Use natural language statement like `Let''s think step by
    step` to explicitly encourage the model to first generate reasoning chains and
    then to prompt with `Therefore, the answer is` to produce answers ([Kojima et
    al. 2022](https://arxiv.org/abs/2205.11916) ). Or a similar statement `Let''s
    work this out it a step by step to be sure we have the right answer` ([Zhou et
    al. 2022](https://arxiv.org/abs/2211.01910)).'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**零样本CoT**。使用自然语言语句如`让我们逐步思考`明确鼓励模型首先生成推理链，然后提示`因此，答案是`来生成答案（[小岛等人，2022](https://arxiv.org/abs/2205.11916)）。或类似语句`让我们逐步解决这个问题，以确保我们有正确的答案`（[周等人，2022](https://arxiv.org/abs/2211.01910)）。'
- en: '[PRE7]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Tips and Extensions
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提示和扩展
- en: '[Self-consistency sampling](#self-consistency-sampling) can improve reasoning
    accuracy by sampling a number of diverse answers and then taking the majority
    vote. ([Wang et al. 2022a](https://arxiv.org/abs/2203.11171))'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[自一致性抽样](#self-consistency-sampling)可以通过抽样多个不同的答案然后进行多数投票来提高推理准确性。([Wang等人，2022a](https://arxiv.org/abs/2203.11171))'
- en: Another approach for ensemble learning is to alter the example order or use
    model generated rationales to replace human-written ones to introduce randomness
    during multiple sample trials. Then aggregate model outputs with a majority vote
    to get final answer. ([Wang et al. 2022b](https://arxiv.org/abs/2207.00747))
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一种集成学习的方法是改变示例顺序或使用模型生成的理由替换人工编写的理由，在多次样本试验中引入随机性。然后用多数投票聚合模型输出以获得最终答案。([Wang等人，2022b](https://arxiv.org/abs/2207.00747))
- en: 'If training examples are only associated with true answers (easy to verify!)
    but no rationales, we can follow the *STaR* (Self-Taught Reasoner; [Zelikman et
    al. 2022](https://arxiv.org/abs/2203.14465)) method : (1) Ask LLM to generate
    reasoning chains and only keep those leading to correct answers; (2) Then fine-tune
    the model with generated rationales and repeat the process until convergence.
    Note that higher temperature is more likely to generate incorrect rationales with
    correct answers. If training examples do not have ground truth answers, maybe
    consider using majority votes as the “correct” answers.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果训练示例仅与真实答案相关（易于验证！）但没有理由，我们可以遵循*STaR*（自学习推理者；[Zelikman等人，2022](https://arxiv.org/abs/2203.14465)）方法：（1）要求LLM生成推理链，仅保留导致正确答案的链条；（2）然后用生成的理由微调模型，并重复该过程直至收敛。请注意，较高的温度更有可能生成带有正确答案的不正确理由。如果训练示例没有地面真实答案，也许可以考虑使用多数投票作为“正确”答案。
- en: Prompts with demonstrations of higher reasoning complexity can achieve better
    performance, where complexity is measured by the number of reasoning steps in
    the chains. When separating reasoning steps, newline `\n` symbol works better
    than `step i`, period `.` or semicolon `;`. ([Fu et al. 2023](https://arxiv.org/abs/2210.00720))
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具有更高推理复杂性演示的提示可以实现更好的性能，其中复杂性由链中推理步骤的数量来衡量。在分离推理步骤时，换行符`\n`比`步骤i`、句号`.`或分号`;`更有效。([Fu等人，2023](https://arxiv.org/abs/2210.00720))
- en: '*Complexity-based consistency* is to explicitly prefer complex chains among
    all the generations by taking majority vote among only top $k$ complex chains.
    ([Fu et al. 2023](https://arxiv.org/abs/2210.00720))'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*基于复杂性的一致性*是明确偏好于所有生成中的复杂链，通过在仅前$k$个复杂链中进行多数投票来实现。([Fu等人，2023](https://arxiv.org/abs/2210.00720))'
- en: Later, [Shum et al. (2023)](https://arxiv.org/abs/2302.12822) found that in
    their experiments CoT prompts with only complex examples can improve the accuracy
    of complex questions, but perform poorly in simple questions; evidence shown on
    GSM8k.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 后来，[Shum等人（2023）](https://arxiv.org/abs/2302.12822) 在实验中发现，只有复杂示例的CoT提示可以提高复杂问题的准确性，但在简单问题上表现不佳；GSM8k上的证据显示。
- en: Changing `Q:` to `Question:` is found to be helpful. ([Fu et al. 2023](https://arxiv.org/abs/2210.00720))
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将`Q：`更改为`问题：`被发现是有帮助的。([Fu等人，2023](https://arxiv.org/abs/2210.00720))
- en: '[Ye & Durrett (2022)](https://arxiv.org/abs/2205.03401) found that the benefit
    of including explanations in the prompt is small to moderate for NLP tasks that
    involve reasoning over text (i.e. QA and NLI) and the effects vary by models.
    They observed that explanations are more likely to be nonfactual than be inconsistent
    (i.e. whether explanation entails prediction). Nonfactual explanations most likely
    lead to incorrect predictions.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Ye & Durrett（2022）](https://arxiv.org/abs/2205.03401) 发现，在提示中包含解释对涉及对文本进行推理的NLP任务（即QA和NLI）的效益较小至中等，并且效果因模型而异。他们观察到解释更有可能是非事实性的，而不是不一致的（即解释是否包含预测）。非事实性解释最有可能导致不正确的预测。'
- en: '*Self-Ask* ([Press et al. 2022](https://arxiv.org/abs/2210.03350)) is a method
    to repeatedly prompt the model to *ask following-up questions* to construct the
    thought process iteratively. Follow-up questions can be answered by search engine
    results. Similarly, *IRCoT* (Interleaving Retrieval CoT; [Trivedi et al. 2022](https://arxiv.org/abs/2212.10509))
    and *ReAct* (Reason + Act; [Yao et al. 2023](https://arxiv.org/abs/2210.03629))
    combines iterative CoT prompting with queries to Wikipedia APIs to search for
    relevant entities and content and then add it back into the context.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*自问*（[Press等人，2022](https://arxiv.org/abs/2210.03350)）是一种反复提示模型*提出后续问题*以迭代构建思维过程的方法。后续问题可以通过搜索引擎结果回答。类似地，*IRCoT*（交错检索CoT；[Trivedi等人，2022](https://arxiv.org/abs/2212.10509)）和*ReAct*（Reason
    + Act；[Yao等人，2023](https://arxiv.org/abs/2210.03629)）将迭代CoT提示与查询维基百科API以搜索相关实体和内容并将其添加回上下文。'
- en: '![](../Images/974cc31bf97f3b260604d94ac6f5ab8d.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/974cc31bf97f3b260604d94ac6f5ab8d.png)'
- en: Fig. 1\. How Self-Ask works with external search queries.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图1。自问如何与外部搜索查询配合工作。
- en: '(Image source: [Press et al. 2022](https://arxiv.org/abs/2210.03350)).'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: （图片来源：[Press等人，2022](https://arxiv.org/abs/2210.03350)）。
- en: '*Tree of Thoughts* ([Yao et al. 2023](https://arxiv.org/abs/2305.10601)) extends
    CoT by exploring multiple reasoning possibilities at each step. It first decomposes
    the problem into multiple thought steps and generates multiple thoughts per step,
    essentially creating a tree structure. The search process can be BFS or DFS while
    each state is evaluated by a classifier (via a prompt) or majority vote.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*思维树*（[Yao等人，2023](https://arxiv.org/abs/2305.10601)）通过在每一步探索多个推理可能性来扩展CoT。它首先将问题分解为多个思考步骤，并在每一步生成多个思考，从根本上创建了一棵树结构。搜索过程可以是BFS或DFS，而每个状态都由分类器（通过提示）或多数投票评估。'
- en: '![](../Images/249ed76ba3eac0e01a55ae914515db77.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/249ed76ba3eac0e01a55ae914515db77.png)'
- en: Fig. 2\. How Self-Ask works with external search queries.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图2。自问如何与外部搜索查询配合工作。
- en: '(Image source: [Yao et al. 2022](https://arxiv.org/abs/2305.10601)).'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: （图片来源：[Yao等人，2022](https://arxiv.org/abs/2305.10601)）。
- en: Automatic Prompt Design
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自动提示设计
- en: Prompt is a sequence of prefix tokens that increase the probability of getting
    desired output given input. Therefore we can treat them as trainable parameters
    and [optimize them directly](https://lilianweng.github.io/posts/2021-01-02-controllable-text-generation/#smart-prompt-design)
    on the embedding space via gradient descent, such as **AutoPrompt** ([Shin et
    al., 2020](https://arxiv.org/abs/2010.15980), **Prefix-Tuning** ([Li & Liang (2021)](https://arxiv.org/abs/2101.00190)),
    **P-tuning** ([Liu et al. 2021](https://arxiv.org/abs/2103.10385)) and **Prompt-Tuning**
    ([Lester et al. 2021](https://arxiv.org/abs/2104.08691)). [This section in my
    “Controllable Neural Text Generation” post](https://lilianweng.github.io/posts/2021-01-02-controllable-text-generation/#smart-prompt-design)
    has a good coverage of them. The trend from AutoPrompt to Prompt-Tuning is that
    the setup gets gradually simplified.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 提示是一系列前缀标记，增加了在给定输入时获得所需输出的概率。因此，我们可以将它们视为可训练参数，并通过梯度下降直接在嵌入空间上进行优化，例如**AutoPrompt**（[Shin等人，2020](https://arxiv.org/abs/2010.15980)）、**Prefix-Tuning**（[Li
    & Liang，2021](https://arxiv.org/abs/2101.00190)）、**P-tuning**（[Liu等人，2021](https://arxiv.org/abs/2103.10385)）和**Prompt-Tuning**（[Lester等人，2021](https://arxiv.org/abs/2104.08691)）。[我“可控神经文本生成”文章中的这一部分](https://lilianweng.github.io/posts/2021-01-02-controllable-text-generation/#smart-prompt-design)对它们有很好的覆盖。从AutoPrompt到Prompt-Tuning的趋势是设置逐渐简化。
- en: '**APE** (Automatic Prompt Engineer; [Zhou et al. 2022](https://arxiv.org/abs/2211.01910))
    is a method to search over a pool of model-generated instruction candidates and
    then filters the candidate set according to a chosen score function to ultimately
    choose the best candidate with highest score.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '**APE**（自动提示工程师；[Zhou等人，2022](https://arxiv.org/abs/2211.01910)）是一种方法，用于在模型生成的指令候选项池中搜索，然后根据选择的评分函数过滤候选集，最终选择得分最高的最佳候选项。'
- en: Prompt LLM to generate instruction candidates based on a small set of demonstrations
    in the form of input-output pairs. E.g. `{{Given desired input-output pairs}}\n\nThe
    instruction is`.
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提示LLM根据少量演示形式的输入-输出对生成指令候选项。例如`{{给定所需的输入-输出对}}\n\n指令是`。
- en: 'Given a dataset of $\mathcal{D}_\text{train} = \{(x, y)\}$, we would like to
    find an instruction $\rho$ such that $\rho^* = \arg\max_\rho \mathbb{E}_{(x, y)
    \in \mathcal{D}_\text{train}} [f(\rho, x, y)]$, where $f(.)$ is a per-sample score
    function, such as execution accuracy $\mathbb{1}[\text{LM}(.\vert \rho, x)=y]$
    or log probability: $p_\text{LM}(y \mid \rho, x)$.'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 给定数据集$\mathcal{D}_\text{train} = \{(x, y)\}$，我们希望找到一个指令$\rho$，使得$\rho^* = \arg\max_\rho
    \mathbb{E}_{(x, y) \in \mathcal{D}_\text{train}} [f(\rho, x, y)]$，其中$f(.)$是一个逐样本评分函数，例如执行准确性$\mathbb{1}[\text{LM}(.\vert
    \rho, x)=y]$或对数概率：$p_\text{LM}(y \mid \rho, x)$。
- en: 'Use an iterative Monte Carlo search method to improve the best candidates by
    proposing semantically similar variants via prompts like `Generate a variation
    of the following instruction while keeping the semantic meaning.\n\nInput: ...\n\nOutput:...`'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用迭代的蒙特卡洛搜索方法通过提示提出语义相似的变体来改进最佳候选项，例如`生成以下指令的变体，保持语义含义。\n\n输入：...\n\n输出：...`
- en: 'To construct chain-of-thought prompts automatically, [Shum et al. (2023)](https://arxiv.org/abs/2302.12822)
    suggested augment-prune-select, a three-step process:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 为了自动构建思维链提示，[Shum等人（2023）](https://arxiv.org/abs/2302.12822)建议增强-修剪-选择，一个三步过程：
- en: '*Augment*: Generate multiple pseudo-chains of thought given question using
    few-shot or zero-shot CoT prompts;'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*增强*：使用少样本或零样本CoT提示生成多个思维伪链；'
- en: '*Prune*: Prune pseudo chains based on whether generated answers match ground
    truths.'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*修剪*：根据生成的答案是否与基本事实匹配来修剪伪链。'
- en: '*Select*: Apply a variance-reduced policy gradient strategy to learn the probability
    distribution over selected examples, while considering the probability distribution
    over examples as policy and the validation set accuracy as reward.'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*选择*：应用方差减少的策略梯度策略来学习所选示例的概率分布，同时考虑示例的概率分布作为策略和验证集准确性作为奖励。'
- en: '[Zhang et al. (2023)](https://arxiv.org/abs/2210.03493) instead adopted *clustering*
    techniques to sample questions and then generates chains. They observed that LLMs
    tend to make certain types of mistakes. One type of errors can be similar in the
    emebedding space and thus get grouped together. By only sampling one or a few
    from frequent-error clusters, we can prevent too many wrong demonstrations of
    one error type and collect a diverse set of examples.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '[Zhang等人（2023）](https://arxiv.org/abs/2210.03493)反而采用*聚类*技术来抽样问题，然后生成链。他们观察到LLMs倾向于产生某些类型的错误。一种类型的错误可能在嵌入空间中相似，因此被分组在一起。通过仅从频繁错误聚类中抽样一个或几个，我们可以防止一种错误类型产生过多错误演示，并收集多样化的示例。'
- en: '*Question clustering*: Embed questions and run $k$-means for clustering.'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*问题聚类*：嵌入问题并运行$k$-means进行聚类。'
- en: '*Demonstration selection*: Select a set of representative questions from each
    cluster; i.e. one demonstration from one cluster. Samples in each cluster are
    sorted by distance to the cluster centroid and those closer to the centroid are
    selected first.'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*演示选择*：从每个聚类中选择一组代表性问题；即每个聚类中选择一个演示。每个聚类中的样本按与聚类中心的距离排序，距离聚类中心更近的先被选择。'
- en: '*Rationale generation*: Use zero-shot CoT to generate reasoning chains for
    selected questions and construct few-shot prompt to run inference.'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*理由生成*：使用零-shot CoT生成选定问题的推理链，并构建少-shot提示来运行推理。'
- en: Augmented Language Models
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 增强语言模型
- en: A survey on augmented language models by [Mialon et al. (2023)](https://arxiv.org/abs/2302.07842)
    has great coverage over multiple categories of language models augmented with
    reasoning skills and the ability of using external tools. Recommend it.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '[Mialon等人（2023）](https://arxiv.org/abs/2302.07842)对增强语言模型进行了调查，涵盖了多个类别的具有推理技能和使用外部工具能力的语言模型。推荐一下。'
- en: Retrieval
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检索
- en: Often we need to complete tasks that require latest knowledge after the model
    pretraining time cutoff or internal/private knowledge base. In that case, the
    model would not know the context if we don’t explicitly provide it in the prompt.
    Many methods for [Open Domain Question Answering](https://lilianweng.github.io/posts/2020-10-29-odqa/)
    depend on first doing retrieval over a knowledge base and then incorporating the
    retrieved content as part of the prompt. The accuracy of such a process depends
    on the quality of both retrieval and generation steps.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们经常需要在模型预训练截止时间或内部/私有知识库之后完成需要最新知识的任务。在这种情况下，如果我们不在提示中明确提供上下文，模型将不知道上下文。许多[开放领域问答](https://lilianweng.github.io/posts/2020-10-29-odqa/)的方法首先依赖于对知识库进行检索，然后将检索到的内容作为提示的一部分。这一过程的准确性取决于检索和生成步骤的质量。
- en: '[Lazaridou et al. (2022)](https://arxiv.org/abs/2203.05115) studied how to
    use Google Search for document retrieval to augment LLMs. Given a question $q$,
    clean text is extracted out of 20 URLs returned by Google, resulting in a set
    of documents. Because these documents are long, each document is split into paragraphs
    of 6 sentences, $\{p\}$. Paragraphs are ranked by TF-IDF based cosine similarity
    between evidence paragraphs and the query. Only the most relevant paragraph is
    used in the prompt to produce an answer $a$.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[Lazaridou等人（2022）](https://arxiv.org/abs/2203.05115)研究了如何利用Google搜索进行文档检索以增强LLMs。给定一个问题$q$，从Google返回的20个URL中提取干净文本，得到一组文档。由于这些文档很长，每个文档被分成6句的段落$\{p\}$。段落根据证据段落和查询之间基于TF-IDF的余弦相似度进行排名。只使用最相关的段落来生成答案$a$的提示。'
- en: For closed-book QA, each demonstration is formatted as follows to construct
    few-shot prompts. Swapping the question with the evidence (longer distance between
    questions and answers) is found to consistently yield lower results across all
    datasets.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 对于闭卷问答，每个演示的格式如下以构建少-shot提示。发现将问题与证据互换（问题和答案之间的距离更远）通常会导致所有数据集的结果一致较低。
- en: '[PRE8]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The answer probability is computed in three ways:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 答案概率有三种计算方式：
- en: '[RAG](https://lilianweng.github.io/posts/2020-10-29-odqa/#RAG) style, $p(a_i
    \mid q) = \sum_{i=1}^n p_\text{tf-idf} (p_i \mid q) \cdot p_\text{LM}(a_i \mid
    q, p_i)$, where $p_\text{tf-idf} (p_i \mid q)$ is the normalized cosine similarities
    between the TF-IDF passage and question representations.'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[RAG](https://lilianweng.github.io/posts/2020-10-29-odqa/#RAG)风格，$p(a_i \mid
    q) = \sum_{i=1}^n p_\text{tf-idf} (p_i \mid q) \cdot p_\text{LM}(a_i \mid q, p_i)$，其中$p_\text{tf-idf}
    (p_i \mid q)$是TF-IDF段落和问题表示之间的归一化余弦相似度。'
- en: Noisy channel inference, $p(a_i\mid q) = \frac{p_\text{LM}(q \mid a_i, p_i)
    \cdot p_\text{LM}(a_i \mid p_i)}{p_\text{LM}(q \mid p_i)}$
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 噪声信道推理，$p(a_i\mid q) = \frac{p_\text{LM}(q \mid a_i, p_i) \cdot p_\text{LM}(a_i
    \mid p_i)}{p_\text{LM}(q \mid p_i)}$
- en: Product-of-Experts (PoE), combines all probabilities used above in addition
    to $p_\text{LM}(p_i \mid q)$.
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 专家乘积（PoE），结合了上述所有概率以及 $p_\text{LM}(p_i \mid q)$。
- en: According to their experiments on generation and classification tasks, among
    three answer reranking scores - PoE > Noisy channel > RAG. Among individual probabilities,
    $p_\text{LM}(a \mid q, p_i)$ and $p_\text{LM}(q \mid p_i, a)$ are found to be
    most informative. $p_\text{LM}(q \mid p_i, a)$ captures how well the question
    can be explained by LM given evidence paragraph and answer and can reliably be
    used for reranking answer candidates.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 根据他们在生成和分类任务上的实验，三个答案重新排名得分中 - PoE > Noisy channel > RAG。在个别概率中，$p_\text{LM}(a
    \mid q, p_i)$ 和 $p_\text{LM}(q \mid p_i, a)$ 被发现是最具信息量的。$p_\text{LM}(q \mid p_i,
    a)$ 捕捉了LM在给定证据段落和答案的情况下解释问题的能力，并可可靠用于重新排列答案候选者。
- en: One observation with [SituatedQA](https://situatedqa.github.io/) dataset for
    questions grounded in different dates is that despite LM (pretraining cutoff is
    year 2020) has access to latest information via Google Search, its performance
    on post-2020 questions are still a lot *worse* than on pre-2020 questions. This
    suggests the existence of some discrepencies or conflicting parametric between
    contextual information and model internal knowledge.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 对于基于不同日期的问题的[SituatedQA](https://situatedqa.github.io/)数据集的一个观察是，尽管LM（预训练截止年份为2020年）通过Google搜索可以获取最新信息，但其在2020年后的问题上的表现仍然比在2020年前的问题上差很多。这表明上下文信息和模型内部知识之间存在一些不一致或冲突的参数。
- en: 'Interestingly it is found to be beneficial even with only “internal retrieval”,
    that is, to generate knowledge about a topic before answering the question ([Liu
    et al. 2022](https://arxiv.org/abs/2110.08387)). First we can use the following
    template to extract knowledge:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，即使只有“内部检索”，即在回答问题之前生成关于主题的知识也是有益的（[Liu et al. 2022](https://arxiv.org/abs/2110.08387)）。首先，我们可以使用以下模板提取知识：
- en: '[PRE9]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: And then with model-generated knowledge, prompt the LM further to get the answer.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 然后通过模型生成的知识，进一步提示LM获取答案。
- en: Programming Language
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 编程语言
- en: Both **PAL** (Program-aided language models); [Gao et al. 2022](https://arxiv.org/abs/2211.10435))
    and **PoT** (Program of Thoughts prompting; [Chen et al. 2022](https://arxiv.org/abs/2211.12588))
    ask LLM to generate programming language statements to resolve natural language
    reasoning problems, hence offloading the solution step to a runtime such as a
    Python interpreter. Such setup decouples complex computation and reasoning. It
    relies on a LM with good enough coding skills.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '**PAL**（程序辅助语言模型；[Gao et al. 2022](https://arxiv.org/abs/2211.10435)）和 **PoT**（思维方案提示程序；[Chen
    et al. 2022](https://arxiv.org/abs/2211.12588)）都要求LLM生成编程语言语句来解决自然语言推理问题，从而将解决步骤转移到运行时，如Python解释器。这种设置将复杂的计算和推理分离开来。它依赖于具有足够良好编码技能的LM。'
- en: '![](../Images/a95265da249cf03858c3e9feebf4985c.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a95265da249cf03858c3e9feebf4985c.png)'
- en: 'Fig. 3\. Comparing CoT and PoT. (Image source: [Chen et al. 2022](https://arxiv.org/abs/2211.12588)).'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图3。比较CoT和PoT。（图片来源：[Chen et al. 2022](https://arxiv.org/abs/2211.12588)）。
- en: External APIs
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 外部APIs
- en: '**TALM** (Tool Augmented Language Models; [Parisi et al. 2022](https://arxiv.org/abs/2205.12255))
    is a language model augmented with text-to-text API calls. LM is guided to generate
    `|tool-call` and `tool input text` conditioned on task input text to construct
    API call requests. When `|result` shows up, the specified tool API is called and
    the returned result gets appended to the text sequence. The final output is generated
    following `|output` token.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '**TALM**（工具增强语言模型；[Parisi et al. 2022](https://arxiv.org/abs/2205.12255)）是一个通过文本到文本API调用增强的语言模型。LM被引导生成`|tool-call`和`tool
    input text`，条件是任务输入文本以构建API调用请求。当`|result`出现时，指定的工具API被调用，并返回的结果被附加到文本序列中。最终输出遵循`|output`标记。'
- en: '![](../Images/f57d91e404e574c25c3f30124859324e.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f57d91e404e574c25c3f30124859324e.png)'
- en: 'Fig. 4\. The format of API calls in TALM. (Image source: [Parisi et al. 2022](https://arxiv.org/abs/2205.12255)).'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 图4。TALM中API调用的格式。（图片来源：[Parisi et al. 2022](https://arxiv.org/abs/2205.12255)）。
- en: TALM adopts a self-play approach to iteratively bootstrap the dataset of tool
    use examples and finetune LM with it. This self-play, defined as a model interacting
    with a tool API, iteratively expands the dataset based on whether a newly added
    tool API can improve the model outputs. Same idea is adopted in Toolformer too,
    described in more details below. The pipeline loosely mimics a RL process where
    LM is the policy network and it is trained by policy gradient with a binary reward
    signal.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: TALM 采用自我对弈方法来迭代地引导工具使用示例数据集，并用其对 LM 进行微调。这种自我对弈被定义为模型与工具 API 交互，根据新添加的工具 API
    是否能改善模型输出来迭代地扩展数据集。Toolformer 中也采用了相同的思想，下面将更详细地描述。该流程松散地模拟了一个强化学习过程，其中 LM 是策略网络，并通过二进制奖励信号进行策略梯度训练。
- en: '![](../Images/55375ab0b6b187f975a94f43f8a9b465.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/55375ab0b6b187f975a94f43f8a9b465.png)'
- en: Fig. 5\. Self-play iterations help boost the model performance.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5\. 自我对弈迭代有助于提升模型性能。
- en: '(Image source: [Parisi et al. 2022](https://arxiv.org/abs/2205.12255)).'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: (图片来源：[Parisi 等人 2022](https://arxiv.org/abs/2205.12255))。
- en: '**Toolformer** ([Schick et al. 2023](https://arxiv.org/abs/2302.04761)) is
    a LM that can use external tools via simple APIs, which is built in a self-supervised
    manner and only requires a handful of demonstrations for each API. The toolbox
    of Toolformer includes:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '**Toolformer** ([Schick 等人 2023](https://arxiv.org/abs/2302.04761)) 是一个 LM，可以通过简单的
    API 使用外部工具，以自监督方式构建，并且每个 API 只需要少量演示。Toolformer 的工具箱包括：'
- en: '*Calculator* to help LM with the lack of precise math skills;'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*计算器* 帮助 LM 解决数学技能不精确的问题；'
- en: '*Q&A system* to help with unfaithful content and hallucination;'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*问答系统* 帮助处理不忠实内容和幻觉；'
- en: '*Search engine* to provide up-to-date information after pretraining cut off
    time;'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*搜索引擎* 在预训练截止时间后提供最新信息；'
- en: '*Translation system* to improve performance on low resource language;'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*翻译系统* 以提高低资源语言的性能；'
- en: '*Calendar* to make LM be aware of time progression.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*日历* 使 LM 意识到时间的推移。'
- en: '![](../Images/3a85241064b28a651baaff3724bc575c.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3a85241064b28a651baaff3724bc575c.png)'
- en: Fig. 6\. Illustration of how to build Toolformer.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6\. 如何构建 Toolformer 的示意图。
- en: '(Image source: [Schick et al. 2023](https://arxiv.org/abs/2302.04761)).'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: (图片来源：[Schick 等人 2023](https://arxiv.org/abs/2302.04761))。
- en: 'Toolformer is trained as follows:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: Toolformer 的训练如下：
- en: '*Prompting to annotate potential API calls*. Ask a pre-trained LM to annotate
    a dataset via few-shot learning with API call usage examples. Formatting example:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*提示注释潜在的 API 调用*。通过少量示例 API 调用使用预训练 LM 对数据集进行注释。格式示例：'
- en: '![](../Images/a50e5b8bdd19eec1cfb63a1c7e43929b.png)'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](../Images/a50e5b8bdd19eec1cfb63a1c7e43929b.png)'
- en: Fig. 7\. How dataset is annotated to do API calls.
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 7\. 如何对数据集进行注释以进行 API 调用。
- en: '(Image source: [Schick et al. 2023](https://arxiv.org/abs/2302.04761)).'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: (图片来源：[Schick 等人 2023](https://arxiv.org/abs/2302.04761))。
- en: 'Each API call is represented as a tuple of (API name, corresponding input),
    $c=(a_c, i_c)$ and its corresponding result is denoted as $r$. The API call sequences
    with and without results are labeled as follows, respectively:'
  id: totrans-121
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个 API 调用表示为 (API 名称，相应输入) 的元组，$c=(a_c, i_c)$，其相应结果表示为 $r$。具有结果和无结果的 API 调用序列分别标记如下：
- en: $$ \begin{aligned} e(c) &= \langle\texttt{API}\rangle a_c(i_c) \langle\texttt{/API}\rangle
    \\ e(c, r) &= \langle\texttt{API}\rangle a_c(i_c) \to r \langle\texttt{/API}\rangle
    \end{aligned} $$
  id: totrans-122
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: $$ \begin{aligned} e(c) &= \langle\texttt{API}\rangle a_c(i_c) \langle\texttt{/API}\rangle
    \\ e(c, r) &= \langle\texttt{API}\rangle a_c(i_c) \to r \langle\texttt{/API}\rangle
    \end{aligned} $$
- en: Sample API calls based on the probabilities $p_\text{LM}(\langle\texttt{API}\rangle
    \mid \text{prompt}(\mathbf{x}), \mathbf{x}_{1:i})$ and select top $k$ candidate
    positions for doing API calls at position $i$ if the probability is larger than
    a threshold.
  id: totrans-123
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于概率 $p_\text{LM}(\langle\texttt{API}\rangle \mid \text{prompt}(\mathbf{x}),
    \mathbf{x}_{1:i})$ 的样本 API 调用，并在概率大于阈值时选择在位置 $i$ 进行 API 调用的前 $k$ 个候选位置。
- en: Then we sample potential API calls from the LM given the sequence $[\text{prompt}(\mathbf{x}),
    x_1, \dots, x_{i-1}, \langle\texttt{API}\rangle]$ as prefix and $\langle\texttt{/API}\rangle$
    as suffix.
  id: totrans-124
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后我们从 LM 中给定序列 $[\text{prompt}(\mathbf{x}), x_1, \dots, x_{i-1}, \langle\texttt{API}\rangle]$
    作为前缀和 $\langle\texttt{/API}\rangle$ 作为后缀中抽样潜在的 API 调用。
- en: '*Filter annotations based on whether API calls help model predict future tokens.*
    Use a self-supervised loss to decide which API calls are actually helpful.'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*根据 API 调用是否帮助模型预测未来标记来筛选注释*。使用自监督损失来决定哪些 API 调用实际上是有帮助的。'
- en: Execute each API call $c_i$ to get corresponding result $r_i$.
  id: totrans-126
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行每个 API 调用 $c_i$ 以获得相应结果 $r_i$。
- en: Compute weighted cross entropy loss for the LM over tokens $x_i, \dots, x_n$
    when the model is prefixed with the prompt. Two versions are computed, one with
    API result and the other with empty sequence $\varepsilon$.
  id: totrans-127
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当模型以提示为前缀时，计算LM在标记$x_i, \dots, x_n$上的加权交叉熵损失。计算两个版本，一个带有API结果，另一个带有空序列$\varepsilon$。
- en: $$ \begin{aligned} L^+_i &= L_i(e(c_i, r_i)) \\ L^-_i &= \min(L_i(\varepsilon),
    L_i(e(c_i, \varepsilon))) \\ \end{aligned} $$
  id: totrans-128
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: $$ \begin{aligned} L^+_i &= L_i(e(c_i, r_i)) \\ L^-_i &= \min(L_i(\varepsilon),
    L_i(e(c_i, \varepsilon))) \\ \end{aligned} $$
- en: Only API calls with $L^-_i - L^+_i$ larger than a threshold are kept, meaning
    that adding this API call and its results help the model predict future tokens.
  id: totrans-129
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 只有$L^-_i - L^+_i$大于阈值的API调用会被保留，这意味着添加此API调用及其结果有助于模型预测未来的标记。
- en: '*Fine-tune LM on this annotated dataset.* The new training sequences are constructed
    as $\mathbf{x}^* = x_{1:i-1}, e(c_i, r_i), x_{i:n}$ . The training data is a combination
    of the original dataset (e.g. a subset of CCNet, as in the paper) and its augmented
    version.'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*在这个注释数据集上微调LM。* 新的训练序列构建为$\mathbf{x}^* = x_{1:i-1}, e(c_i, r_i), x_{i:n}$。训练数据是原始数据集的组合（例如，如论文中所述的CCNet的子集）及其增强版本。'
- en: At inference time, decoding runs until the model produces “$\to$ " token, indicating
    that it is expecting response from an API call next.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 推理时，解码会一直运行，直到模型产生“$\to$”标记，表示它正在等待下一个API调用的响应。
- en: Toolformer currently does not support tool use in a chain (i.e. using the output
    of one tool as an input for another tool) or in an interactive way (i.e. adopt
    API response after human selection). Both are interesting future directions to
    expand the model for.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: Toolformer目前不支持链式工具使用（即使用一个工具的输出作为另一个工具的输入）或交互方式（即在人类选择后采用API响应）。这两种方式都是未来扩展模型的有趣方向。
- en: Citation
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引用
- en: 'Cited as:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 引用为：
- en: Weng, Lilian. (Mar 2023). Prompt Engineering. Lil’Log. https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/.
  id: totrans-135
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 翁，莉莉安。 (2023年3月)。提示工程。Lil’Log。https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/。
- en: Or
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 或
- en: '[PRE10]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Useful Resources
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 有用资源
- en: '[OpenAI Cookbook](https://github.com/openai/openai-cookbook) has many in-depth
    examples for how to utilize LLM efficiently.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[OpenAI Cookbook](https://github.com/openai/openai-cookbook)有许多深入的示例，展示如何高效利用LLM。'
- en: '[LangChain](https://langchain.readthedocs.io/en/latest/), a library for combining
    language models with other components to build applications.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[LangChain](https://langchain.readthedocs.io/en/latest/)，一个用于将语言模型与其他组件结合构建应用程序的库。'
- en: '[Prompt Engineering Guide](https://github.com/dair-ai/Prompt-Engineering-Guide)
    repo contains a pretty comprehensive collection of education materials on prompt
    engineering.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Prompt Engineering Guide](https://github.com/dair-ai/Prompt-Engineering-Guide)存储库包含了关于提示工程的教育材料的相当全面的收集。'
- en: '[learnprompting.org](https://learnprompting.org/docs/intro)'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[learnprompting.org](https://learnprompting.org/docs/intro)'
- en: '[PromptPerfect](https://promptperfect.jina.ai)'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[PromptPerfect](https://promptperfect.jina.ai)'
- en: '[Semantic Kernel](https://github.com/microsoft/semantic-kernel)'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Semantic Kernel](https://github.com/microsoft/semantic-kernel)'
- en: References
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Zhao et al. [“Calibrate Before Use: Improving Few-shot Performance of Language
    Models.”](https://arxiv.org/abs/2102.09690) ICML 2021'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] 赵等人 [“使用前校准：改善语言模型的少样本性能。”](https://arxiv.org/abs/2102.09690) ICML 2021'
- en: '[2] Liu et al. [“What Makes Good In-Context Examples for GPT-3?”](https://arxiv.org/abs/2101.06804)
    arXiv preprint arXiv:2101.06804 (2021).'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] 刘等人 [“GPT-3的上下文示例是什么使之优秀？”](https://arxiv.org/abs/2101.06804) arXiv预印本
    arXiv:2101.06804 (2021)。'
- en: '[3] Lu et al. [“Fantastically Ordered Prompts and Where to Find Them: Overcoming
    Few-Shot Prompt Order Sensitivity.”](https://arxiv.org/abs/2104.08786) ACL 2022'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] 卢等人 [“奇妙有序的提示及其发现之处：克服少样本提示顺序敏感性。”](https://arxiv.org/abs/2104.08786) ACL
    2022'
- en: '[4] Ye et al. [“In-Context Instruction Learning.”](https://arxiv.org/abs/2302.14691)
    arXiv preprint arXiv:2302.14691 (2023).'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] 叶等人 [“上下文指导学习。”](https://arxiv.org/abs/2302.14691) arXiv预印本 arXiv:2302.14691
    (2023)。'
- en: '[5] Su et al. [“Selective annotation makes language models better few-shot
    learners.”](https://arxiv.org/abs/2209.01975) arXiv preprint arXiv:2209.01975
    (2022).'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] 苏等人 [“选择性注释使语言模型更好的少样本学习者。”](https://arxiv.org/abs/2209.01975) arXiv预印本
    arXiv:2209.01975 (2022)。'
- en: '[6] Rubin et al. [“Learning to retrieve prompts for in-context learning.”](https://arxiv.org/abs/2112.08633)
    NAACL-HLT 2022'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] 鲁宾等人 [“学习为上下文学习检索提示。”](https://arxiv.org/abs/2112.08633) NAACL-HLT 2022'
- en: '[7] Wei et al. [“Chain of thought prompting elicits reasoning in large language
    models.”](https://arxiv.org/abs/2201.11903) NeurIPS 2022'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] 魏等人 [“思维链提示引发大型语言模型的推理。”](https://arxiv.org/abs/2201.11903) NeurIPS 2022'
- en: '[8] Wang et al. [“Self-Consistency Improves Chain of Thought Reasoning in Language
    Models.”](https://arxiv.org/abs/2203.11171) ICLR 2023.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] 王等人 [“自一致性改善语言模型中的思维链推理。”](https://arxiv.org/abs/2203.11171) ICLR 2023.'
- en: '[9] Diao et al. [“Active Prompting with Chain-of-Thought for Large Language
    Models.”](https://arxiv.org/abs/2302.12246) arXiv preprint arXiv:2302.12246 (2023).'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] 刁等人 [“大型语言模型的主动提示与思维链。”](https://arxiv.org/abs/2302.12246) arXiv预印本 arXiv:2302.12246
    (2023).'
- en: '[10] Zelikman et al. [“STaR: Bootstrapping Reasoning With Reasoning.”](https://arxiv.org/abs/2203.14465)
    arXiv preprint arXiv:2203.14465 (2022).'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] 泽利克曼等人 [“STaR：用推理引导推理。”](https://arxiv.org/abs/2203.14465) arXiv预印本 arXiv:2203.14465
    (2022).'
- en: '[11] Ye & Durrett. [“The unreliability of explanations in few-shot in-context
    learning.”](https://arxiv.org/abs/2205.03401) arXiv preprint arXiv:2205.03401
    (2022).'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] 叶 & 达雷特 [“少样本情境学习中解释的不可靠性。”](https://arxiv.org/abs/2205.03401) arXiv预印本
    arXiv:2205.03401 (2022).'
- en: '[12] Trivedi et al. [“Interleaving retrieval with chain-of-thought reasoning
    for knowledge-intensive multi-step questions.”](https://arxiv.org/abs/2212.10509)
    arXiv preprint arXiv:2212.10509 (2022).'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '[12] 特里韦迪等人 [“交错检索与思维链推理相结合，用于知识密集型多步问题。”](https://arxiv.org/abs/2212.10509)
    arXiv预印本 arXiv:2212.10509 (2022).'
- en: '[13] Press et al. [“Measuring and narrowing the compositionality gap in language
    models.”](https://arxiv.org/abs/2210.03350) arXiv preprint arXiv:2210.03350 (2022).'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '[13] 普雷斯等人 [“衡量和缩小语言模型中的组合性差距。”](https://arxiv.org/abs/2210.03350) arXiv预印本
    arXiv:2210.03350 (2022).'
- en: '[14] Yao et al. [“ReAct: Synergizing reasoning and acting in language models.”](https://arxiv.org/abs/2210.03629)
    ICLR 2023.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '[14] 姚等人 [“ReAct：在语言模型中协同推理和行动。”](https://arxiv.org/abs/2210.03629) ICLR 2023.'
- en: '[15] Fu et al. [“Complexity-based prompting for multi-step reasoning.”](https://arxiv.org/abs/2210.00720)
    arXiv preprint arXiv:2210.00720 (2022).'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '[15] 傅等人 [“基于复杂性的多步推理提示。”](https://arxiv.org/abs/2210.00720) arXiv预印本 arXiv:2210.00720
    (2022).'
- en: '[16] Wang et al. [“Rationale-augmented ensembles in language models.”](https://arxiv.org/abs/2207.00747)
    arXiv preprint arXiv:2207.00747 (2022).'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '[16] 王等人 [“基于原因的集成在语言模型中。”](https://arxiv.org/abs/2207.00747) arXiv预印本 arXiv:2207.00747
    (2022).'
- en: '[17] Zhang et al. [“Automatic chain of thought prompting in large language
    models.”](https://arxiv.org/abs/2210.03493) arXiv preprint arXiv:2210.03493 (2022).'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '[17] 张等人 [“大型语言模型中的自动思维链提示。”](https://arxiv.org/abs/2210.03493) arXiv预印本 arXiv:2210.03493
    (2022).'
- en: '[18] Shum et al. [“Automatic Prompt Augmentation and Selection with Chain-of-Thought
    from Labeled Data.”](https://arxiv.org/abs/2302.12822) arXiv preprint arXiv:2302.12822
    (2023).'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '[18] 沈等人 [“使用标记数据的思维链自动提示增强和选择。”](https://arxiv.org/abs/2302.12822) arXiv预印本
    arXiv:2302.12822 (2023).'
- en: '[19] Zhou et al. [“Large Language Models Are Human-Level Prompt Engineers.”](https://arxiv.org/abs/2211.01910)
    ICLR 2023.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '[19] 周等人 [“大型语言模型是人类级别的提示工程师。”](https://arxiv.org/abs/2211.01910) ICLR 2023.'
- en: '[20] Lazaridou et al. [“Internet augmented language models through few-shot
    prompting for open-domain question answering.”](https://arxiv.org/abs/2203.05115)
    arXiv preprint arXiv:2203.05115 (2022).'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '[20] 拉扎里多等人 [“通过少样本提示增强互联网语言模型，用于开放域问答。”](https://arxiv.org/abs/2203.05115)
    arXiv预印本 arXiv:2203.05115 (2022).'
- en: '[21] Chen et al. [“Program of Thoughts Prompting: Disentangling Computation
    from Reasoning for Numerical Reasoning Tasks.”](https://arxiv.org/abs/2211.12588)
    arXiv preprint arXiv:2211.12588 (2022).'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '[21] 陈等人 [“思维提示程序：将计算与推理分离，用于数值推理任务。”](https://arxiv.org/abs/2211.12588) arXiv预印本
    arXiv:2211.12588 (2022).'
- en: '[22] Gao et al. [“PAL: Program-aided language models.”](https://arxiv.org/abs/2211.10435)
    arXiv preprint arXiv:2211.10435 (2022).'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '[22] 高等人 [“PAL：程序辅助语言模型。”](https://arxiv.org/abs/2211.10435) arXiv预印本 arXiv:2211.10435
    (2022).'
- en: '[23] Parisi et al. [“TALM: Tool Augmented Language Models”](https://arxiv.org/abs/2205.12255)
    arXiv preprint arXiv:2205.12255 (2022).'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '[23] 帕里西等人 [“TALM：工具增强语言模型”](https://arxiv.org/abs/2205.12255) arXiv预印本 arXiv:2205.12255
    (2022).'
- en: '[24] Schick et al. [“Toolformer: Language Models Can Teach Themselves to Use
    Tools.”](https://arxiv.org/abs/2302.04761) arXiv preprint arXiv:2302.04761 (2023).'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '[24] 施克等人 [“Toolformer：语言模型可以自我教导使用工具。”](https://arxiv.org/abs/2302.04761)
    arXiv预印本 arXiv:2302.04761 (2023).'
- en: '[25] Mialon et al. [“Augmented Language Models: a Survey”](https://arxiv.org/abs/2302.07842)
    arXiv preprint arXiv:2302.07842 (2023).'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '[25] 米亚隆等人 [“增强语言模型：一项调查”](https://arxiv.org/abs/2302.07842) arXiv预印本 arXiv:2302.07842
    (2023).'
- en: '[26] Yao et al. [“Tree of Thoughts: Deliberate Problem Solving with Large Language
    Models.”](https://arxiv.org/abs/2305.10601) arXiv preprint arXiv:2305.10601 (2023).'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '[26] 姚等人 [“思维之树：大型语言模型的刻意问题解决。”](https://arxiv.org/abs/2305.10601) arXiv预印本
    arXiv:2305.10601 (2023).'
