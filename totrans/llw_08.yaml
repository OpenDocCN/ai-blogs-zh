- en: Generalized Visual Language Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://lilianweng.github.io/posts/2022-06-09-vlm/](https://lilianweng.github.io/posts/2022-06-09-vlm/)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Processing images to generate text, such as image captioning and visual question-answering,
    has been studied for years. Traditionally such systems rely on an object detection
    network as a vision encoder to capture visual features and then produce text via
    a text decoder. Given a large amount of existing literature, in this post, I would
    like to only focus on one approach for solving vision language tasks, which is
    to *extend pre-trained [generalized language models](https://lilianweng.github.io/posts/2019-01-31-lm/)
    to be capable of consuming visual signals*.
  prefs: []
  type: TYPE_NORMAL
- en: 'I roughly group such vision language models (VLMs) into four buckets:'
  prefs: []
  type: TYPE_NORMAL
- en: Translating images into embedding features that can be jointly trained with
    token embeddings.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Learning good image embeddings that can work as a prefix for a frozen, pre-trained
    language model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using a specially designed cross-attention mechanism to fuse visual information
    into layers of the language model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Combine vision and language models without any training.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Jointly Training with Image and Text
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One straightforward approach to fuse visual information into language models
    is to treat images as normal text tokens and train the model on a sequence of
    joint representations of both text and images. Precisely, images are divided into
    multiple smaller patches and each patch is treated as one “token” in the input
    sequence.
  prefs: []
  type: TYPE_NORMAL
- en: '**VisualBERT** ([Li et al. 2019](https://arxiv.org/abs/1908.03557)) feeds both
    text inputs and image regions into [BERT](https://lilianweng.github.io/posts/2019-01-31-lm/#bert)
    such that it is able to discover the internal alignment between images and text
    with self-attention mechanism.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/04805c92244b2a6ca1c28a058c909698.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 1\. VisualBERT is trained on the combination of both text and image embeddings.
    (Image source: [Li et al. 2019](https://arxiv.org/abs/1908.03557))'
  prefs: []
  type: TYPE_NORMAL
- en: 'Similar to [text embedding in BERT](https://lilianweng.github.io/posts/2019-01-31-lm/#input-embedding),
    each visual embedding in VisualBERT also sums up three types of embeddings, tokenized
    features $f_o$, segmentation embedding $f_s$ and position embedding $f_p$, precisely:'
  prefs: []
  type: TYPE_NORMAL
- en: $f_o$ is a visual feature vector computed for a bounding region of the image
    by a convolutional neural network;
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: $f_s$ is a segment embedding to indicate whether the embedding is for vision
    not for text;
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: $f_p$ is a position embedding used for aligning the order of bounding regions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The model is trained on MS COCO image caption dataset with both text and image
    as inputs to predict text captions, using two visually-grounded language model
    objectives:'
  prefs: []
  type: TYPE_NORMAL
- en: '*[MLM](https://lilianweng.github.io/posts/2019-01-31-lm/#pre-training-tasks)
    with the image*. The model needs to predict masked text tokens, while image embeddings
    always stay not masked.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Sentence-image prediction*. When provided with an image and two associated
    captions, one of two captions might be a random unrelated caption with 50% probability.
    The model is asked to distinguish these two situations.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: According to ablation experiments, the most important configuration is to fuse
    visual information early on into the transformer layers and to pretrain the model
    on the COCO caption dataset. Initialization from a pre-trained BERT and the adoption
    of the sentence-image prediction training objective have relatively small impacts.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/33fd1da995e446900a8f24fa7ffe773b.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig. 2\. Ablation study results of VisualBERT on NLVR.
  prefs: []
  type: TYPE_NORMAL
- en: '(Image source: [Li et al. 2019](https://arxiv.org/abs/1908.03557))'
  prefs: []
  type: TYPE_NORMAL
- en: VisualBERT outperforms SoTA at the time on NLVR and Flickr30K, but still has
    some performance gap with SoTA on VQA.
  prefs: []
  type: TYPE_NORMAL
- en: '**SimVLM** (Simple Visual Language Model; [Wang et al. 2022](https://arxiv.org/abs/2108.10904))
    is a simple *prefix language model*, where the prefix sequence is processed with
    bi-directional attention like BERT, but the main input sequence only has causal
    attention like [GPT](#gpt). Images are encoded as prefix tokens such that the
    model can fully consume the visual information and then generates associated text
    in an autoregressive manner.'
  prefs: []
  type: TYPE_NORMAL
- en: Inspired by [ViT](https://arxiv.org/abs/2010.11929) and [CoAtNet](https://arxiv.org/abs/2106.04803),
    SimVLM splits the image into smaller patches in a flatten 1D sequence of patches.
    They use the convolutional stage consisting of the first 3 blocks of ResNet to
    extract contextualized patches and this setup is found to work better than a naive
    linear projection.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7903fee47cc616db23c48aceaa75fe08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 3\. Training architecture for SimVLM, where the image patches are processed
    by the cross-attention encoder and the text decoder has causal attention. (Image
    source: [Wang et al. 2022](https://arxiv.org/abs/2108.10904))'
  prefs: []
  type: TYPE_NORMAL
- en: Training data for SimVLM consists of a large number of image-text pairs from
    ALIGN ([Jia et al. 2021](https://arxiv.org/abs/2102.05918)) and text-only data
    from C4 dataset ([Raffel et al. 2019](https://arxiv.org/abs/1910.10683)). They
    mix the two pretraining datasets within each batch, containing 4,096 image-text
    pairs (ALIGN) and 512 text-only documents (C4).
  prefs: []
  type: TYPE_NORMAL
- en: According to ablation studies, it is important to have both image-text and text-only
    data for training. The PrefixLM objective outperforms both [span corruption](https://arxiv.org/abs/1910.10683)
    and naive LM.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3219ec9ffff101f12a0876523adad212.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig. 4\. Ablation study results of SimVLM on VQA.
  prefs: []
  type: TYPE_NORMAL
- en: '(Image source: [Wang et al. 2022](https://arxiv.org/abs/2108.10904))'
  prefs: []
  type: TYPE_NORMAL
- en: '**CM3** (Causally-Masked Multimodal Modeling; [Aghajanyan, et al. 2022](https://arxiv.org/abs/2201.07520))
    is a hyper-text language model, learning to generate the content (hypertext markup,
    hyperlinks and images) of large scale HTML web pages of CC-NEWS and Wikipedia
    articles. The resulting CM3 models can generate rich structured, multi-modal outputs
    while conditioning on arbitrary masked document contexts.'
  prefs: []
  type: TYPE_NORMAL
- en: Architecture-wise, CM3 is an autoregressive model. However, in order to combine
    causal and masked language modeling, CM3 also masks out a small number of long
    token spans and tries to generate them at the *end* of the sequences.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/148134bc59d948026f718cfea3362875.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig. 5\. Illustration of how a causally masked language model works.
  prefs: []
  type: TYPE_NORMAL
- en: '(Image source: [Aghajanyan, et al. 2022](https://arxiv.org/abs/2201.07520))'
  prefs: []
  type: TYPE_NORMAL
- en: The training dataset for CM3 contains close to 1T Web data. During preprocessing,
    images are first downloaded from `src` and resized to 256 x 256 with random cropping.
    Then they are tokenized by [VQVAE-GAN](https://arxiv.org/abs/2012.09841), resulting
    in 256 tokens per image. These tokens, joined with spaces, are inserted back into
    the `src` attribute.
  prefs: []
  type: TYPE_NORMAL
- en: 'CM3 can be used to complete several types of tasks by prompt engineering:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Image in-filling:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Conditional image in-filling:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Conditional image generation:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Image captions:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Entity disambiguation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Learned Image Embedding as (Frozen) LM Prefix
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What if we don’t want to change the language model parameters when adapting
    it to handle visual signals? Instead we learn such an embedding space for images
    that it is compatible with the language model’s.
  prefs: []
  type: TYPE_NORMAL
- en: Inspired by [prefix](https://arxiv.org/abs/2101.00190) or [prompt](https://arxiv.org/abs/2104.08691)
    [tuning](https://lilianweng.github.io/posts/2021-01-02-controllable-text-generation/#prefix-tuning),
    both **Frozen** ([Tsimpoukelli et al. 2021](https://arxiv.org/abs/2106.13884))
    and **ClipCap** ([Mokady, Hertz & Hertz, 2021](https://arxiv.org/abs/2111.09734))
    only update the parameters of the vision module during training to produce image
    embeddings that can work with a pretrained, *frozen* language model. Both are
    trained with aligned image caption [datasets](#image-caption-datasets) to produce
    the next text token in caption conditioned on the image and previous text tokens.
    The powerful language capability is retained by freezing LM parameters. In addition,
    even though such setup is trained with limited image caption data, they can also
    rely on the encyclopedic knowledge of the language model at test time.
  prefs: []
  type: TYPE_NORMAL
- en: The vision encoder of Frozen is based on NF-ResNet-50 and uses the final output
    vector of the NF-Resnet after the global pooling layer. The Frozen VLM can be
    used as a multi-model few-shot learner to adapt to new tasks at test time for
    zero-shot or few-shot transfer with a sequence of interleaved images and text.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/39915668c65743b6afe797fc2c883f3a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 6\. Illustration of Frozen model (left) training architecture and (right)
    testing pipeline. (Image source: [Tsimpoukelli et al. 2021](https://arxiv.org/abs/2106.13884))'
  prefs: []
  type: TYPE_NORMAL
- en: Experiments showed that fine-tuning the pre-trained LM interestingly leads to
    worse performance on VQA tasks. It is important to initialize the language model
    from a pre-trained version, as training from scratch (${Frozen}_\text{scratch}$)
    does not show any meaningful progress. The baseline ${Frozen}_\text{train-blind}$
    blacks out the image but still can achieve decent performance because of the innate
    power of the pre-trained LM.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6e96307ec485a14c317cde8a72f08961.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 7\. Performance of different versions of Frozen on (left) VQAv2 and (right)
    OKVQA, trained on Conceptual Captions. "Frozen scratch" does not load a pre-trained
    LM and is trained from scratch. "Frozen finetuned" has the language model finetuned,
    while "Frozen" keeps LM frozen. "Frozen train-blind" blacks out the image. (Image
    source: [Tsimpoukelli et al. 2021](https://arxiv.org/abs/2106.13884))'
  prefs: []
  type: TYPE_NORMAL
- en: ClipCap relies on [CLIP](https://lilianweng.github.io/posts/2021-05-31-contrastive/#clip)
    ([Radford et al. 2021](https://arxiv.org/abs/2103.00020)) for vision encoding,
    but it needs to be processed by a light mapping network $F$ such that image embedding
    vectors are translated into the same semantic space as the pre-trained LM. The
    network $F$ maps CLIP embeddings into a sequence of $k$ embedding vectors, each
    with the same dimension as a word embedding in GPT2\. Increasing the prefix size
    $k$ helps improve the performance. Both CLIP vision encoder and the LM are *frozen*
    during training and only the mapping network $F$ is learned. They found that when
    LM is frozen, $F$ should be a transformer, with 8 multi-head self-attention layers
    with 8 heads each, but when LM can be fine-tuned, a MLP is enough.
  prefs: []
  type: TYPE_NORMAL
- en: Even though ClipCap only trains such a minimum set of parameters, it still achieves
    decent performance on image captioning tasks, comparable with SoTA at the time
    (e.g. [Oscar](https://arxiv.org/abs/2004.06165), [VLP](https://arxiv.org/abs/1909.11059),
    [BUTD](https://arxiv.org/abs/1707.07998)). Hence they postulate that “the CLIP
    space already encapsulates the required information, and adapting it towards specific
    styles does not contribute to flexibility.”
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7a14eaa5ec5445a4dd4407c35565b388.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 8\. Overview of ClipCap training pipeline where only the mapping network
    needs to be train to transform CLIP image embedding to work with the pre-trained
    LM. (Image source: [Mokady, Hertz & Hertz, 2021](https://arxiv.org/abs/2111.09734))'
  prefs: []
  type: TYPE_NORMAL
- en: The fun fact is - because ClipCap translates CLIP image embeddings into LM space,
    the processed prefixes can be even interpreted as words.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a71cee53eb073e7cc9eb78c63882d33a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 9\. The learned image embedding can be interpreted as text, containing
    words related to the image context. (Image source: [Mokady, Hertz & Hertz, 2021](https://arxiv.org/abs/2111.09734))'
  prefs: []
  type: TYPE_NORMAL
- en: Text-Image Cross-Attention Fuse Mechanisms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To more efficiently fuse visual information into different layers of the language
    model, we can consider a specially designed cross-attention fuse mechanism to
    balance the mixture of text generation capacity and visual information.
  prefs: []
  type: TYPE_NORMAL
- en: '**VisualGPT** ([Chen et al. 2021](https://arxiv.org/abs/2102.10407)) employs
    a self-resurrecting encoder-decoder attention mechanism to quickly adapt the pre-trained
    LM with a small amount of in-domain image-text data.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1471812a350408ba124b3fc400ff98d1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 10\. Illustration of VisualGPT architecture. (Image source: [Chen et al.
    2021](https://arxiv.org/abs/2102.10407))'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let $I$ be the output of a visual encoder and $H$ be the hidden state of the
    LM decoder. VisualGPT introduced a self-resurrecting activation unit (SRAU) to
    control the tradeoff between a mixture of pre-trained linguistic information $H$
    and visual component, $\text{EncDecAttn}(H, I)$ via two complementary gates $B^\text{vis}$
    and $B^\text{lan}$:'
  prefs: []
  type: TYPE_NORMAL
- en: $$ \begin{aligned} & B^\text{vis} \otimes \text{EncDecAttn}(H, I) + B^\text{lan}
    \otimes H \\ \text{where } & B^\text{vis}[i,j] = \sigma(H[i,j]) \mathbb{1}[\sigma(H[i,j])
    > \tau] \\ & B^\text{lan}[i,j] = (1 - \sigma(H[i,j])) \mathbb{1}[1 - \sigma(H[i,j])
    > \tau] \\ \end{aligned} $$ where $\otimes$ is element-wise multiplication and
    $[i,j]$ denotes one element in the matrix. $\tau$ is a predefined threshold hyperparameter.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f699949bce5910b8ce0d8c456f859880.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 11\. Comparison of different models trained on 0.1% and 1% of the MS COCO
    and Conceptual Caption datasets. (Image source: [Chen et al. 2021](https://arxiv.org/abs/2102.10407))'
  prefs: []
  type: TYPE_NORMAL
- en: '**VC-GPT** (Visual Conditioned GPT; [Luo et al. 2022](https://arxiv.org/abs/2201.12723))
    combines a pretrained visual transformer (CLIP-ViT) as visual encoder and a pretrained
    LM as language decoder.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3811e2d7c934aafafc838cc40d1aff29.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig. 12\. Illustration of VC-GPT training framework.
  prefs: []
  type: TYPE_NORMAL
- en: '(Image source: [Luo et al. 2022](https://arxiv.org/abs/2201.12723))'
  prefs: []
  type: TYPE_NORMAL
- en: The CLIP-ViT takes a sequence of image patches as inputs and outputs representation
    for each patch. To avoid catastrophic forgetting, instead of injecting the visual
    information directly into GPT2, VC-GPT introduces extra cross-attention layers
    on top of the output of visual encoder and language decoder. Then a *self-ensemble*
    module linearly combines the single model language decoder logits $h^G$ and cross-model
    vision-language fused module logits $h^\text{fuse}$. The self-ensemble module
    (see “VC-GPT w/o SE” in Fig. 13) is important for the performance.
  prefs: []
  type: TYPE_NORMAL
- en: $$ \text{logits} = W^G h^G + W^\text{fuse}h^\text{fuse} $$
  prefs: []
  type: TYPE_NORMAL
- en: where $W^G$ is a linear projection of the language decoder, initialized by the
    word embedding matrix of GPT2 and $W^\text{fuse}$ is a linear projection of the
    fusion module and initialized randomly.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fc54343d36c4e8761edc641c226dd3e9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 13\. Performance of VC-GPT on the MS COCO test set, in comparison with
    other end-to-end image captioning baseline models. Metric abbreviation: C = CIDEr;
    B = BLEU; M = METEOR; S = SPICE. (Image source: [Luo et al. 2022](https://arxiv.org/abs/2201.12723))'
  prefs: []
  type: TYPE_NORMAL
- en: '**MERLOT** ([Zellers, et al. 2021](https://arxiv.org/abs/2106.02636)) is trained
    with 6 millions of YouTube videos with transcribed speech ([YT-Temporal-180M](https://rowanzellers.com/merlot/#data))
    to learn both spatial (frame-level) and temporal (video-level) objectives and
    demonstrated strong performance on VQA and visual reasoning tasks when fine-tuned.'
  prefs: []
  type: TYPE_NORMAL
- en: Each video $\mathcal{V}$ is split into multiple segments $\{ \boldsymbol{s}_t
    \}$, each segment $\boldsymbol{s}_t$ containing an image frame $\mathbf{I}_t$
    extracted from the middle timestep and $L=32$ tokens of words associated. Images
    are encoded by a learned image encoder and words are encoded using a learned embedding.
    Then both are encoded together within a joint vision-language transformer.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are 3 learning objectives in MERLOT:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Masked language modeling* (MLM) is useful especially because in videos, people
    tend to ramble, resulting in many repeated keywords or filler words.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Contrastive frame-caption matching* uses the language-only part from the joint
    vision-language transformer. Matched representations for each frame $\mathbf{I}_t$
    and caption $\boldsymbol{w}_t$ are treated as positive examples, while the negative
    examples come from all other frame-caption pairs in the minibatch.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Temporal reordering* learns temporal reasoning: scramble random $i$ frames
    and replace the segment-level position embeddings with a random and unique position
    embedding. The random position embeddings are learned, allowing the model to unshuffle
    these “‘shuffled’” frames conditioned on correctly-ordered ones. The loss is to
    predict whether $t_i < t_j$ or $t_j < t_i$ for each frame-frame pair.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/f0513712f175a8afb4dbbd66b1de46dc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 14\. Illustration of MERLOT training framework: (Left) contrastive frame-caption
    matching training; (Right) joint vision-language transformer is trained with MLM
    loss, as well as on the temporal reordering task to unshuffle scrambled video
    frames. (Image source: [Zellers, et al. 2021](https://arxiv.org/abs/2106.02636))'
  prefs: []
  type: TYPE_NORMAL
- en: Ablation studies showed that it is important to (1) train on videos instead
    of images, (2) scale up the size and diversity of the training dataset and (3)
    use diverse objectives to encourage full-stack multimodal reasoning.
  prefs: []
  type: TYPE_NORMAL
- en: '**Flamingo** ([Alayrac et al. 2022](https://arxiv.org/abs/2204.14198)) is a
    visual language model that accepts text interleaved with images/videos and outputs
    free-form text. Flamingo connects a pretrained LM and a pretrained vision encoder
    (i.e. CLIP image encoder) via a transformer-based mapper. To more efficiently
    incorporate vision signals, Flamingo adopts a [Perceiver](https://arxiv.org/abs/2103.03206)-based
    architecture to produce a few hundreds of tokens out of a large number of visual
    input features and then use cross-attention layers interleaved with the LM layers
    to fuse visual information into the language decoding process. The training objective
    is an autoregressive, NLL loss.'
  prefs: []
  type: TYPE_NORMAL
- en: The Perceiver resampler receives spatio-temporal features from the vision encoder
    of image/video inputs to produce fixed-size visual tokens.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The frozen LM is equipped with newly initialized cross-attention layers interleaved
    between the pretrained LM layers. Thus the LM can generate text conditioned on
    the above visual tokens.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Similar to ClipCap, both pretrained models are *frozen* during training and
    thus Flamingo is only trained to harmoniously connect existing, powerful language
    and vision models together. Tha main difference between ClipCap and Flamingo is
    that the former treats the image embedding as simple prefix for LM, while the
    latter uses the gated cross-attention-dense layer to fuse image information. In
    addition, Flamingo incorporates a lot more training data than ClipCap.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2bf98c6271754a9e0d52754eccfe5df0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 15\. Overview of the Flamingo model. (Image source: [Alayrac et al. 2022](https://arxiv.org/abs/2204.14198))'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/800cb1ee227e9e9a0f9a84611bc5faf2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 16\. The architecture illustration and pseudo code of the gated cross-attention-dense
    layer in Flamingo. (Image source: [Alayrac et al. 2022](https://arxiv.org/abs/2204.14198))'
  prefs: []
  type: TYPE_NORMAL
- en: To easily handle text with interleaved images, masking in Flamingo is designed
    such that text token only cross-attends to visual tokens corresponding to the
    *last* preceding image, largely reducing the number of visual tokens that a certain
    text token can see. They found this works better than allowing text tokens to
    attend to all preceding images directly. Text still can attend to all previous
    images because there is a causal self-attention dependency in the text encoder.
    This design can deal with an arbitrary number of images in the context.
  prefs: []
  type: TYPE_NORMAL
- en: They scraped 43 million webpages from the Internet, named MultiModal MassiveWeb
    (M3W) dataset, containing text with interleaved images. In addition, Flamingo
    is also trained on paired image/text and video/text datasets, including [ALIGN,
    LTIP and VTP](#pair-image-text-datasets).
  prefs: []
  type: TYPE_NORMAL
- en: 'Data processing of the Internet dataset includes:'
  prefs: []
  type: TYPE_NORMAL
- en: The input Web page text is processed by inserting `<image>` tags at the location
    of visual inputs, as well as special tokens, `<BOS>` (beginning of sentence) and
    `<EOC>` (end of chunks; always at the end of the document, before any image tag).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: From each document, they sample a random subsequence of $L = 256$ tokens and
    take up to $N = 5$ images included in the sampled sequence (using only the first
    $N$ within that sampled subsequence if there are more, or padding to $N$ if fewer)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A function $\phi: [1,L] \to [0,N]$ is computed to track the text and image
    interleaving order, which assigns to each text position the index of the last
    image/video appearing before this position; 0 if no preceding visual data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since Flamingo is trained on a mixture of three different datasets, it optimizes
    for a weighted sum of dataset-specific NLL losses. Tuning the dataset weights
    is very important for the final performance. In practice, instead of round-robin
    between datasets, they actually sample one batch from each dataset and apply a
    weighted sum of these gradients in each update. Gradient accumulation across different
    heterogeneous datasets can be viewed as a mean to stabilize training, as it reduces
    the gradient variance between each update.
  prefs: []
  type: TYPE_NORMAL
- en: At test time, Flamingo naturally supports few-shot learning since it can work
    with any sequence of interleaved text and images. And more examples in the context
    contribute to better performance.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d05fdf0d17e3e5afcaa4ae7f1c2253fc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 17\. Larger model sizes and more few-shot examples lead to better performance.
    (Image source: [Alayrac et al. 2022](https://arxiv.org/abs/2204.14198))'
  prefs: []
  type: TYPE_NORMAL
- en: Flamingo outperforms SoTA fine-tuned models on 6 out of the 16 tasks despite
    even when not using any fine-tuning but only few-shot prompting. Fine-tuning Flamingo
    is expensive and it is difficult to do hyperparemeter tuning, but it does lead
    to better results.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7553830dcd528a22aca36ad5bc8589db.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 18\. Performance of Flamingo model using different numbers of shots and
    of different sizes, in comparison with SoTA fine-tuned baseline. (Image source:
    [Alayrac et al. 2022](https://arxiv.org/abs/2204.14198))'
  prefs: []
  type: TYPE_NORMAL
- en: '**CoCa** (Contrastive Captioner; [Yu & Wang et al., 2022](https://arxiv.org/abs/2205.01917))
    captures both the merits of contrastive learning and image-to-caption generation.
    It is a model jointly trained with contrastive loss on CLIP-style representation
    and generative loss on image captioning, achieving SoTA zero-shot transfer on
    a variety of multi-modal evaluation tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cc5713a39fcf936bf8800a889548ce5c.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig. 19\. Overview of CoCa training framework.
  prefs: []
  type: TYPE_NORMAL
- en: '(Image source: [Yu & Wang et al., 2022](https://arxiv.org/abs/2205.01917))'
  prefs: []
  type: TYPE_NORMAL
- en: CoCa is pretrained from *scratch*, using web-scale alt-text data [ALIGN](#pair-image-text-datasets)
    and annotated images by treating all labels as texts in [JTB-3B](#pair-image-text-datasets).
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two major training components in CoCa. The final loss is a weighted
    sum of the following two losses, with weight scalars $\lambda_\text{cap}=2.0,
    \lambda_\text{con} = 1.0$.:'
  prefs: []
  type: TYPE_NORMAL
- en: $\mathcal{L}_\text{con}$ - *Dual-encoder contrastive learning* optimizes the
    symmetric contrastive learning loss, similar to CLIP.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '$\mathcal{L}_\text{cap}$ - *Encoder-decoder captioning* has the decoder predict
    the caption based on the latent encoded features from the image encoder, by optimizing
    an autoregressive loss. The text decoder is decoupled into two components, *unimodal*
    and *multimodal*; a good balance is to split the decoder by half for these two
    components:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The bottom unimodal component encodes the input text with causally-masked self-attention.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The top multimodal component applies both causally-masked self-attention and
    cross-attention to the output of the vision encoder.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: CoCa performs better than the contrastive-only model and on par with the captioning-only
    model on VQA. Captioning loss is found to be beneficial to the zero-shot classification
    capacity too.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/96bfd34fae5394dc1f4ecf557a97ebba.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 20\. Illustration of how CoCa can be used to solve various downstream
    tasks at test time. (Image source: [Yu & Wang et al., 2022](https://arxiv.org/abs/2205.01917))'
  prefs: []
  type: TYPE_NORMAL
- en: They use task-specific attention pooling, or attention pooler, as a natural
    task adapter, as they found that a single pooled image embedding helps visual
    recognition tasks (e.g. ImageNet classification), while a more fine-grained embedding
    helps multimodal understanding tasks (e.g. VQA). A pooler is a single multi-head
    attention layer with $n_\text{query}$ learnable queries (note that $\mathbf{X}
    \in \mathbb{R}^{L \times d}$, $\mathbf{W}^q \in \mathbb{R}^{d \times d_q}$, and
    $d_k = d_q$), with the encoder output as both keys and values. CoCa uses attentional
    poolers in pretraining for generative loss $n_\text{query} = 256$ and contrastive
    loss $n_\text{query} = 1$. This enables the model to obtain strong performance
    as a *frozen* encoder where we only learn a new pooler to aggregate features.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/431a3afbe8b5b4b2a596118aac4a29c1.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig. 21\. Pseudo code for CoCa architecture and training.
  prefs: []
  type: TYPE_NORMAL
- en: '(Image source: [Yu & Wang et al., 2022](https://arxiv.org/abs/2205.01917))'
  prefs: []
  type: TYPE_NORMAL
- en: No Training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Finally it is possible to solve vision language tasks by stitching pretrained
    language and vision models together without training any additional parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Decoding Guided with Vision-based Scores
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**MAGiC** (iMAge-Guided text generatIon with CLIP; [Su et al. 2022](https://arxiv.org/abs/2205.02655))
    does [guided decoding](https://lilianweng.github.io/posts/2021-01-02-controllable-text-generation/#guided-decoding)
    according to a CLIP-based score named *magic score* to sample the next token,
    without fine-tuning. The generated text is encouraged to be relevant to the given
    image, while still stay coherent to the previously generated text.'
  prefs: []
  type: TYPE_NORMAL
- en: The next token $x_t$ at a time step $t$ is selected according to the following
    equation. Model confidence and degeneration penalty ([Su et al. 2022](https://arxiv.org/abs/2202.06417))
    are added to avoid corrupted generation from LM.
  prefs: []
  type: TYPE_NORMAL
- en: $$ \begin{aligned} & x_t = \arg\max_{v \in \mathcal{V}^{(k)}} \big\{ (1-\alpha)
    \underbrace{p(v \vert \boldsymbol{x}_{<t})}_\text{model confidence} - \alpha \underbrace{\max_{1
    \leq j \leq t-1} { \text{cosine}(h_v, h_{x_j})}}_\text{degeneration penalty} +
    \beta \underbrace{f_\text{magic}(v \vert \mathcal{I}, \boldsymbol{x}_{<t}, \mathcal{V}^{(k)})}_\text{magic
    score} \big\} \\ \text{where } & f_\text{magic} ( v \vert \mathcal{I}, \mathbf{x}_{<t},
    \mathcal{V}^{(k)} ) = \frac{ \exp(\text{CLIP}(\mathcal{I}, [\boldsymbol{x}_{<t}:v]))
    }{ \sum_{z \in \mathcal{V}^{(k)}} \exp(\text{CLIP}(\mathcal{I}, [\boldsymbol{x}_{<t}:z]))
    } = \frac{ \exp\big({h^\text{image}(\mathcal{I})}^\top h^\text{text}([\boldsymbol{x}_{<t}:v])\big)
    }{ \sum_{z \in \mathcal{V}^{(k)}} \exp\big({h^\text{image}(\mathcal{I})}^\top
    h^\text{text}([\boldsymbol{x}_{<t}:z])\big) } \end{aligned} $$
  prefs: []
  type: TYPE_NORMAL
- en: where $\mathcal{I}$ is the input image; $\mathcal{V}^{(k)}$ contains top-$k$
    possible tokens predicted by the language model $p$; $\boldsymbol{x}_{<t}$ refers
    to the past generated tokens before time step $t$; $h_v$ is the representation
    of the token $v$ computed by LM conditioned on the concatenation of $\boldsymbol{x}_{<t}$
    and $v$; $h^\text{image}(.)$ and $h^\text{text}(.)$ are embeddings generated by
    CLIP image and text encoders, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: MAGiC has decent performance compared to other unsupervised approaches, but
    still has big gaps with supervised methods.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/af6d81ae1dfa1eefec9253f477230eab.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 22\. Image captioning performance on COCO and Flickr30k. (Image source:
    [Su et al. 2022](https://arxiv.org/abs/2205.02655))'
  prefs: []
  type: TYPE_NORMAL
- en: Language as Communication Interface
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For knowledge-based VQA tasks, PICa (Prompts GPT-3 via the use of Image Captions;
    [Yang et al. 2021](https://arxiv.org/abs/2109.05014)) first converts the images
    into captions or tags and then uses few-shot examples to prompt GPT3 to provide
    answers. Image captions or tags are extracted by some existing models (e.g. [VinVL](https://openaccess.thecvf.com/content/CVPR2021/html/Zhang_VinVL_Revisiting_Visual_Representations_in_Vision-Language_Models_CVPR_2021_paper.html))
    or Azure Tagging API. And GPT3 is considered as an unstructured, implicit knowledge
    base.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9d129e64746fd76bf05fdbd613ba380d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 23\. How PICa works for $n$-shot VQA at inference time. (Image source:
    [Yang et al. 2021](https://arxiv.org/abs/2109.05014))'
  prefs: []
  type: TYPE_NORMAL
- en: 'PICa explored two ways to improve few-shot examples to achieve better results:'
  prefs: []
  type: TYPE_NORMAL
- en: In-context examples are selected based on how *similar* they are to the question
    using CLIP embedding.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Multi-query ensembling* is to prompt the model multiple times to get multiple
    answers and the one with highest logprob is selected.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This simple approach with only 16 examples improved SoTA on OK-VQA by +8.6 points
    and got decent performance on VQAv2.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1ceebc33e3551e5655df9947cf46c180.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 24\. Performance of PICa on OK-VQA. "PICa-Base" has random in-context
    examples, while "PICa-Full" incorporates both similar in-context example selection
    and multi-query ensembling. (Image source: [Yang et al. 2021](https://arxiv.org/abs/2109.05014))'
  prefs: []
  type: TYPE_NORMAL
- en: '**Socratic Models** (SM) ([Zeng et al. 2022](https://arxiv.org/abs/2204.00598))
    is a framework to *compose* multiple pretrained models for different modality
    via language (prompting) into one model without further training. Here language
    is considered as the intermediate representation by which different models can
    exchange information. The key idea is to use *multi-model multimodal prompting*,
    in which output of a non-language model is inserted into a language prompt and
    then it is used for LM for reasoning.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s examine a concrete example. Given an ego-centric video (images + audio),
    SM can produce a summary of the person’s activity using text-to-text LM, image-to-text
    VLM and speech-to-text ALM. They are chained as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/25829fe3dfd5f930d17b13a851a25de8.png)'
  prefs: []
  type: TYPE_IMG
- en: '(Image source: [Zeng et al. 2022](https://arxiv.org/abs/2204.00598))'
  prefs: []
  type: TYPE_NORMAL
- en: the VLM detects visual entities;
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: the LM suggests sounds that may be heard;
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: the ALM chooses the most likely sound;
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: the LM suggests possible activities;
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: the VLM ranks the most likely activity;
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: the LM generates a summary of the Socratic interaction.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/cf05fc42ca2fada1f27920632fbf608f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 25\. Illustration of the Socratic Model solution for image captioning.
    (Image source: [Zeng et al. 2022](https://arxiv.org/abs/2204.00598))'
  prefs: []
  type: TYPE_NORMAL
- en: SM can generate image captions by first using VLM to zero-shot predict different
    place categories, object categories, image type and the number of people; and
    then the VLM-filled language prompt is fed into a causal LM to generate caption
    candidates. The Socratic approach still has performance gap with ClipCap on image
    captioning but pretty decent given it does not involve any training.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6771cae635b1221253b8389292b66651.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 26\. Comparison of image captioning performance of different models on
    random 100 COCO text examples. (Image source: [Zeng et al. 2022](https://arxiv.org/abs/2204.00598))'
  prefs: []
  type: TYPE_NORMAL
- en: SM framework is very flexible and can be used on a lot more complicated tasks
    other than image captions. For example, the egocentric perception (User inputs
    + VLM + LM + ALM) task is to take as inputs egocentric videos to (1) summarize
    content; (2) answer free-form reasoning questions; (3) and do forecasting.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7d91575b24543282e6c6f8ae3f5c7145.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 27\. The Socratic Model approach for generating captions and question
    answering based on the egocentric videos. (Image source: [Zeng et al. 2022](https://arxiv.org/abs/2204.00598))'
  prefs: []
  type: TYPE_NORMAL
- en: Datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Image Caption Datasets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*MS COCO* ([Chen et al. 2015](https://arxiv.org/abs/1504.00325)): contains
    328K images and each paired with 5 independent captions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*NoCaps* ([Agrawal et al., 2019](https://arxiv.org/abs/1812.08658)) is designed
    to measure generalization to unseen classes and concepts, where in-domain contains
    images portraying only COCO classes, near-domain contains both COCO and novel
    classes, and out-of-domain consists of only novel classes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Conceptual Captions* ([Sharma et al. 2018](https://aclanthology.org/P18-1238/))
    contains 3 million pairs of images and captions, mined from the web and post-processed.
    To focus on the concepts, specific entities in this dataset are replaced with
    general notions (e.g. a politician’s name is replaced with “politician”)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Crisscrossed Captions (CxC)* ([Parekh et al. 2021](https://arxiv.org/abs/2004.15020))
    contains 247,315 human-labeled annotations including positive and negative associations
    between image pairs, caption pairs and image-caption pairs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Concadia* ([Kreiss et al. 2021](https://arxiv.org/abs/2104.08376)) is a Wikipedia-based
    dataset containing 96,918 images with corresponding English-language descriptions,
    captions, and surrounding context.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pair Image-Text Datasets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: (*) Not a public dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '*ALIGN* ([Jia et al., 2021](https://arxiv.org/abs/2102.05918)) contains 1.8
    billion images with alt-text. The dataset is large but noisy with only minimal
    frequency-based filtration.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(*) *LTIP* (Long text & image pairs; [Alayrac et al. 2022](https://arxiv.org/abs/2204.14198)):
    312 million images, paired with descriptive captions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(*) *VTP* (Video & text pairs; [Alayrac et al. 2022](https://arxiv.org/abs/2204.14198)):
    27 million short videos (~22 seconds on average), paired with descriptive captions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (*) *JFT-300M* / *JFT-3B* are internal Google datasets, containing 300M / 3B
    images annotated with a class-hierarchy of around 30k labels via a semi-automatic
    pipeline. Thus the data and associated labels are noisy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluation Tasks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Visual Question-Answering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Given an image and a question, the task is to correctly answer the question.
  prefs: []
  type: TYPE_NORMAL
- en: '*VQAv2* ([Goyal et al., 2017](https://arxiv.org/abs/1612.00837)) contains 1+
    million questions about 200K images from COCO.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*OK-VQA* ([Marino et al. 2019](https://arxiv.org/abs/1906.00067)) contains
    14K open-ended questions that require outside knowledge (e.g. from Wikipedia).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*A-OKVQA*: the augmented successor of OK-VQA, with no overlapped questions
    with OK-VAQ.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*TextVQA* ([Singh, et al. 2019](https://arxiv.org/abs/1904.08920)) contains
    45,336 questions on 28,408 images that require reasoning about text to answer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*VizWiz* ([Gurari, et al. 2018](https://arxiv.org/abs/1802.08218)) contains
    over 31,000 visual questions originating from blind people who each took a picture
    using a mobile phone and recorded a spoken question about it, together with 10
    crowdsourced answers per visual question.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visual Language Reasoning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*VCR* (Visual Commonsense Reasoning; [Zellers et al. 2018](https://arxiv.org/abs/1811.10830))
    contains 290k multiple choice QA questions derived from 110k movie scenes, with
    focus on visual commonsense.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*NLVR2* (Natural Language for Visual Reasoning; [Suhr et al. 2019](https://arxiv.org/abs/1811.00491))
    contains 100k+ examples of sentences paired with web images and the task is to
    determine whether a natural language caption is true about a pair of images, with
    a focus on semantic diversity.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Flickr30K* ([Jia et al. 2015](https://arxiv.org/abs/1509.04942)) contains
    30k images collected from Flickr and 250k annotations and the task is to select
    the bounding regions given spans of a sentence.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*SNLI-VE* (Visual Entailment; [Xie et al. 2019](https://arxiv.org/abs/1901.06706))
    is built on top of SNLI and Flickr30K and the task is to reason about the relationship
    between an image premise and a text hypothesis.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Video QA and Understanding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*MSR-VTT* (MSR Video to Text; [Xu et al. 2016](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/06/cvpr16.msr-vtt.tmei_-1.pdf))
    contains 10K web video clips with 41.2 hours and 200K clip-sentence pairs in total;
    the task is to translate videos to text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*ActivityNet-QA* ([Yu et al. 2019](https://arxiv.org/abs/1906.02467)) contains
    58,000 human-annotated QA pairs on 5,800 videos derived from the popular [ActivityNet](http://activity-net.org/index.html)
    dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*TGIF* (Tumblr GIF; [Li et al. .2016](https://arxiv.org/abs/1604.02748)) contains
    100K animated GIFs and 120K sentences describing visual content of the animated
    GIFs, randomly selected posts published between May and June of 2015 on Tumblr.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*TGIF-QA* contains 165K QA pairs for the animated GIFs from the TGIF dataset.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*LSMDC* (Large Scale Movie Description Challenge; [Rohrbach et al. 2015](https://arxiv.org/abs/1501.02530))
    contains 118,081 short video clips extracted from 202 movies. Each video has a
    caption, either extracted from the movie script or from transcribed DVS (descriptive
    video services) for the visually impaired.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*TVQA* ([Lei et al. 2018](https://arxiv.org/abs/1809.01696)) / *TVQA+* ([Lei
    et al. 2019](https://arxiv.org/abs/1904.11574)) is a large-scale video QA dataset
    based on 6 popular TV shows (Friends, The Big Bang Theory, How I Met Your Mother,
    House M.D., Grey’s Anatomy, Castle). It consists of 152.5K QA pairs from 21.8K
    video clips, spanning over 460 hours of video.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*DramaQA* ([Choi et al. 2020](https://arxiv.org/abs/2005.03356)) is a large-scale
    video QA dataset based on a Korean popular TV show, “Another Miss Oh”. This dataset
    contains four levels of QA on difficulty and multi-level character-centered story
    descriptions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*VLEP* (Video-and-Language Event Prediction; [Lei et al. 2020](https://arxiv.org/abs/2010.07999))
    contains 28,726 future event prediction examples (along with their rationales)
    from 10,234 diverse TV Show and YouTube Lifestyle Vlog video clips.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Citation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Cited as:'
  prefs: []
  type: TYPE_NORMAL
- en: Weng, Lilian. (Jun 2022). Generalized visual language models. Lil’Log. https://lilianweng.github.io/posts/2022-06-09-vlm/.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Or
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] Li et al. [“VisualBERT: A Simple and Performant Baseline for Vision and
    Language.”](https://arxiv.org/abs/1908.03557) arXiv preprint:1908.03557 (2019).'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] Wang et al. [“SimVLM: Simple Visual Language Model Pretraining with Weak
    Supervision.”](https://arxiv.org/abs/2108.10904) ICLR 2022.'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] Aghajanyan, et al. [“CM3: A Causal Masked Multimodal Model of the Internet.”](https://arxiv.org/abs/2201.07520)
    arXiv preprint arXiv: 2201.07520 (2022).'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] Tsimpoukelli et al. [“Multimodal Few-Shot Learning with Frozen Language
    Models.”](https://arxiv.org/abs/2106.13884) NeuriPS 2021.'
  prefs: []
  type: TYPE_NORMAL
- en: '[5] Mokady, Hertz & Hertz. [“ClipCap: CLIP Prefix for Image Captioning.”](https://arxiv.org/abs/2111.09734)
    2021.'
  prefs: []
  type: TYPE_NORMAL
- en: '[6] Chen et al. [“VisualGPT: Data-efficient Adaptation of Pretrained Language
    Models for Image Captioning.”](https://arxiv.org/abs/2102.10407) arXiv preprint
    arXiv:2111.09734 (2021).'
  prefs: []
  type: TYPE_NORMAL
- en: '[7] Luo et al. [“A Frustratingly Simple Approach for End-to-End Image Captioning.”](https://arxiv.org/abs/2201.12723)
    arXiv preprint arXiv:2201.12723 (2022).'
  prefs: []
  type: TYPE_NORMAL
- en: '[8] Zellers et al. [“MERLOT: Multimodal neural script knowledge models.”](https://arxiv.org/abs/2106.02636)
    NeuriPS 2021.'
  prefs: []
  type: TYPE_NORMAL
- en: '[9] Alayrac et al. [“Flamingo: a Visual Language Model for Few-Shot Learning.”](https://arxiv.org/abs/2204.14198)
    arXiv preprint arXiv:2204.14198 (2022).'
  prefs: []
  type: TYPE_NORMAL
- en: '[10] Yu & Wang et al. [“CoCa: Contrastive Captioners are Image-Text Foundation
    Models.”](https://arxiv.org/abs/2205.01917) arXiv preprint arXiv:2205.01917 (2022).'
  prefs: []
  type: TYPE_NORMAL
- en: '[11] Yang et al. [“An Empirical Study of GPT-3 for Few-Shot Knowledge-Based
    VQA.”](https://arxiv.org/abs/2109.05014) arXiv preprint arXiv:2109.05014 (2021).'
  prefs: []
  type: TYPE_NORMAL
- en: '[12] Su et al. [“Language models can see: Plugging visual controls in text
    generation.”](https://arxiv.org/abs/2205.02655) arXiv preprint arXiv:2205.02655
    (2022).'
  prefs: []
  type: TYPE_NORMAL
- en: '[13] Zeng et al. [“Socratic Models: Composing Zero-Shot Multimodal Reasoning
    with Language.”](https://arxiv.org/abs/2204.00598) arXiv preprint arXiv:2204.00598
    (2022).'
  prefs: []
  type: TYPE_NORMAL
