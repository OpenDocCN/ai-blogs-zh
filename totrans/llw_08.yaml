- en: Generalized Visual Language Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通用视觉语言模型
- en: 原文：[https://lilianweng.github.io/posts/2022-06-09-vlm/](https://lilianweng.github.io/posts/2022-06-09-vlm/)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://lilianweng.github.io/posts/2022-06-09-vlm/](https://lilianweng.github.io/posts/2022-06-09-vlm/)
- en: Processing images to generate text, such as image captioning and visual question-answering,
    has been studied for years. Traditionally such systems rely on an object detection
    network as a vision encoder to capture visual features and then produce text via
    a text decoder. Given a large amount of existing literature, in this post, I would
    like to only focus on one approach for solving vision language tasks, which is
    to *extend pre-trained [generalized language models](https://lilianweng.github.io/posts/2019-01-31-lm/)
    to be capable of consuming visual signals*.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 处理图像以生成文本，如图像字幕和视觉问答，已经研究多年。传统上，这种系统依赖于对象检测网络作为视觉编码器来捕获视觉特征，然后通过文本解码器生成文本。鉴于大量现有文献，在本文中，我只想专注于解决视觉语言任务的一种方法，即*扩展预训练的[通用语言模型](https://lilianweng.github.io/posts/2019-01-31-lm/)以能够接收视觉信号*。
- en: 'I roughly group such vision language models (VLMs) into four buckets:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我大致将这种视觉语言模型（VLMs）分为四类：
- en: Translating images into embedding features that can be jointly trained with
    token embeddings.
  id: totrans-4
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将图像翻译为嵌入特征，可以与标记嵌入一起进行联合训练。
- en: Learning good image embeddings that can work as a prefix for a frozen, pre-trained
    language model.
  id: totrans-5
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 学习良好的图像嵌入，可以作为冻结的、预训练语言模型的前缀。
- en: Using a specially designed cross-attention mechanism to fuse visual information
    into layers of the language model.
  id: totrans-6
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用特殊设计的交叉注意力机制将视觉信息融入语言模型的层中。
- en: Combine vision and language models without any training.
  id: totrans-7
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将视觉和语言模型结合而无需任何训练。
- en: Jointly Training with Image and Text
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 与图像和文本联合训练
- en: One straightforward approach to fuse visual information into language models
    is to treat images as normal text tokens and train the model on a sequence of
    joint representations of both text and images. Precisely, images are divided into
    multiple smaller patches and each patch is treated as one “token” in the input
    sequence.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 将视觉信息融入语言模型的一种直接方法是将图像视为普通文本标记，并在同时表示文本和图像的序列上训练模型。具体来说，图像被划分为多个较小的补丁，每个补丁被视为输入序列中的一个“标记”。
- en: '**VisualBERT** ([Li et al. 2019](https://arxiv.org/abs/1908.03557)) feeds both
    text inputs and image regions into [BERT](https://lilianweng.github.io/posts/2019-01-31-lm/#bert)
    such that it is able to discover the internal alignment between images and text
    with self-attention mechanism.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**VisualBERT**（[Li等，2019](https://arxiv.org/abs/1908.03557)）将文本输入和图像区域输入到[BERT](https://lilianweng.github.io/posts/2019-01-31-lm/#bert)中，以便通过自注意机制发现图像和文本之间的内部对齐。'
- en: '![](../Images/04805c92244b2a6ca1c28a058c909698.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/04805c92244b2a6ca1c28a058c909698.png)'
- en: 'Fig. 1\. VisualBERT is trained on the combination of both text and image embeddings.
    (Image source: [Li et al. 2019](https://arxiv.org/abs/1908.03557))'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图1。VisualBERT在文本和图像嵌入的组合上进行训练。（图片来源：[Li等，2019](https://arxiv.org/abs/1908.03557)）
- en: 'Similar to [text embedding in BERT](https://lilianweng.github.io/posts/2019-01-31-lm/#input-embedding),
    each visual embedding in VisualBERT also sums up three types of embeddings, tokenized
    features $f_o$, segmentation embedding $f_s$ and position embedding $f_p$, precisely:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于[BERT中的文本嵌入](https://lilianweng.github.io/posts/2019-01-31-lm/#input-embedding)，VisualBERT中的每个视觉嵌入也总结了三种类型的嵌入，标记化特征
    $f_o$，分段嵌入 $f_s$ 和位置嵌入 $f_p$，具体如下：
- en: $f_o$ is a visual feature vector computed for a bounding region of the image
    by a convolutional neural network;
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: $f_o$ 是由卷积神经网络计算出的图像边界区域的视觉特征向量；
- en: $f_s$ is a segment embedding to indicate whether the embedding is for vision
    not for text;
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: $f_s$ 是一个段落嵌入，用于指示嵌入是否用于视觉而非文本；
- en: $f_p$ is a position embedding used for aligning the order of bounding regions.
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: $f_p$ 是用于对齐边界区域顺序的位置嵌入。
- en: 'The model is trained on MS COCO image caption dataset with both text and image
    as inputs to predict text captions, using two visually-grounded language model
    objectives:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型在 MS COCO 图像字幕数据集上进行训练，同时将文本和图像作为输入，以预测文本字幕，使用两种视觉基础语言模型目标：
- en: '*[MLM](https://lilianweng.github.io/posts/2019-01-31-lm/#pre-training-tasks)
    with the image*. The model needs to predict masked text tokens, while image embeddings
    always stay not masked.'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*[MLM](https://lilianweng.github.io/posts/2019-01-31-lm/#pre-training-tasks)与图像*。模型需要预测被屏蔽的文本标记，而图像嵌入始终保持未被屏蔽。'
- en: '*Sentence-image prediction*. When provided with an image and two associated
    captions, one of two captions might be a random unrelated caption with 50% probability.
    The model is asked to distinguish these two situations.'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*句子-图像预测*。当提供一张图像和两个相关的字幕时，其中一个字幕可能是一个与之无关的随机字幕，概率为50%。模型被要求区分这两种情况。'
- en: According to ablation experiments, the most important configuration is to fuse
    visual information early on into the transformer layers and to pretrain the model
    on the COCO caption dataset. Initialization from a pre-trained BERT and the adoption
    of the sentence-image prediction training objective have relatively small impacts.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 根据消融实验，最重要的配置是将视觉信息早期融入到变压器层中，并在COCO字幕数据集上对模型进行预训练。从预训练的BERT初始化和采用句子-图像预测训练目标对模型影响较小。
- en: '![](../Images/33fd1da995e446900a8f24fa7ffe773b.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/33fd1da995e446900a8f24fa7ffe773b.png)'
- en: Fig. 2\. Ablation study results of VisualBERT on NLVR.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图2。VisualBERT在NLVR上的消融研究结果。
- en: '(Image source: [Li et al. 2019](https://arxiv.org/abs/1908.03557))'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: (图片来源：[李等人，2019](https://arxiv.org/abs/1908.03557))
- en: VisualBERT outperforms SoTA at the time on NLVR and Flickr30K, but still has
    some performance gap with SoTA on VQA.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: VisualBERT在NLVR和Flickr30K上的表现超越了当时的最先进技术，但在VQA上仍存在一些性能差距。
- en: '**SimVLM** (Simple Visual Language Model; [Wang et al. 2022](https://arxiv.org/abs/2108.10904))
    is a simple *prefix language model*, where the prefix sequence is processed with
    bi-directional attention like BERT, but the main input sequence only has causal
    attention like [GPT](#gpt). Images are encoded as prefix tokens such that the
    model can fully consume the visual information and then generates associated text
    in an autoregressive manner.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**SimVLM**（简单视觉语言模型；[王等人，2022](https://arxiv.org/abs/2108.10904)）是一个简单的*前缀语言模型*，其中前缀序列像BERT一样进行双向注意力处理，但主要输入序列只有像[GPT](#gpt)那样的因果关注。图像被编码为前缀标记，以便模型可以充分利用视觉信息，然后以自回归方式生成相关文本。'
- en: Inspired by [ViT](https://arxiv.org/abs/2010.11929) and [CoAtNet](https://arxiv.org/abs/2106.04803),
    SimVLM splits the image into smaller patches in a flatten 1D sequence of patches.
    They use the convolutional stage consisting of the first 3 blocks of ResNet to
    extract contextualized patches and this setup is found to work better than a naive
    linear projection.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 受[ViT](https://arxiv.org/abs/2010.11929)和[CoAtNet](https://arxiv.org/abs/2106.04803)的启发，SimVLM将图像分割成较小的补丁，在一个扁平的1D补丁序列中。他们使用由ResNet的前3个块组成的卷积阶段来提取具有上下文的补丁，这种设置被发现比天真的线性投影效果更好。
- en: '![](../Images/7903fee47cc616db23c48aceaa75fe08.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7903fee47cc616db23c48aceaa75fe08.png)'
- en: 'Fig. 3\. Training architecture for SimVLM, where the image patches are processed
    by the cross-attention encoder and the text decoder has causal attention. (Image
    source: [Wang et al. 2022](https://arxiv.org/abs/2108.10904))'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图3。SimVLM的训练架构，其中图像补丁由交叉注意力编码器处理，文本解码器具有因果关注。（图片来源：[王等人，2022](https://arxiv.org/abs/2108.10904))
- en: Training data for SimVLM consists of a large number of image-text pairs from
    ALIGN ([Jia et al. 2021](https://arxiv.org/abs/2102.05918)) and text-only data
    from C4 dataset ([Raffel et al. 2019](https://arxiv.org/abs/1910.10683)). They
    mix the two pretraining datasets within each batch, containing 4,096 image-text
    pairs (ALIGN) and 512 text-only documents (C4).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: SimVLM的训练数据包括来自ALIGN的大量图像-文本对（[贾等人，2021](https://arxiv.org/abs/2102.05918)）和来自C4数据集的仅文本数据（[Raffel等人，2019](https://arxiv.org/abs/1910.10683)）。他们在每个批次中混合这两个预训练数据集，包含4,096个图像-文本对（ALIGN）和512个仅文本文档（C4）。
- en: According to ablation studies, it is important to have both image-text and text-only
    data for training. The PrefixLM objective outperforms both [span corruption](https://arxiv.org/abs/1910.10683)
    and naive LM.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 根据消融研究，对于训练来说同时拥有图像-文本和仅文本数据是重要的。PrefixLM目标优于[span corruption](https://arxiv.org/abs/1910.10683)和天真LM。
- en: '![](../Images/3219ec9ffff101f12a0876523adad212.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3219ec9ffff101f12a0876523adad212.png)'
- en: Fig. 4\. Ablation study results of SimVLM on VQA.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图4。SimVLM在VQA上的消融研究结果。
- en: '(Image source: [Wang et al. 2022](https://arxiv.org/abs/2108.10904))'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: (图片来源：[王等人，2022](https://arxiv.org/abs/2108.10904))
- en: '**CM3** (Causally-Masked Multimodal Modeling; [Aghajanyan, et al. 2022](https://arxiv.org/abs/2201.07520))
    is a hyper-text language model, learning to generate the content (hypertext markup,
    hyperlinks and images) of large scale HTML web pages of CC-NEWS and Wikipedia
    articles. The resulting CM3 models can generate rich structured, multi-modal outputs
    while conditioning on arbitrary masked document contexts.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '**CM3**（因果屏蔽多模态建模；[Aghajanyan等人，2022](https://arxiv.org/abs/2201.07520)）是一个超文本语言模型，学习生成CC-NEWS和维基百科文章的大规模HTML网页的内容（超文本标记、超链接和图像）。生成的CM3模型可以在任意屏蔽文档上下文的条件下生成丰富结构化的多模态输出。'
- en: Architecture-wise, CM3 is an autoregressive model. However, in order to combine
    causal and masked language modeling, CM3 also masks out a small number of long
    token spans and tries to generate them at the *end* of the sequences.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在架构上，CM3是一个自回归模型。然而，为了结合因果和屏蔽语言建模，CM3还会屏蔽一小部分长标记跨度，并尝试在序列的*末尾*生成它们。
- en: '![](../Images/148134bc59d948026f718cfea3362875.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/148134bc59d948026f718cfea3362875.png)'
- en: Fig. 5\. Illustration of how a causally masked language model works.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图5. 描述因果屏蔽语言模型的工作原理。
- en: '(Image source: [Aghajanyan, et al. 2022](https://arxiv.org/abs/2201.07520))'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: (图片来源：[Aghajanyan等人，2022](https://arxiv.org/abs/2201.07520))
- en: The training dataset for CM3 contains close to 1T Web data. During preprocessing,
    images are first downloaded from `src` and resized to 256 x 256 with random cropping.
    Then they are tokenized by [VQVAE-GAN](https://arxiv.org/abs/2012.09841), resulting
    in 256 tokens per image. These tokens, joined with spaces, are inserted back into
    the `src` attribute.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: CM3的训练数据集包含接近1T的Web数据。在预处理过程中，首先从`src`下载图像，并将其调整大小为256 x 256，并进行随机裁剪。然后，它们通过[VQVAE-GAN](https://arxiv.org/abs/2012.09841)进行标记化，每个图像产生256个标记。这些标记与空格连接后插入回`src`属性。
- en: 'CM3 can be used to complete several types of tasks by prompt engineering:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: CM3可以通过提示工程完成几种类型的任务：
- en: 'Image in-filling:'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像填充：
- en: '[PRE0]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Conditional image in-filling:'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有条件的图像填充：
- en: '[PRE1]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Conditional image generation:'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有条件的图像生成：
- en: '[PRE2]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Image captions:'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图片说明：
- en: '[PRE3]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Entity disambiguation
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实体消歧
- en: '[PRE4]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Learned Image Embedding as (Frozen) LM Prefix
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 作为（Frozen）LM前缀的学习图像嵌入
- en: What if we don’t want to change the language model parameters when adapting
    it to handle visual signals? Instead we learn such an embedding space for images
    that it is compatible with the language model’s.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们不想在调整语言模型以处理视觉信号时改变语言模型参数怎么办？相反，我们学习这样一个图像嵌入空间，使其与语言模型兼容。
- en: Inspired by [prefix](https://arxiv.org/abs/2101.00190) or [prompt](https://arxiv.org/abs/2104.08691)
    [tuning](https://lilianweng.github.io/posts/2021-01-02-controllable-text-generation/#prefix-tuning),
    both **Frozen** ([Tsimpoukelli et al. 2021](https://arxiv.org/abs/2106.13884))
    and **ClipCap** ([Mokady, Hertz & Hertz, 2021](https://arxiv.org/abs/2111.09734))
    only update the parameters of the vision module during training to produce image
    embeddings that can work with a pretrained, *frozen* language model. Both are
    trained with aligned image caption [datasets](#image-caption-datasets) to produce
    the next text token in caption conditioned on the image and previous text tokens.
    The powerful language capability is retained by freezing LM parameters. In addition,
    even though such setup is trained with limited image caption data, they can also
    rely on the encyclopedic knowledge of the language model at test time.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 受到[prefix](https://arxiv.org/abs/2101.00190)或[prompt](https://arxiv.org/abs/2104.08691)[调整](https://lilianweng.github.io/posts/2021-01-02-controllable-text-generation/#prefix-tuning)的启发，**Frozen**（[Tsimpoukelli等人，2021](https://arxiv.org/abs/2106.13884)）和**ClipCap**（[Mokady,
    Hertz & Hertz, 2021](https://arxiv.org/abs/2111.09734)）在训练期间仅更新视觉模块的参数，以生成可以与预训练的*冻结*语言模型配合使用的图像嵌入。两者都是使用对齐的图像标题[数据集](#image-caption-datasets)进行训练，以在图像和先前文本标记的条件下生成标题中的下一个文本标记。通过冻结LM参数来保留强大的语言能力。此外，即使这种设置是使用有限的图像标题数据进行训练的，它们在测试时也可以依赖语言模型的百科知识。
- en: The vision encoder of Frozen is based on NF-ResNet-50 and uses the final output
    vector of the NF-Resnet after the global pooling layer. The Frozen VLM can be
    used as a multi-model few-shot learner to adapt to new tasks at test time for
    zero-shot or few-shot transfer with a sequence of interleaved images and text.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: Frozen的视觉编码器基于NF-ResNet-50，并在全局池化层之后使用NF-Resnet的最终输出向量。Frozen VLM可以作为多模型少样本学习器，在测试时用于零样本或少样本转移，使用交错图像和文本序列。
- en: '![](../Images/39915668c65743b6afe797fc2c883f3a.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/39915668c65743b6afe797fc2c883f3a.png)'
- en: 'Fig. 6\. Illustration of Frozen model (left) training architecture and (right)
    testing pipeline. (Image source: [Tsimpoukelli et al. 2021](https://arxiv.org/abs/2106.13884))'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图6. 冻结模型（左）训练架构和（右）测试流程的示意图。（图片来源：[Tsimpoukelli等人，2021](https://arxiv.org/abs/2106.13884)）
- en: Experiments showed that fine-tuning the pre-trained LM interestingly leads to
    worse performance on VQA tasks. It is important to initialize the language model
    from a pre-trained version, as training from scratch (${Frozen}_\text{scratch}$)
    does not show any meaningful progress. The baseline ${Frozen}_\text{train-blind}$
    blacks out the image but still can achieve decent performance because of the innate
    power of the pre-trained LM.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 实验证明，微调预训练的LM有趣地导致VQA任务表现更差。从预训练版本初始化语言模型非常重要，因为从头开始训练（${Frozen}_\text{scratch}$）没有显示出任何有意义的进展。基线${Frozen}_\text{train-blind}$虽然将图像变黑，但仍然能够取得不错的表现，这是因为预训练LM的固有能力。
- en: '![](../Images/6e96307ec485a14c317cde8a72f08961.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6e96307ec485a14c317cde8a72f08961.png)'
- en: 'Fig. 7\. Performance of different versions of Frozen on (left) VQAv2 and (right)
    OKVQA, trained on Conceptual Captions. "Frozen scratch" does not load a pre-trained
    LM and is trained from scratch. "Frozen finetuned" has the language model finetuned,
    while "Frozen" keeps LM frozen. "Frozen train-blind" blacks out the image. (Image
    source: [Tsimpoukelli et al. 2021](https://arxiv.org/abs/2106.13884))'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图7. 在概念字幕上训练的Frozen不同版本在（左）VQAv2和（右）OKVQA上的表现。“Frozen scratch”不加载预训练的LM，而是从头开始训练。“Frozen
    finetuned”对语言模型进行了微调，而“Frozen”保持LM冻结。“Frozen train-blind”将图像变黑。（图片来源：[Tsimpoukelli等人，2021](https://arxiv.org/abs/2106.13884)）
- en: ClipCap relies on [CLIP](https://lilianweng.github.io/posts/2021-05-31-contrastive/#clip)
    ([Radford et al. 2021](https://arxiv.org/abs/2103.00020)) for vision encoding,
    but it needs to be processed by a light mapping network $F$ such that image embedding
    vectors are translated into the same semantic space as the pre-trained LM. The
    network $F$ maps CLIP embeddings into a sequence of $k$ embedding vectors, each
    with the same dimension as a word embedding in GPT2\. Increasing the prefix size
    $k$ helps improve the performance. Both CLIP vision encoder and the LM are *frozen*
    during training and only the mapping network $F$ is learned. They found that when
    LM is frozen, $F$ should be a transformer, with 8 multi-head self-attention layers
    with 8 heads each, but when LM can be fine-tuned, a MLP is enough.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: ClipCap依赖于[CLIP](https://lilianweng.github.io/posts/2021-05-31-contrastive/#clip)（[Radford等人，2021](https://arxiv.org/abs/2103.00020)）进行视觉编码，但需要通过一个轻量级映射网络$F$进行处理，使图像嵌入向量转换为与预训练LM相同的语义空间。网络$F$将CLIP嵌入映射为一系列$k$个嵌入向量，每个向量的维度与GPT2中的单词嵌入相同。增加前缀大小$k$有助于提高性能。在训练期间，CLIP视觉编码器和LM都是*冻结*的，只有映射网络$F$是可学习的。他们发现，当LM被冻结时，$F$应该是一个transformer，具有8个多头自注意力层，每个层有8个头，但当LM可以进行微调时，一个MLP就足够了。
- en: Even though ClipCap only trains such a minimum set of parameters, it still achieves
    decent performance on image captioning tasks, comparable with SoTA at the time
    (e.g. [Oscar](https://arxiv.org/abs/2004.06165), [VLP](https://arxiv.org/abs/1909.11059),
    [BUTD](https://arxiv.org/abs/1707.07998)). Hence they postulate that “the CLIP
    space already encapsulates the required information, and adapting it towards specific
    styles does not contribute to flexibility.”
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 即使ClipCap只训练了一组最少的参数，它仍然在图像字幕任务上取得了不错的表现，与当时的最先进技术（例如[Oscar](https://arxiv.org/abs/2004.06165)，[VLP](https://arxiv.org/abs/1909.11059)，[BUTD](https://arxiv.org/abs/1707.07998)）相媲美。因此，他们假设“CLIP空间已经包含了所需的信息，并且将其调整到特定风格并不会增加灵活性。”
- en: '![](../Images/7a14eaa5ec5445a4dd4407c35565b388.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7a14eaa5ec5445a4dd4407c35565b388.png)'
- en: 'Fig. 8\. Overview of ClipCap training pipeline where only the mapping network
    needs to be train to transform CLIP image embedding to work with the pre-trained
    LM. (Image source: [Mokady, Hertz & Hertz, 2021](https://arxiv.org/abs/2111.09734))'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图8. ClipCap训练流程概述，只需训练映射网络以将CLIP图像嵌入转换为与预训练LM配合的形式。（图片来源：[Mokady，Hertz＆Hertz，2021](https://arxiv.org/abs/2111.09734)）
- en: The fun fact is - because ClipCap translates CLIP image embeddings into LM space,
    the processed prefixes can be even interpreted as words.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的事实是 - 因为ClipCap将CLIP图像嵌入转换为LM空间，处理后的前缀甚至可以被解释为单词。
- en: '![](../Images/a71cee53eb073e7cc9eb78c63882d33a.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a71cee53eb073e7cc9eb78c63882d33a.png)'
- en: 'Fig. 9\. The learned image embedding can be interpreted as text, containing
    words related to the image context. (Image source: [Mokady, Hertz & Hertz, 2021](https://arxiv.org/abs/2111.09734))'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9\. 学习到的图像嵌入可以被解释为文本，包含与图像内容相关的词语。（图片来源：[Mokady, Hertz & Hertz, 2021](https://arxiv.org/abs/2111.09734)）
- en: Text-Image Cross-Attention Fuse Mechanisms
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本-图像交叉注意力融合机制
- en: To more efficiently fuse visual information into different layers of the language
    model, we can consider a specially designed cross-attention fuse mechanism to
    balance the mixture of text generation capacity and visual information.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更有效地将视觉信息融入语言模型的不同层中，我们可以考虑一种特别设计的交叉注意力融合机制，以平衡文本生成能力和视觉信息的混合。
- en: '**VisualGPT** ([Chen et al. 2021](https://arxiv.org/abs/2102.10407)) employs
    a self-resurrecting encoder-decoder attention mechanism to quickly adapt the pre-trained
    LM with a small amount of in-domain image-text data.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '**VisualGPT**（[Chen et al. 2021](https://arxiv.org/abs/2102.10407)）采用了一种自我复活的编码器-解码器注意力机制，快速适应预训练的
    LM 与少量领域内的图像文本数据。'
- en: '![](../Images/1471812a350408ba124b3fc400ff98d1.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1471812a350408ba124b3fc400ff98d1.png)'
- en: 'Fig. 10\. Illustration of VisualGPT architecture. (Image source: [Chen et al.
    2021](https://arxiv.org/abs/2102.10407))'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10\. VisualGPT 架构示意图。（图片来源：[Chen et al. 2021](https://arxiv.org/abs/2102.10407)）
- en: 'Let $I$ be the output of a visual encoder and $H$ be the hidden state of the
    LM decoder. VisualGPT introduced a self-resurrecting activation unit (SRAU) to
    control the tradeoff between a mixture of pre-trained linguistic information $H$
    and visual component, $\text{EncDecAttn}(H, I)$ via two complementary gates $B^\text{vis}$
    and $B^\text{lan}$:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 设 $I$ 为视觉编码器的输出，$H$ 为 LM 解码器的隐藏状态。VisualGPT 引入了一种自我复活激活单元（SRAU），通过两个互补门 $B^\text{vis}$
    和 $B^\text{lan}$ 控制预训练语言信息 $H$ 和视觉组件 $\text{EncDecAttn}(H, I)$ 的权衡：
- en: $$ \begin{aligned} & B^\text{vis} \otimes \text{EncDecAttn}(H, I) + B^\text{lan}
    \otimes H \\ \text{where } & B^\text{vis}[i,j] = \sigma(H[i,j]) \mathbb{1}[\sigma(H[i,j])
    > \tau] \\ & B^\text{lan}[i,j] = (1 - \sigma(H[i,j])) \mathbb{1}[1 - \sigma(H[i,j])
    > \tau] \\ \end{aligned} $$ where $\otimes$ is element-wise multiplication and
    $[i,j]$ denotes one element in the matrix. $\tau$ is a predefined threshold hyperparameter.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \begin{aligned} & B^\text{vis} \otimes \text{EncDecAttn}(H, I) + B^\text{lan}
    \otimes H \\ \text{其中 } & B^\text{vis}[i,j] = \sigma(H[i,j]) \mathbb{1}[\sigma(H[i,j])
    > \tau] \\ & B^\text{lan}[i,j] = (1 - \sigma(H[i,j])) \mathbb{1}[1 - \sigma(H[i,j])
    > \tau] \\ \end{aligned} $$ 其中 $\otimes$ 是逐元素乘法，$[i,j]$ 表示矩阵中的一个元素。$\tau$ 是预定义的阈值超参数。
- en: '![](../Images/f699949bce5910b8ce0d8c456f859880.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f699949bce5910b8ce0d8c456f859880.png)'
- en: 'Fig. 11\. Comparison of different models trained on 0.1% and 1% of the MS COCO
    and Conceptual Caption datasets. (Image source: [Chen et al. 2021](https://arxiv.org/abs/2102.10407))'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11\. 在 MS COCO 和 Conceptual Caption 数据集的 0.1% 和 1% 训练的不同模型的比较。（图片来源：[Chen
    et al. 2021](https://arxiv.org/abs/2102.10407)）
- en: '**VC-GPT** (Visual Conditioned GPT; [Luo et al. 2022](https://arxiv.org/abs/2201.12723))
    combines a pretrained visual transformer (CLIP-ViT) as visual encoder and a pretrained
    LM as language decoder.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '**VC-GPT**（Visual Conditioned GPT；[Luo et al. 2022](https://arxiv.org/abs/2201.12723)）将预训练的视觉变换器（CLIP-ViT）作为视觉编码器，将预训练的
    LM 作为语言解码器。'
- en: '![](../Images/3811e2d7c934aafafc838cc40d1aff29.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3811e2d7c934aafafc838cc40d1aff29.png)'
- en: Fig. 12\. Illustration of VC-GPT training framework.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12\. VC-GPT 训练框架示意图。
- en: '(Image source: [Luo et al. 2022](https://arxiv.org/abs/2201.12723))'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: （图片来源：[Luo et al. 2022](https://arxiv.org/abs/2201.12723)）
- en: The CLIP-ViT takes a sequence of image patches as inputs and outputs representation
    for each patch. To avoid catastrophic forgetting, instead of injecting the visual
    information directly into GPT2, VC-GPT introduces extra cross-attention layers
    on top of the output of visual encoder and language decoder. Then a *self-ensemble*
    module linearly combines the single model language decoder logits $h^G$ and cross-model
    vision-language fused module logits $h^\text{fuse}$. The self-ensemble module
    (see “VC-GPT w/o SE” in Fig. 13) is important for the performance.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: CLIP-ViT 将一系列图像块作为输入，并为每个块输出表示。为了避免灾难性遗忘，VC-GPT 在视觉编码器和语言解码器的输出之上引入了额外的交叉注意力层。然后，一个
    *自我集成* 模块线性组合了单模型语言解码器 logits $h^G$ 和跨模型视觉-语言融合模块 logits $h^\text{fuse}$。自我集成模块（见图
    13 中的 “VC-GPT w/o SE”）对性能至关重要。
- en: $$ \text{logits} = W^G h^G + W^\text{fuse}h^\text{fuse} $$
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \text{logits} = W^G h^G + W^\text{fuse}h^\text{fuse} $$
- en: where $W^G$ is a linear projection of the language decoder, initialized by the
    word embedding matrix of GPT2 and $W^\text{fuse}$ is a linear projection of the
    fusion module and initialized randomly.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $W^G$ 是语言解码器的线性投影，由 GPT2 的词嵌入矩阵初始化，而 $W^\text{fuse}$ 是融合模块的线性投影，随机初始化。
- en: '![](../Images/fc54343d36c4e8761edc641c226dd3e9.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fc54343d36c4e8761edc641c226dd3e9.png)'
- en: 'Fig. 13\. Performance of VC-GPT on the MS COCO test set, in comparison with
    other end-to-end image captioning baseline models. Metric abbreviation: C = CIDEr;
    B = BLEU; M = METEOR; S = SPICE. (Image source: [Luo et al. 2022](https://arxiv.org/abs/2201.12723))'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13\. VC-GPT 在 MS COCO 测试集上的性能，与其他端到端图像字幕基线模型进行比较。 指标缩写：C = CIDEr；B = BLEU；M
    = METEOR；S = SPICE。 （图片来源：[Luo et al. 2022](https://arxiv.org/abs/2201.12723))
- en: '**MERLOT** ([Zellers, et al. 2021](https://arxiv.org/abs/2106.02636)) is trained
    with 6 millions of YouTube videos with transcribed speech ([YT-Temporal-180M](https://rowanzellers.com/merlot/#data))
    to learn both spatial (frame-level) and temporal (video-level) objectives and
    demonstrated strong performance on VQA and visual reasoning tasks when fine-tuned.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '**MERLOT** ([Zellers, et al. 2021](https://arxiv.org/abs/2106.02636)) 是通过对带有转录语音的
    600 万个 YouTube 视频（[YT-Temporal-180M](https://rowanzellers.com/merlot/#data)）进行训练而得到的，以学习空间（帧级）和时间（视频级）目标，并在微调时在
    VQA 和视觉推理任务上表现出色。'
- en: Each video $\mathcal{V}$ is split into multiple segments $\{ \boldsymbol{s}_t
    \}$, each segment $\boldsymbol{s}_t$ containing an image frame $\mathbf{I}_t$
    extracted from the middle timestep and $L=32$ tokens of words associated. Images
    are encoded by a learned image encoder and words are encoded using a learned embedding.
    Then both are encoded together within a joint vision-language transformer.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 每个视频 $\mathcal{V}$ 被分割成多个片段 $\{ \boldsymbol{s}_t \}$，每个片段 $\boldsymbol{s}_t$
    包含从中间时间步提取的图像帧 $\mathbf{I}_t$ 和 $L=32$ 个相关的单词标记。 图像由学习的图像编码器编码，单词使用学习的嵌入进行编码。
    然后两者在联合视觉-语言变压器中一起进行编码。
- en: 'There are 3 learning objectives in MERLOT:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: MERLOT 有 3 个学习目标：
- en: '*Masked language modeling* (MLM) is useful especially because in videos, people
    tend to ramble, resulting in many repeated keywords or filler words.'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*掩码语言建模*（MLM）特别有用，因为在视频中，人们往往会啰嗦，导致许多重复的关键词或填充词。'
- en: '*Contrastive frame-caption matching* uses the language-only part from the joint
    vision-language transformer. Matched representations for each frame $\mathbf{I}_t$
    and caption $\boldsymbol{w}_t$ are treated as positive examples, while the negative
    examples come from all other frame-caption pairs in the minibatch.'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*对比帧-标题匹配* 使用联合视觉-语言变压器中的仅语言部分。 每个帧 $\mathbf{I}_t$ 和标题 $\boldsymbol{w}_t$ 的匹配表示被视为正例，而负例来自小批量中的所有其他帧-标题对。'
- en: '*Temporal reordering* learns temporal reasoning: scramble random $i$ frames
    and replace the segment-level position embeddings with a random and unique position
    embedding. The random position embeddings are learned, allowing the model to unshuffle
    these “‘shuffled’” frames conditioned on correctly-ordered ones. The loss is to
    predict whether $t_i < t_j$ or $t_j < t_i$ for each frame-frame pair.'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*时间重新排序* 学习时间推理：打乱随机 $i$ 帧，并用随机和唯一的位置嵌入替换段级位置嵌入。 学习随机位置嵌入，使模型能够在正确排序的帧的条件下对这些“‘打乱’”帧进行整理。
    损失是为每个帧-帧对预测 $t_i < t_j$ 或 $t_j < t_i$。'
- en: '![](../Images/f0513712f175a8afb4dbbd66b1de46dc.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f0513712f175a8afb4dbbd66b1de46dc.png)'
- en: 'Fig. 14\. Illustration of MERLOT training framework: (Left) contrastive frame-caption
    matching training; (Right) joint vision-language transformer is trained with MLM
    loss, as well as on the temporal reordering task to unshuffle scrambled video
    frames. (Image source: [Zellers, et al. 2021](https://arxiv.org/abs/2106.02636))'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14\. MERLOT 训练框架示意图：（左）对比帧-标题匹配训练；（右）联合视觉-语言变压器使用 MLM 损失进行训练，同时进行时间重新排序任务以对打乱的视频帧进行整理。
    （图片来源：[Zellers, et al. 2021](https://arxiv.org/abs/2106.02636)）
- en: Ablation studies showed that it is important to (1) train on videos instead
    of images, (2) scale up the size and diversity of the training dataset and (3)
    use diverse objectives to encourage full-stack multimodal reasoning.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 消融研究表明，重要的是（1）在视频上进行训练而不是在图像上，（2）扩大训练数据集的规模和多样性，以及（3）使用多样的目标来鼓励全栈多模态推理。
- en: '**Flamingo** ([Alayrac et al. 2022](https://arxiv.org/abs/2204.14198)) is a
    visual language model that accepts text interleaved with images/videos and outputs
    free-form text. Flamingo connects a pretrained LM and a pretrained vision encoder
    (i.e. CLIP image encoder) via a transformer-based mapper. To more efficiently
    incorporate vision signals, Flamingo adopts a [Perceiver](https://arxiv.org/abs/2103.03206)-based
    architecture to produce a few hundreds of tokens out of a large number of visual
    input features and then use cross-attention layers interleaved with the LM layers
    to fuse visual information into the language decoding process. The training objective
    is an autoregressive, NLL loss.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '**Flamingo** ([Alayrac 等人 2022](https://arxiv.org/abs/2204.14198)) 是一个接受文本交错图像/视频并输出自由文本的视觉语言模型。Flamingo
    通过基于 transformer 的映射器连接了一个预训练语言模型和一个预训练视觉编码器（即 CLIP 图像编码器）。为了更有效地整合视觉信号，Flamingo
    采用了基于 [Perceiver](https://arxiv.org/abs/2103.03206) 的架构，从大量的视觉输入特征中产生数百个标记，然后使用交错于语言模型层的交叉注意力层将视觉信息融入语言解码过程中。训练目标是自回归的
    NLL 损失。'
- en: The Perceiver resampler receives spatio-temporal features from the vision encoder
    of image/video inputs to produce fixed-size visual tokens.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Perceiver 重采样器接收来自图像/视频输入的视觉编码器的时空特征，以生成固定大小的视觉标记。
- en: The frozen LM is equipped with newly initialized cross-attention layers interleaved
    between the pretrained LM layers. Thus the LM can generate text conditioned on
    the above visual tokens.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 冻结的语言模型配备了在预训练语言模型层之间交错初始化的交叉注意力层。因此，语言模型可以生成基于上述视觉标记的文本。
- en: Similar to ClipCap, both pretrained models are *frozen* during training and
    thus Flamingo is only trained to harmoniously connect existing, powerful language
    and vision models together. Tha main difference between ClipCap and Flamingo is
    that the former treats the image embedding as simple prefix for LM, while the
    latter uses the gated cross-attention-dense layer to fuse image information. In
    addition, Flamingo incorporates a lot more training data than ClipCap.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 与 ClipCap 类似，训练期间两个预训练模型都是*冻结*的，因此 Flamingo 只是被训练来和谐地连接现有的强大语言和视觉模型。ClipCap
    和 Flamingo 的主要区别在于前者将图像嵌入视为语言模型的简单前缀，而后者使用门控交叉注意力密集层来融合图像信息。此外，Flamingo 比 ClipCap
    包含更多的训练数据。
- en: '![](../Images/2bf98c6271754a9e0d52754eccfe5df0.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2bf98c6271754a9e0d52754eccfe5df0.png)'
- en: 'Fig. 15\. Overview of the Flamingo model. (Image source: [Alayrac et al. 2022](https://arxiv.org/abs/2204.14198))'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15\. Flamingo 模型概述。 (图片来源：[Alayrac 等人 2022](https://arxiv.org/abs/2204.14198))
- en: '![](../Images/800cb1ee227e9e9a0f9a84611bc5faf2.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/800cb1ee227e9e9a0f9a84611bc5faf2.png)'
- en: 'Fig. 16\. The architecture illustration and pseudo code of the gated cross-attention-dense
    layer in Flamingo. (Image source: [Alayrac et al. 2022](https://arxiv.org/abs/2204.14198))'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16\. Flamingo 中门控交叉注意力密集层的架构示意图和伪代码。 (图片来源：[Alayrac 等人 2022](https://arxiv.org/abs/2204.14198))
- en: To easily handle text with interleaved images, masking in Flamingo is designed
    such that text token only cross-attends to visual tokens corresponding to the
    *last* preceding image, largely reducing the number of visual tokens that a certain
    text token can see. They found this works better than allowing text tokens to
    attend to all preceding images directly. Text still can attend to all previous
    images because there is a causal self-attention dependency in the text encoder.
    This design can deal with an arbitrary number of images in the context.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 为了轻松处理文本与交错图像，Flamingo 中的掩码设计使得文本标记仅与*最后*一个前置图像对应的视觉标记进行交叉注意力，大大减少了某个文本标记可以看到的视觉标记数量。他们发现这比允许文本标记直接参与所有前置图像的效果更好。文本仍然可以参与所有先前的图像，因为文本编码器中存在因果自注意力依赖。这种设计可以处理上下文中任意数量的图像。
- en: They scraped 43 million webpages from the Internet, named MultiModal MassiveWeb
    (M3W) dataset, containing text with interleaved images. In addition, Flamingo
    is also trained on paired image/text and video/text datasets, including [ALIGN,
    LTIP and VTP](#pair-image-text-datasets).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 他们从互联网上爬取了 4300 万个网页，命名为 MultiModal MassiveWeb (M3W) 数据集，包含交错图像的文本。此外，Flamingo
    还在配对的图像/文本和视频/文本数据集上进行训练，包括 [ALIGN, LTIP 和 VTP](#pair-image-text-datasets)。
- en: 'Data processing of the Internet dataset includes:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 互联网数据集的数据处理包括：
- en: The input Web page text is processed by inserting `<image>` tags at the location
    of visual inputs, as well as special tokens, `<BOS>` (beginning of sentence) and
    `<EOC>` (end of chunks; always at the end of the document, before any image tag).
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入的网页文本通过在视觉输入位置插入`<image>`标签以及特殊标记`<BOS>`（句子开头）和`<EOC>`（块结束；始终在文档末尾，在任何图像标记之前）进行处理。
- en: From each document, they sample a random subsequence of $L = 256$ tokens and
    take up to $N = 5$ images included in the sampled sequence (using only the first
    $N$ within that sampled subsequence if there are more, or padding to $N$ if fewer)
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从每个文档中，他们随机抽取一个长度为$L = 256$的子序列，并在抽样序列中包含最多$N = 5$个图像（如果有更多，则仅使用该抽样子序列中的前$N$个，如果较少，则填充为$N$）
- en: 'A function $\phi: [1,L] \to [0,N]$ is computed to track the text and image
    interleaving order, which assigns to each text position the index of the last
    image/video appearing before this position; 0 if no preceding visual data.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '计算一个函数$\phi: [1,L] \to [0,N]$来跟踪文本和图像的交错顺序，为每个文本位置分配在该位置之前最后出现的图像/视频的索引；如果没有先前的视觉数据，则为0。'
- en: Since Flamingo is trained on a mixture of three different datasets, it optimizes
    for a weighted sum of dataset-specific NLL losses. Tuning the dataset weights
    is very important for the final performance. In practice, instead of round-robin
    between datasets, they actually sample one batch from each dataset and apply a
    weighted sum of these gradients in each update. Gradient accumulation across different
    heterogeneous datasets can be viewed as a mean to stabilize training, as it reduces
    the gradient variance between each update.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Flamingo是在三种不同数据集的混合上进行训练的，它优化了数据集特定的NLL损失的加权和。调整数据集权重对最终性能非常重要。在实践中，他们实际上不是在数据集之间轮流，而是从每个数据集中抽取一个批次，并在每次更新中应用这些梯度的加权和。跨不同异构数据集的梯度累积可以被视为稳定训练的一种方法，因为它减少了每次更新之间的梯度方差。
- en: At test time, Flamingo naturally supports few-shot learning since it can work
    with any sequence of interleaved text and images. And more examples in the context
    contribute to better performance.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在测试时，Flamingo自然支持少样本学习，因为它可以处理任何交错文本和图像序列。在上下文中的更多示例有助于提高性能。
- en: '![](../Images/d05fdf0d17e3e5afcaa4ae7f1c2253fc.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d05fdf0d17e3e5afcaa4ae7f1c2253fc.png)'
- en: 'Fig. 17\. Larger model sizes and more few-shot examples lead to better performance.
    (Image source: [Alayrac et al. 2022](https://arxiv.org/abs/2204.14198))'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图17。更大的模型尺寸和更多的少样本示例会带来更好的性能。 (图片来源：[Alayrac等人，2022](https://arxiv.org/abs/2204.14198))
- en: Flamingo outperforms SoTA fine-tuned models on 6 out of the 16 tasks despite
    even when not using any fine-tuning but only few-shot prompting. Fine-tuning Flamingo
    is expensive and it is difficult to do hyperparemeter tuning, but it does lead
    to better results.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管Flamingo在16项任务中有6项表现优于SoTA微调模型，即使在没有使用任何微调而仅使用少量提示的情况下。微调Flamingo很昂贵，很难进行超参数调整，但确实会带来更好的结果。
- en: '![](../Images/7553830dcd528a22aca36ad5bc8589db.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7553830dcd528a22aca36ad5bc8589db.png)'
- en: 'Fig. 18\. Performance of Flamingo model using different numbers of shots and
    of different sizes, in comparison with SoTA fine-tuned baseline. (Image source:
    [Alayrac et al. 2022](https://arxiv.org/abs/2204.14198))'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图18。Flamingo模型在使用不同数量的提示和不同大小时的性能，与SoTA微调基线进行比较。 (图片来源：[Alayrac等人，2022](https://arxiv.org/abs/2204.14198))
- en: '**CoCa** (Contrastive Captioner; [Yu & Wang et al., 2022](https://arxiv.org/abs/2205.01917))
    captures both the merits of contrastive learning and image-to-caption generation.
    It is a model jointly trained with contrastive loss on CLIP-style representation
    and generative loss on image captioning, achieving SoTA zero-shot transfer on
    a variety of multi-modal evaluation tasks.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '**CoCa**（对比式字幕生成器；[Yu & Wang等人，2022](https://arxiv.org/abs/2205.01917)）捕捉了对比学习和图像到字幕生成的优点。它是一个模型，同时在CLIP风格表示上进行对比损失训练和在图像字幕生成上进行生成损失训练，实现了在各种多模态评估任务上的SoTA零样本转移。'
- en: '![](../Images/cc5713a39fcf936bf8800a889548ce5c.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cc5713a39fcf936bf8800a889548ce5c.png)'
- en: Fig. 19\. Overview of CoCa training framework.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图19。CoCa训练框架概述。
- en: '(Image source: [Yu & Wang et al., 2022](https://arxiv.org/abs/2205.01917))'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: (图片来源：[Yu & Wang等人，2022](https://arxiv.org/abs/2205.01917))
- en: CoCa is pretrained from *scratch*, using web-scale alt-text data [ALIGN](#pair-image-text-datasets)
    and annotated images by treating all labels as texts in [JTB-3B](#pair-image-text-datasets).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: CoCa是从*头开始*预训练的，使用网络规模的alt-text数据[ALIGN](#pair-image-text-datasets)和通过将所有标签视为文本在[JTB-3B](#pair-image-text-datasets)中注释的图像。
- en: 'There are two major training components in CoCa. The final loss is a weighted
    sum of the following two losses, with weight scalars $\lambda_\text{cap}=2.0,
    \lambda_\text{con} = 1.0$.:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: CoCa中有两个主要的训练组件。最终损失是以下两个损失的加权和，权重标量为$\lambda_\text{cap}=2.0, \lambda_\text{con}
    = 1.0$：
- en: $\mathcal{L}_\text{con}$ - *Dual-encoder contrastive learning* optimizes the
    symmetric contrastive learning loss, similar to CLIP.
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: $\mathcal{L}_\text{con}$ - *双编码器对比学习* 优化对称对比学习损失，类似于CLIP。
- en: '$\mathcal{L}_\text{cap}$ - *Encoder-decoder captioning* has the decoder predict
    the caption based on the latent encoded features from the image encoder, by optimizing
    an autoregressive loss. The text decoder is decoupled into two components, *unimodal*
    and *multimodal*; a good balance is to split the decoder by half for these two
    components:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: $\mathcal{L}_\text{cap}$ - *编码器-解码器字幕生成*使解码器基于图像编码器的潜在编码特征预测字幕，通过优化自回归损失。文本解码器分为两个组件，*单模态*和*多模态*；一个很好的平衡是将解码器一分为二：
- en: The bottom unimodal component encodes the input text with causally-masked self-attention.
  id: totrans-123
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 底部的单模态组件使用因果屏蔽的自注意力来编码输入文本。
- en: The top multimodal component applies both causally-masked self-attention and
    cross-attention to the output of the vision encoder.
  id: totrans-124
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 顶部的多模态组件对视觉编码器的输出应用因果屏蔽的自注意力和交叉注意力。
- en: CoCa performs better than the contrastive-only model and on par with the captioning-only
    model on VQA. Captioning loss is found to be beneficial to the zero-shot classification
    capacity too.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: CoCa在VQA上表现优于仅对比模型，并与仅字幕模型持平。发现字幕损失对零样本分类能力也有益处。
- en: '![](../Images/96bfd34fae5394dc1f4ecf557a97ebba.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/96bfd34fae5394dc1f4ecf557a97ebba.png)'
- en: 'Fig. 20\. Illustration of how CoCa can be used to solve various downstream
    tasks at test time. (Image source: [Yu & Wang et al., 2022](https://arxiv.org/abs/2205.01917))'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 图20。CoCa如何在测试时用于解决各种下游任务的示意图。（图片来源：[Yu & Wang等人，2022](https://arxiv.org/abs/2205.01917)）
- en: They use task-specific attention pooling, or attention pooler, as a natural
    task adapter, as they found that a single pooled image embedding helps visual
    recognition tasks (e.g. ImageNet classification), while a more fine-grained embedding
    helps multimodal understanding tasks (e.g. VQA). A pooler is a single multi-head
    attention layer with $n_\text{query}$ learnable queries (note that $\mathbf{X}
    \in \mathbb{R}^{L \times d}$, $\mathbf{W}^q \in \mathbb{R}^{d \times d_q}$, and
    $d_k = d_q$), with the encoder output as both keys and values. CoCa uses attentional
    poolers in pretraining for generative loss $n_\text{query} = 256$ and contrastive
    loss $n_\text{query} = 1$. This enables the model to obtain strong performance
    as a *frozen* encoder where we only learn a new pooler to aggregate features.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 他们使用任务特定的注意力池化，或者说注意力池器，作为一种自然的任务适配器，因为他们发现单个池化图像嵌入有助于视觉识别任务（例如ImageNet分类），而更精细的嵌入则有助于多模态理解任务（例如VQA）。一个池化器是一个具有$n_\text{query}$可学习查询的单个多头注意力层（注意$\mathbf{X}
    \in \mathbb{R}^{L \times d}$，$\mathbf{W}^q \in \mathbb{R}^{d \times d_q}$，$d_k
    = d_q$），编码器输出作为键和值。CoCa在预训练中使用注意力池器进行生成损失$n_\text{query} = 256$和对比损失$n_\text{query}
    = 1$。这使得模型能够在*冻结*编码器的情况下获得强大的性能，我们只学习一个新的池化器来聚合特征。
- en: '![](../Images/431a3afbe8b5b4b2a596118aac4a29c1.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/431a3afbe8b5b4b2a596118aac4a29c1.png)'
- en: Fig. 21\. Pseudo code for CoCa architecture and training.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图21。CoCa架构和训练的伪代码。
- en: '(Image source: [Yu & Wang et al., 2022](https://arxiv.org/abs/2205.01917))'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: （图片来源：[Yu & Wang等人，2022](https://arxiv.org/abs/2205.01917)）
- en: No Training
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无需训练
- en: Finally it is possible to solve vision language tasks by stitching pretrained
    language and vision models together without training any additional parameters.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，可以通过将预训练的语言和视觉模型拼接在一起来解决视觉语言任务，而无需训练任何额外的参数。
- en: Decoding Guided with Vision-based Scores
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用基于视觉得分的引导解码
- en: '**MAGiC** (iMAge-Guided text generatIon with CLIP; [Su et al. 2022](https://arxiv.org/abs/2205.02655))
    does [guided decoding](https://lilianweng.github.io/posts/2021-01-02-controllable-text-generation/#guided-decoding)
    according to a CLIP-based score named *magic score* to sample the next token,
    without fine-tuning. The generated text is encouraged to be relevant to the given
    image, while still stay coherent to the previously generated text.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '**MAGiC**（iMAge-Guided文本生成与CLIP；[Su等人，2022](https://arxiv.org/abs/2205.02655)）根据基于CLIP的*魔法分数*进行引导解码，以采样下一个标记，无需微调。生成的文本被鼓励与给定图像相关，同时仍然与先前生成的文本保持连贯。'
- en: The next token $x_t$ at a time step $t$ is selected according to the following
    equation. Model confidence and degeneration penalty ([Su et al. 2022](https://arxiv.org/abs/2202.06417))
    are added to avoid corrupted generation from LM.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个时间步 $t$ 的下一个标记 $x_t$ 根据以下方程选择。为了避免 LM 生成错误，模型置信度和退化惩罚（[Su et al. 2022](https://arxiv.org/abs/2202.06417)）被添加。
- en: $$ \begin{aligned} & x_t = \arg\max_{v \in \mathcal{V}^{(k)}} \big\{ (1-\alpha)
    \underbrace{p(v \vert \boldsymbol{x}_{<t})}_\text{model confidence} - \alpha \underbrace{\max_{1
    \leq j \leq t-1} { \text{cosine}(h_v, h_{x_j})}}_\text{degeneration penalty} +
    \beta \underbrace{f_\text{magic}(v \vert \mathcal{I}, \boldsymbol{x}_{<t}, \mathcal{V}^{(k)})}_\text{magic
    score} \big\} \\ \text{where } & f_\text{magic} ( v \vert \mathcal{I}, \mathbf{x}_{<t},
    \mathcal{V}^{(k)} ) = \frac{ \exp(\text{CLIP}(\mathcal{I}, [\boldsymbol{x}_{<t}:v]))
    }{ \sum_{z \in \mathcal{V}^{(k)}} \exp(\text{CLIP}(\mathcal{I}, [\boldsymbol{x}_{<t}:z]))
    } = \frac{ \exp\big({h^\text{image}(\mathcal{I})}^\top h^\text{text}([\boldsymbol{x}_{<t}:v])\big)
    }{ \sum_{z \in \mathcal{V}^{(k)}} \exp\big({h^\text{image}(\mathcal{I})}^\top
    h^\text{text}([\boldsymbol{x}_{<t}:z])\big) } \end{aligned} $$
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \begin{aligned} & x_t = \arg\max_{v \in \mathcal{V}^{(k)}} \big\{ (1-\alpha)
    \underbrace{p(v \vert \boldsymbol{x}_{<t})}_\text{模型置信度} - \alpha \underbrace{\max_{1
    \leq j \leq t-1} { \text{cosine}(h_v, h_{x_j})}}_\text{退化惩罚} + \beta \underbrace{f_\text{magic}(v
    \vert \mathcal{I}, \boldsymbol{x}_{<t}, \mathcal{V}^{(k)})}_\text{魔法分数} \big\}
    \\ \text{其中 } & f_\text{magic} ( v \vert \mathcal{I}, \mathbf{x}_{<t}, \mathcal{V}^{(k)}
    ) = \frac{ \exp(\text{CLIP}(\mathcal{I}, [\boldsymbol{x}_{<t}:v])) }{ \sum_{z
    \in \mathcal{V}^{(k)}} \exp(\text{CLIP}(\mathcal{I}, [\boldsymbol{x}_{<t}:z]))
    } = \frac{ \exp\big({h^\text{image}(\mathcal{I})}^\top h^\text{text}([\boldsymbol{x}_{<t}:v])\big)
    }{ \sum_{z \in \mathcal{V}^{(k)}} \exp\big({h^\text{image}(\mathcal{I})}^\top
    h^\text{text}([\boldsymbol{x}_{<t}:z])\big) } \end{aligned} $$
- en: where $\mathcal{I}$ is the input image; $\mathcal{V}^{(k)}$ contains top-$k$
    possible tokens predicted by the language model $p$; $\boldsymbol{x}_{<t}$ refers
    to the past generated tokens before time step $t$; $h_v$ is the representation
    of the token $v$ computed by LM conditioned on the concatenation of $\boldsymbol{x}_{<t}$
    and $v$; $h^\text{image}(.)$ and $h^\text{text}(.)$ are embeddings generated by
    CLIP image and text encoders, respectively.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathcal{I}$ 是输入图像；$\mathcal{V}^{(k)}$ 包含语言模型 $p$ 预测的前 $k$ 个可能标记；$\boldsymbol{x}_{<t}$
    指的是时间步 $t$ 之前生成的标记；$h_v$ 是 LM 在 $\boldsymbol{x}_{<t}$ 和 $v$ 的连接上计算的标记 $v$ 的表示；$h^\text{image}(.)$
    和 $h^\text{text}(.)$ 是由 CLIP 图像和文本编码器生成的嵌入。
- en: MAGiC has decent performance compared to other unsupervised approaches, but
    still has big gaps with supervised methods.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 与监督方法相比，MAGiC 的性能相当不错，但与无监督方法仍存在较大差距。
- en: '![](../Images/af6d81ae1dfa1eefec9253f477230eab.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/af6d81ae1dfa1eefec9253f477230eab.png)'
- en: 'Fig. 22\. Image captioning performance on COCO and Flickr30k. (Image source:
    [Su et al. 2022](https://arxiv.org/abs/2205.02655))'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 图 22\. COCO 和 Flickr30k 上的图像字幕性能。 (图片来源：[Su et al. 2022](https://arxiv.org/abs/2205.02655))
- en: Language as Communication Interface
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 语言作为通信接口
- en: For knowledge-based VQA tasks, PICa (Prompts GPT-3 via the use of Image Captions;
    [Yang et al. 2021](https://arxiv.org/abs/2109.05014)) first converts the images
    into captions or tags and then uses few-shot examples to prompt GPT3 to provide
    answers. Image captions or tags are extracted by some existing models (e.g. [VinVL](https://openaccess.thecvf.com/content/CVPR2021/html/Zhang_VinVL_Revisiting_Visual_Representations_in_Vision-Language_Models_CVPR_2021_paper.html))
    or Azure Tagging API. And GPT3 is considered as an unstructured, implicit knowledge
    base.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 对于基于知识的 VQA 任务，PICa（通过图像字幕提示 GPT-3；[Yang et al. 2021](https://arxiv.org/abs/2109.05014)）首先将图像转换为字幕或标记，然后使用少量示例提示
    GPT3 提供答案。 图像字幕或标记是由一些现有模型（例如 [VinVL](https://openaccess.thecvf.com/content/CVPR2021/html/Zhang_VinVL_Revisiting_Visual_Representations_in_Vision-Language_Models_CVPR_2021_paper.html)）或
    Azure 标记 API 提取的。 GPT3 被视为一个非结构化的、隐式的知识库。
- en: '![](../Images/9d129e64746fd76bf05fdbd613ba380d.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9d129e64746fd76bf05fdbd613ba380d.png)'
- en: 'Fig. 23\. How PICa works for $n$-shot VQA at inference time. (Image source:
    [Yang et al. 2021](https://arxiv.org/abs/2109.05014))'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 图 23\. 在推理时，PICa 如何处理 $n$-shot VQA。 (图片来源：[Yang et al. 2021](https://arxiv.org/abs/2109.05014))
- en: 'PICa explored two ways to improve few-shot examples to achieve better results:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: PICa 探索了两种改进少量示例以获得更好结果的方法：
- en: In-context examples are selected based on how *similar* they are to the question
    using CLIP embedding.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据 CLIP 嵌入，选择与问题*相似*的上下文示例。
- en: '*Multi-query ensembling* is to prompt the model multiple times to get multiple
    answers and the one with highest logprob is selected.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*多次查询集成* 是多次提示模型以获得多个答案，并选择具有最高对数概率的答案。'
- en: This simple approach with only 16 examples improved SoTA on OK-VQA by +8.6 points
    and got decent performance on VQAv2.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 这种简单的方法只用了16个例子就在OK-VQA上提高了+8.6分，并在VQAv2上表现出色。
- en: '![](../Images/1ceebc33e3551e5655df9947cf46c180.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1ceebc33e3551e5655df9947cf46c180.png)'
- en: 'Fig. 24\. Performance of PICa on OK-VQA. "PICa-Base" has random in-context
    examples, while "PICa-Full" incorporates both similar in-context example selection
    and multi-query ensembling. (Image source: [Yang et al. 2021](https://arxiv.org/abs/2109.05014))'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 图24。PICa在OK-VQA上的表现。“PICa-Base”具有随机的上下文示例，而“PICa-Full”结合了类似的上下文示例选择和多查询集成。（图片来源：[Yang等人，2021](https://arxiv.org/abs/2109.05014)）
- en: '**Socratic Models** (SM) ([Zeng et al. 2022](https://arxiv.org/abs/2204.00598))
    is a framework to *compose* multiple pretrained models for different modality
    via language (prompting) into one model without further training. Here language
    is considered as the intermediate representation by which different models can
    exchange information. The key idea is to use *multi-model multimodal prompting*,
    in which output of a non-language model is inserted into a language prompt and
    then it is used for LM for reasoning.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '**苏格拉底模型**（SM）（[曾等人，2022](https://arxiv.org/abs/2204.00598)）是一个框架，用于通过语言（提示）将多个预训练模型组合成一个模型，无需进一步训练。在这里，语言被视为不同模型可以交换信息的中间表示。关键思想是使用*多模型多模态提示*，其中非语言模型的输出被插入到语言提示中，然后用于LM进行推理。'
- en: 'Let’s examine a concrete example. Given an ego-centric video (images + audio),
    SM can produce a summary of the person’s activity using text-to-text LM, image-to-text
    VLM and speech-to-text ALM. They are chained as follows:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个具体的例子。给定一个自我中心视频（图像+音频），SM可以使用文本到文本LM、图像到文本VLM和语音到文本ALM生成人物活动的摘要。它们的链接如下：
- en: '![](../Images/25829fe3dfd5f930d17b13a851a25de8.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/25829fe3dfd5f930d17b13a851a25de8.png)'
- en: '(Image source: [Zeng et al. 2022](https://arxiv.org/abs/2204.00598))'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: （图片来源：[曾等人，2022](https://arxiv.org/abs/2204.00598)）
- en: the VLM detects visual entities;
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: VLM检测视觉实体；
- en: the LM suggests sounds that may be heard;
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: LM建议可能听到的声音；
- en: the ALM chooses the most likely sound;
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ALM选择最可能的声音；
- en: the LM suggests possible activities;
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: LM建议可能的活动；
- en: the VLM ranks the most likely activity;
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: VLM对最可能的活动进行排名；
- en: the LM generates a summary of the Socratic interaction.
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: LM生成苏格拉底互动的摘要。
- en: '![](../Images/cf05fc42ca2fada1f27920632fbf608f.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cf05fc42ca2fada1f27920632fbf608f.png)'
- en: 'Fig. 25\. Illustration of the Socratic Model solution for image captioning.
    (Image source: [Zeng et al. 2022](https://arxiv.org/abs/2204.00598))'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 图25。苏格拉底模型解决图像字幕的示意图。（图片来源：[曾等人，2022](https://arxiv.org/abs/2204.00598)）
- en: SM can generate image captions by first using VLM to zero-shot predict different
    place categories, object categories, image type and the number of people; and
    then the VLM-filled language prompt is fed into a causal LM to generate caption
    candidates. The Socratic approach still has performance gap with ClipCap on image
    captioning but pretty decent given it does not involve any training.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: SM可以通过首先使用VLM零样本预测不同的地点类别、物体类别、图像类型和人数来生成图像字幕；然后将VLM填充的语言提示馈送到因果LM中生成字幕候选。苏格拉底方法在图像字幕方面仍然与ClipCap存在性能差距，但考虑到它不涉及任何训练，表现相当不错。
- en: '![](../Images/6771cae635b1221253b8389292b66651.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6771cae635b1221253b8389292b66651.png)'
- en: 'Fig. 26\. Comparison of image captioning performance of different models on
    random 100 COCO text examples. (Image source: [Zeng et al. 2022](https://arxiv.org/abs/2204.00598))'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 图26。不同模型在随机100个COCO文本示例上的图像字幕性能比较。（图片来源：[曾等人，2022](https://arxiv.org/abs/2204.00598)）
- en: SM framework is very flexible and can be used on a lot more complicated tasks
    other than image captions. For example, the egocentric perception (User inputs
    + VLM + LM + ALM) task is to take as inputs egocentric videos to (1) summarize
    content; (2) answer free-form reasoning questions; (3) and do forecasting.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: SM框架非常灵活，可以用于除图像字幕之外的更复杂的任务。例如，自我中心感知（用户输入+VLM+LM+ALM）任务是将自我中心视频作为输入，（1）总结内容；（2）回答自由形式的推理问题；（3）进行预测。
- en: '![](../Images/7d91575b24543282e6c6f8ae3f5c7145.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7d91575b24543282e6c6f8ae3f5c7145.png)'
- en: 'Fig. 27\. The Socratic Model approach for generating captions and question
    answering based on the egocentric videos. (Image source: [Zeng et al. 2022](https://arxiv.org/abs/2204.00598))'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 图27。基于自我中心视频生成字幕和问答的苏格拉底模型方法。（图片来源：[曾等人，2022](https://arxiv.org/abs/2204.00598)）
- en: Datasets
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据集
- en: Image Caption Datasets
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图像字幕数据集
- en: '*MS COCO* ([Chen et al. 2015](https://arxiv.org/abs/1504.00325)): contains
    328K images and each paired with 5 independent captions.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*MS COCO*（[Chen等人，2015](https://arxiv.org/abs/1504.00325)）：包含了32.8万张图片，每张图片配有5个独立的标题。'
- en: '*NoCaps* ([Agrawal et al., 2019](https://arxiv.org/abs/1812.08658)) is designed
    to measure generalization to unseen classes and concepts, where in-domain contains
    images portraying only COCO classes, near-domain contains both COCO and novel
    classes, and out-of-domain consists of only novel classes.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*NoCaps*（[Agrawal等人，2019](https://arxiv.org/abs/1812.08658)）旨在衡量对未见类别和概念的泛化能力，其中领域内包含仅展示COCO类别的图片，近领域包含COCO和新颖类别，领域外包含仅新颖类别。'
- en: '*Conceptual Captions* ([Sharma et al. 2018](https://aclanthology.org/P18-1238/))
    contains 3 million pairs of images and captions, mined from the web and post-processed.
    To focus on the concepts, specific entities in this dataset are replaced with
    general notions (e.g. a politician’s name is replaced with “politician”)'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Conceptual Captions*（[Sharma等人，2018](https://aclanthology.org/P18-1238/)）包含了300万对图片和标题，从网络中挖掘并进行后处理。为了专注于概念，该数据集中的特定实体被替换为一般概念（例如，政治家的名字被替换为“政治家”）。'
- en: '*Crisscrossed Captions (CxC)* ([Parekh et al. 2021](https://arxiv.org/abs/2004.15020))
    contains 247,315 human-labeled annotations including positive and negative associations
    between image pairs, caption pairs and image-caption pairs.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Crisscrossed Captions（CxC）*（[Parekh等人，2021](https://arxiv.org/abs/2004.15020)）包含了247,315个人工标注的注释，包括图片对、标题对和图片-标题对之间的正面和负面关联。'
- en: '*Concadia* ([Kreiss et al. 2021](https://arxiv.org/abs/2104.08376)) is a Wikipedia-based
    dataset containing 96,918 images with corresponding English-language descriptions,
    captions, and surrounding context.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Concadia*（[Kreiss等人，2021](https://arxiv.org/abs/2104.08376)）是一个基于维基百科的数据集，包含了96,918张图片及其对应的英语描述、标题和周围环境。'
- en: Pair Image-Text Datasets
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图像-文本配对数据集
- en: (*) Not a public dataset.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: (*) 非公开数据集。
- en: '*ALIGN* ([Jia et al., 2021](https://arxiv.org/abs/2102.05918)) contains 1.8
    billion images with alt-text. The dataset is large but noisy with only minimal
    frequency-based filtration.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*ALIGN*（[Jia等人，2021](https://arxiv.org/abs/2102.05918)）包含了18亿张带有替代文本的图片。该数据集规模庞大，但噪音较大，仅进行了最小程度的基于频率的过滤。'
- en: '(*) *LTIP* (Long text & image pairs; [Alayrac et al. 2022](https://arxiv.org/abs/2204.14198)):
    312 million images, paired with descriptive captions.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (*) *LTIP*（长文本和图像对；[Alayrac等人，2022](https://arxiv.org/abs/2204.14198)）：3.12亿张图片，配有描述性标题。
- en: '(*) *VTP* (Video & text pairs; [Alayrac et al. 2022](https://arxiv.org/abs/2204.14198)):
    27 million short videos (~22 seconds on average), paired with descriptive captions.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (*) *VTP*（视频和文本对；[Alayrac等人，2022](https://arxiv.org/abs/2204.14198)）：2700万个短视频（平均约22秒），配有描述性标题。
- en: (*) *JFT-300M* / *JFT-3B* are internal Google datasets, containing 300M / 3B
    images annotated with a class-hierarchy of around 30k labels via a semi-automatic
    pipeline. Thus the data and associated labels are noisy.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (*) *JFT-300M* / *JFT-3B*是Google内部数据集，包含了300M / 3B张图片，通过半自动化流程进行了大约30k标签的类层次标注。因此，数据和相关标签存在噪音。
- en: Evaluation Tasks
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估任务
- en: Visual Question-Answering
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 视觉问答
- en: Given an image and a question, the task is to correctly answer the question.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一张图片和一个问题，任务是正确回答这个问题。
- en: '*VQAv2* ([Goyal et al., 2017](https://arxiv.org/abs/1612.00837)) contains 1+
    million questions about 200K images from COCO.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*VQAv2*（[Goyal等人，2017](https://arxiv.org/abs/1612.00837)）包含了来自COCO的约200K张图片的100多万个问题。'
- en: '*OK-VQA* ([Marino et al. 2019](https://arxiv.org/abs/1906.00067)) contains
    14K open-ended questions that require outside knowledge (e.g. from Wikipedia).'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*OK-VQA*（[Marino等人，2019](https://arxiv.org/abs/1906.00067)）包含了14K个需要外部知识（例如来自维基百科）的开放式问题。'
- en: '*A-OKVQA*: the augmented successor of OK-VQA, with no overlapped questions
    with OK-VAQ.'
  id: totrans-188
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*A-OKVQA*：OK-VQA的增强版本，与OK-VAQ没有重叠的问题。'
- en: '*TextVQA* ([Singh, et al. 2019](https://arxiv.org/abs/1904.08920)) contains
    45,336 questions on 28,408 images that require reasoning about text to answer.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*TextVQA*（[Singh等人，2019](https://arxiv.org/abs/1904.08920)）包含了28,408张图片上的45,336个需要推理文本才能回答的问题。'
- en: '*VizWiz* ([Gurari, et al. 2018](https://arxiv.org/abs/1802.08218)) contains
    over 31,000 visual questions originating from blind people who each took a picture
    using a mobile phone and recorded a spoken question about it, together with 10
    crowdsourced answers per visual question.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*VizWiz*（[Gurari等人，2018](https://arxiv.org/abs/1802.08218)）包含了来自盲人的超过31,000个视觉问题，每个问题都是一个使用手机拍摄的图片，并录制了一个关于图片的口头问题，以及每个视觉问题的10个众包答案。'
- en: Visual Language Reasoning
  id: totrans-191
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 视觉语言推理
- en: '*VCR* (Visual Commonsense Reasoning; [Zellers et al. 2018](https://arxiv.org/abs/1811.10830))
    contains 290k multiple choice QA questions derived from 110k movie scenes, with
    focus on visual commonsense.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*VCR*（视觉常识推理；[Zellers et al. 2018](https://arxiv.org/abs/1811.10830)）包含了从110k个电影场景衍生出的290k个多项选择问答问题，重点关注视觉常识。'
- en: '*NLVR2* (Natural Language for Visual Reasoning; [Suhr et al. 2019](https://arxiv.org/abs/1811.00491))
    contains 100k+ examples of sentences paired with web images and the task is to
    determine whether a natural language caption is true about a pair of images, with
    a focus on semantic diversity.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*NLVR2*（自然语言视觉推理；[Suhr et al. 2019](https://arxiv.org/abs/1811.00491)）包含了100k+个句子与网络图像配对的示例，任务是确定自然语言标题是否对一对图像正确，重点关注语义多样性。'
- en: '*Flickr30K* ([Jia et al. 2015](https://arxiv.org/abs/1509.04942)) contains
    30k images collected from Flickr and 250k annotations and the task is to select
    the bounding regions given spans of a sentence.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Flickr30K*（[Jia et al. 2015](https://arxiv.org/abs/1509.04942)）包含了从Flickr收集的30k张图片和250k个注释，任务是根据句子的跨度选择边界区域。'
- en: '*SNLI-VE* (Visual Entailment; [Xie et al. 2019](https://arxiv.org/abs/1901.06706))
    is built on top of SNLI and Flickr30K and the task is to reason about the relationship
    between an image premise and a text hypothesis.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*SNLI-VE*（视觉蕴涵；[Xie et al. 2019](https://arxiv.org/abs/1901.06706)）建立在SNLI和Flickr30K之上，任务是推理图像前提和文本假设之间的关系。'
- en: Video QA and Understanding
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 视频问答和理解
- en: '*MSR-VTT* (MSR Video to Text; [Xu et al. 2016](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/06/cvpr16.msr-vtt.tmei_-1.pdf))
    contains 10K web video clips with 41.2 hours and 200K clip-sentence pairs in total;
    the task is to translate videos to text.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*MSR-VTT*（MSR视频到文本；[Xu et al. 2016](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/06/cvpr16.msr-vtt.tmei_-1.pdf)）包含了10k个网络视频剪辑，总时长41.2小时，共有200k个剪辑-句子对；任务是将视频翻译成文本。'
- en: '*ActivityNet-QA* ([Yu et al. 2019](https://arxiv.org/abs/1906.02467)) contains
    58,000 human-annotated QA pairs on 5,800 videos derived from the popular [ActivityNet](http://activity-net.org/index.html)
    dataset.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*ActivityNet-QA*（[Yu et al. 2019](https://arxiv.org/abs/1906.02467)）包含了从流行的[ActivityNet](http://activity-net.org/index.html)数据集中衍生出的5,800个视频上的58,000个人工注释问答对。'
- en: '*TGIF* (Tumblr GIF; [Li et al. .2016](https://arxiv.org/abs/1604.02748)) contains
    100K animated GIFs and 120K sentences describing visual content of the animated
    GIFs, randomly selected posts published between May and June of 2015 on Tumblr.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*TGIF*（Tumblr GIF；[Li et al. .2016](https://arxiv.org/abs/1604.02748)）包含了100k个动画GIF和120k个描述动画GIF视觉内容的句子，随机选择自Tumblr
    2015年5月至6月发布的帖子。'
- en: '*TGIF-QA* contains 165K QA pairs for the animated GIFs from the TGIF dataset.'
  id: totrans-200
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*TGIF-QA* 包含了来自TGIF数据集的165K个动画GIF的问答对。'
- en: '*LSMDC* (Large Scale Movie Description Challenge; [Rohrbach et al. 2015](https://arxiv.org/abs/1501.02530))
    contains 118,081 short video clips extracted from 202 movies. Each video has a
    caption, either extracted from the movie script or from transcribed DVS (descriptive
    video services) for the visually impaired.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*LSMDC*（大规模电影描述挑战赛；[Rohrbach et al. 2015](https://arxiv.org/abs/1501.02530)）包含了从202部电影中提取的118,081个短视频剪辑。每个视频都有一个标题，可以是从电影剧本中提取的，也可以是为视觉障碍者转录的DVS（描述性视频服务）中提取的。'
- en: '*TVQA* ([Lei et al. 2018](https://arxiv.org/abs/1809.01696)) / *TVQA+* ([Lei
    et al. 2019](https://arxiv.org/abs/1904.11574)) is a large-scale video QA dataset
    based on 6 popular TV shows (Friends, The Big Bang Theory, How I Met Your Mother,
    House M.D., Grey’s Anatomy, Castle). It consists of 152.5K QA pairs from 21.8K
    video clips, spanning over 460 hours of video.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*TVQA*（[Lei et al. 2018](https://arxiv.org/abs/1809.01696)）/ *TVQA+*（[Lei et
    al. 2019](https://arxiv.org/abs/1904.11574)）是一个基于6个热门电视节目（Friends, The Big Bang
    Theory, How I Met Your Mother, House M.D., Grey’s Anatomy, Castle）的大规模视频问答数据集。它包含了来自21.8k个视频剪辑的152.5k个问答对，视频总时长超过460小时。'
- en: '*DramaQA* ([Choi et al. 2020](https://arxiv.org/abs/2005.03356)) is a large-scale
    video QA dataset based on a Korean popular TV show, “Another Miss Oh”. This dataset
    contains four levels of QA on difficulty and multi-level character-centered story
    descriptions.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*DramaQA*（[Choi et al. 2020](https://arxiv.org/abs/2005.03356)）是一个基于韩国热门电视节目“另一个吴海”的大规模视频问答数据集。该数据集包含了四个难度级别和多级角色中心故事描述的问答。'
- en: '*VLEP* (Video-and-Language Event Prediction; [Lei et al. 2020](https://arxiv.org/abs/2010.07999))
    contains 28,726 future event prediction examples (along with their rationales)
    from 10,234 diverse TV Show and YouTube Lifestyle Vlog video clips.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*VLEP*（Video-and-Language Event Prediction；[Lei et al. 2020](https://arxiv.org/abs/2010.07999)）包含了来自10,234个不同的电视节目和YouTube生活视频剪辑的28,726个未来事件预测示例（以及它们的理由）。'
- en: Citation
  id: totrans-205
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引用
- en: 'Cited as:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 被引用为：
- en: Weng, Lilian. (Jun 2022). Generalized visual language models. Lil’Log. https://lilianweng.github.io/posts/2022-06-09-vlm/.
  id: totrans-207
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 翁，莉莲。 (2022年6月)。广义视觉语言模型。Lil’Log。https://lilianweng.github.io/posts/2022-06-09-vlm/。
- en: Or
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 或
- en: '[PRE5]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: References
  id: totrans-210
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Li et al. [“VisualBERT: A Simple and Performant Baseline for Vision and
    Language.”](https://arxiv.org/abs/1908.03557) arXiv preprint:1908.03557 (2019).'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] 李等人[“VisualBERT：视觉和语言的简单且高效基线。”](https://arxiv.org/abs/1908.03557) arXiv预印本:1908.03557
    (2019)。'
- en: '[2] Wang et al. [“SimVLM: Simple Visual Language Model Pretraining with Weak
    Supervision.”](https://arxiv.org/abs/2108.10904) ICLR 2022.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] 王等人[“SimVLM: 简单的视觉语言模型预训练与弱监督。”](https://arxiv.org/abs/2108.10904) ICLR
    2022。'
- en: '[3] Aghajanyan, et al. [“CM3: A Causal Masked Multimodal Model of the Internet.”](https://arxiv.org/abs/2201.07520)
    arXiv preprint arXiv: 2201.07520 (2022).'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] 阿加加尼扬等人[“CM3：互联网的因果蒙版多模态模型。”](https://arxiv.org/abs/2201.07520) arXiv预印本arXiv:2201.07520
    (2022)。'
- en: '[4] Tsimpoukelli et al. [“Multimodal Few-Shot Learning with Frozen Language
    Models.”](https://arxiv.org/abs/2106.13884) NeuriPS 2021.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Tsimpoukelli等人[“冻结语言模型的多模态少样本学习。”](https://arxiv.org/abs/2106.13884) NeuriPS
    2021。'
- en: '[5] Mokady, Hertz & Hertz. [“ClipCap: CLIP Prefix for Image Captioning.”](https://arxiv.org/abs/2111.09734)
    2021.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Mokady，赫兹和赫兹。[“ClipCap: 用于图像字幕的CLIP前缀。”](https://arxiv.org/abs/2111.09734)
    2021。'
- en: '[6] Chen et al. [“VisualGPT: Data-efficient Adaptation of Pretrained Language
    Models for Image Captioning.”](https://arxiv.org/abs/2102.10407) arXiv preprint
    arXiv:2111.09734 (2021).'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] 陈等人[“VisualGPT：用于图像字幕的预训练语言模型的数据高效适应。”](https://arxiv.org/abs/2102.10407)
    arXiv预印本arXiv:2111.09734 (2021)。'
- en: '[7] Luo et al. [“A Frustratingly Simple Approach for End-to-End Image Captioning.”](https://arxiv.org/abs/2201.12723)
    arXiv preprint arXiv:2201.12723 (2022).'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] 罗等人[“端到端图像字幕的一个令人沮丧地简单方法。”](https://arxiv.org/abs/2201.12723) arXiv预印本arXiv:2201.12723
    (2022)。'
- en: '[8] Zellers et al. [“MERLOT: Multimodal neural script knowledge models.”](https://arxiv.org/abs/2106.02636)
    NeuriPS 2021.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] Zellers等人[“MERLOT: 多模态神经脚本知识模型。”](https://arxiv.org/abs/2106.02636) NeuriPS
    2021。'
- en: '[9] Alayrac et al. [“Flamingo: a Visual Language Model for Few-Shot Learning.”](https://arxiv.org/abs/2204.14198)
    arXiv preprint arXiv:2204.14198 (2022).'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] Alayrac等人[“Flamingo: 一种用于少样本学习的视觉语言模型。”](https://arxiv.org/abs/2204.14198)
    arXiv预印本arXiv:2204.14198 (2022)。'
- en: '[10] Yu & Wang et al. [“CoCa: Contrastive Captioners are Image-Text Foundation
    Models.”](https://arxiv.org/abs/2205.01917) arXiv preprint arXiv:2205.01917 (2022).'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] 于和王等人[“CoCa：对比字幕生成器是图像文本基础模型。”](https://arxiv.org/abs/2205.01917) arXiv预印本arXiv:2205.01917
    (2022)。'
- en: '[11] Yang et al. [“An Empirical Study of GPT-3 for Few-Shot Knowledge-Based
    VQA.”](https://arxiv.org/abs/2109.05014) arXiv preprint arXiv:2109.05014 (2021).'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] 杨等人[“GPT-3在少样本基于知识的VQA中的实证研究。”](https://arxiv.org/abs/2109.05014) arXiv预印本arXiv:2109.05014
    (2021)。'
- en: '[12] Su et al. [“Language models can see: Plugging visual controls in text
    generation.”](https://arxiv.org/abs/2205.02655) arXiv preprint arXiv:2205.02655
    (2022).'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '[12] 苏等人[“语言模型可以看到：在文本生成中插入视觉控制。”](https://arxiv.org/abs/2205.02655) arXiv预印本arXiv:2205.02655
    (2022)。'
- en: '[13] Zeng et al. [“Socratic Models: Composing Zero-Shot Multimodal Reasoning
    with Language.”](https://arxiv.org/abs/2204.00598) arXiv preprint arXiv:2204.00598
    (2022).'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '[13] 曾等人[“苏格拉底模型：用语言组合零样本多模态推理。”](https://arxiv.org/abs/2204.00598) arXiv预印本arXiv:2204.00598
    (2022)。'
