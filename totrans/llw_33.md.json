["```py\n/usr/bin/ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\" \n```", "```py\n# Install python virtualenv brew install pyenv-virtualenv # Create a virtual environment of any name you like with Python 3.6.4 support pyenv virtualenv 3.6.4 workspace # Activate the virtualenv named \"workspace\" pyenv activate workspace \n```", "```py\ngit clone https://github.com/openai/gym.git cd gym pip install -e . \n```", "```py\nbrew install cmake boost boost-python sdl2 swig wget \n```", "```py\npip install -e '.[atari]' \n```", "```py\ngit clone git@github.com:lilianweng/deep-reinforcement-learning-gym.git cd deep-reinforcement-learning-gym pip install -e .  # install the \"playground\" project. pip install -r requirements.txt  # install required packages. \n```", "```py\nimport gym env = gym.make(\"MsPacman-v0\") \n```", "```py\nfrom collections import defaultdict Q = defaultdict(float) gamma = 0.99  # Discounting factor alpha = 0.5  # soft update param   env = gym.make(\"CartPole-v0\") actions = range(env.action_space)   def update_Q(s, r, a, s_next, done):  max_q_next = max([Q[s_next, a] for a in actions]) # Do not include the next state's value if currently at the terminal state. Q[s, a] += alpha * (r + gamma * max_q_next * (1.0 - done) - Q[s, a]) \n```", "```py\nimport gym   class DiscretizedObservationWrapper(gym.ObservationWrapper):  \"\"\"This wrapper converts a Box observation into a single integer. \"\"\" def __init__(self, env, n_bins=10, low=None, high=None): super().__init__(env) assert isinstance(env.observation_space, Box)   low = self.observation_space.low if low is None else low high = self.observation_space.high if high is None else high   self.n_bins = n_bins self.val_bins = [np.linspace(l, h, n_bins + 1) for l, h in zip(low.flatten(), high.flatten())] self.observation_space = Discrete(n_bins ** low.flatten().shape[0])   def _convert_to_one_number(self, digits): return sum([d * ((self.n_bins + 1) ** i) for i, d in enumerate(digits)])   def observation(self, observation): digits = [np.digitize([x], bins)[0] for x, bins in zip(observation.flatten(), self.val_bins)] return self._convert_to_one_number(digits)     env = DiscretizedObservationWrapper(  env, n_bins=8, low=[-2.4, -2.0, -0.42, -3.5], high=[2.4, 2.0, 0.42, 3.5] ) \n```", "```py\nimport gym import numpy as np n_steps = 100000 epsilon = 0.1  # 10% chances to apply a random action   def act(ob):  if np.random.random() < epsilon: # action_space.sample() is a convenient function to get a random action # that is compatible with this given action space. return env.action_space.sample()   # Pick the action with highest q value. qvals = {a: q[state, a] for a in actions} max_q = max(qvals.values()) # In case multiple actions have the same maximum q value. actions_with_max_q = [a for a, q in qvals.items() if q == max_q] return np.random.choice(actions_with_max_q)   ob = env.reset() rewards = [] reward = 0.0   for step in range(n_steps):  a = act(ob) ob_next, r, done, _ = env.step(a) update_Q(ob, r, a, ob_next, done) reward += r if done: rewards.append(reward) reward = 0.0 ob = env.reset() else: ob = ob_next \n```", "```py\nimport gym env = gym.make('CartPole-v1') # The observation space is `Box(4,)`, a 4-element vector. observation_size = env.observation_space.shape[0] \n```", "```py\nimport tensorflow as tf def dense_nn(inputs, layers_sizes, scope_name):  \"\"\"Creates a densely connected multi-layer neural network. inputs: the input tensor layers_sizes (list<int>): defines the number of units in each layer. The output layer has the size layers_sizes[-1]. \"\"\" with tf.variable_scope(scope_name): for i, size in enumerate(layers_sizes): inputs = tf.layers.dense( inputs, size, # Add relu activation only for internal layers. activation=tf.nn.relu if i < len(layers_sizes) - 1 else None, kernel_initializer=tf.contrib.layers.xavier_initializer(), name=scope_name + '_l' + str(i) ) return inputs \n```", "```py\nbatch_size = 32  # A tunable hyperparameter.   states = tf.placeholder(tf.float32, shape=(batch_size, observation_size), name='state') states_next = tf.placeholder(tf.float32, shape=(batch_size, observation_size), name='state_next') actions = tf.placeholder(tf.int32, shape=(batch_size,), name='action') rewards = tf.placeholder(tf.float32, shape=(batch_size,), name='reward') done_flags = tf.placeholder(tf.float32, shape=(batch_size,), name='done') \n```", "```py\nq = dense(states, [32, 32, 2], name='Q_primary') q_target = dense(states_next, [32, 32, 2], name='Q_target') \n```", "```py\n# The prediction by the primary Q network for the actual actions. action_one_hot = tf.one_hot(actions, act_size, 1.0, 0.0, name='action_one_hot') pred = tf.reduce_sum(q * action_one_hot, reduction_indices=-1, name='q_acted')   # The optimization target defined by the Bellman equation and the target network. max_q_next_by_target = tf.reduce_max(q_target, axis=-1) y = rewards + (1. - done_flags) * gamma * max_q_next_by_target   # The loss measures the mean squared error between prediction and target. loss = tf.reduce_mean(tf.square(pred - tf.stop_gradient(y)), name=\"loss_mse_train\") optimizer = tf.train.AdamOptimizer(0.001).minimize(loss, name=\"adam_optim\") \n```", "```py\n# Get all the variables in the Q primary network. q_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"Q_primary\") # Get all the variables in the Q target network. q_target_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"Q_target\") assert len(q_vars) == len(q_target_vars)   def update_target_q_net_hard():  # Hard update sess.run([v_t.assign(v) for v_t, v in zip(q_target_vars, q_vars)])   def update_target_q_net_soft(tau=0.05):  # Soft update: polyak averaging. sess.run([v_t.assign(v_t * (1. - tau) + v * tau) for v_t, v in zip(q_target_vars, q_vars)]) \n```", "```py\nactions_next = tf.placeholder(tf.int32, shape=(None,), name='action_next') actions_selected_by_q = tf.argmax(q, axis=-1, name='action_selected') \n```", "```py\nactions_next_flatten = actions_next + tf.range(0, batch_size) * q_target.shape[1] max_q_next_target = tf.gather(tf.reshape(q_target, [-1]), actions_next_flatten) y = rewards + (1. - done_flags) * gamma * max_q_next_by_target \n```", "```py\n# batch_data is a dict with keys, \u2018s', \u2018a', \u2018r', \u2018s_next' and \u2018done', containing a batch of transitions. actions_next = sess.run(actions_selected_by_q, {states: batch_data['s_next']}) \n```", "```py\nq_hidden = dense_nn(states, [32], name='Q_primary_hidden') adv = dense_nn(q_hidden, [32, env.action_space.n], name='Q_primary_adv') v = dense_nn(q_hidden, [32, 1], name='Q_primary_v')   # Average dueling q = v + (adv - tf.reduce_mean(adv, reduction_indices=1, keepdims=True)) \n```", "```py\n# Inputs states = tf.placeholder(tf.float32, shape=(None, obs_size), name='state') actions = tf.placeholder(tf.int32, shape=(None,), name='action') returns = tf.placeholder(tf.float32, shape=(None,), name='return') \n```", "```py\n# Policy network pi = dense_nn(states, [32, 32, env.action_space.n], name='pi_network') sampled_actions = tf.squeeze(tf.multinomial(pi, 1))  # For sampling actions according to probabilities.   with tf.variable_scope('pi_optimize'):  loss_pi = tf.reduce_mean( returns * tf.nn.sparse_softmax_cross_entropy_with_logits( logits=pi, labels=actions), name='loss_pi') optim_pi = tf.train.AdamOptimizer(0.001).minimize(loss_pi, name='adam_optim_pi') \n```", "```py\n# env = gym.make(...) # gamma = 0.99 # sess = tf.Session(...)   def act(ob):  return sess.run(sampled_actions, {states: [ob]})   for _ in range(n_episodes):  ob = env.reset() done = False   obs = [] actions = [] rewards = [] returns = []   while not done: a = act(ob) new_ob, r, done, info = env.step(a)   obs.append(ob) actions.append(a) rewards.append(r) ob = new_ob   # Estimate returns backwards. return_so_far = 0.0 for r in rewards[::-1]: return_so_far = gamma * return_so_far + r returns.append(return_so_far)   returns = returns[::-1]   # Update the policy network with the data from one episode. sess.run([optim_pi], feed_dict={ states: np.array(obs), actions: np.array(actions), returns: np.array(returns), }) \n```", "```py\n# Inputs states = tf.placeholder(tf.float32, shape=(None, observation_size), name='state') actions = tf.placeholder(tf.int32, shape=(None,), name='action') td_targets = tf.placeholder(tf.float32, shape=(None,), name='td_target')   # Actor: action probabilities actor = dense_nn(states, [32, 32, env.action_space.n], name='actor')   # Critic: action value (Q-value) critic = dense_nn(states, [32, 32, 1], name='critic')   action_ohe = tf.one_hot(actions, act_size, 1.0, 0.0, name='action_one_hot') pred_value = tf.reduce_sum(critic * action_ohe, reduction_indices=-1, name='q_acted') td_errors = td_targets - tf.reshape(pred_value, [-1])   with tf.variable_scope('critic_train'):  loss_c = tf.reduce_mean(tf.square(td_errors)) optim_c = tf.train.AdamOptimizer(0.01).minimize(loss_c)   with tf.variable_scope('actor_train'):  loss_a = tf.reduce_mean( tf.stop_gradient(td_errors) * tf.nn.sparse_softmax_cross_entropy_with_logits( logits=actor, labels=actions), name='loss_actor') optim_a = tf.train.AdamOptimizer(0.01).minimize(loss_a)   train_ops = [optim_c, optim_a] \n```"]