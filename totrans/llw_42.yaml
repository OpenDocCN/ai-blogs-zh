- en: From GAN to WGAN
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从GAN到WGAN
- en: 原文：[https://lilianweng.github.io/posts/2017-08-20-gan/](https://lilianweng.github.io/posts/2017-08-20-gan/)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://lilianweng.github.io/posts/2017-08-20-gan/](https://lilianweng.github.io/posts/2017-08-20-gan/)
- en: '[Updated on 2018-09-30: thanks to Yoonju, we have this post translated in [Korean](https://github.com/yjucho1/articles/blob/master/fromGANtoWGAN/readme.md)!]'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[2018-09-30更新：感谢Yoonju，我们有这篇文章的[韩文翻译](https://github.com/yjucho1/articles/blob/master/fromGANtoWGAN/readme.md)!]'
- en: '[Updated on 2019-04-18: this post is also available on [arXiv](https://arxiv.org/abs/1904.08994).]'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[2019-04-18更新：这篇文章也可以在[arXiv](https://arxiv.org/abs/1904.08994)上找到。]'
- en: '[Generative adversarial network](https://arxiv.org/pdf/1406.2661.pdf) (GAN)
    has shown great results in many generative tasks to replicate the real-world rich
    content such as images, human language, and music. It is inspired by game theory:
    two models, a generator and a critic, are competing with each other while making
    each other stronger at the same time. However, it is rather challenging to train
    a GAN model, as people are facing issues like training instability or failure
    to converge.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[生成对抗网络](https://arxiv.org/pdf/1406.2661.pdf)（GAN）在许多生成任务中展现出了出色的结果，如复制现实世界丰富内容，如图像、人类语言和音乐。它受到博弈论的启发：两个模型，一个生成器和一个评论者，彼此竞争，同时使彼此变得更强。然而，训练GAN模型相当具有挑战性，因为人们面临着训练不稳定或无法收敛的问题。'
- en: Here I would like to explain the maths behind the generative adversarial network
    framework, why it is hard to be trained, and finally introduce a modified version
    of GAN intended to solve the training difficulties.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我想解释生成对抗网络框架背后的数学原理，为什么它难以训练，最后介绍一个旨在解决训练困难的修改版GAN。
- en: Kullback–Leibler and Jensen–Shannon Divergence
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kullback–Leibler和Jensen–Shannon散度
- en: Before we start examining GANs closely, let us first review two metrics for
    quantifying the similarity between two probability distributions.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始仔细研究GAN之前，让我们首先回顾两个用于量化两个概率分布相似性的度量标准。
- en: (1) [KL (Kullback–Leibler) divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)
    measures how one probability distribution $p$ diverges from a second expected
    probability distribution $q$.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: (1) [KL（Kullback–Leibler）散度](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)衡量一个概率分布$p$与第二个期望概率分布$q$之间的差异。
- en: $$ D_{KL}(p \| q) = \int_x p(x) \log \frac{p(x)}{q(x)} dx $$
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: $$ D_{KL}(p \| q) = \int_x p(x) \log \frac{p(x)}{q(x)} dx $$
- en: $D_{KL}$ achieves the minimum zero when $p(x)$ == $q(x)$ everywhere.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 当$p(x)$ == $q(x)$时，$D_{KL}$达到最小值零。
- en: It is noticeable according to the formula that KL divergence is asymmetric.
    In cases where $p(x)$ is close to zero, but $q(x)$ is significantly non-zero,
    the $q$’s effect is disregarded. It could cause buggy results when we just want
    to measure the similarity between two equally important distributions.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 根据公式可以看出，KL散度是不对称的。在$p(x)$接近零但$q(x)$显著非零的情况下，会忽略$q$的影响。当我们只想衡量两个同等重要分布之间的相似性时，可能会导致错误的结果。
- en: (2) [Jensen–Shannon Divergence](https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence)
    is another measure of similarity between two probability distributions, bounded
    by $[0, 1]$. JS divergence is symmetric (yay!) and more smooth. Check this [Quora
    post](https://www.quora.com/Why-isnt-the-Jensen-Shannon-divergence-used-more-often-than-the-Kullback-Leibler-since-JS-is-symmetric-thus-possibly-a-better-indicator-of-distance)
    if you are interested in reading more about the comparison between KL divergence
    and JS divergence.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: (2) [Jensen–Shannon散度](https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence)是另一种衡量两个概率分布相似性的方法，范围在$[0,
    1]$之间。JS散度是对称的（耶！）且更加平滑。如果您对KL散度和JS散度之间的比较感兴趣，请查看这篇[Quora帖子](https://www.quora.com/Why-isnt-the-Jensen-Shannon-divergence-used-more-often-than-the-Kullback-Leibler-since-JS-is-symmetric-thus-possibly-a-better-indicator-of-distance)。
- en: $$ D_{JS}(p \| q) = \frac{1}{2} D_{KL}(p \| \frac{p + q}{2}) + \frac{1}{2} D_{KL}(q
    \| \frac{p + q}{2}) $$![](../Images/78bf48c2d963829a7e8b146ecef953b4.png)
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: $$ D_{JS}(p \| q) = \frac{1}{2} D_{KL}(p \| \frac{p + q}{2}) + \frac{1}{2} D_{KL}(q
    \| \frac{p + q}{2}) $$![](../Images/78bf48c2d963829a7e8b146ecef953b4.png)
- en: Fig. 1\. Given two Gaussian distribution, $p$ with mean=0 and std=1 and $q$
    with mean=1 and std=1\. The average of two distributions is labelled as $m=(p+q)/2$.
    KL divergence $D_{KL}$ is asymmetric but JS divergence $D_{JS}$ is symmetric.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图1。给定两个高斯分布，$p$的均值=0，标准差=1，$q$的均值=1，标准差=1。两个分布的平均值标记为$m=(p+q)/2$。KL散度$D_{KL}$是不对称的，但JS散度$D_{JS}$是对称的。
- en: Some believe ([Huszar, 2015](https://arxiv.org/pdf/1511.05101.pdf)) that one
    reason behind GANs’ big success is switching the loss function from asymmetric
    KL divergence in traditional maximum-likelihood approach to symmetric JS divergence.
    We will discuss more on this point in the next section.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 一些人认为（[Huszar, 2015](https://arxiv.org/pdf/1511.05101.pdf)）GAN取得巨大成功的一个原因是将损失函数从传统的最大似然方法中的不对称KL散度转换为对称JS散度。我们将在下一节中更多讨论这一点。
- en: Generative Adversarial Network (GAN)
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成对抗网络（GAN）
- en: 'GAN consists of two models:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: GAN由两个模型组成：
- en: A discriminator $D$ estimates the probability of a given sample coming from
    the real dataset. It works as a critic and is optimized to tell the fake samples
    from the real ones.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个鉴别器$D$估计给定样本来自真实数据集的概率。它作为批评者，被优化为区分假样本和真实样本。
- en: A generator $G$ outputs synthetic samples given a noise variable input $z$ ($z$
    brings in potential output diversity). It is trained to capture the real data
    distribution so that its generative samples can be as real as possible, or in
    other words, can trick the discriminator to offer a high probability.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成器$G$给出合成样本，给定一个噪声变量输入$z$（$z$带来潜在的输出多样性）。它被训练来捕捉真实数据分布，使得其生成的样本尽可能真实，或者换句话说，可以欺骗鉴别器提供高概率。
- en: '![](../Images/d27243d75d975c2e35050e6d803bdd93.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d27243d75d975c2e35050e6d803bdd93.png)'
- en: 'Fig. 2\. Architecture of a generative adversarial network. (Image source: [www.kdnuggets.com/2017/01/generative-...-learning.html](http://www.kdnuggets.com/2017/01/generative-adversarial-networks-hot-topic-machine-learning.html))'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图2. 生成对抗网络的架构。（图片来源：[www.kdnuggets.com/2017/01/generative-...-learning.html](http://www.kdnuggets.com/2017/01/generative-adversarial-networks-hot-topic-machine-learning.html)）
- en: 'These two models compete against each other during the training process: the
    generator $G$ is trying hard to trick the discriminator, while the critic model
    $D$ is trying hard not to be cheated. This interesting zero-sum game between two
    models motivates both to improve their functionalities.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个模型在训练过程中相互竞争：生成器$G$努力欺骗鉴别器，而批评模型$D$努力不被欺骗。这两个模型之间的有趣的零和博弈激励它们改进功能。
- en: Given,
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 给定，
- en: '| Symbol | Meaning | Notes |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| 符号 | 含义 | 备注 |'
- en: '| --- | --- | --- |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| $p_{z}$ | Data distribution over noise input $z$ | Usually, just uniform.
    |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| $p_{z}$ | 噪声输入$z$上的数据分布 | 通常是均匀的。 |'
- en: '| $p_{g}$ | The generator’s distribution over data $x$ |  |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| $p_{g}$ | 生成器对数据$x$的分布 |  |'
- en: '| $p_{r}$ | Data distribution over real sample $x$ |  |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| $p_{r}$ | 真实样本$x$上的数据分布 |  |'
- en: On one hand, we want to make sure the discriminator $D$’s decisions over real
    data are accurate by maximizing $\mathbb{E}_{x \sim p_{r}(x)} [\log D(x)]$. Meanwhile,
    given a fake sample $G(z), z \sim p_z(z)$, the discriminator is expected to output
    a probability, $D(G(z))$, close to zero by maximizing $\mathbb{E}_{z \sim p_{z}(z)}
    [\log (1 - D(G(z)))]$.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 一方面，我们希望通过最大化$\mathbb{E}_{x \sim p_{r}(x)} [\log D(x)]$来确保鉴别器$D$对真实数据的决策是准确的。同时，对于一个假样本$G(z),
    z \sim p_z(z)$，期望鉴别器输出一个接近零的概率，即$D(G(z))$，通过最大化$\mathbb{E}_{z \sim p_{z}(z)} [\log
    (1 - D(G(z)))]$。
- en: On the other hand, the generator is trained to increase the chances of $D$ producing
    a high probability for a fake example, thus to minimize $\mathbb{E}_{z \sim p_{z}(z)}
    [\log (1 - D(G(z)))]$.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，生成器被训练来增加$D$对假例产生高概率的机会，从而最小化$\mathbb{E}_{z \sim p_{z}(z)} [\log (1 - D(G(z)))]$。
- en: 'When combining both aspects together, $D$ and $G$ are playing a **minimax game**
    in which we should optimize the following loss function:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 当将两个方面结合在一起时，$D$和$G$在一个**极小极大博弈**中，我们应该优化以下损失函数：
- en: $$ \begin{aligned} \min_G \max_D L(D, G) & = \mathbb{E}_{x \sim p_{r}(x)} [\log
    D(x)] + \mathbb{E}_{z \sim p_z(z)} [\log(1 - D(G(z)))] \\ & = \mathbb{E}_{x \sim
    p_{r}(x)} [\log D(x)] + \mathbb{E}_{x \sim p_g(x)} [\log(1 - D(x)] \end{aligned}
    $$
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \begin{aligned} \min_G \max_D L(D, G) & = \mathbb{E}_{x \sim p_{r}(x)} [\log
    D(x)] + \mathbb{E}_{z \sim p_z(z)} [\log(1 - D(G(z)))] \\ & = \mathbb{E}_{x \sim
    p_{r}(x)} [\log D(x)] + \mathbb{E}_{x \sim p_g(x)} [\log(1 - D(x)] \end{aligned}
    $$
- en: ($\mathbb{E}_{x \sim p_{r}(x)} [\log D(x)]$ has no impact on $G$ during gradient
    descent updates.)
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: （在梯度下降更新过程中，$\mathbb{E}_{x \sim p_{r}(x)} [\log D(x)]$对$G$没有影响。）
- en: What is the optimal value for D?
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 鉴别器$D$的最佳值是多少？
- en: Now we have a well-defined loss function. Let’s first examine what is the best
    value for $D$.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一个明确定义的损失函数。让我们首先检查$D$的最佳值是多少。
- en: $$ L(G, D) = \int_x \bigg( p_{r}(x) \log(D(x)) + p_g (x) \log(1 - D(x)) \bigg)
    dx $$
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: $$ L(G, D) = \int_x \bigg( p_{r}(x) \log(D(x)) + p_g (x) \log(1 - D(x)) \bigg)
    dx $$
- en: Since we are interested in what is the best value of $D(x)$ to maximize $L(G,
    D)$, let us label
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们对最大化$L(G, D)$时$D(x)$的最佳值感兴趣，让我们标记
- en: $$ \tilde{x} = D(x), A=p_{r}(x), B=p_g(x) $$
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \tilde{x} = D(x), A=p_{r}(x), B=p_g(x) $$
- en: 'And then what is inside the integral (we can safely ignore the integral because
    $x$ is sampled over all the possible values) is:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 然后积分中的内容是什么（我们可以安全地忽略积分，因为$x$是在所有可能值上进行采样的）：
- en: $$ \begin{aligned} f(\tilde{x}) & = A log\tilde{x} + B log(1-\tilde{x}) \\ \frac{d
    f(\tilde{x})}{d \tilde{x}} & = A \frac{1}{ln10} \frac{1}{\tilde{x}} - B \frac{1}{ln10}
    \frac{1}{1 - \tilde{x}} \\ & = \frac{1}{ln10} (\frac{A}{\tilde{x}} - \frac{B}{1-\tilde{x}})
    \\ & = \frac{1}{ln10} \frac{A - (A + B)\tilde{x}}{\tilde{x} (1 - \tilde{x})} \\
    \end{aligned} $$
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \begin{aligned} f(\tilde{x}) & = A log\tilde{x} + B log(1-\tilde{x}) \\ \frac{d
    f(\tilde{x})}{d \tilde{x}} & = A \frac{1}{ln10} \frac{1}{\tilde{x}} - B \frac{1}{ln10}
    \frac{1}{1 - \tilde{x}} \\ & = \frac{1}{ln10} (\frac{A}{\tilde{x}} - \frac{B}{1-\tilde{x}})
    \\ & = \frac{1}{ln10} \frac{A - (A + B)\tilde{x}}{\tilde{x} (1 - \tilde{x})} \\
    \end{aligned} $$
- en: 'Thus, set $\frac{d f(\tilde{x})}{d \tilde{x}} = 0$, we get the best value of
    the discriminator: $D^*(x) = \tilde{x}^* = \frac{A}{A + B} = \frac{p_{r}(x)}{p_{r}(x)
    + p_g(x)} \in [0, 1]$.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，设$\frac{d f(\tilde{x})}{d \tilde{x}} = 0$，我们得到鉴别器的最佳值：$D^*(x) = \tilde{x}^*
    = \frac{A}{A + B} = \frac{p_{r}(x)}{p_{r}(x) + p_g(x)} \in [0, 1]$。
- en: Once the generator is trained to its optimal, $p_g$ gets very close to $p_{r}$.
    When $p_g = p_{r}$, $D^*(x)$ becomes $1/2$.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦生成器训练到最佳状态，$p_g$会非常接近$p_{r}$。当$p_g = p_{r}$时，$D^*(x)$变为$1/2$。
- en: What is the global optimal?
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 全局最优是什么？
- en: 'When both $G$ and $D$ are at their optimal values, we have $p_g = p_{r}$ and
    $D^*(x) = 1/2$ and the loss function becomes:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 当$G$和$D$都达到最佳值时，我们有$p_g = p_{r}$和$D^*(x) = 1/2$，损失函数变为：
- en: $$ \begin{aligned} L(G, D^*) &= \int_x \bigg( p_{r}(x) \log(D^*(x)) + p_g (x)
    \log(1 - D^*(x)) \bigg) dx \\ &= \log \frac{1}{2} \int_x p_{r}(x) dx + \log \frac{1}{2}
    \int_x p_g(x) dx \\ &= -2\log2 \end{aligned} $$
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \begin{aligned} L(G, D^*) &= \int_x \bigg( p_{r}(x) \log(D^*(x)) + p_g (x)
    \log(1 - D^*(x)) \bigg) dx \\ &= \log \frac{1}{2} \int_x p_{r}(x) dx + \log \frac{1}{2}
    \int_x p_g(x) dx \\ &= -2\log2 \end{aligned} $$
- en: What does the loss function represent?
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 损失函数代表什么？
- en: 'According to the formula listed in the [previous section](https://lilianweng.github.io/posts/2017-08-20-gan/#kullbackleibler-and-jensenshannon-divergence),
    JS divergence between $p_{r}$ and $p_g$ can be computed as:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 根据[前一节](https://lilianweng.github.io/posts/2017-08-20-gan/#kullbackleibler-and-jensenshannon-divergence)中列出的公式，$p_{r}$和$p_g$之间的JS散度可以计算为：
- en: $$ \begin{aligned} D_{JS}(p_{r} \| p_g) =& \frac{1}{2} D_{KL}(p_{r} || \frac{p_{r}
    + p_g}{2}) + \frac{1}{2} D_{KL}(p_{g} || \frac{p_{r} + p_g}{2}) \\ =& \frac{1}{2}
    \bigg( \log2 + \int_x p_{r}(x) \log \frac{p_{r}(x)}{p_{r} + p_g(x)} dx \bigg)
    + \\& \frac{1}{2} \bigg( \log2 + \int_x p_g(x) \log \frac{p_g(x)}{p_{r} + p_g(x)}
    dx \bigg) \\ =& \frac{1}{2} \bigg( \log4 + L(G, D^*) \bigg) \end{aligned} $$
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \begin{aligned} D_{JS}(p_{r} \| p_g) =& \frac{1}{2} D_{KL}(p_{r} || \frac{p_{r}
    + p_g}{2}) + \frac{1}{2} D_{KL}(p_{g} || \frac{p_{r} + p_g}{2}) \\ =& \frac{1}{2}
    \bigg( \log2 + \int_x p_{r}(x) \log \frac{p_{r}(x)}{p_{r} + p_g(x)} dx \bigg)
    + \\& \frac{1}{2} \bigg( \log2 + \int_x p_g(x) \log \frac{p_g(x)}{p_{r} + p_g(x)}
    dx \bigg) \\ =& \frac{1}{2} \bigg( \log4 + L(G, D^*) \bigg) \end{aligned} $$
- en: Thus,
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，
- en: $$ L(G, D^*) = 2D_{JS}(p_{r} \| p_g) - 2\log2 $$
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: $$ L(G, D^*) = 2D_{JS}(p_{r} \| p_g) - 2\log2 $$
- en: Essentially the loss function of GAN quantifies the similarity between the generative
    data distribution $p_g$ and the real sample distribution $p_{r}$ by JS divergence
    when the discriminator is optimal. The best $G^*$ that replicates the real data
    distribution leads to the minimum $L(G^*, D^*) = -2\log2$ which is aligned with
    equations above.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，GAN的损失函数通过JS散度量化了在鉴别器最优时生成数据分布$p_g$与真实样本分布$p_{r}$之间的相似性。最佳的$G^*$复制真实数据分布导致最小的$L(G^*,
    D^*) = -2\log2$，与上述方程一致。
- en: '**Other Variations of GAN**: There are many variations of GANs in different
    contexts or designed for different tasks. For example, for semi-supervised learning,
    one idea is to update the discriminator to output real class labels, $1, \dots,
    K-1$, as well as one fake class label $K$. The generator model aims to trick the
    discriminator to output a classification label smaller than $K$.'
  id: totrans-52
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**GAN的其他变体**：在不同背景或为不同任务设计的许多GAN变体。例如，对于半监督学习，一个想法是更新鉴别器以输出真实类标签$1, \dots,
    K-1$，以及一个假类标签$K$。生成器模型旨在欺骗鉴别器输出小于$K$的分类标签。'
- en: '**Tensorflow Implementation**: [carpedm20/DCGAN-tensorflow](https://github.com/carpedm20/DCGAN-tensorflow)'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '**Tensorflow 实现**：[carpedm20/DCGAN-tensorflow](https://github.com/carpedm20/DCGAN-tensorflow)'
- en: Problems in GANs
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GAN中的问题
- en: Although GAN has shown great success in the realistic image generation, the
    training is not easy; The process is known to be slow and unstable.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管GAN在逼真图像生成方面取得了巨大成功，但训练并不容易；这个过程被认为是缓慢且不稳定的。
- en: Hard to achieve Nash equilibrium
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 很难实现纳什均衡
- en: '[Salimans et al. (2016)](http://papers.nips.cc/paper/6125-improved-techniques-for-training-gans.pdf)
    discussed the problem with GAN’s gradient-descent-based training procedure. Two
    models are trained simultaneously to find a [Nash equilibrium](https://en.wikipedia.org/wiki/Nash_equilibrium)
    to a two-player non-cooperative game. However, each model updates its cost independently
    with no respect to another player in the game. Updating the gradient of both models
    concurrently cannot guarantee a convergence.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '[Salimans等人（2016）](http://papers.nips.cc/paper/6125-improved-techniques-for-training-gans.pdf)讨论了GAN基于梯度下降的训练过程中的问题。两个模型同时训练以找到两人非合作博弈的[纳什均衡](https://en.wikipedia.org/wiki/Nash_equilibrium)。然而，每个模型独立地更新其成本，而不考虑游戏中的另一个玩家。同时更新两个模型的梯度不能保证收敛。'
- en: Let’s check out a simple example to better understand why it is difficult to
    find a Nash equilibrium in an non-cooperative game. Suppose one player takes control
    of $x$ to minimize $f_1(x) = xy$, while at the same time the other player constantly
    updates $y$ to minimize $f_2(y) = -xy$.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个简单的例子，更好地理解为什么在非合作博弈中很难找到纳什均衡。假设一个玩家控制$x$以最小化$f_1(x) = xy$，同时另一个玩家不断更新$y$以最小化$f_2(y)
    = -xy$。
- en: Because $\frac{\partial f_1}{\partial x} = y$ and $\frac{\partial f_2}{\partial
    y} = -x$, we update $x$ with $x-\eta \cdot y$ and $y$ with $y+ \eta \cdot x$ simulitanously
    in one iteration, where $\eta$ is the learning rate. Once $x$ and $y$ have different
    signs, every following gradient update causes huge oscillation and the instability
    gets worse in time, as shown in Fig. 3.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 因为$\frac{\partial f_1}{\partial x} = y$和$\frac{\partial f_2}{\partial y} = -x$，我们在一个迭代中同时更新$x$为$x-\eta
    \cdot y$和$y$为$y+ \eta \cdot x$，其中$\eta$是学习率。一旦$x$和$y$有不同的符号，每次后续梯度更新都会导致巨大的振荡，并且随着时间的推移不稳定性变得更加严重，如图3所示。
- en: '![](../Images/e7a06563aba4ac52b2e059ea035dc9b2.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e7a06563aba4ac52b2e059ea035dc9b2.png)'
- en: Fig. 3\. A simulation of our example for updating $x$ to minimize $xy$ and updating
    $y$ to minimize $-xy$. The learning rate $\eta = 0.1$. With more iterations, the
    oscillation grows more and more unstable.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图3。我们的示例模拟了更新$x$以最小化$xy$和更新$y$以最小化$-xy$的过程。学习率$\eta = 0.1$。随着更多迭代，振荡变得越来越不稳定。
- en: Low dimensional supports
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 低维支撑
- en: '| Term | Explanation |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| 术语 | 解释 |'
- en: '| --- | --- |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| [Manifold](https://en.wikipedia.org/wiki/Manifold) | A topological space
    that locally resembles Euclidean space near each point. Precisely, when this Euclidean
    space is of **dimension $n$**, the manifold is referred as **$n$-manifold**. |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| [流形](https://en.wikipedia.org/wiki/Manifold) | 在每个点附近局部类似于欧几里得空间的拓扑空间。准确地说，当这个欧几里得空间的维度为**$n$**时，流形被称为**$n$-流形**。'
- en: '| [Support](https://en.wikipedia.org/wiki/Support_(mathematics)) | A real-valued
    function $f$ is the subset of the domain containing those elements which are not
    mapped to **zero**. |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| [支撑](https://en.wikipedia.org/wiki/Support_(mathematics)) | 实值函数$f$是包含那些不映射到**零**的域的子集。
    |'
- en: '[Arjovsky and Bottou (2017)](https://arxiv.org/pdf/1701.04862.pdf) discussed
    the problem of the [supports](https://en.wikipedia.org/wiki/Support_(mathematics))
    of $p_r$ and $p_g$ lying on low dimensional [manifolds](https://en.wikipedia.org/wiki/Manifold)
    and how it contributes to the instability of GAN training thoroughly in a very
    theoretical paper [“Towards principled methods for training generative adversarial
    networks”](https://arxiv.org/pdf/1701.04862.pdf).'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '[Arjovsky和Bottou（2017）](https://arxiv.org/pdf/1701.04862.pdf)讨论了$p_r$和$p_g$的[支撑](https://en.wikipedia.org/wiki/Support_(mathematics))位于低维[流形](https://en.wikipedia.org/wiki/Manifold)上的问题，以及它如何在一篇非常理论的论文“Towards
    principled methods for training generative adversarial networks”中全面讨论了GAN训练的不稳定性。'
- en: The dimensions of many real-world datasets, as represented by $p_r$, only appear
    to be **artificially high**. They have been found to concentrate in a lower dimensional
    manifold. This is actually the fundamental assumption for [Manifold Learning](http://scikit-learn.org/stable/modules/manifold.html).
    Thinking of the real world images, once the theme or the contained object is fixed,
    the images have a lot of restrictions to follow, i.e., a dog should have two ears
    and a tail, and a skyscraper should have a straight and tall body, etc. These
    restrictions keep images aways from the possibility of having a high-dimensional
    free form.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 许多真实世界数据集的维度，如 $p_r$ 所代表的，只是**人为地高**。已经发现它们集中在一个较低维度的流形中。这实际上是[流形学习](http://scikit-learn.org/stable/modules/manifold.html)的基本假设。想象一下真实世界的图像，一旦主题或包含的对象固定，图像就有很多限制要遵循，即，一只狗应该有两只耳朵和一条尾巴，一座摩天大楼应该有笔直而高耸的身体等。这些限制使图像远离了具有高维度自由形式的可能性。
- en: $p_g$ lies in a low dimensional manifolds, too. Whenever the generator is asked
    to a much larger image like 64x64 given a small dimension, such as 100, noise
    variable input $z$, the distribution of colors over these 4096 pixels has been
    defined by the small 100-dimension random number vector and can hardly fill up
    the whole high dimensional space.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: $p_g$ 也位于低维流形中。每当生成器被要求生成一个更大的图像，比如 64x64，给定一个小维度，比如 100，噪声变量输入 $z$，这 4096 个像素的颜色分布已经由小的
    100 维随机数向量定义，并且几乎无法填满整个高维空间。
- en: Because both $p_g$ and $p_r$ rest in low dimensional manifolds, they are almost
    certainly gonna be disjoint (See Fig. 4). When they have disjoint supports, we
    are always capable of finding a perfect discriminator that separates real and
    fake samples 100% correctly. Check the [paper](https://arxiv.org/pdf/1701.04862.pdf)
    if you are curious about the proof.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 因为 $p_g$ 和 $p_r$ 都位于低维流形中，它们几乎肯定是不相交的（见图 4）。当它们具有不相交的支持时，我们总是能够找到一个完美的鉴别器，将真假样本完全正确地分开。如果你对证明感兴趣，请查看[论文](https://arxiv.org/pdf/1701.04862.pdf)。
- en: '![](../Images/ccff23a535f0a6816bf4bf4f24779101.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ccff23a535f0a6816bf4bf4f24779101.png)'
- en: Fig. 4\. Low dimensional manifolds in high dimension space can hardly have overlaps.
    (Left) Two lines in a three-dimension space. (Right) Two surfaces in a three-dimension
    space.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4\. 高维空间中的低维流形几乎不可能重叠。（左）三维空间中的两条线。（右）三维空间中的两个曲面。
- en: Vanishing gradient
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 梯度消失
- en: When the discriminator is perfect, we are guaranteed with $D(x) = 1, \forall
    x \in p_r$ and $D(x) = 0, \forall x \in p_g$. Therefore the loss function $L$
    falls to zero and we end up with no gradient to update the loss during learning
    iterations. Fig. 5 demonstrates an experiment when the discriminator gets better,
    the gradient vanishes fast.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 当鉴别器完美时，我们保证 $D(x) = 1, \forall x \in p_r$ 和 $D(x) = 0, \forall x \in p_g$。因此损失函数
    $L$ 降至零，我们最终没有梯度来更新学习迭代中的损失。图 5 展示了一个实验，当鉴别器变得更好时，梯度迅速消失。
- en: '![](../Images/125b72a1c2964a8a6428074fc2da5061.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/125b72a1c2964a8a6428074fc2da5061.png)'
- en: 'Fig. 5\. First, a DCGAN is trained for 1, 10 and 25 epochs. Then, with the
    **generator fixed**, a discriminator is trained from scratch and measure the gradients
    with the original cost function. We see the gradient norms **decay quickly** (in
    log scale), in the best case 5 orders of magnitude after 4000 discriminator iterations.
    (Image source: [Arjovsky and Bottou, 2017](https://arxiv.org/pdf/1701.04862.pdf))'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5\. 首先，DCGAN 被训练了 1、10 和 25 个 epochs。然后，**固定生成器**，从头开始训练一个鉴别器，并用原始成本函数测量梯度。我们看到梯度范数**迅速衰减**（以对数刻度表示），在最好的情况下，在
    4000 个鉴别器迭代后衰减了 5 个数量级。（图片来源：[Arjovsky and Bottou, 2017](https://arxiv.org/pdf/1701.04862.pdf)）
- en: 'As a result, training a GAN faces a **dilemma**:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，训练 GAN 面临着一个**困境**：
- en: If the discriminator behaves badly, the generator does not have accurate feedback
    and the loss function cannot represent the reality.
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果鉴别器表现不佳，生成器就无法获得准确的反馈，损失函数无法代表现实。
- en: If the discriminator does a great job, the gradient of the loss function drops
    down to close to zero and the learning becomes super slow or even jammed.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果鉴别器表现出色，损失函数的梯度会下降到接近零，学习变得非常缓慢甚至陷入僵局。
- en: This dilemma clearly is capable to make the GAN training very tough.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这个困境显然能够使 GAN 训练变得非常艰难。
- en: Mode collapse
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模式坍塌
- en: During the training, the generator may collapse to a setting where it always
    produces same outputs. This is a common failure case for GANs, commonly referred
    to as **Mode Collapse**. Even though the generator might be able to trick the
    corresponding discriminator, it fails to learn to represent the complex real-world
    data distribution and gets stuck in a small space with extremely low variety.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，生成器可能会崩溃到始终产生相同输出的设置。这是 GAN 的常见失败案例，通常称为**模式崩溃**。即使生成器可能能够欺骗相应的鉴别器，但它未能学会表示复杂的真实世界数据分布，并且陷入了一个极低多样性的小空间中。
- en: '![](../Images/608080aa51a617958b6ead22f82a3c9b.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/608080aa51a617958b6ead22f82a3c9b.png)'
- en: 'Fig. 6\. A DCGAN model is trained with an MLP network with 4 layers, 512 units
    and ReLU activation function, configured to lack a strong inductive bias for image
    generation. The results shows a significant degree of mode collapse. (Image source:
    [Arjovsky, Chintala, & Bottou, 2017.](https://arxiv.org/pdf/1701.07875.pdf))'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6\. 使用具有 4 层、512 个单元和 ReLU 激活函数的 MLP 网络训练 DCGAN 模型，配置为缺乏图像生成的强归纳偏差。结果显示了显著的模式崩溃。（图片来源：[Arjovsky,
    Chintala, & Bottou, 2017.](https://arxiv.org/pdf/1701.07875.pdf)）
- en: Lack of a proper evaluation metric
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 缺乏适当的评估指标
- en: Generative adversarial networks are not born with a good objection function
    that can inform us the training progress. Without a good evaluation metric, it
    is like working in the dark. No good sign to tell when to stop; No good indicator
    to compare the performance of multiple models.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 生成对抗网络并没有一个可以告诉我们训练进度的良好目标函数。缺乏一个良好的评估指标，就像在黑暗中工作一样。没有好的信号告诉何时停止；没有好的指标来比较多个模型的性能。
- en: Improved GAN Training
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 改进的 GAN 训练
- en: The following suggestions are proposed to help stabilize and improve the training
    of GANs.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 提出以下建议以帮助稳定和改进 GAN 的训练。
- en: First five methods are practical techniques to achieve faster convergence of
    GAN training, proposed in [“Improve Techniques for Training GANs”](http://papers.nips.cc/paper/6125-improved-techniques-for-training-gans.pdf).
    The last two are proposed in [“Towards principled methods for training generative
    adversarial networks”](https://arxiv.org/pdf/1701.04862.pdf) to solve the problem
    of disjoint distributions.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 前五种方法是实现更快收敛 GAN 训练的实用技术，提出于[“改进 GAN 训练技术”](http://papers.nips.cc/paper/6125-improved-techniques-for-training-gans.pdf)。最后两种是在[“为生成对抗网络训练提供原则方法”](https://arxiv.org/pdf/1701.04862.pdf)中提出的，以解决分布不一致的问题。
- en: (1) **Feature Matching**
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: (1) **特征匹配**
- en: Feature matching suggests to optimize the discriminator to inspect whether the
    generator’s output matches expected statistics of the real samples. In such a
    scenario, the new loss function is defined as $| \mathbb{E}_{x \sim p_r} f(x)
    - \mathbb{E}_{z \sim p_z(z)}f(G(z)) |_2^2 $, where $f(x)$ can be any computation
    of statistics of features, such as mean or median.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 特征匹配建议优化鉴别器以检查生成器的输出是否与真实样本的预期统计匹配。在这种情况下，新的损失函数定义为 $| \mathbb{E}_{x \sim p_r}
    f(x) - \mathbb{E}_{z \sim p_z(z)}f(G(z)) |_2^2 $，其中 $f(x)$ 可以是特征统计的任何计算，如均值或中位数。
- en: (2) **Minibatch Discrimination**
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: (2) **小批量鉴别**
- en: With minibatch discrimination, the discriminator is able to digest the relationship
    between training data points in one batch, instead of processing each point independently.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 通过小批量鉴别，鉴别器能够在一个批次中消化训练数据点之间的关系，而不是独立处理每个点。
- en: In one minibatch, we approximate the closeness between every pair of samples,
    $c(x_i, x_j)$, and get the overall summary of one data point by summing up how
    close it is to other samples in the same batch, $o(x_i) = \sum_{j} c(x_i, x_j)$.
    Then $o(x_i)$ is explicitly added to the input of the model.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个小批量中，我们近似每对样本之间的接近程度，$c(x_i, x_j)$，并通过将它们与同一批次中其他样本的接近程度相加来得到一个数据点的总结，$o(x_i)
    = \sum_{j} c(x_i, x_j)$。然后将 $o(x_i)$ 明确添加到模型的输入中。
- en: (3) **Historical Averaging**
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: (3) **历史平均**
- en: For both models, add $ | \Theta - \frac{1}{t} \sum_{i=1}^t \Theta_i |^2 $ into
    the loss function, where $\Theta$ is the model parameter and $\Theta_i$ is how
    the parameter is configured at the past training time $i$. This addition piece
    penalizes the training speed when $\Theta$ is changing too dramatically in time.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 对于两个模型，将 $ | \Theta - \frac{1}{t} \sum_{i=1}^t \Theta_i |^2 $ 添加到损失函数中，其中 $\Theta$
    是模型参数，$\Theta_i$ 是参数在过去训练时间 $i$ 的配置。这个额外部分在 $\Theta$ 在时间上变化太剧烈时惩罚训练速度。
- en: (4) **One-sided Label Smoothing**
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: (4) **单边标签平滑**
- en: When feeding the discriminator, instead of providing 1 and 0 labels, use soften
    values such as 0.9 and 0.1\. It is shown to reduce the networks’ vulnerability.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在馈送鉴别器时，不提供1和0标签，而是使用软化的值，如0.9和0.1。已经证明可以减少网络的脆弱性。
- en: (5) **Virtual Batch Normalization** (VBN)
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: (5) **虚拟批次归一化**（VBN）
- en: Each data sample is normalized based on a fixed batch (*“reference batch”*)
    of data rather than within its minibatch. The reference batch is chosen once at
    the beginning and stays the same through the training.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 每个数据样本基于一个固定批次（“参考批次”）的数据进行归一化，而不是在其小批次内进行。参考批次在开始时选择一次，并在训练过程中保持不变。
- en: '**Theano Implementation**: [openai/improved-gan](https://github.com/openai/improved-gan)'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '**Theano实现**：[openai/improved-gan](https://github.com/openai/improved-gan)'
- en: (6) **Adding Noises**.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: (6) **添加噪声**。
- en: Based on the discussion in the [previous section](https://lilianweng.github.io/posts/2017-08-20-gan/#low-dimensional-supports),
    we now know $p_r$ and $p_g$ are disjoint in a high dimensional space and it causes
    the problem of vanishing gradient. To artificially “spread out” the distribution
    and to create higher chances for two probability distributions to have overlaps,
    one solution is to add continuous noises onto the inputs of the discriminator
    $D$.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 根据[上一节](https://lilianweng.github.io/posts/2017-08-20-gan/#low-dimensional-supports)的讨论，我们现在知道$p_r$和$p_g$在高维空间中是不相交的，这导致了梯度消失的问题。为了人为地“扩散”分布并增加两个概率分布重叠的机会，一个解决方案是在鉴别器$D$的输入上添加连续噪声。
- en: (7) **Use Better Metric of Distribution Similarity**
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: (7) **使用更好的分布相似度度量**
- en: The loss function of the vanilla GAN measures the JS divergence between the
    distributions of $p_r$ and $p_g$. This metric fails to provide a meaningful value
    when two distributions are disjoint.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 原始GAN的损失函数衡量了$p_r$和$p_g$的分布之间的JS散度。当两个分布不相交时，这个度量无法提供有意义的值。
- en: '[Wasserstein metric](https://en.wikipedia.org/wiki/Wasserstein_metric) is proposed
    to replace JS divergence because it has a much smoother value space. See more
    in the next section.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '[Wasserstein度量](https://en.wikipedia.org/wiki/Wasserstein_metric)被提出来替代JS散度，因为它具有更加平滑的值空间。更多内容请参见下一节。'
- en: Wasserstein GAN (WGAN)
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Wasserstein GAN（WGAN）
- en: What is Wasserstein distance?
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是Wasserstein距离？
- en: '[Wasserstein Distance](https://en.wikipedia.org/wiki/Wasserstein_metric) is
    a measure of the distance between two probability distributions. It is also called
    **Earth Mover’s distance**, short for EM distance, because informally it can be
    interpreted as the minimum energy cost of moving and transforming a pile of dirt
    in the shape of one probability distribution to the shape of the other distribution.
    The cost is quantified by: the amount of dirt moved x the moving distance.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '[Wasserstein距离](https://en.wikipedia.org/wiki/Wasserstein_metric)是两个概率分布之间的距离度量。它也被称为**地球移动者距离**，简称EM距离，因为非正式地可以解释为将一个概率分布形状的一堆土移动和转换为另一个分布形状的最小能量成本。成本由：移动的土量
    x 移动距离来量化。'
- en: 'Let us first look at a simple case where the probability domain is *discrete*.
    For example, suppose we have two distributions $P$ and $Q$, each has four piles
    of dirt and both have ten shovelfuls of dirt in total. The numbers of shovelfuls
    in each dirt pile are assigned as follows:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先看一个简单的情况，其中概率领域是*离散*的。例如，假设我们有两个分布$P$和$Q$，每个都有四堆土，总共有十铲土。每堆土中的铲土数量分配如下：
- en: $$ P_1 = 3, P_2 = 2, P_3 = 1, P_4 = 4\\ Q_1 = 1, Q_2 = 2, Q_3 = 4, Q_4 = 3 $$
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: $$ P_1 = 3, P_2 = 2, P_3 = 1, P_4 = 4\\ Q_1 = 1, Q_2 = 2, Q_3 = 4, Q_4 = 3 $$
- en: 'In order to change $P$ to look like $Q$, as illustrated in Fig. 7, we:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使$P$看起来像$Q$，如图7所示，我们：
- en: First move 2 shovelfuls from $P_1$ to $P_2$ => $(P_1, Q_1)$ match up.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先将2铲土从$P_1$移动到$P_2$ => $(P_1, Q_1)$匹配。
- en: Then move 2 shovelfuls from $P_2$ to $P_3$ => $(P_2, Q_2)$ match up.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后将2铲土从$P_2$移动到$P_3$ => $(P_2, Q_2)$匹配。
- en: Finally move 1 shovelfuls from $Q_3$ to $Q_4$ => $(P_3, Q_3)$ and $(P_4, Q_4)$
    match up.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后将1铲土从$Q_3$移动到$Q_4$ => $(P_3, Q_3)$和$(P_4, Q_4)$匹配。
- en: 'If we label the cost to pay to make $P_i$ and $Q_i$ match as $\delta_i$, we
    would have $\delta_{i+1} = \delta_i + P_i - Q_i$ and in the example:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将使$P_i$和$Q_i$匹配的成本标记为$\delta_i$，那么我们将有$\delta_{i+1} = \delta_i + P_i - Q_i$，在这个例子中：
- en: $$ \begin{aligned} \delta_0 &= 0\\ \delta_1 &= 0 + 3 - 1 = 2\\ \delta_2 &= 2
    + 2 - 2 = 2\\ \delta_3 &= 2 + 1 - 4 = -1\\ \delta_4 &= -1 + 4 - 3 = 0 \end{aligned}
    $$
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \begin{aligned} \delta_0 &= 0\\ \delta_1 &= 0 + 3 - 1 = 2\\ \delta_2 &= 2
    + 2 - 2 = 2\\ \delta_3 &= 2 + 1 - 4 = -1\\ \delta_4 &= -1 + 4 - 3 = 0 \end{aligned}
    $$
- en: Finally the Earth Mover’s distance is $W = \sum \vert \delta_i \vert = 5$.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，地球移动者距离是$W = \sum \vert \delta_i \vert = 5$。
- en: '![](../Images/325bf18eac38b915dc5d7202e6e9f834.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/325bf18eac38b915dc5d7202e6e9f834.png)'
- en: Fig. 7\. Step-by-step plan of moving dirt between piles in $P$ and $Q$ to make
    them match.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7\. 在 $P$ 和 $Q$ 之间移动污垢的逐步计划。
- en: 'When dealing with the continuous probability domain, the distance formula becomes:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 处理连续概率域时，距离公式变为：
- en: $$ W(p_r, p_g) = \inf_{\gamma \sim \Pi(p_r, p_g)} \mathbb{E}_{(x, y) \sim \gamma}[\|
    x-y \|] $$
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: $$ W(p_r, p_g) = \inf_{\gamma \sim \Pi(p_r, p_g)} \mathbb{E}_{(x, y) \sim \gamma}[\|
    x-y \|] $$
- en: In the formula above, $\Pi(p_r, p_g)$ is the set of all possible joint probability
    distributions between $p_r$ and $p_g$. One joint distribution $\gamma \in \Pi(p_r,
    p_g)$ describes one dirt transport plan, same as the discrete example above, but
    in the continuous probability space. Precisely $\gamma(x, y)$ states the percentage
    of dirt should be transported from point $x$ to $y$ so as to make $x$ follows
    the same probability distribution of $y$. That’s why the marginal distribution
    over $x$ adds up to $p_g$, $\sum_{x} \gamma(x, y) = p_g(y)$ (Once we finish moving
    the planned amount of dirt from every possible $x$ to the target $y$, we end up
    with exactly what $y$ has according to $p_g$.) and vice versa $\sum_{y} \gamma(x,
    y) = p_r(x)$.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的公式中，$\Pi(p_r, p_g)$ 是 $p_r$ 和 $p_g$ 之间所有可能的联合概率分布的集合。一个联合分布 $\gamma \in
    \Pi(p_r, p_g)$ 描述了一个污垢传输计划，与上面的离散例子类似，但在连续概率空间中。准确地说，$\gamma(x, y)$ 表示应该从点 $x$
    传输到点 $y$ 的污垢百分比，以使 $x$ 遵循 $y$ 的相同概率分布。这就是为什么关于 $x$ 的边际分布加起来等于 $p_g$，$\sum_{x}
    \gamma(x, y) = p_g(y)$（一旦我们完成从每个可能的 $x$ 移动计划的污垢量到目标 $y$，我们最终得到了完全符合 $p_g$ 的 $y$）以及反之
    $\sum_{y} \gamma(x, y) = p_r(x)$。
- en: 'When treating $x$ as the starting point and $y$ as the destination, the total
    amount of dirt moved is $\gamma(x, y)$ and the travelling distance is $| x-y |$
    and thus the cost is $\gamma(x, y) \cdot | x-y |$. The expected cost averaged
    across all the $(x,y)$ pairs can be easily computed as:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 将 $x$ 视为起点，$y$ 视为目的地时，移动的总污垢量为 $\gamma(x, y)$，行程距离为 $| x-y |$，因此成本为 $\gamma(x,
    y) \cdot | x-y |$。通过所有 $(x,y)$ 对的期望成本可以轻松计算为：
- en: $$ \sum_{x, y} \gamma(x, y) \| x-y \| = \mathbb{E}_{x, y \sim \gamma} \| x-y
    \| $$
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \sum_{x, y} \gamma(x, y) \| x-y \| = \mathbb{E}_{x, y \sim \gamma} \| x-y
    \| $$
- en: Finally, we take the minimum one among the costs of all dirt moving solutions
    as the EM distance. In the definition of Wasserstein distance, the $\inf$ ([infimum](https://en.wikipedia.org/wiki/Infimum_and_supremum),
    also known as *greatest lower bound*) indicates that we are only interested in
    the smallest cost.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将所有污垢移动解决方案的成本中最小的一个作为 EM 距离。在 Wasserstein 距离的定义中，$\inf$（[infimum](https://en.wikipedia.org/wiki/Infimum_and_supremum)，也称为*最大下界*）表示我们只对最小成本感兴趣。
- en: Why Wasserstein is better than JS or KL divergence?
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么 Wasserstein 距离比 JS 或 KL 散度更好？
- en: Even when two distributions are located in lower dimensional manifolds without
    overlaps, Wasserstein distance can still provide a meaningful and smooth representation
    of the distance in-between.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 即使两个分布位于低维流形中且没有重叠，Wasserstein 距离仍然可以提供有意义且平滑的距离表示。
- en: The WGAN paper exemplified the idea with a simple example.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: WGAN 论文用一个简单的例子阐释了这个想法。
- en: 'Suppose we have two probability distributions, $P$ and $Q$:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有两个概率分布，$P$ 和 $Q$：
- en: $$ \forall (x, y) \in P, x = 0 \text{ and } y \sim U(0, 1)\\ \forall (x, y)
    \in Q, x = \theta, 0 \leq \theta \leq 1 \text{ and } y \sim U(0, 1)\\ $$![](../Images/613cc8800885d396cceb23c5779ec4e2.png)
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \forall (x, y) \in P, x = 0 \text{ and } y \sim U(0, 1)\\ \forall (x, y)
    \in Q, x = \theta, 0 \leq \theta \leq 1 \text{ and } y \sim U(0, 1)\\ $$![](../Images/613cc8800885d396cceb23c5779ec4e2.png)
- en: Fig. 8\. There is no overlap between $P$ and $Q$ when $\theta \neq 0$.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8\. 当 $\theta \neq 0$ 时，$P$ 和 $Q$ 之间没有重叠。
- en: 'When $\theta \neq 0$:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 当 $\theta \neq 0$ 时：
- en: $$ \begin{aligned} D_{KL}(P \| Q) &= \sum_{x=0, y \sim U(0, 1)} 1 \cdot \log\frac{1}{0}
    = +\infty \\ D_{KL}(Q \| P) &= \sum_{x=\theta, y \sim U(0, 1)} 1 \cdot \log\frac{1}{0}
    = +\infty \\ D_{JS}(P, Q) &= \frac{1}{2}(\sum_{x=0, y \sim U(0, 1)} 1 \cdot \log\frac{1}{1/2}
    + \sum_{x=0, y \sim U(0, 1)} 1 \cdot \log\frac{1}{1/2}) = \log 2\\ W(P, Q) &=
    |\theta| \end{aligned} $$
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \begin{aligned} D_{KL}(P \| Q) &= \sum_{x=0, y \sim U(0, 1)} 1 \cdot \log\frac{1}{0}
    = +\infty \\ D_{KL}(Q \| P) &= \sum_{x=\theta, y \sim U(0, 1)} 1 \cdot \log\frac{1}{0}
    = +\infty \\ D_{JS}(P, Q) &= \frac{1}{2}(\sum_{x=0, y \sim U(0, 1)} 1 \cdot \log\frac{1}{1/2}
    + \sum_{x=0, y \sim U(0, 1)} 1 \cdot \log\frac{1}{1/2}) = \log 2\\ W(P, Q) &=
    |\theta| \end{aligned} $$
- en: 'But when $\theta = 0$, two distributions are fully overlapped:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 但是当 $\theta = 0$ 时，两个分布完全重叠：
- en: $$ \begin{aligned} D_{KL}(P \| Q) &= D_{KL}(Q \| P) = D_{JS}(P, Q) = 0\\ W(P,
    Q) &= 0 = \lvert \theta \rvert \end{aligned} $$
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \begin{aligned} D_{KL}(P \| Q) &= D_{KL}(Q \| P) = D_{JS}(P, Q) = 0\\ W(P,
    Q) &= 0 = \lvert \theta \rvert \end{aligned} $$
- en: $D_{KL}$ gives us inifity when two distributions are disjoint. The value of
    $D_{JS}$ has sudden jump, not differentiable at $\theta = 0$. Only Wasserstein
    metric provides a smooth measure, which is super helpful for a stable learning
    process using gradient descents.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 当两个分布不相交时，$D_{KL}$给我们无穷大。$D_{JS}$的值突然跳跃，在$\theta = 0$处不可微。只有Wasserstein度量提供了一个平滑的度量，这对使用梯度下降进行稳定学习过程非常有帮助。
- en: Use Wasserstein distance as GAN loss function
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Wasserstein距离作为GAN的损失函数
- en: 'It is intractable to exhaust all the possible joint distributions in $\Pi(p_r,
    p_g)$ to compute $\inf_{\gamma \sim \Pi(p_r, p_g)}$. Thus the authors proposed
    a smart transformation of the formula based on the Kantorovich-Rubinstein duality
    to:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 无法穷尽所有可能的联合分布$\Pi(p_r, p_g)$来计算$\inf_{\gamma \sim \Pi(p_r, p_g)}$。因此，作者根据Kantorovich-Rubinstein对偶提出了一个智能的公式转换，以：
- en: $$ W(p_r, p_g) = \frac{1}{K} \sup_{\| f \|_L \leq K} \mathbb{E}_{x \sim p_r}[f(x)]
    - \mathbb{E}_{x \sim p_g}[f(x)] $$
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: $$ W(p_r, p_g) = \frac{1}{K} \sup_{\| f \|_L \leq K} \mathbb{E}_{x \sim p_r}[f(x)]
    - \mathbb{E}_{x \sim p_g}[f(x)] $$
- en: where $\sup$ ([supremum](https://en.wikipedia.org/wiki/Infimum_and_supremum))
    is the opposite of $inf$ (infimum); we want to measure the least upper bound or,
    in even simpler words, the maximum value.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: $\sup$（[上确界](https://en.wikipedia.org/wiki/Infimum_and_supremum)）是$inf$（下确界）的对立面；我们希望衡量最小上界，或者更简单地说，最大值。
- en: '**Lipschitz continuity?**'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '**Lipschitz连续性？**'
- en: The function $f$ in the new form of Wasserstein metric is demanded to satisfy
    $| f |_L \leq K$, meaning it should be [K-Lipschitz continuous](https://en.wikipedia.org/wiki/Lipschitz_continuity).
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 新形式的Wasserstein度量中的函数$f$要求满足$| f |_L \leq K$，意味着它应该是[K-Lipschitz连续的](https://en.wikipedia.org/wiki/Lipschitz_continuity)。
- en: 'A real-valued function $f: \mathbb{R} \rightarrow \mathbb{R}$ is called $K$-Lipschitz
    continuous if there exists a real constant $K \geq 0$ such that, for all $x_1,
    x_2 \in \mathbb{R}$,'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '一个实值函数$f: \mathbb{R} \rightarrow \mathbb{R}$如果存在一个实常数$K \geq 0$，使得对于所有的$x_1,
    x_2 \in \mathbb{R}$，'
- en: $$ \lvert f(x_1) - f(x_2) \rvert \leq K \lvert x_1 - x_2 \rvert $$
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \lvert f(x_1) - f(x_2) \rvert \leq K \lvert x_1 - x_2 \rvert $$
- en: Here $K$ is known as a Lipschitz constant for function $f(.)$. Functions that
    are everywhere continuously differentiable is Lipschitz continuous, because the
    derivative, estimated as $\frac{\lvert f(x_1) - f(x_2) \rvert}{\lvert x_1 - x_2
    \rvert}$, has bounds. However, a Lipschitz continuous function may not be everywhere
    differentiable, such as $f(x) = \lvert x \rvert$.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 这里$K$被称为函数$f(.)$的Lipschitz常数。处处连续可微的函数是Lipschitz连续的，因为导数，估计为$\frac{\lvert f(x_1)
    - f(x_2) \rvert}{\lvert x_1 - x_2 \rvert}$，有界。然而，Lipschitz连续函数可能不是处处可微的，比如$f(x)
    = \lvert x \rvert$。
- en: Explaining how the transformation happens on the Wasserstein distance formula
    is worthy of a long post by itself, so I skip the details here. If you are interested
    in how to compute Wasserstein metric using linear programming, or how to transfer
    Wasserstein metric into its dual form according to the Kantorovich-Rubinstein
    Duality, read this [awesome post](https://vincentherrmann.github.io/blog/wasserstein/).
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 解释Wasserstein距离公式的转换如何发生值得单独写一篇长文，所以我在这里略过细节。如果你对如何使用线性规划计算Wasserstein度量，或者如何根据Kantorovich-Rubinstein对偶将Wasserstein度量转换为其对偶形式感兴趣，请阅读这篇[精彩文章](https://vincentherrmann.github.io/blog/wasserstein/)。
- en: Suppose this function $f$ comes from a family of K-Lipschitz continuous functions,
    $\{ f_w \}_{w \in W}$, parameterized by $w$. In the modified Wasserstein-GAN,
    the “discriminator” model is used to learn $w$ to find a good $f_w$ and the loss
    function is configured as measuring the Wasserstein distance between $p_r$ and
    $p_g$.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 假设这个函数$f$来自一个K-Lipschitz连续函数的家族$\{ f_w \}_{w \in W}$，由参数$w$参数化。在修改后的Wasserstein-GAN中，“鉴别器”模型用于学习$w$以找到一个好的$f_w$，损失函数被配置为衡量$p_r$和$p_g$之间的Wasserstein距离。
- en: $$ L(p_r, p_g) = W(p_r, p_g) = \max_{w \in W} \mathbb{E}_{x \sim p_r}[f_w(x)]
    - \mathbb{E}_{z \sim p_r(z)}[f_w(g_\theta(z))] $$
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: $$ L(p_r, p_g) = W(p_r, p_g) = \max_{w \in W} \mathbb{E}_{x \sim p_r}[f_w(x)]
    - \mathbb{E}_{z \sim p_r(z)}[f_w(g_\theta(z))] $$
- en: Thus the “discriminator” is not a direct critic of telling the fake samples
    apart from the real ones anymore. Instead, it is trained to learn a $K$-Lipschitz
    continuous function to help compute Wasserstein distance. As the loss function
    decreases in the training, the Wasserstein distance gets smaller and the generator
    model’s output grows closer to the real data distribution.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，“鉴别器”不再是一个直接的批评者，告诉假样本与真实样本的区别。相反，它被训练为学习一个$K$-Lipschitz连续函数，以帮助计算Wasserstein距离。随着训练中损失函数的减小，Wasserstein距离变小，生成器模型的输出越来越接近真实数据分布。
- en: 'One big problem is to maintain the $K$-Lipschitz continuity of $f_w$ during
    the training in order to make everything work out. The paper presents a simple
    but very practical trick: After every gradient update, clamp the weights $w$ to
    a small window, such as $[-0.01, 0.01]$, resulting in a compact parameter space
    $W$ and thus $f_w$ obtains its lower and upper bounds to preserve the Lipschitz
    continuity.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 一个大问题是在训练过程中保持$f_w$的$K$-Lipschitz连续性，以使一切顺利进行。论文提出了一个简单但非常实用的技巧：在每次梯度更新后，将权重$w$夹紧到一个小窗口，例如$[-0.01,
    0.01]$，从而得到一个紧凑的参数空间$W$，因此$f_w$获得其下限和上限以保持Lipschitz连续性。
- en: '![](../Images/dcb1848c71b82768c8468e976c94d00c.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dcb1848c71b82768c8468e976c94d00c.png)'
- en: 'Fig. 9\. Algorithm of Wasserstein generative adversarial network. (Image source:
    [Arjovsky, Chintala, & Bottou, 2017.](https://arxiv.org/pdf/1701.07875.pdf))'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 图9. Wasserstein生成对抗网络的算法。（图片来源：[Arjovsky, Chintala, & Bottou, 2017.](https://arxiv.org/pdf/1701.07875.pdf))
- en: 'Compared to the original GAN algorithm, the WGAN undertakes the following changes:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 与原始GAN算法相比，WGAN进行了以下更改：
- en: After every gradient update on the critic function, clamp the weights to a small
    fixed range, $[-c, c]$.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在对评论函数进行每次梯度更新后，将权重夹紧到一个小的固定范围，$[-c, c]$。
- en: Use a new loss function derived from the Wasserstein distance, no logarithm
    anymore. The “discriminator” model does not play as a direct critic but a helper
    for estimating the Wasserstein metric between real and generated data distribution.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用从Wasserstein距离导出的新损失函数，不再使用对数。 “鉴别器”模型不再直接充当评论家，而是辅助估计真实数据和生成数据分布之间的Wasserstein度量。
- en: Empirically the authors recommended [RMSProp](http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf)
    optimizer on the critic, rather than a momentum based optimizer such as [Adam](https://arxiv.org/abs/1412.6980v8)
    which could cause instability in the model training. I haven’t seen clear theoretical
    explanation on this point through.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作者经验性地推荐在评论家上使用[RMSProp](http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf)优化器，而不是像[Adam](https://arxiv.org/abs/1412.6980v8)这样基于动量的优化器，后者可能导致模型训练不稳定。我还没有看到关于这一点的清晰理论解释。
- en: '* * *'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Sadly, Wasserstein GAN is not perfect. Even the authors of the original WGAN
    paper mentioned that *“Weight clipping is a clearly terrible way to enforce a
    Lipschitz constraint”* (Oops!). WGAN still suffers from unstable training, slow
    convergence after weight clipping (when clipping window is too large), and vanishing
    gradients (when clipping window is too small).
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 遗憾的是，Wasserstein GAN并不完美。即使原始WGAN论文的作者们提到*“权重夹紧明显是一种强制Lipschitz约束的糟糕方式”*（哎呀！）。WGAN仍然存在训练不稳定、在夹紧权重后收敛缓慢（当夹紧窗口太大时）以及梯度消失（当夹紧窗口太小）等问题。
- en: Some improvement, precisely replacing weight clipping with **gradient penalty**,
    has been discussed in [Gulrajani et al. 2017](https://arxiv.org/pdf/1704.00028.pdf).
    I will leave this to a future post.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 一些改进，准确地用**梯度惩罚**替换权重夹紧，已在[Gulrajani等人2017年](https://arxiv.org/pdf/1704.00028.pdf)中讨论过。我将把这留到未来的帖子中。
- en: 'Example: Create New Pokemons!'
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 例子：创造新宝可梦！
- en: Just for fun, I tried out [carpedm20/DCGAN-tensorflow](https://github.com/carpedm20/DCGAN-tensorflow)
    on a tiny dataset, [Pokemon sprites](https://github.com/PokeAPI/sprites/). The
    dataset only has 900-ish pokemon images, including different levels of same pokemon
    species.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 为了好玩，我尝试在一个微小的数据集上使用[carpedm20/DCGAN-tensorflow](https://github.com/carpedm20/DCGAN-tensorflow)，这个数据集是[Pokemon
    sprites](https://github.com/PokeAPI/sprites/)。这个数据集只有大约900张宝可梦图片，包括同一宝可梦物种不同等级的图片。
- en: Let’s check out what types of new pokemons the model is able to create. Unfortunately
    due to the tiny training data, the new pokemons only have rough shapes without
    details. The shapes and colors do look better with more training epoches! Hooray!
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看模型能够创造出什么类型的新宝可梦。不幸的是，由于训练数据量很小，新宝可梦只有粗略的形状而没有细节。随着更多的训练轮次，形状和颜色看起来会更好！万岁！
- en: '![](../Images/518543f5aca0561d1fdfb024eac77a96.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/518543f5aca0561d1fdfb024eac77a96.png)'
- en: Fig. 10\. Train [carpedm20/DCGAN-tensorflow](https://github.com/carpedm20/DCGAN-tensorflow)
    on a set of Pokemon sprite images. The sample outputs are listed after training
    epoches = 7, 21, 49.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 图10. 在一组Pokemon精灵图像上训练[carpedm20/DCGAN-tensorflow](https://github.com/carpedm20/DCGAN-tensorflow)。在训练轮次=
    7、21、49之后列出样本输出。
- en: If you are interested in a commented version of [carpedm20/DCGAN-tensorflow](https://github.com/carpedm20/DCGAN-tensorflow)
    and how to modify it to train WGAN and WGAN with gradient penalty, check [lilianweng/unified-gan-tensorflow](https://github.com/lilianweng/unified-gan-tensorflow).
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对[carpedm20/DCGAN-tensorflow](https://github.com/carpedm20/DCGAN-tensorflow)的注释版本以及如何修改它来训练WGAN和带有梯度惩罚的WGAN感兴趣，请查看[lilianweng/unified-gan-tensorflow](https://github.com/lilianweng/unified-gan-tensorflow)。
- en: '* * *'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 'Cited as:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '引用为:'
- en: '[PRE0]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: OR
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 或者
- en: '[PRE1]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: References
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Goodfellow, Ian, et al. [“Generative adversarial nets.”](https://arxiv.org/pdf/1406.2661.pdf)
    NIPS, 2014.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Goodfellow, Ian, 等. [“生成对抗网络。”](https://arxiv.org/pdf/1406.2661.pdf) NIPS,
    2014.'
- en: '[2] Tim Salimans, et al. [“Improved techniques for training gans.”](http://papers.nips.cc/paper/6125-improved-techniques-for-training-gans.pdf)
    NIPS 2016.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Tim Salimans, 等. [“改进的生成对抗网络训练技术。”](http://papers.nips.cc/paper/6125-improved-techniques-for-training-gans.pdf)
    NIPS 2016.'
- en: '[3] Martin Arjovsky and Léon Bottou. [“Towards principled methods for training
    generative adversarial networks.”](https://arxiv.org/pdf/1701.04862.pdf) arXiv
    preprint arXiv:1701.04862 (2017).'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Martin Arjovsky 和 Léon Bottou. [“朝着训练生成对抗网络的原则方法。”](https://arxiv.org/pdf/1701.04862.pdf)
    arXiv预印本 arXiv:1701.04862 (2017).'
- en: '[4] Martin Arjovsky, Soumith Chintala, and Léon Bottou. [“Wasserstein GAN.”](https://arxiv.org/pdf/1701.07875.pdf)
    arXiv preprint arXiv:1701.07875 (2017).'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Martin Arjovsky, Soumith Chintala, 和 Léon Bottou. [“Wasserstein GAN。”](https://arxiv.org/pdf/1701.07875.pdf)
    arXiv预印本 arXiv:1701.07875 (2017).'
- en: '[5] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, Aaron
    Courville. [Improved training of wasserstein gans.](https://arxiv.org/pdf/1704.00028.pdf)
    arXiv preprint arXiv:1704.00028 (2017).'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, Aaron
    Courville. [改进的Wasserstein GAN训练。](https://arxiv.org/pdf/1704.00028.pdf) arXiv预印本
    arXiv:1704.00028 (2017).'
- en: '[6] [Computing the Earth Mover’s Distance under Transformations](http://robotics.stanford.edu/~scohen/research/emdg/emdg.html)'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] [在变换下计算地球移动距离](http://robotics.stanford.edu/~scohen/research/emdg/emdg.html)'
- en: '[7] [Wasserstein GAN and the Kantorovich-Rubinstein Duality](https://vincentherrmann.github.io/blog/wasserstein/)'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] [Wasserstein GAN和Kantorovich-Rubinstein对偶性](https://vincentherrmann.github.io/blog/wasserstein/)'
- en: '[8] [zhuanlan.zhihu.com/p/25071913](https://zhuanlan.zhihu.com/p/25071913)'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] [zhuanlan.zhihu.com/p/25071913](https://zhuanlan.zhihu.com/p/25071913)'
- en: '[9] Ferenc Huszár. [“How (not) to Train your Generative Model: Scheduled Sampling,
    Likelihood, Adversary?.”](https://arxiv.org/pdf/1511.05101.pdf) arXiv preprint
    arXiv:1511.05101 (2015).'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] Ferenc Huszár. [“如何（不）训练生成模型：定期抽样、似然、对手？”](https://arxiv.org/pdf/1511.05101.pdf)
    arXiv预印本 arXiv:1511.05101 (2015).'
