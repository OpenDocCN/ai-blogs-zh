- en: 'Predict Stock Prices Using RNN: Part 1'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用RNN预测股价：第1部分
- en: 原文：[https://lilianweng.github.io/posts/2017-07-08-stock-rnn-part-1/](https://lilianweng.github.io/posts/2017-07-08-stock-rnn-part-1/)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://lilianweng.github.io/posts/2017-07-08-stock-rnn-part-1/](https://lilianweng.github.io/posts/2017-07-08-stock-rnn-part-1/)
- en: This is a tutorial for how to build a recurrent neural network using Tensorflow
    to predict stock market prices. The full working code is available in [github.com/lilianweng/stock-rnn](https://github.com/lilianweng/stock-rnn).
    If you don’t know what is recurrent neural network or LSTM cell, feel free to
    check [my previous post](https://lilianweng.github.io/posts/2017-06-21-overview/#recurrent-neural-network).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个关于如何使用Tensorflow构建递归神经网络来预测股市价格的教程。完整的可运行代码可在[github.com/lilianweng/stock-rnn](https://github.com/lilianweng/stock-rnn)中找到。如果你不知道什么是递归神经网络或LSTM单元，可以查看[我的先前文章](https://lilianweng.github.io/posts/2017-06-21-overview/#recurrent-neural-network)。
- en: '*One thing I would like to emphasize that because my motivation for writing
    this post is more on demonstrating how to build and train an RNN model in Tensorflow
    and less on solve the stock prediction problem, I didn’t try hard on improving
    the prediction outcomes. You are more than welcome to take my [code](https://github.com/lilianweng/stock-rnn)
    as a reference point and add more stock prediction related ideas to improve it.
    Enjoy!*'
  id: totrans-3
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*我想强调一件事，因为我撰写这篇文章的动机更多地是为了演示如何在Tensorflow中构建和训练RNN模型，而不是解决股票预测问题，所以我没有努力改进预测结果。欢迎参考我的[代码](https://github.com/lilianweng/stock-rnn)，并添加更多与股票预测相关的想法来改进它。祝好！*'
- en: Overview of Existing Tutorials
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 现有教程概述
- en: 'There are many tutorials on the Internet, like:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 互联网上有许多教程，比如：
- en: '[A noob’s guide to implementing RNN-LSTM using Tensorflow](http://monik.in/a-noobs-guide-to-implementing-rnn-lstm-using-tensorflow/)'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[一个新手实现RNN-LSTM使用Tensorflow的指南](http://monik.in/a-noobs-guide-to-implementing-rnn-lstm-using-tensorflow/)'
- en: '[TensorFlow RNN Tutorial](https://svds.com/tensorflow-rnn-tutorial/)'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[TensorFlow RNN教程](https://svds.com/tensorflow-rnn-tutorial/)'
- en: '[LSTM by Example using Tensorflow](https://medium.com/towards-data-science/lstm-by-example-using-tensorflow-feb0c1968537)'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用Tensorflow示例的LSTM](https://medium.com/towards-data-science/lstm-by-example-using-tensorflow-feb0c1968537)'
- en: '[How to build a Recurrent Neural Network in TensorFlow](https://medium.com/@erikhallstrm/hello-world-rnn-83cd7105b767)'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何在TensorFlow中构建递归神经网络](https://medium.com/@erikhallstrm/hello-world-rnn-83cd7105b767)'
- en: '[RNNs in Tensorflow, a Practical Guide and Undocumented Features](http://www.wildml.com/2016/08/rnns-in-tensorflow-a-practical-guide-and-undocumented-features/)'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Tensorflow中的RNN，实用指南和未记录的功能](http://www.wildml.com/2016/08/rnns-in-tensorflow-a-practical-guide-and-undocumented-features/)'
- en: '[Sequence prediction using recurrent neural networks(LSTM) with TensorFlow](http://mourafiq.com/2016/05/15/predicting-sequences-using-rnn-in-tensorflow.html)'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用TensorFlow进行序列预测的递归神经网络(LSTM)](http://mourafiq.com/2016/05/15/predicting-sequences-using-rnn-in-tensorflow.html)'
- en: '[Anyone Can Learn To Code an LSTM-RNN in Python](https://iamtrask.github.io/2015/11/15/anyone-can-code-lstm/)'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[任何人都可以学会用Python编写LSTM-RNN](https://iamtrask.github.io/2015/11/15/anyone-can-code-lstm/)'
- en: '[How to do time series prediction using RNNs, TensorFlow and Cloud ML Engine](https://medium.com/google-cloud/how-to-do-time-series-prediction-using-rnns-and-tensorflow-and-cloud-ml-engine-2ad2eeb189e8)'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何使用RNNs、TensorFlow和Cloud ML Engine进行时间序列预测](https://medium.com/google-cloud/how-to-do-time-series-prediction-using-rnns-and-tensorflow-and-cloud-ml-engine-2ad2eeb189e8)'
- en: 'Despite all these existing tutorials, I still want to write a new one mainly
    for three reasons:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管存在这些现有教程，我仍然想写一个新的主要有三个原因：
- en: Early tutorials cannot cope with the new version any more, as Tensorflow is
    still under development and changes on API interfaces are being made fast.
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 早期的教程已经无法适应新版本了，因为Tensorflow仍在开发中，API接口的变化很快。
- en: Many tutorials use synthetic data in the examples. Well, I would like to play
    with the real world data.
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 许多教程在示例中使用合成数据。嗯，我想要使用真实世界的数据来尝试。
- en: Some tutorials assume that you have known something about Tensorflow API beforehand,
    which makes the reading a bit difficult.
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一些教程假设你事先了解Tensorflow API的一些内容，这使得阅读有点困难。
- en: After reading a bunch of examples, I would like to suggest taking the [official
    example](https://github.com/tensorflow/models/tree/master/tutorials/rnn/ptb) on
    Penn Tree Bank (PTB) dataset as your starting point. The PTB example showcases
    a RNN model in a pretty and modular design pattern, but it might prevent you from
    easily understanding the model structure. Hence, here I will build up the graph
    in a very straightforward manner.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读了一堆例子后，我建议从Penn Tree Bank（PTB）数据集中的[官方示例](https://github.com/tensorflow/models/tree/master/tutorials/rnn/ptb)开始。PTB示例展示了一个漂亮且模块化设计模式的RNN模型，但这可能会阻碍你轻松理解模型结构。因此，在这里我将以非常直接的方式构建图形。
- en: The Goal
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 目标
- en: I will explain how to build an RNN model with LSTM cells to predict the prices
    of S&P500 index. The dataset can be downloaded from [Yahoo! Finance ^GSPC](https://finance.yahoo.com/quote/%5EGSPC/history?p=%5EGSPC).
    In the following example, I used S&P 500 data from Jan 3, 1950 (the maximum date
    that Yahoo! Finance is able to trace back to) to Jun 23, 2017\. The dataset provides
    several price points per day. For simplicity, we will only use the daily **close
    prices** for prediction. Meanwhile, I will demonstrate how to use [TensorBoard](https://www.tensorflow.org/get_started/summaries_and_tensorboard)
    for easily debugging and model tracking.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我将解释如何使用带有LSTM单元的RNN模型来预测标准普尔500指数的价格。数据集可以从[Yahoo! Finance ^GSPC](https://finance.yahoo.com/quote/%5EGSPC/history?p=%5EGSPC)下载。在以下示例中，我使用了从1950年1月3日（Yahoo!
    Finance能够追溯到的最大日期）到2017年6月23日的标准普尔500数据。数据集每天提供几个价格点。为简单起见，我们将仅使用每日的**收盘价**进行预测。同时，我将演示如何使用[TensorBoard](https://www.tensorflow.org/get_started/summaries_and_tensorboard)进行轻松调试和模型跟踪。
- en: 'As a quick recap: the recurrent neural network (RNN) is a type of artificial
    neural network with self-loop in its hidden layer(s), which enables RNN to use
    the previous state of the hidden neuron(s) to learn the current state given the
    new input. RNN is good at processing sequential data. Long short-term memory (LSTM)
    cell is a specially designed working unit that helps RNN better memorize the long-term
    context.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 简要回顾：递归神经网络（RNN）是一种具有隐藏层自环的人工神经网络类型，这使得RNN能够利用隐藏神经元的先前状态来学习给定新输入时的当前状态。RNN擅长处理序列数据。长短期记忆（LSTM）单元是一种特别设计的工作单元，帮助RNN更好地记忆长期上下文。
- en: For more information in depth, please read [my previous post](https://lilianweng.github.io/posts/2017-06-21-overview/#recurrent-neural-network)
    or [this awesome post](http://colah.github.io/posts/2015-08-Understanding-LSTMs/).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 欲了解更深入的信息，请阅读[我的先前文章](https://lilianweng.github.io/posts/2017-06-21-overview/#recurrent-neural-network)或[这篇精彩的文章](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)。
- en: Data Preparation
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据准备
- en: The stock prices is a time series of length $N$, defined as $p_0, p_1, \dots,
    p_{N-1}$ in which $p_i$ is the close price on day $i$, $0 \le i < N$. Imagine
    that we have a sliding window of a fixed size $w$ (later, we refer to this as
    `input_size`) and every time we move the window to the right by size $w$, so that
    there is no overlap between data in all the sliding windows.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 股票价格是一个长度为$N$的时间序列，定义为$p_0, p_1, \dots, p_{N-1}$，其中$p_i$是第$i$天的收盘价，$0 \le i
    < N$。想象一下，我们有一个固定大小为$w$的滑动窗口（稍后，我们将称之为`input_size`），每次我们将窗口向右移动$w$的大小，以便所有滑动窗口中的数据之间没有重叠。
- en: '![](../Images/56ecd91f38f2fe1c3936c0452ee1067d.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/56ecd91f38f2fe1c3936c0452ee1067d.png)'
- en: Fig. 1\. The S&P 500 prices in time. We use content in one sliding windows to
    make prediction for the next, while there is no overlap between two consecutive
    windows.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图1。标准普尔500指数的价格随时间变化。我们使用一个滑动窗口中的内容来预测下一个窗口的内容，而两个连续窗口之间没有重叠。
- en: 'The RNN model we are about to build has LSTM cells as basic hidden units. We
    use values from the very beginning in the first sliding window $W_0$ to the window
    $W_t$ at time $t$:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们即将构建的RNN模型以LSTM单元作为基本隐藏单元。我们使用从第一个滑动窗口$W_0$一直到时间$t$的窗口$W_t$中的值：
- en: $$ \begin{aligned} W_0 &= (p_0, p_1, \dots, p_{w-1}) \\ W_1 &= (p_w, p_{w+1},
    \dots, p_{2w-1}) \\ \dots \\ W_t &= (p_{tw}, p_{tw+1}, \dots, p_{(t+1)w-1}) \end{aligned}
    $$
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \begin{aligned} W_0 &= (p_0, p_1, \dots, p_{w-1}) \\ W_1 &= (p_w, p_{w+1},
    \dots, p_{2w-1}) \\ \dots \\ W_t &= (p_{tw}, p_{tw+1}, \dots, p_{(t+1)w-1}) \end{aligned}
    $$
- en: 'to predict the prices in the following window $w_{t+1}$:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 来预测接下来窗口$w_{t+1}$中的价格：
- en: $$ W_{t+1} = (p_{(t+1)w}, p_{(t+1)w+1}, \dots, p_{(t+2)w-1}) $$
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: $$ W_{t+1} = (p_{(t+1)w}, p_{(t+1)w+1}, \dots, p_{(t+2)w-1}) $$
- en: Essentially we try to learn an approximation function, $f(W_0, W_1, \dots, W_t)
    \approx W_{t+1}$.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，我们试图学习一个近似函数，$f(W_0, W_1, \dots, W_t) \approx W_{t+1}$。
- en: '![](../Images/2b88867f0dfc3dd86d13952d601ea872.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2b88867f0dfc3dd86d13952d601ea872.png)'
- en: Fig. 2 The unrolled version of RNN.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图2 RNN的展开版本。
- en: Considering how [back propagation through time (BPTT)](https://en.wikipedia.org/wiki/Backpropagation_through_time)
    works, we usually train RNN in a “unrolled” version so that we don’t have to do
    propagation computation too far back and save the training complication.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到[时间反向传播（BPTT）](https://en.wikipedia.org/wiki/Backpropagation_through_time)的工作原理，我们通常以“展开”的方式训练RNN，这样我们就不必进行太远的传播计算，从而简化训练过程。
- en: 'Here is the explanation on `num_steps` from [Tensorflow’s tutorial](tensorflow.org/tutorials/recurrent):'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是来自[Tensorflow教程](tensorflow.org/tutorials/recurrent)关于`num_steps`的解释：
- en: By design, the output of a recurrent neural network (RNN) depends on arbitrarily
    distant inputs. Unfortunately, this makes backpropagation computation difficult.
    In order to make the learning process tractable, it is common practice to create
    an “unrolled” version of the network, which contains a fixed number (`num_steps`)
    of LSTM inputs and outputs. The model is then trained on this finite approximation
    of the RNN. This can be implemented by feeding inputs of length `num_steps` at
    a time and performing a backward pass after each such input block.
  id: totrans-36
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 按设计，循环神经网络（RNN）的输出取决于任意远的输入。不幸的是，这使得反向传播计算变得困难。为了使学习过程可行，通常会创建网络的“展开”版本，其中包含固定数量（`num_steps`）的LSTM输入和输出。然后在这个有限的RNN近似上进行训练。这可以通过每次输入长度为`num_steps`的输入并在每个这样的输入块之后执行反向传播来实现。
- en: The sequence of prices are first split into non-overlapped small windows. Each
    contains `input_size` numbers and each is considered as one independent input
    element. Then any `num_steps` consecutive input elements are grouped into one
    training input, forming an **“un-rolled”** version of RNN for training on Tensorfow.
    The corresponding label is the input element right after them.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 价格序列首先被分割成不重叠的小窗口。每个窗口包含`input_size`个数字，每个被视为一个独立的输入元素。然后，任何连续的`num_steps`个输入元素被分组成一个训练输入，形成了一个用于在Tensorfow上训练的**“展开”**版本的RNN。相应的标签是它们之后的输入元素。
- en: 'For instance, if `input_size=3` and `num_steps=2`, my first few training examples
    would look like:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果`input_size=3`和`num_steps=2`，我的前几个训练示例会是这样的：
- en: $$ \begin{aligned} \text{Input}_1 &= [[p_0, p_1, p_2], [p_3, p_4, p_5]]\quad\text{Label}_1
    = [p_6, p_7, p_8] \\ \text{Input}_2 &= [[p_3, p_4, p_5], [p_6, p_7, p_8]]\quad\text{Label}_2
    = [p_9, p_{10}, p_{11}] \\ \text{Input}_3 &= [[p_6, p_7, p_8], [p_9, p_{10}, p_{11}]]\quad\text{Label}_3
    = [p_{12}, p_{13}, p_{14}] \end{aligned} $$
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \begin{aligned} \text{Input}_1 &= [[p_0, p_1, p_2], [p_3, p_4, p_5]]\quad\text{Label}_1
    = [p_6, p_7, p_8] \\ \text{Input}_2 &= [[p_3, p_4, p_5], [p_6, p_7, p_8]]\quad\text{Label}_2
    = [p_9, p_{10}, p_{11}] \\ \text{Input}_3 &= [[p_6, p_7, p_8], [p_9, p_{10}, p_{11}]]\quad\text{Label}_3
    = [p_{12}, p_{13}, p_{14}] \end{aligned} $$
- en: 'Here is the key part for formatting the data:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是数据格式化的关键部分：
- en: '[PRE0]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The complete code of data formatting is [here](https://github.com/lilianweng/stock-rnn/blob/master/data_wrapper.py).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 数据格式化的完整代码在[这里](https://github.com/lilianweng/stock-rnn/blob/master/data_wrapper.py)。
- en: Train / Test Split
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练/测试分割
- en: Since we always want to predict the future, we take the **latest 10%** of data
    as the test data.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们总是想要预测未来，我们将最新的10%数据作为测试数据。
- en: Normalization
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 归一化
- en: The S&P 500 index increases in time, bringing about the problem that most values
    in the test set are out of the scale of the train set and thus the model has to
    *predict some numbers it has never seen before*. Sadly and unsurprisingly, it
    does a tragic job. See Fig. 3.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 标普500指数随时间增长，导致测试集中的大多数值超出了训练集的范围，因此模型必须*预测它从未见过的一些数字*。遗憾而不足为奇的是，它表现得很糟糕。见图3。
- en: '![](../Images/f2fa89e29bd6f8edc3932923b5285bb8.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f2fa89e29bd6f8edc3932923b5285bb8.png)'
- en: Fig. 3 A very sad example when the RNN model have to predict numbers out of
    the scale of the training data.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图3 当RNN模型必须预测超出训练数据范围的数字时的一个非常悲伤的例子。
- en: 'To solve the out-of-scale issue, I normalize the prices in each sliding window.
    The task becomes predicting the relative change rates instead of the absolute
    values. In a normalized sliding window $W’_t$ at time $t$, all the values are
    divided by the last unknown price—the last price in $W_{t-1}$:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决尺度不一致的问题，我在每个滑动窗口中对价格进行了归一化处理。任务变成了预测相对变化率而不是绝对值。在时间$t$的归一化滑动窗口$W’_t$中，所有的值都被最后一个未知价格——$W_{t-1}$中的最后一个价格所除：
- en: $$ W’_t = (\frac{p_{tw}}{p_{tw-1}}, \frac{p_{tw+1}}{p_{tw-1}}, \dots, \frac{p_{(t+1)w-1}}{p_{tw-1}})
    $$
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: $$ W’_t = (\frac{p_{tw}}{p_{tw-1}}, \frac{p_{tw+1}}{p_{tw-1}}, \dots, \frac{p_{(t+1)w-1}}{p_{tw-1}}
    ) $$
- en: Here is a data archive [stock-data-lilianweng.tar.gz](https://drive.google.com/open?id=1QKVkiwgCNJsdQMEsfoi6KpqoPgc4O6DD)
    of S & P 500 stock prices I crawled up to Jul, 2017\. Feel free to play with it
    :)
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个数据存档[stock-data-lilianweng.tar.gz](https://drive.google.com/open?id=1QKVkiwgCNJsdQMEsfoi6KpqoPgc4O6DD)，包含我在2017年7月之前爬取的标普500股票价格。随意使用它
    :)
- en: Model Construction
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型构建
- en: Definitions
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义
- en: '`lstm_size`: number of units in one LSTM layer.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lstm_size`: 一个LSTM层中的单元数。'
- en: '`num_layers`: number of stacked LSTM layers.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_layers`: 堆叠的LSTM层数。'
- en: '`keep_prob`: percentage of cell units to keep in the [dropout](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf)
    operation.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`keep_prob`: 在[dropout](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf)操作中保留的细胞单位的百分比。'
- en: '`init_learning_rate`: the learning rate to start with.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`init_learning_rate`: 起始学习率。'
- en: '`learning_rate_decay`: decay ratio in later training epochs.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`learning_rate_decay`: 后续训练epoch中的衰减比率。'
- en: '`init_epoch`: number of epochs using the constant `init_learning_rate`.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`init_epoch`: 使用恒定`init_learning_rate`的迭代次数。'
- en: '`max_epoch`: total number of epochs in training'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_epoch`: 训练中的总迭代次数'
- en: '`input_size`: size of the sliding window / one training data point'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_size`: 滑动窗口/一个训练数据点的大小'
- en: '`batch_size`: number of data points to use in one mini-batch.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_size`: 一个小批次中要使用的数据点数量。'
- en: The LSTM model has `num_layers` stacked LSTM layer(s) and each layer contains
    `lstm_size` number of LSTM cells. Then a [dropout](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf)
    mask with keep probability `keep_prob` is applied to the output of every LSTM
    cell. The goal of dropout is to remove the potential strong dependency on one
    dimension so as to prevent overfitting.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM模型有`num_layers`个堆叠的LSTM层，每个层包含`lstm_size`个LSTM单元。然后，对每个LSTM单元的输出应用保留概率为`keep_prob`的[dropout](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf)掩码。dropout的目标是消除对某一维度的潜在强依赖，以防止过拟合。
- en: The training requires `max_epoch` epochs in total; an [epoch](http://www.fon.hum.uva.nl/praat/manual/epoch.html)
    is a single full pass of all the training data points. In one epoch, the training
    data points are split into mini-batches of size `batch_size`. We send one mini-batch
    to the model for one BPTT learning. The learning rate is set to `init_learning_rate`
    during the first `init_epoch` epochs and then decay by $\times$ `learning_rate_decay`
    during every succeeding epoch.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 训练总共需要`max_epoch`个迭代；一个[epoch](http://www.fon.hum.uva.nl/praat/manual/epoch.html)是所有训练数据点的完整遍历。在一个epoch中，训练数据点被分成大小为`batch_size`的小批次。我们将一个小批次发送到模型进行一次BPTT学习。在前`init_epoch`个epoch期间，学习率设置为`init_learning_rate`，然后在每个后续epoch中按$\times$
    `learning_rate_decay`衰减。
- en: '[PRE1]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Define Graph
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义图
- en: A [`tf.Graph`](https://www.tensorflow.org/api_docs/python/tf/Graph) is not attached
    to any real data. It defines the flow of how to process the data and how to run
    the computation. Later, this graph can be fed with data within a [`tf.session`](https://www.tensorflow.org/api_docs/python/tf/Session)
    and at this moment the computation happens for real.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[`tf.Graph`](https://www.tensorflow.org/api_docs/python/tf/Graph)没有连接到任何真实数据。它定义了如何处理数据以及如何运行计算的流程。稍后，这个图可以在[`tf.session`](https://www.tensorflow.org/api_docs/python/tf/Session)中被输入数据，并且此时计算会真正发生。
- en: '**— Let’s start going through some code —**'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '**— 让我们开始阅读一些代码 —**'
- en: (1) Initialize a new graph first.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: (1) 首先初始化一个新图。
- en: '[PRE2]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: (2) How the graph works should be defined within its scope.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: (2) 图的工作原理应在其范围内定义。
- en: '[PRE3]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: (3) Define the data required for computation. Here we need three input variables,
    all defined as [`tf.placeholder`](https://www.tensorflow.org/versions/master/api_docs/python/tf/placeholder)
    because we don’t know what they are at the graph construction stage.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: (3) 定义计算所需的数据。在这里，我们需要三个输入变量，全部定义为[`tf.placeholder`](https://www.tensorflow.org/versions/master/api_docs/python/tf/placeholder)，因为在图构建阶段我们不知道它们是什么。
- en: '`inputs`: the training data *X*, a tensor of shape (# data examples, `num_steps`,
    `input_size`); the number of data examples is unknown, so it is `None`. In our
    case, it would be `batch_size` in training session. Check the [input format example](#input_format_example)
    if confused.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs`: 训练数据*X*，一个形状为(# 数据示例，`num_steps`，`input_size`)的张量；数据示例的数量未知，因此为`None`。在我们的情况下，它将在训练会话中成为`batch_size`。如果感到困惑，请查看[input
    format example](#input_format_example)。'
- en: '`targets`: the training label *y*, a tensor of shape (# data examples, `input_size`).'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`targets`: 训练标签*y*，一个形状为(# 数据示例，`input_size`)的张量。'
- en: '`learning_rate`: a simple float.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`learning_rate`: 一个简单的浮点数。'
- en: '[PRE4]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: (4) This function returns one [LSTMCell](https://www.tensorflow.org/versions/r1.0/api_docs/python/tf/contrib/rnn/LSTMCell)
    with or without dropout operation.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: (4) 此函数返回一个带有或不带有dropout操作的[LSTMCell](https://www.tensorflow.org/versions/r1.0/api_docs/python/tf/contrib/rnn/LSTMCell)。
- en: '[PRE5]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: (5) Let’s stack the cells into multiple layers if needed. `MultiRNNCell` helps
    connect sequentially multiple simple cells to compose one cell.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: (5) 如果需要，让我们将单元格堆叠成多个层。`MultiRNNCell`有助于将多个简单单元格按顺序连接起来组成一个单元格。
- en: '[PRE6]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: (6) [`tf.nn.dynamic_rnn`](https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn)
    constructs a recurrent neural network specified by `cell` (RNNCell). It returns
    a pair of (model outpus, state), where the outputs `val` is of size (`batch_size`,
    `num_steps`, `lstm_size`) by default. The state refers to the current state of
    the LSTM cell, not consumed here.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: (6) [`tf.nn.dynamic_rnn`](https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn)构建由`cell`（RNNCell）指定的递归神经网络。默认情况下，它返回一对（模型输出，状态），其中输出`val`的大小为（`batch_size`，`num_steps`，`lstm_size`）。状态指的是LSTM单元格的当前状态，在此处未使用。
- en: '[PRE7]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: (7) [`tf.transpose`](https://www.tensorflow.org/api_docs/python/tf/transpose)
    converts the outputs from the dimension (`batch_size`, `num_steps`, `lstm_size`)
    to (`num_steps`, `batch_size`, `lstm_size`). Then the last output is picked.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: (7) [`tf.transpose`](https://www.tensorflow.org/api_docs/python/tf/transpose)将输出从维度（`batch_size`，`num_steps`，`lstm_size`）转换为（`num_steps`，`batch_size`，`lstm_size`）。然后选择最后一个输出。
- en: '[PRE8]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: (8) Define weights and biases between the hidden and output layers.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: (8) 定义隐藏层和输出层之间的权重和偏置。
- en: '[PRE9]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: (9) We use mean square error as the loss metric and [the RMSPropOptimizer algorithm](http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf)
    for gradient descent optimization.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: (9) 我们使用均方误差作为损失度量，并使用[the RMSPropOptimizer算法](http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf)进行梯度下降优化。
- en: '[PRE10]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Start Training Session
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 开始训练会话
- en: (1) To start training the graph with real data, we need to start a [`tf.session`](https://www.tensorflow.org/api_docs/python/tf/Session)
    first.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: (1) 要开始使用真实数据训练图形，我们首先需要启动一个[`tf.session`](https://www.tensorflow.org/api_docs/python/tf/Session)。
- en: '[PRE11]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: (2) Initialize the variables as defined.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: (2) 根据定义初始化变量。
- en: '[PRE12]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: (0) The learning rates for training epochs should have been precomputed beforehand.
    The index refers to the epoch index.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: (0) 训练时期的学习率应该事先计算好。索引指的是时期索引。
- en: '[PRE13]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: (3) Each loop below completes one epoch training.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: (3) 下面的每个循环完成一个时期的训练。
- en: '[PRE14]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: (4) Don’t forget to save your trained model at the end.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: (4) 不要忘记在最后保存您训练好的模型。
- en: '[PRE15]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The complete code is available [here](https://github.com/lilianweng/stock-rnn/blob/master/build_graph.py).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的代码在[这里](https://github.com/lilianweng/stock-rnn/blob/master/build_graph.py)可用。
- en: Use TensorBoard
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用TensorBoard
- en: Building the graph without visualization is like drawing in the dark, very obscure
    and error-prone. [Tensorboard](https://github.com/tensorflow/tensorboard) provides
    easy visualization of the graph structure and the learning process. Check out
    this [hand-on tutorial](https://youtu.be/eBbEDRsCmv4), only 20 min, but it is
    very practical and showcases several live demos.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在没有可视化的情况下构建图形就像在黑暗中绘画，非常模糊且容易出错。[Tensorboard](https://github.com/tensorflow/tensorboard)提供了图形结构和学习过程的简单可视化。查看这个[实践教程](https://youtu.be/eBbEDRsCmv4)，只需20分钟，但非常实用，并展示了几个实时演示。
- en: '**Brief Summary**'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '**简要总结**'
- en: Use `with [tf.name_scope](https://www.tensorflow.org/api_docs/python/tf/name_scope)("your_awesome_module_name"):`
    to wrap elements working on the similar goal together.
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`with [tf.name_scope](https://www.tensorflow.org/api_docs/python/tf/name_scope)("your_awesome_module_name"):`将致力于相似目标的元素包装在一起。
- en: Many `tf.*` methods accepts `name=` argument. Assigning a customized name can
    make your life much easier when reading the graph.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 许多`tf.*`方法接受`name=`参数。分配一个自定义名称可以在阅读图时使您的生活更轻松。
- en: Methods like [`tf.summary.scalar`](https://www.tensorflow.org/api_docs/python/tf/summary/scalar)
    and [`tf.summary.histogram`](https://www.tensorflow.org/api_docs/python/tf/summary/histogram)
    help track the values of variables in the graph during iterations.
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 像[`tf.summary.scalar`](https://www.tensorflow.org/api_docs/python/tf/summary/scalar)和[`tf.summary.histogram`](https://www.tensorflow.org/api_docs/python/tf/summary/histogram)这样的方法有助于在迭代过程中跟踪图中变量的值。
- en: In the training session, define a log file using [`tf.summary.FileWriter`](https://www.tensorflow.org/api_docs/python/tf/summary/FileWriter).
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在培训会话中，使用[`tf.summary.FileWriter`](https://www.tensorflow.org/api_docs/python/tf/summary/FileWriter)定义一个日志文件。
- en: '[PRE16]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Later, write the training progress and summary results into the file.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 稍后，将训练进度和摘要结果写入文件。
- en: '[PRE17]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '![](../Images/411595a9adf0df339315eaa6562ec414.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/411595a9adf0df339315eaa6562ec414.png)'
- en: Fig. 4a The RNN graph built by the example code. The "train" module has been
    "removed from the main graph", as it is not a real part of the model during the
    prediction time.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图4a 由示例代码构建的RNN图。"train"模块已从主图中"移除"，因为在预测时不是模型的真正部分。
- en: '![](../Images/97b488cce9096211de3ac82005dcf119.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/97b488cce9096211de3ac82005dcf119.png)'
- en: Fig. 4b Click the "output_layer" module to expand it and check the structure
    in details.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图4b 点击"output_layer"模块以展开并详细检查结构。
- en: The full working code is available in [github.com/lilianweng/stock-rnn](https://github.com/lilianweng/stock-rnn).
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的工作代码可在[github.com/lilianweng/stock-rnn](https://github.com/lilianweng/stock-rnn)中找到。
- en: Results
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结果
- en: I used the following configuration in the experiment.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我在实验中使用了以下配置。
- en: '[PRE18]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: (Thanks to Yury for cathcing a bug that I had in the price normalization. Instead
    of using the last price of the previous time window, I ended up with using the
    last price in the same window. The following plots have been corrected.)
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: （感谢Yury发现了我在价格归一化中的一个错误。我最终使用了同一时间窗口中的最后价格，而不是使用前一个时间窗口的最后价格。以下图表已经更正。）
- en: Overall predicting the stock prices is not an easy task. Especially after normalization,
    the price trends look very noisy.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 总体来说，预测股票价格并不是一项容易的任务。特别是在归一化之后，价格趋势看起来非常嘈杂。
- en: '![](../Images/146df55d32c5be9691487675748b7822.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/146df55d32c5be9691487675748b7822.png)'
- en: Fig. 5a Predictoin results for the last 200 days in test data. Model is trained
    with input_size=1 and lstm_size=32.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图5a 针对测试数据中最后200天的预测结果。模型使用input_size=1和lstm_size=32进行训练。
- en: '![](../Images/612756510988f4965ec39745e32ad9f4.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/612756510988f4965ec39745e32ad9f4.png)'
- en: Fig. 5b Predictoin results for the last 200 days in test data. Model is trained
    with input_size=1 and lstm_size=128.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图5b 针对测试数据中最后200天的预测结果。模型使用input_size=1和lstm_size=128进行训练。
- en: '![](../Images/fc015f52515ba338298c6ce1bf8c7521.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fc015f52515ba338298c6ce1bf8c7521.png)'
- en: Fig. 5c Predictoin results for the last 200 days in test data. Model is trained
    with input_size=5, lstm_size=128 and max_epoch=75 (instead of 50).
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 图5c 针对测试数据中最后200天的预测结果。模型使用input_size=5，lstm_size=128和max_epoch=75进行训练（而不是50）。
- en: The example code in this tutorial is available in [github.com/lilianweng/stock-rnn:scripts](https://github.com/lilianweng/stock-rnn/tree/master/scripts).
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程中的示例代码可在[github.com/lilianweng/stock-rnn:scripts](https://github.com/lilianweng/stock-rnn/tree/master/scripts)中找到。
- en: '(Updated on Sep 14, 2017) The model code has been updated to be wrapped into
    a class: [LstmRNN](https://github.com/lilianweng/stock-rnn/blob/master/model_rnn.py).
    The model training can be triggered by [main.py](https://github.com/lilianweng/stock-rnn/blob/master/main.py),
    such as:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: (更新于2017年9月14日) 模型代码已更新为封装在一个类中：[LstmRNN](https://github.com/lilianweng/stock-rnn/blob/master/model_rnn.py)。模型训练可以通过[main.py](https://github.com/lilianweng/stock-rnn/blob/master/main.py)触发，例如：
- en: '[PRE19]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
