- en: From Autoencoder to Beta-VAE
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://lilianweng.github.io/posts/2018-08-12-vae/](https://lilianweng.github.io/posts/2018-08-12-vae/)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[Updated on 2019-07-18: add a section on [VQ-VAE & VQ-VAE-2](#vq-vae-and-vq-vae-2).]'
  prefs: []
  type: TYPE_NORMAL
- en: '[Updated on 2019-07-26: add a section on [TD-VAE](#td-vae).]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Autocoder is invented to reconstruct high-dimensional data using a neural network
    model with a narrow bottleneck layer in the middle (oops, this is probably not
    true for [Variational Autoencoder](#vae-variational-autoencoder), and we will
    investigate it in details in later sections). A nice byproduct is dimension reduction:
    the bottleneck layer captures a compressed latent encoding. Such a low-dimensional
    representation can be used as en embedding vector in various applications (i.e.
    search), help data compression, or reveal the underlying data generative factors.'
  prefs: []
  type: TYPE_NORMAL
- en: Notation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '| Symbol | Mean |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| $\mathcal{D}$ | The dataset, $\mathcal{D} = \{ \mathbf{x}^{(1)}, \mathbf{x}^{(2)},
    \dots, \mathbf{x}^{(n)} \}$, contains $n$ data samples; $\vert\mathcal{D}\vert
    =n $. |'
  prefs: []
  type: TYPE_TB
- en: '| $\mathbf{x}^{(i)}$ | Each data point is a vector of $d$ dimensions, $\mathbf{x}^{(i)}
    = [x^{(i)}_1, x^{(i)}_2, \dots, x^{(i)}_d]$. |'
  prefs: []
  type: TYPE_TB
- en: '| $\mathbf{x}$ | One data sample from the dataset, $\mathbf{x} \in \mathcal{D}$.
    |'
  prefs: []
  type: TYPE_TB
- en: '| $\mathbf{x}’$ | The reconstructed version of $\mathbf{x}$. |'
  prefs: []
  type: TYPE_TB
- en: '| $\tilde{\mathbf{x}}$ | The corrupted version of $\mathbf{x}$. |'
  prefs: []
  type: TYPE_TB
- en: '| $\mathbf{z}$ | The compressed code learned in the bottleneck layer. |'
  prefs: []
  type: TYPE_TB
- en: '| $a_j^{(l)}$ | The activation function for the $j$-th neuron in the $l$-th
    hidden layer. |'
  prefs: []
  type: TYPE_TB
- en: '| $g_{\phi}(.)$ | The **encoding** function parameterized by $\phi$. |'
  prefs: []
  type: TYPE_TB
- en: '| $f_{\theta}(.)$ | The **decoding** function parameterized by $\theta$. |'
  prefs: []
  type: TYPE_TB
- en: '| $q_{\phi}(\mathbf{z}\vert\mathbf{x})$ | Estimated posterior probability function,
    also known as **probabilistic encoder**. |'
  prefs: []
  type: TYPE_TB
- en: '| $p_{\theta}(\mathbf{x}\vert\mathbf{z})$ | Likelihood of generating true data
    sample given the latent code, also known as **probabilistic decoder**. |'
  prefs: []
  type: TYPE_TB
- en: Autoencoder
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Autoencoder** is a neural network designed to learn an identity function
    in an unsupervised way to reconstruct the original input while compressing the
    data in the process so as to discover a more efficient and compressed representation.
    The idea was originated in [the 1980s](https://en.wikipedia.org/wiki/Autoencoder),
    and later promoted by the seminal paper by [Hinton & Salakhutdinov, 2006](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.459.3788&rep=rep1&type=pdf).'
  prefs: []
  type: TYPE_NORMAL
- en: 'It consists of two networks:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Encoder* network: It translates the original high-dimension input into the
    latent low-dimensional code. The input size is larger than the output size.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Decoder* network: The decoder network recovers the data from the code, likely
    with larger and larger output layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/fbaa850ba69a2c444ce906ada3e057b5.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig. 1\. Illustration of autoencoder model architecture.
  prefs: []
  type: TYPE_NORMAL
- en: The encoder network essentially accomplishes the [dimensionality reduction](https://en.wikipedia.org/wiki/Dimensionality_reduction),
    just like how we would use Principal Component Analysis (PCA) or Matrix Factorization
    (MF) for. In addition, the autoencoder is explicitly optimized for the data reconstruction
    from the code. A good intermediate representation not only can capture latent
    variables, but also benefits a full [decompression](https://ai.googleblog.com/2016/09/image-compression-with-neural-networks.html)
    process.
  prefs: []
  type: TYPE_NORMAL
- en: The model contains an encoder function $g(.)$ parameterized by $\phi$ and a
    decoder function $f(.)$ parameterized by $\theta$. The low-dimensional code learned
    for input $\mathbf{x}$ in the bottleneck layer is $\mathbf{z} = g_\phi(\mathbf{x})$
    and the reconstructed input is $\mathbf{x}’ = f_\theta(g_\phi(\mathbf{x}))$.
  prefs: []
  type: TYPE_NORMAL
- en: 'The parameters $(\theta, \phi)$ are learned together to output a reconstructed
    data sample same as the original input, $\mathbf{x} \approx f_\theta(g_\phi(\mathbf{x}))$,
    or in other words, to learn an identity function. There are various metrics to
    quantify the difference between two vectors, such as cross entropy when the activation
    function is sigmoid, or as simple as MSE loss:'
  prefs: []
  type: TYPE_NORMAL
- en: $$ L_\text{AE}(\theta, \phi) = \frac{1}{n}\sum_{i=1}^n (\mathbf{x}^{(i)} - f_\theta(g_\phi(\mathbf{x}^{(i)})))^2
    $$
  prefs: []
  type: TYPE_NORMAL
- en: Denoising Autoencoder
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since the autoencoder learns the identity function, we are facing the risk of
    “overfitting” when there are more network parameters than the number of data points.
  prefs: []
  type: TYPE_NORMAL
- en: 'To avoid overfitting and improve the robustness, **Denoising Autoencoder**
    (Vincent et al. 2008) proposed a modification to the basic autoencoder. The input
    is partially corrupted by adding noises to or masking some values of the input
    vector in a stochastic manner, $\tilde{\mathbf{x}} \sim \mathcal{M}_\mathcal{D}(\tilde{\mathbf{x}}
    \vert \mathbf{x})$. Then the model is trained to recover the original input (note:
    not the corrupt one).'
  prefs: []
  type: TYPE_NORMAL
- en: $$ \begin{aligned} \tilde{\mathbf{x}}^{(i)} &\sim \mathcal{M}_\mathcal{D}(\tilde{\mathbf{x}}^{(i)}
    \vert \mathbf{x}^{(i)})\\ L_\text{DAE}(\theta, \phi) &= \frac{1}{n} \sum_{i=1}^n
    (\mathbf{x}^{(i)} - f_\theta(g_\phi(\tilde{\mathbf{x}}^{(i)})))^2 \end{aligned}
    $$
  prefs: []
  type: TYPE_NORMAL
- en: where $\mathcal{M}_\mathcal{D}$ defines the mapping from the true data samples
    to the noisy or corrupted ones.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/663dd6fdf5647420331a0f6ea92f1659.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig. 2\. Illustration of denoising autoencoder model architecture.
  prefs: []
  type: TYPE_NORMAL
- en: This design is motivated by the fact that humans can easily recognize an object
    or a scene even the view is partially occluded or corrupted. To “repair” the partially
    destroyed input, the denoising autoencoder has to discover and capture relationship
    between dimensions of input in order to infer missing pieces.
  prefs: []
  type: TYPE_NORMAL
- en: For high dimensional input with high redundancy, like images, the model is likely
    to depend on evidence gathered from a combination of many input dimensions to
    recover the denoised version rather than to overfit one dimension. This builds
    up a good foundation for learning *robust* latent representation.
  prefs: []
  type: TYPE_NORMAL
- en: The noise is controlled by a stochastic mapping $\mathcal{M}_\mathcal{D}(\tilde{\mathbf{x}}
    \vert \mathbf{x})$, and it is not specific to a particular type of corruption
    process (i.e. masking noise, Gaussian noise, salt-and-pepper noise, etc.). Naturally
    the corruption process can be equipped with prior knowledge
  prefs: []
  type: TYPE_NORMAL
- en: 'In the experiment of the original DAE paper, the noise is applied in this way:
    a fixed proportion of input dimensions are selected at random and their values
    are forced to 0\. Sounds a lot like dropout, right? Well, the denoising autoencoder
    was proposed in 2008, 4 years before the dropout paper ([Hinton, et al. 2012](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf))
    ;)'
  prefs: []
  type: TYPE_NORMAL
- en: Sparse Autoencoder
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Sparse Autoencoder** applies a “sparse” constraint on the hidden unit activation
    to avoid overfitting and improve robustness. It forces the model to only have
    a small number of hidden units being activated at the same time, or in other words,
    one hidden neuron should be inactivate most of time.'
  prefs: []
  type: TYPE_NORMAL
- en: Recall that common [activation functions](http://cs231n.github.io/neural-networks-1/#actfun)
    include sigmoid, tanh, relu, leaky relu, etc. A neuron is activated when the value
    is close to 1 and inactivate with a value close to 0.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s say there are $s_l$ neurons in the $l$-th hidden layer and the activation
    function for the $j$-th neuron in this layer is labelled as $a^{(l)}_j(.)$, $j=1,
    \dots, s_l$. The fraction of activation of this neuron $\hat{\rho}_j$ is expected
    to be a small number $\rho$, known as *sparsity parameter*; a common config is
    $\rho = 0.05$.
  prefs: []
  type: TYPE_NORMAL
- en: $$ \hat{\rho}_j^{(l)} = \frac{1}{n} \sum_{i=1}^n [a_j^{(l)}(\mathbf{x}^{(i)})]
    \approx \rho $$
  prefs: []
  type: TYPE_NORMAL
- en: This constraint is achieved by adding a penalty term into the loss function.
    The KL-divergence $D_\text{KL}$ measures the difference between two Bernoulli
    distributions, one with mean $\rho$ and the other with mean $\hat{\rho}_j^{(l)}$.
    The hyperparameter $\beta$ controls how strong the penalty we want to apply on
    the sparsity loss.
  prefs: []
  type: TYPE_NORMAL
- en: $$ \begin{aligned} L_\text{SAE}(\theta) &= L(\theta) + \beta \sum_{l=1}^L \sum_{j=1}^{s_l}
    D_\text{KL}(\rho \| \hat{\rho}_j^{(l)}) \\ &= L(\theta) + \beta \sum_{l=1}^L \sum_{j=1}^{s_l}
    \rho\log\frac{\rho}{\hat{\rho}_j^{(l)}} + (1-\rho)\log\frac{1-\rho}{1-\hat{\rho}_j^{(l)}}
    \end{aligned} $$![](../Images/45b50efe9198dbf95325bd5aaf88e80f.png)
  prefs: []
  type: TYPE_NORMAL
- en: Fig. 4\. The KL divergence between a Bernoulli distribution with mean $\rho=0.25$
    and a Bernoulli distribution with mean $0 \leq \hat{\rho} \leq 1$.
  prefs: []
  type: TYPE_NORMAL
- en: '**$k$-Sparse Autoencoder**'
  prefs: []
  type: TYPE_NORMAL
- en: 'In $k$-Sparse Autoencoder ([Makhzani and Frey, 2013](https://arxiv.org/abs/1312.5663)),
    the sparsity is enforced by only keeping the top k highest activations in the
    bottleneck layer with linear activation function. First we run feedforward through
    the encoder network to get the compressed code: $\mathbf{z} = g(\mathbf{x})$.
    Sort the values in the code vector $\mathbf{z}$. Only the k largest values are
    kept while other neurons are set to 0\. This can be done in a ReLU layer with
    an adjustable threshold too. Now we have a sparsified code: $\mathbf{z}’ = \text{Sparsify}(\mathbf{z})$.
    Compute the output and the loss from the sparsified code, $L = |\mathbf{x} - f(\mathbf{z}’)
    |_2^2$. And, the back-propagation only goes through the top k activated hidden
    units!'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/94f1dbc940d432694638ccd177fbc65b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 5\. Filters of the k-sparse autoencoder for different sparsity levels
    k, learnt from MNIST with 1000 hidden units.. (Image source: [Makhzani and Frey,
    2013](https://arxiv.org/abs/1312.5663))'
  prefs: []
  type: TYPE_NORMAL
- en: Contractive Autoencoder
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Similar to sparse autoencoder, **Contractive Autoencoder** ([Rifai, et al, 2011](http://www.icml-2011.org/papers/455_icmlpaper.pdf))
    encourages the learned representation to stay in a contractive space for better
    robustness.
  prefs: []
  type: TYPE_NORMAL
- en: 'It adds a term in the loss function to penalize the representation being too
    sensitive to the input, and thus improve the robustness to small perturbations
    around the training data points. The sensitivity is measured by the Frobenius
    norm of the Jacobian matrix of the encoder activations with respect to the input:'
  prefs: []
  type: TYPE_NORMAL
- en: $$ \|J_f(\mathbf{x})\|_F^2 = \sum_{ij} \Big( \frac{\partial h_j(\mathbf{x})}{\partial
    x_i} \Big)^2 $$
  prefs: []
  type: TYPE_NORMAL
- en: where $h_j$ is one unit output in the compressed code $\mathbf{z} = f(x)$.
  prefs: []
  type: TYPE_NORMAL
- en: This penalty term is the sum of squares of all partial derivatives of the learned
    encoding with respect to input dimensions. The authors claimed that empirically
    this penalty was found to carve a representation that corresponds to a lower-dimensional
    non-linear manifold, while staying more invariant to majority directions orthogonal
    to the manifold.
  prefs: []
  type: TYPE_NORMAL
- en: 'VAE: Variational Autoencoder'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The idea of **Variational Autoencoder** ([Kingma & Welling, 2014](https://arxiv.org/abs/1312.6114)),
    short for **VAE**, is actually less similar to all the autoencoder models above,
    but deeply rooted in the methods of variational bayesian and graphical model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of mapping the input into a *fixed* vector, we want to map it into
    a distribution. Let’s label this distribution as $p_\theta$, parameterized by
    $\theta$. The relationship between the data input $\mathbf{x}$ and the latent
    encoding vector $\mathbf{z}$ can be fully defined by:'
  prefs: []
  type: TYPE_NORMAL
- en: Prior $p_\theta(\mathbf{z})$
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Likelihood $p_\theta(\mathbf{x}\vert\mathbf{z})$
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Posterior $p_\theta(\mathbf{z}\vert\mathbf{x})$
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Assuming that we know the real parameter $\theta^{*}$ for this distribution.
    In order to generate a sample that looks like a real data point $\mathbf{x}^{(i)}$,
    we follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: First, sample a $\mathbf{z}^{(i)}$ from a prior distribution $p_{\theta^*}(\mathbf{z})$.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then a value $\mathbf{x}^{(i)}$ is generated from a conditional distribution
    $p_{\theta^*}(\mathbf{x} \vert \mathbf{z} = \mathbf{z}^{(i)})$.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The optimal parameter $\theta^{*}$ is the one that maximizes the probability
    of generating real data samples:'
  prefs: []
  type: TYPE_NORMAL
- en: $$ \theta^{*} = \arg\max_\theta \prod_{i=1}^n p_\theta(\mathbf{x}^{(i)}) $$
  prefs: []
  type: TYPE_NORMAL
- en: 'Commonly we use the log probabilities to convert the product on RHS to a sum:'
  prefs: []
  type: TYPE_NORMAL
- en: $$ \theta^{*} = \arg\max_\theta \sum_{i=1}^n \log p_\theta(\mathbf{x}^{(i)})
    $$
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let’s update the equation to better demonstrate the data generation process
    so as to involve the encoding vector:'
  prefs: []
  type: TYPE_NORMAL
- en: $$ p_\theta(\mathbf{x}^{(i)}) = \int p_\theta(\mathbf{x}^{(i)}\vert\mathbf{z})
    p_\theta(\mathbf{z}) d\mathbf{z} $$
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately it is not easy to compute $p_\theta(\mathbf{x}^{(i)})$ in this
    way, as it is very expensive to check all the possible values of $\mathbf{z}$
    and sum them up. To narrow down the value space to facilitate faster search, we
    would like to introduce a new approximation function to output what is a likely
    code given an input $\mathbf{x}$, $q_\phi(\mathbf{z}\vert\mathbf{x})$, parameterized
    by $\phi$.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d5c9b2ea68c45318c9953d6a0b324b21.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig. 6\. The graphical model involved in Variational Autoencoder. Solid lines
    denote the generative distribution $p\_\theta(.)$ and dashed lines denote the
    distribution $q\_\phi (\mathbf{z}\vert\mathbf{x})$ to approximate the intractable
    posterior $p\_\theta (\mathbf{z}\vert\mathbf{x})$.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now the structure looks a lot like an autoencoder:'
  prefs: []
  type: TYPE_NORMAL
- en: The conditional probability $p_\theta(\mathbf{x} \vert \mathbf{z})$ defines
    a generative model, similar to the decoder $f_\theta(\mathbf{x} \vert \mathbf{z})$
    introduced above. $p_\theta(\mathbf{x} \vert \mathbf{z})$ is also known as *probabilistic
    decoder*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The approximation function $q_\phi(\mathbf{z} \vert \mathbf{x})$ is the *probabilistic
    encoder*, playing a similar role as $g_\phi(\mathbf{z} \vert \mathbf{x})$ above.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Loss Function: ELBO'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The estimated posterior $q_\phi(\mathbf{z}\vert\mathbf{x})$ should be very close
    to the real one $p_\theta(\mathbf{z}\vert\mathbf{x})$. We can use [Kullback-Leibler
    divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)
    to quantify the distance between these two distributions. KL divergence $D_\text{KL}(X|Y)$
    measures how much information is lost if the distribution Y is used to represent
    X.
  prefs: []
  type: TYPE_NORMAL
- en: In our case we want to minimize $D_\text{KL}( q_\phi(\mathbf{z}\vert\mathbf{x})
    | p_\theta(\mathbf{z}\vert\mathbf{x}) )$ with respect to $\phi$.
  prefs: []
  type: TYPE_NORMAL
- en: 'But why use $D_\text{KL}(q_\phi | p_\theta)$ (reversed KL) instead of $D_\text{KL}(p_\theta
    | q_\phi)$ (forward KL)? Eric Jang has a great explanation in his [post](https://blog.evjang.com/2016/08/variational-bayes.html)
    on Bayesian Variational methods. As a quick recap:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7749354d0701a2463de5fd31e37aaba7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 7\. Forward and reversed KL divergence have different demands on how to
    match two distributions. (Image source: [blog.evjang.com/2016/08/variational-bayes.html](https://blog.evjang.com/2016/08/variational-bayes.html))'
  prefs: []
  type: TYPE_NORMAL
- en: 'Forward KL divergence: $D_\text{KL}(P|Q) = \mathbb{E}_{z\sim P(z)} \log\frac{P(z)}{Q(z)}$;
    we have to ensure that Q(z)>0 wherever P(z)>0\. The optimized variational distribution
    $q(z)$ has to cover over the entire $p(z)$.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Reversed KL divergence: $D_\text{KL}(Q|P) = \mathbb{E}_{z\sim Q(z)} \log\frac{Q(z)}{P(z)}$;
    minimizing the reversed KL divergence squeezes the $Q(z)$ under $P(z)$.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s now expand the equation:'
  prefs: []
  type: TYPE_NORMAL
- en: $$ \begin{aligned} & D_\text{KL}( q_\phi(\mathbf{z}\vert\mathbf{x}) \| p_\theta(\mathbf{z}\vert\mathbf{x})
    ) & \\ &=\int q_\phi(\mathbf{z} \vert \mathbf{x}) \log\frac{q_\phi(\mathbf{z}
    \vert \mathbf{x})}{p_\theta(\mathbf{z} \vert \mathbf{x})} d\mathbf{z} & \\ &=\int
    q_\phi(\mathbf{z} \vert \mathbf{x}) \log\frac{q_\phi(\mathbf{z} \vert \mathbf{x})p_\theta(\mathbf{x})}{p_\theta(\mathbf{z},
    \mathbf{x})} d\mathbf{z} & \scriptstyle{\text{; Because }p(z \vert x) = p(z, x)
    / p(x)} \\ &=\int q_\phi(\mathbf{z} \vert \mathbf{x}) \big( \log p_\theta(\mathbf{x})
    + \log\frac{q_\phi(\mathbf{z} \vert \mathbf{x})}{p_\theta(\mathbf{z}, \mathbf{x})}
    \big) d\mathbf{z} & \\ &=\log p_\theta(\mathbf{x}) + \int q_\phi(\mathbf{z} \vert
    \mathbf{x})\log\frac{q_\phi(\mathbf{z} \vert \mathbf{x})}{p_\theta(\mathbf{z},
    \mathbf{x})} d\mathbf{z} & \scriptstyle{\text{; Because }\int q(z \vert x) dz
    = 1}\\ &=\log p_\theta(\mathbf{x}) + \int q_\phi(\mathbf{z} \vert \mathbf{x})\log\frac{q_\phi(\mathbf{z}
    \vert \mathbf{x})}{p_\theta(\mathbf{x}\vert\mathbf{z})p_\theta(\mathbf{z})} d\mathbf{z}
    & \scriptstyle{\text{; Because }p(z, x) = p(x \vert z) p(z)} \\ &=\log p_\theta(\mathbf{x})
    + \mathbb{E}_{\mathbf{z}\sim q_\phi(\mathbf{z} \vert \mathbf{x})}[\log \frac{q_\phi(\mathbf{z}
    \vert \mathbf{x})}{p_\theta(\mathbf{z})} - \log p_\theta(\mathbf{x} \vert \mathbf{z})]
    &\\ &=\log p_\theta(\mathbf{x}) + D_\text{KL}(q_\phi(\mathbf{z}\vert\mathbf{x})
    \| p_\theta(\mathbf{z})) - \mathbb{E}_{\mathbf{z}\sim q_\phi(\mathbf{z}\vert\mathbf{x})}\log
    p_\theta(\mathbf{x}\vert\mathbf{z}) & \end{aligned} $$
  prefs: []
  type: TYPE_NORMAL
- en: 'So we have:'
  prefs: []
  type: TYPE_NORMAL
- en: $$ D_\text{KL}( q_\phi(\mathbf{z}\vert\mathbf{x}) \| p_\theta(\mathbf{z}\vert\mathbf{x})
    ) =\log p_\theta(\mathbf{x}) + D_\text{KL}(q_\phi(\mathbf{z}\vert\mathbf{x}) \|
    p_\theta(\mathbf{z})) - \mathbb{E}_{\mathbf{z}\sim q_\phi(\mathbf{z}\vert\mathbf{x})}\log
    p_\theta(\mathbf{x}\vert\mathbf{z}) $$
  prefs: []
  type: TYPE_NORMAL
- en: Once rearrange the left and right hand side of the equation,
  prefs: []
  type: TYPE_NORMAL
- en: $$ \log p_\theta(\mathbf{x}) - D_\text{KL}( q_\phi(\mathbf{z}\vert\mathbf{x})
    \| p_\theta(\mathbf{z}\vert\mathbf{x}) ) = \mathbb{E}_{\mathbf{z}\sim q_\phi(\mathbf{z}\vert\mathbf{x})}\log
    p_\theta(\mathbf{x}\vert\mathbf{z}) - D_\text{KL}(q_\phi(\mathbf{z}\vert\mathbf{x})
    \| p_\theta(\mathbf{z})) $$
  prefs: []
  type: TYPE_NORMAL
- en: 'The LHS of the equation is exactly what we want to maximize when learning the
    true distributions: we want to maximize the (log-)likelihood of generating real
    data (that is $\log p_\theta(\mathbf{x})$) and also minimize the difference between
    the real and estimated posterior distributions (the term $D_\text{KL}$ works like
    a regularizer). Note that $p_\theta(\mathbf{x})$ is fixed with respect to $q_\phi$.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The negation of the above defines our loss function:'
  prefs: []
  type: TYPE_NORMAL
- en: $$ \begin{aligned} L_\text{VAE}(\theta, \phi) &= -\log p_\theta(\mathbf{x})
    + D_\text{KL}( q_\phi(\mathbf{z}\vert\mathbf{x}) \| p_\theta(\mathbf{z}\vert\mathbf{x})
    )\\ &= - \mathbb{E}_{\mathbf{z} \sim q_\phi(\mathbf{z}\vert\mathbf{x})} \log p_\theta(\mathbf{x}\vert\mathbf{z})
    + D_\text{KL}( q_\phi(\mathbf{z}\vert\mathbf{x}) \| p_\theta(\mathbf{z}) ) \\
    \theta^{*}, \phi^{*} &= \arg\min_{\theta, \phi} L_\text{VAE} \end{aligned} $$
  prefs: []
  type: TYPE_NORMAL
- en: In Variational Bayesian methods, this loss function is known as the *variational
    lower bound*, or *evidence lower bound*. The “lower bound” part in the name comes
    from the fact that KL divergence is always non-negative and thus $-L_\text{VAE}$
    is the lower bound of $\log p_\theta (\mathbf{x})$.
  prefs: []
  type: TYPE_NORMAL
- en: $$ -L_\text{VAE} = \log p_\theta(\mathbf{x}) - D_\text{KL}( q_\phi(\mathbf{z}\vert\mathbf{x})
    \| p_\theta(\mathbf{z}\vert\mathbf{x}) ) \leq \log p_\theta(\mathbf{x}) $$
  prefs: []
  type: TYPE_NORMAL
- en: Therefore by minimizing the loss, we are maximizing the lower bound of the probability
    of generating real data samples.
  prefs: []
  type: TYPE_NORMAL
- en: Reparameterization Trick
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The expectation term in the loss function invokes generating samples from $\mathbf{z}
    \sim q_\phi(\mathbf{z}\vert\mathbf{x})$. Sampling is a stochastic process and
    therefore we cannot backpropagate the gradient. To make it trainable, the reparameterization
    trick is introduced: It is often possible to express the random variable $\mathbf{z}$
    as a deterministic variable $\mathbf{z} = \mathcal{T}_\phi(\mathbf{x}, \boldsymbol{\epsilon})$,
    where $\boldsymbol{\epsilon}$ is an auxiliary independent random variable, and
    the transformation function $\mathcal{T}_\phi$ parameterized by $\phi$ converts
    $\boldsymbol{\epsilon}$ to $\mathbf{z}$.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, a common choice of the form of $q_\phi(\mathbf{z}\vert\mathbf{x})$
    is a multivariate Gaussian with a diagonal covariance structure:'
  prefs: []
  type: TYPE_NORMAL
- en: $$ \begin{aligned} \mathbf{z} &\sim q_\phi(\mathbf{z}\vert\mathbf{x}^{(i)})
    = \mathcal{N}(\mathbf{z}; \boldsymbol{\mu}^{(i)}, \boldsymbol{\sigma}^{2(i)}\boldsymbol{I})
    & \\ \mathbf{z} &= \boldsymbol{\mu} + \boldsymbol{\sigma} \odot \boldsymbol{\epsilon}
    \text{, where } \boldsymbol{\epsilon} \sim \mathcal{N}(0, \boldsymbol{I}) & \scriptstyle{\text{;
    Reparameterization trick.}} \end{aligned} $$
  prefs: []
  type: TYPE_NORMAL
- en: where $\odot$ refers to element-wise product.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/41855f7d735adddd93cd89aa2f9cf8d5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 8\. Illustration of how the reparameterization trick makes the $\mathbf{z}$
    sampling process trainable.(Image source: Slide 12 in Kingma’s NIPS 2015 workshop
    [talk](http://dpkingma.com/wordpress/wp-content/uploads/2015/12/talk_nips_workshop_2015.pdf))'
  prefs: []
  type: TYPE_NORMAL
- en: The reparameterization trick works for other types of distributions too, not
    only Gaussian. In the multivariate Gaussian case, we make the model trainable
    by learning the mean and variance of the distribution, $\mu$ and $\sigma$, explicitly
    using the reparameterization trick, while the stochasticity remains in the random
    variable $\boldsymbol{\epsilon} \sim \mathcal{N}(0, \boldsymbol{I})$.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/eb5cf06531f510b6d53f21193762025f.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig. 9\. Illustration of variational autoencoder model with the multivariate
    Gaussian assumption.
  prefs: []
  type: TYPE_NORMAL
- en: Beta-VAE
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If each variable in the inferred latent representation $\mathbf{z}$ is only
    sensitive to one single generative factor and relatively invariant to other factors,
    we will say this representation is disentangled or factorized. One benefit that
    often comes with disentangled representation is *good interpretability* and easy
    generalization to a variety of tasks.
  prefs: []
  type: TYPE_NORMAL
- en: For example, a model trained on photos of human faces might capture the gentle,
    skin color, hair color, hair length, emotion, whether wearing a pair of glasses
    and many other relatively independent factors in separate dimensions. Such a disentangled
    representation is very beneficial to facial image generation.
  prefs: []
  type: TYPE_NORMAL
- en: 'β-VAE ([Higgins et al., 2017](https://openreview.net/forum?id=Sy2fzU9gl)) is
    a modification of Variational Autoencoder with a special emphasis to discover
    disentangled latent factors. Following the same incentive in VAE, we want to maximize
    the probability of generating real data, while keeping the distance between the
    real and estimated posterior distributions small (say, under a small constant
    $\delta$):'
  prefs: []
  type: TYPE_NORMAL
- en: $$ \begin{aligned} &\max_{\phi, \theta} \mathbb{E}_{\mathbf{x}\sim\mathcal{D}}[\mathbb{E}_{\mathbf{z}
    \sim q_\phi(\mathbf{z}\vert\mathbf{x})} \log p_\theta(\mathbf{x}\vert\mathbf{z})]\\
    &\text{subject to } D_\text{KL}(q_\phi(\mathbf{z}\vert\mathbf{x})\|p_\theta(\mathbf{z}))
    < \delta \end{aligned} $$
  prefs: []
  type: TYPE_NORMAL
- en: 'We can rewrite it as a Lagrangian with a Lagrangian multiplier $\beta$ under
    the [KKT condition](https://www.cs.cmu.edu/~ggordon/10725-F12/slides/16-kkt.pdf).
    The above optimization problem with only one inequality constraint is equivalent
    to maximizing the following equation $\mathcal{F}(\theta, \phi, \beta)$:'
  prefs: []
  type: TYPE_NORMAL
- en: $$ \begin{aligned} \mathcal{F}(\theta, \phi, \beta) &= \mathbb{E}_{\mathbf{z}
    \sim q_\phi(\mathbf{z}\vert\mathbf{x})} \log p_\theta(\mathbf{x}\vert\mathbf{z})
    - \beta(D_\text{KL}(q_\phi(\mathbf{z}\vert\mathbf{x})\|p_\theta(\mathbf{z})) -
    \delta) & \\ & = \mathbb{E}_{\mathbf{z} \sim q_\phi(\mathbf{z}\vert\mathbf{x})}
    \log p_\theta(\mathbf{x}\vert\mathbf{z}) - \beta D_\text{KL}(q_\phi(\mathbf{z}\vert\mathbf{x})\|p_\theta(\mathbf{z}))
    + \beta \delta & \\ & \geq \mathbb{E}_{\mathbf{z} \sim q_\phi(\mathbf{z}\vert\mathbf{x})}
    \log p_\theta(\mathbf{x}\vert\mathbf{z}) - \beta D_\text{KL}(q_\phi(\mathbf{z}\vert\mathbf{x})\|p_\theta(\mathbf{z}))
    & \scriptstyle{\text{; Because }\beta,\delta\geq 0} \end{aligned} $$
  prefs: []
  type: TYPE_NORMAL
- en: 'The loss function of $\beta$-VAE is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: $$ L_\text{BETA}(\phi, \beta) = - \mathbb{E}_{\mathbf{z} \sim q_\phi(\mathbf{z}\vert\mathbf{x})}
    \log p_\theta(\mathbf{x}\vert\mathbf{z}) + \beta D_\text{KL}(q_\phi(\mathbf{z}\vert\mathbf{x})\|p_\theta(\mathbf{z}))
    $$
  prefs: []
  type: TYPE_NORMAL
- en: where the Lagrangian multiplier $\beta$ is considered as a hyperparameter.
  prefs: []
  type: TYPE_NORMAL
- en: Since the negation of $L_\text{BETA}(\phi, \beta)$ is the lower bound of the
    Lagrangian $\mathcal{F}(\theta, \phi, \beta)$. Minimizing the loss is equivalent
    to maximizing the Lagrangian and thus works for our initial optimization problem.
  prefs: []
  type: TYPE_NORMAL
- en: When $\beta=1$, it is same as VAE. When $\beta > 1$, it applies a stronger constraint
    on the latent bottleneck and limits the representation capacity of $\mathbf{z}$.
    For some conditionally independent generative factors, keeping them disentangled
    is the most efficient representation. Therefore a higher $\beta$ encourages more
    efficient latent encoding and further encourages the disentanglement. Meanwhile,
    a higher $\beta$ may create a trade-off between reconstruction quality and the
    extent of disentanglement.
  prefs: []
  type: TYPE_NORMAL
- en: '[Burgess, et al. (2017)](https://arxiv.org/pdf/1804.03599.pdf) discussed the
    distentangling in $\beta$-VAE in depth with an inspiration by the [information
    bottleneck theory](https://lilianweng.github.io/posts/2017-09-28-information-bottleneck/)
    and further proposed a modification to $\beta$-VAE to better control the encoding
    representation capacity.'
  prefs: []
  type: TYPE_NORMAL
- en: VQ-VAE and VQ-VAE-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **VQ-VAE** (“Vector Quantised-Variational AutoEncoder”; [van den Oord, et
    al. 2017](http://papers.nips.cc/paper/7210-neural-discrete-representation-learning.pdf))
    model learns a discrete latent variable by the encoder, since discrete representations
    may be a more natural fit for problems like language, speech, reasoning, etc.
  prefs: []
  type: TYPE_NORMAL
- en: Vector quantisation (VQ) is a method to map $K$-dimensional vectors into a finite
    set of “code” vectors. The process is very much similar to [KNN](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm)
    algorithm. The optimal centroid code vector that a sample should be mapped to
    is the one with minimum euclidean distance.
  prefs: []
  type: TYPE_NORMAL
- en: Let $\mathbf{e} \in \mathbb{R}^{K \times D}, i=1, \dots, K$ be the latent embedding
    space (also known as “codebook”) in VQ-VAE, where $K$ is the number of latent
    variable categories and $D$ is the embedding size. An individual embedding vector
    is $\mathbf{e}_i \in \mathbb{R}^{D}, i=1, \dots, K$.
  prefs: []
  type: TYPE_NORMAL
- en: 'The encoder output $E(\mathbf{x}) = \mathbf{z}_e$ goes through a nearest-neighbor
    lookup to match to one of $K$ embedding vectors and then this matched code vector
    becomes the input for the decoder $D(.)$:'
  prefs: []
  type: TYPE_NORMAL
- en: $$ \mathbf{z}_q(\mathbf{x}) = \text{Quantize}(E(\mathbf{x})) = \mathbf{e}_k
    \text{ where } k = \arg\min_i \|E(\mathbf{x}) - \mathbf{e}_i \|_2 $$
  prefs: []
  type: TYPE_NORMAL
- en: Note that the discrete latent variables can have different shapes in differnet
    applications; for example, 1D for speech, 2D for image and 3D for video.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1211731c716acf903f4a02920d8114f3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 10\. The architecture of VQ-VAE (Image source: [van den Oord, et al. 2017](http://papers.nips.cc/paper/7210-neural-discrete-representation-learning.pdf))'
  prefs: []
  type: TYPE_NORMAL
- en: 'Because argmin() is non-differentiable on a discrete space, the gradients $\nabla_z
    L$ from decoder input $\mathbf{z}_q$ is copied to the encoder output $\mathbf{z}_e$.
    Other than reconstruction loss, VQ-VAE also optimizes:'
  prefs: []
  type: TYPE_NORMAL
- en: '*VQ loss*: The L2 error between the embedding space and the encoder outputs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Commitment loss*: A measure to encourage the encoder output to stay close
    to the embedding space and to prevent it from fluctuating too frequently from
    one code vector to another.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $$ L = \underbrace{\|\mathbf{x} - D(\mathbf{e}_k)\|_2^2}_{\textrm{reconstruction
    loss}} + \underbrace{\|\text{sg}[E(\mathbf{x})] - \mathbf{e}_k\|_2^2}_{\textrm{VQ
    loss}} + \underbrace{\beta \|E(\mathbf{x}) - \text{sg}[\mathbf{e}_k]\|_2^2}_{\textrm{commitment
    loss}} $$
  prefs: []
  type: TYPE_NORMAL
- en: where $\text{sq}[.]$ is the `stop_gradient` operator.
  prefs: []
  type: TYPE_NORMAL
- en: 'The embedding vectors in the codebook is updated through EMA (exponential moving
    average). Given a code vector $\mathbf{e}_i$, say we have $n_i$ encoder output
    vectors, $\{\mathbf{z}_{i,j}\}_{j=1}^{n_i}$, that are quantized to $\mathbf{e}_i$:'
  prefs: []
  type: TYPE_NORMAL
- en: $$ N_i^{(t)} = \gamma N_i^{(t-1)} + (1-\gamma)n_i^{(t)}\;\;\; \mathbf{m}_i^{(t)}
    = \gamma \mathbf{m}_i^{(t-1)} + (1-\gamma)\sum_{j=1}^{n_i^{(t)}}\mathbf{z}_{i,j}^{(t)}\;\;\;
    \mathbf{e}_i^{(t)} = \mathbf{m}_i^{(t)} / N_i^{(t)} $$
  prefs: []
  type: TYPE_NORMAL
- en: where $(t)$ refers to batch sequence in time. $N_i$ and $\mathbf{m}_i$ are accumulated
    vector count and volume, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: VQ-VAE-2 ([Ali Razavi, et al. 2019](https://arxiv.org/abs/1906.00446)) is a
    two-level hierarchical VQ-VAE combined with self-attention autoregressive model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Stage 1 is to **train a hierarchical VQ-VAE**: The design of hierarchical latent
    variables intends to separate local patterns (i.e., texture) from global information
    (i.e., object shapes). The training of the larger bottom level codebook is conditioned
    on the smaller top level code too, so that it does not have to learn everything
    from scratch.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Stage 2 is to **learn a prior over the latent discrete codebook** so that we
    sample from it and generate images. In this way, the decoder can receive input
    vectors sampled from a similar distribution as the one in training. A powerful
    autoregressive model enhanced with multi-headed self-attention layers is used
    to capture the prior distribution (like [PixelSNAIL; Chen et al 2017](https://arxiv.org/abs/1712.09763)).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Considering that VQ-VAE-2 depends on discrete latent variables configured in
    a simple hierarchical setting, the quality of its generated images are pretty
    amazing.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6e8e56d6998931c779239492d44de09c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 11\. Architecture of hierarchical VQ-VAE and multi-stage image generation.
    (Image source: [Ali Razavi, et al. 2019](https://arxiv.org/abs/1906.00446))'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/390c6de721e8e7327f408c84d666689f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 12\. The VQ-VAE-2 algorithm. (Image source: [Ali Razavi, et al. 2019](https://arxiv.org/abs/1906.00446))'
  prefs: []
  type: TYPE_NORMAL
- en: TD-VAE
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**TD-VAE** (“Temporal Difference VAE”; [Gregor et al., 2019](https://arxiv.org/abs/1806.03107))
    works with sequential data. It relies on three main ideas, described below.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b4646845891ff2e79d7125fc7d66bc11.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig. 13\. State-space model as a Markov Chain model.
  prefs: []
  type: TYPE_NORMAL
- en: '**1\. State-Space Models**'
  prefs: []
  type: TYPE_NORMAL
- en: In (latent) state-space models, a sequence of unobserved hidden states $\mathbf{z}
    = (z_1, \dots, z_T)$ determine the observation states $\mathbf{x} = (x_1, \dots,
    x_T)$. Each time step in the Markov chain model in Fig. 13 can be trained in a
    similar manner as in Fig. 6, where the intractable posterior $p(z \vert x)$ is
    approximated by a function $q(z \vert x)$.
  prefs: []
  type: TYPE_NORMAL
- en: '**2\. Belief State**'
  prefs: []
  type: TYPE_NORMAL
- en: An agent should learn to encode all the past states to reason about the future,
    named as *belief state*, $b_t = belief(x_1, \dots, x_t) = belief(b_{t-1}, x_t)$.
    Given this, the distribution of future states conditioned on the past can be written
    as $p(x_{t+1}, \dots, x_T \vert x_1, \dots, x_t) \approx p(x_{t+1}, \dots, x_T
    \vert b_t)$. The hidden states in a recurrent policy are used as the agent’s belief
    state in TD-VAE. Thus we have $b_t = \text{RNN}(b_{t-1}, x_t)$.
  prefs: []
  type: TYPE_NORMAL
- en: '**3\. Jumpy Prediction**'
  prefs: []
  type: TYPE_NORMAL
- en: Further, an agent is expected to imagine distant futures based on all the information
    gathered so far, suggesting the capability of making jumpy predictions, that is,
    predicting states several steps further into the future.
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall what we have learned from the variance lower bound [above](#loss-function-elbo):'
  prefs: []
  type: TYPE_NORMAL
- en: $$ \begin{aligned} \log p(x) &\geq \log p(x) - D_\text{KL}(q(z|x)\|p(z|x)) \\
    &= \mathbb{E}_{z\sim q} \log p(x|z) - D_\text{KL}(q(z|x)\|p(z)) \\ &= \mathbb{E}_{z
    \sim q} \log p(x|z) - \mathbb{E}_{z \sim q} \log \frac{q(z|x)}{p(z)} \\ &= \mathbb{E}_{z
    \sim q}[\log p(x|z) -\log q(z|x) + \log p(z)] \\ &= \mathbb{E}_{z \sim q}[\log
    p(x, z) -\log q(z|x)] \\ \log p(x) &\geq \mathbb{E}_{z \sim q}[\log p(x, z) -\log
    q(z|x)] \end{aligned} $$
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let’s model the distribution of the state $x_t$ as a probability function
    conditioned on all the past states $x_{<t}$ and two latent variables, $z_t$ and
    $z_{t-1}$, at current time step and one step back:'
  prefs: []
  type: TYPE_NORMAL
- en: $$ \log p(x_t|x_{
  prefs: []
  type: TYPE_NORMAL
- en: 'Continue expanding the equation:'
  prefs: []
  type: TYPE_NORMAL
- en: $$ \begin{aligned} & \log p(x_t|x_{
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice two things:'
  prefs: []
  type: TYPE_NORMAL
- en: The red terms can be ignored according to Markov assumptions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The blue term is expanded according to Markov assumptions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The green term is expanded to include an one-step prediction back to the past
    as a smoothing distribution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Precisely, there are four types of distributions to learn:'
  prefs: []
  type: TYPE_NORMAL
- en: '$p_D(.)$ is the **decoder** distribution:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: $p(x_t \mid z_t)$ is the encoder by the common definition;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $p(x_t \mid z_t) \to p_D(x_t \mid z_t)$;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '$p_T(.)$ is the **transition** distribution:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: $p(z_t \mid z_{t-1})$ captures the sequential dependency between latent variables;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $p(z_t \mid z_{t-1}) \to p_T(z_t \mid z_{t-1})$;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '$p_B(.)$ is the **belief** distribution:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Both $p(z_{t-1} \mid x_{<t})$ and $q(z_t \mid x_{\leq t})$ can use the belief
    states to predict the latent variables;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $p(z_{t-1} \mid x_{<t}) \to p_B(z_{t-1} \mid b_{t-1})$;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $q(z_{t} \mid x_{\leq t}) \to p_B(z_t \mid b_t)$;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '$p_S(.)$ is the **smoothing** distribution:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The back-to-past smoothing term $q(z_{t-1} \mid z_t, x_{\leq t})$ can be rewritten
    to be dependent of belief states too;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $q(z_{t-1} \mid z_t, x_{\leq t}) \to p_S(z_{t-1} \mid z_t, b_{t-1}, b_t)$;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To incorporate the idea of jumpy prediction, the sequential ELBO has to not
    only work on $t, t+1$, but also two distant timestamp $t_1 < t_2$. Here is the
    final TD-VAE objective function to maximize:'
  prefs: []
  type: TYPE_NORMAL
- en: $$ J_{t_1, t_2} = \mathbb{E}[ \log p_D(x_{t_2}|z_{t_2}) + \log p_B(z_{t_1}|b_{t_1})
    + \log p_T(z_{t_2}|z_{t_1}) - \log p_B(z_{t_2}|b_{t_2}) - \log p_S(z_{t_1}|z_{t_2},
    b_{t_1}, b_{t_2})] $$![](../Images/9774b76a16c22bfb088fb2fb9004368e.png)
  prefs: []
  type: TYPE_NORMAL
- en: 'Fig. 14\. A detailed overview of TD-VAE architecture, very nicely done. (Image
    source: [TD-VAE paper](https://arxiv.org/abs/1806.03107))'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 'Cited as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] Geoffrey E. Hinton, and Ruslan R. Salakhutdinov. [“Reducing the dimensionality
    of data with neural networks.”](https://pdfs.semanticscholar.org/c50d/ca78e97e335d362d6b991ae0e1448914e9a3.pdf)
    Science 313.5786 (2006): 504-507.'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] Pascal Vincent, et al. [“Extracting and composing robust features with
    denoising autoencoders.”](http://www.cs.toronto.edu/~larocheh/publications/icml-2008-denoising-autoencoders.pdf)
    ICML, 2008.'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] Pascal Vincent, et al. [“Stacked denoising autoencoders: Learning useful
    representations in a deep network with a local denoising criterion.”](http://www.jmlr.org/papers/volume11/vincent10a/vincent10a.pdf).
    Journal of machine learning research 11.Dec (2010): 3371-3408.'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever,
    and Ruslan R. Salakhutdinov. “Improving neural networks by preventing co-adaptation
    of feature detectors.” arXiv preprint arXiv:1207.0580 (2012).'
  prefs: []
  type: TYPE_NORMAL
- en: '[5] [Sparse Autoencoder](https://web.stanford.edu/class/cs294a/sparseAutoencoder.pdf)
    by Andrew Ng.'
  prefs: []
  type: TYPE_NORMAL
- en: '[6] Alireza Makhzani, Brendan Frey (2013). [“k-sparse autoencoder”](https://arxiv.org/abs/1312.5663).
    ICLR 2014.'
  prefs: []
  type: TYPE_NORMAL
- en: '[7] Salah Rifai, et al. [“Contractive auto-encoders: Explicit invariance during
    feature extraction.”](http://www.icml-2011.org/papers/455_icmlpaper.pdf) ICML,
    2011.'
  prefs: []
  type: TYPE_NORMAL
- en: '[8] Diederik P. Kingma, and Max Welling. [“Auto-encoding variational bayes.”](https://arxiv.org/abs/1312.6114)
    ICLR 2014.'
  prefs: []
  type: TYPE_NORMAL
- en: '[9] [Tutorial - What is a variational autoencoder?](https://jaan.io/what-is-variational-autoencoder-vae-tutorial/)
    on jaan.io'
  prefs: []
  type: TYPE_NORMAL
- en: '[10] Youtube tutorial: [Variational Autoencoders](https://www.youtube.com/watch?v=9zKuYvjFFS8)
    by Arxiv Insights'
  prefs: []
  type: TYPE_NORMAL
- en: '[11] [“A Beginner’s Guide to Variational Methods: Mean-Field Approximation”](https://blog.evjang.com/2016/08/variational-bayes.html)
    by Eric Jang.'
  prefs: []
  type: TYPE_NORMAL
- en: '[12] Carl Doersch. [“Tutorial on variational autoencoders.”](https://arxiv.org/abs/1606.05908)
    arXiv:1606.05908, 2016.'
  prefs: []
  type: TYPE_NORMAL
- en: '[13] Irina Higgins, et al. ["$\beta$-VAE: Learning basic visual concepts with
    a constrained variational framework."](https://openreview.net/forum?id=Sy2fzU9gl)
    ICLR 2017.'
  prefs: []
  type: TYPE_NORMAL
- en: '[14] Christopher P. Burgess, et al. [“Understanding disentangling in beta-VAE.”](https://arxiv.org/abs/1804.03599)
    NIPS 2017.'
  prefs: []
  type: TYPE_NORMAL
- en: '[15] Aaron van den Oord, et al. [“Neural Discrete Representation Learning”](https://arxiv.org/abs/1711.00937)
    NIPS 2017.'
  prefs: []
  type: TYPE_NORMAL
- en: '[16] Ali Razavi, et al. [“Generating Diverse High-Fidelity Images with VQ-VAE-2”](https://arxiv.org/abs/1906.00446).
    arXiv preprint arXiv:1906.00446 (2019).'
  prefs: []
  type: TYPE_NORMAL
- en: '[17] Xi Chen, et al. [“PixelSNAIL: An Improved Autoregressive Generative Model.”](https://arxiv.org/abs/1712.09763)
    arXiv preprint arXiv:1712.09763 (2017).'
  prefs: []
  type: TYPE_NORMAL
- en: '[18] Karol Gregor, et al. [“Temporal Difference Variational Auto-Encoder.”](https://arxiv.org/abs/1806.03107)
    ICLR 2019.'
  prefs: []
  type: TYPE_NORMAL
- en: '[autoencoder](https://lilianweng.github.io/tags/autoencoder/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[generative-model](https://lilianweng.github.io/tags/generative-model/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[image-generation](https://lilianweng.github.io/tags/image-generation/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[«'
  prefs: []
  type: TYPE_NORMAL
- en: Flow-based Deep Generative Models](https://lilianweng.github.io/posts/2018-10-13-flow-models/)
    [»
  prefs: []
  type: TYPE_NORMAL
- en: Attention? Attention!](https://lilianweng.github.io/posts/2018-06-24-attention/)[](https://twitter.com/intent/tweet/?text=From%20Autoencoder%20to%20Beta-VAE&url=https%3a%2f%2flilianweng.github.io%2fposts%2f2018-08-12-vae%2f&hashtags=autoencoder%2cgenerative-model%2cimage-generation)[](https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2flilianweng.github.io%2fposts%2f2018-08-12-vae%2f&title=From%20Autoencoder%20to%20Beta-VAE&summary=From%20Autoencoder%20to%20Beta-VAE&source=https%3a%2f%2flilianweng.github.io%2fposts%2f2018-08-12-vae%2f)[](https://reddit.com/submit?url=https%3a%2f%2flilianweng.github.io%2fposts%2f2018-08-12-vae%2f&title=From%20Autoencoder%20to%20Beta-VAE)[](https://facebook.com/sharer/sharer.php?u=https%3a%2f%2flilianweng.github.io%2fposts%2f2018-08-12-vae%2f)[](https://api.whatsapp.com/send?text=From%20Autoencoder%20to%20Beta-VAE%20-%20https%3a%2f%2flilianweng.github.io%2fposts%2f2018-08-12-vae%2f)[](https://telegram.me/share/url?text=From%20Autoencoder%20to%20Beta-VAE&url=https%3a%2f%2flilianweng.github.io%2fposts%2f2018-08-12-vae%2f)©
    2024 [Lil'Log](https://lilianweng.github.io/) Powered by [Hugo](https://gohugo.io/)
    & [PaperMod](https://git.io/hugopapermod)[](#top "Go to Top (Alt + G)")
  prefs: []
  type: TYPE_NORMAL
