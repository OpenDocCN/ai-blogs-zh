- en: Anatomize Deep Learning with Information Theory
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用信息理论解剖深度学习
- en: 原文：[https://lilianweng.github.io/posts/2017-09-28-information-bottleneck/](https://lilianweng.github.io/posts/2017-09-28-information-bottleneck/)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://lilianweng.github.io/posts/2017-09-28-information-bottleneck/](https://lilianweng.github.io/posts/2017-09-28-information-bottleneck/)
- en: Professor Naftali Tishby passed away in 2021\. Hope the post can introduce his
    cool idea of information bottleneck to more people.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: Naftali Tishby教授于2021年去世。希望这篇文章能向更多人介绍他关于信息瓶颈的酷炫想法。
- en: 'Recently I watched the talk [“Information Theory in Deep Learning”](https://youtu.be/bLqJHjXihK8)
    by Prof Naftali Tishby and found it very interesting. He presented how to apply
    the information theory to study the growth and transformation of deep neural networks
    during training. Using the [Information Bottleneck (IB)](https://arxiv.org/pdf/physics/0004057.pdf)
    method, he proposed a new learning bound for deep neural networks (DNN), as the
    traditional learning theory fails due to the exponentially large number of parameters.
    Another keen observation is that DNN training involves two distinct phases: First,
    the network is trained to fully represent the input data and minimize the generalization
    error; then, it learns to forget the irrelevant details by compressing the representation
    of the input.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 最近我观看了Naftali Tishby教授的讲座[“深度学习中的信息理论”](https://youtu.be/bLqJHjXihK8)，发现非常有趣。他展示了如何将信息理论应用于研究深度神经网络在训练过程中的增长和转变。利用[信息瓶颈（IB）](https://arxiv.org/pdf/physics/0004057.pdf)方法，他为深度神经网络（DNN）提出了一个新的学习界限，因为传统的学习理论由于参数数量的指数增长而失败。另一个敏锐的观察是，DNN训练涉及两个不同的阶段：首先，网络被训练以完全表示输入数据并最小化泛化误差；然后，它通过压缩输入的表示来学会忘记不相关的细节。
- en: Most of the materials in this post are from Prof Tishby’s talk and [related
    papers](https://lilianweng.github.io/posts/2017-09-28-information-bottleneck/#references).
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇文章中的大部分材料来自Tishby教授的讲座和[相关论文](https://lilianweng.github.io/posts/2017-09-28-information-bottleneck/#references)。
- en: Basic Concepts
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基本概念
- en: '[**Markov Chain**](https://en.wikipedia.org/wiki/Markov_chain)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[**马尔可夫链**](https://en.wikipedia.org/wiki/Markov_chain)'
- en: A Markov process is a [“memoryless”](http://mathworld.wolfram.com/Memoryless.html)
    (also called “Markov Property”) stochastic process. A Markov chain is a type of
    Markov process containing multiple discrete states. That is being said, the conditional
    probability of future states of the process is only determined by the current
    state and does not depend on the past states.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 马尔可夫过程是一种“无记忆”（也称为“马尔可夫性质”）的随机过程。马尔可夫链是包含多个离散状态的马尔可夫过程的一种类型。也就是说，过程的未来状态的条件概率仅由当前状态决定，不依赖于过去状态。
- en: '[**Kullback–Leibler (KL) Divergence**](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[**Kullback–Leibler（KL）散度**](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)'
- en: KL divergence measures how one probability distribution $p$ diverges from a
    second expected probability distribution $q$. It is asymmetric.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: KL散度衡量了一个概率分布$p$与第二个期望概率分布$q$之间的偏离程度。它是不对称的。
- en: $$ \begin{aligned} D_{KL}(p \| q) &= \sum_x p(x) \log \frac{p(x)}{q(x)} \\ &=
    - \sum_x p(x)\log q(x) + \sum_x p(x)\log p(x) \\ &= H(P, Q) - H(P) \end{aligned}
    $$
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \begin{aligned} D_{KL}(p \| q) &= \sum_x p(x) \log \frac{p(x)}{q(x)} \\ &=
    - \sum_x p(x)\log q(x) + \sum_x p(x)\log p(x) \\ &= H(P, Q) - H(P) \end{aligned}
    $$
- en: $D_{KL}$ achieves the minimum zero when $p(x)$ == $q(x)$ everywhere.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 当$p(x)$ == $q(x)$时，$D_{KL}$达到最小值零。
- en: '[**Mutual Information**](https://en.wikipedia.org/wiki/Mutual_information)'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[**互信息**](https://en.wikipedia.org/wiki/Mutual_information)'
- en: Mutual information measures the mutual dependence between two variables. It
    quantifies the “amount of information” obtained about one random variable through
    the other random variable. Mutual information is symmetric.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 互信息度量了两个变量之间的相互依赖关系。它量化了通过另一个随机变量获得的“信息量”。互信息是对称的。
- en: $$ \begin{aligned} I(X;Y) &= D_{KL}[p(x,y) \| p(x)p(y)] \\ &= \sum_{x \in X,
    y \in Y} p(x, y) \log(\frac{p(x, y)}{p(x)p(y)}) \\ &= \sum_{x \in X, y \in Y}
    p(x, y) \log(\frac{p(x|y)}{p(x)}) \\ &= H(X) - H(X|Y) \\ \end{aligned} $$
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \begin{aligned} I(X;Y) &= D_{KL}[p(x,y) \| p(x)p(y)] \\ &= \sum_{x \in X,
    y \in Y} p(x, y) \log(\frac{p(x, y)}{p(x)p(y)}) \\ &= \sum_{x \in X, y \in Y}
    p(x, y) \log(\frac{p(x|y)}{p(x)}) \\ &= H(X) - H(X|Y) \\ \end{aligned} $$
- en: '[**Data Processing Inequality (DPI)**](https://en.wikipedia.org/wiki/Data_processing_inequality)'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '[**数据处理不等式（DPI）**](https://en.wikipedia.org/wiki/Data_processing_inequality)'
- en: 'For any markov chain: $X \to Y \to Z$, we would have $I(X; Y) \geq I(X; Z)$.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何马尔可夫链：$X \to Y \to Z$，我们有 $I(X; Y) \geq I(X; Z)$。
- en: A deep neural network can be viewed as a Markov chain, and thus when we are
    moving down the layers of a DNN, the mutual information between the layer and
    the input can only decrease.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 一个深度神经网络可以被视为马尔可夫链，因此当我们沿着DNN的层向下移动时，层与输入之间的互信息只能减少。
- en: '[**Reparametrization invariance**](https://en.wikipedia.org/wiki/Parametrization#Parametrization_invariance)'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[**重参数化不变性**](https://en.wikipedia.org/wiki/Parametrization#Parametrization_invariance)'
- en: 'For two invertible functions $\phi$, $\psi$, the mutual information still holds:
    $I(X; Y) = I(\phi(X); \psi(Y))$.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 对于两个可逆函数$\phi$，$\psi$，互信息仍然成立：$I(X; Y) = I(\phi(X); \psi(Y))$。
- en: For example, if we shuffle the weights in one layer of DNN, it would not affect
    the mutual information between this layer and another.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们对DNN的一个层中的权重进行洗牌，这不会影响该层与另一层之间的互信息。
- en: Deep Neural Networks as Markov Chains
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度神经网络作为马尔可夫链
- en: The training data contains sampled observations from the joint distribution
    of $X$ and $Y$. The input variable $X$ and weights of hidden layers are all high-dimensional
    random variable. The ground truth target $Y$ and the predicted value $\hat{Y}$
    are random variables of smaller dimensions in the classification settings.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据包含从$X$和$Y$的联合分布中抽样的观测。输入变量$X$和隐藏层的权重都是高维随机变量。真实目标$Y$和预测值$\hat{Y}$在分类设置中是维度较小的随机变量。
- en: '![](../Images/299525d24d61a09c81f086b94d3fac0a.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/299525d24d61a09c81f086b94d3fac0a.png)'
- en: 'Fig. 1\. The structure of a deep neural network, which consists of the target
    label $Y$, input layer $X$, hidden layers $h\_1, \dots, h\_m$ and the final prediction
    $\hat{Y}$. (Image source: [Tishby and Zaslavsky, 2015](https://arxiv.org/pdf/1503.02406.pdf))'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图1\. 一个深度神经网络的结构，包括目标标签$Y$，输入层$X$，隐藏层$h\_1, \dots, h\_m$和最终预测$\hat{Y}$。（图片来源：[Tishby
    and Zaslavsky, 2015](https://arxiv.org/pdf/1503.02406.pdf)）
- en: 'If we label the hidden layers of a DNN as $h_1, h_2, \dots, h_m$ as in Fig.
    1, we can view each layer as one state of a Markov Chain: $ h_i \to h_{i+1}$.
    According to DPI, we would have:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们像图1中标记DNN的隐藏层为$h_1, h_2, \dots, h_m$，我们可以将每一层视为马尔可夫链的一个状态：$ h_i \to h_{i+1}$。根据DPI，我们会有：
- en: $$ \begin{aligned} H(X) \geq I(X; h_1) \geq I(X; h_2) \geq \dots \geq I(X; h_m)
    \geq I(X; \hat{Y}) \\ I(X; Y) \geq I(h_1; Y) \geq I(h_2; Y) \geq \dots \geq I(h_m;
    Y) \geq I(\hat{Y}; Y) \end{aligned} $$
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \begin{aligned} H(X) \geq I(X; h_1) \geq I(X; h_2) \geq \dots \geq I(X; h_m)
    \geq I(X; \hat{Y}) \\ I(X; Y) \geq I(h_1; Y) \geq I(h_2; Y) \geq \dots \geq I(h_m;
    Y) \geq I(\hat{Y}; Y) \end{aligned} $$
- en: A DNN is designed to learn how to describe $X$ to predict $Y$ and eventually,
    to compress $X$ to only hold the information related to $Y$. Tishby describes
    this processing as *“successive refinement of relevant information”*.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 一个深度神经网络（DNN）被设计成学习如何描述$X$以预测$Y，并最终将$X$压缩，只保留与$Y$相关的信息。Tishby将这种处理描述为*“相关信息的逐步细化”*。
- en: Information Plane Theorem
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 信息平面定理
- en: A DNN has successive internal representations of $X$, a set of hidden layers
    $\{T_i\}$. The *information plane* theorem characterizes each layer by its encoder
    and decoder information. The encoder is a representation of the input data $X$,
    while the decoder translates the information in the current layer to the target
    ouput $Y$.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 一个DNN具有$X$的连续内部表示，一组隐藏层$\{T_i\}$。*信息平面*定理通过其编码器和解码器信息来表征每一层。编码器是输入数据$X$的表示，而解码器将当前层中的信息转换为目标输出$Y$。
- en: 'Precisely, in an information plane plot:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 精确地，在信息平面图中：
- en: '**X-axis**: The sample complexity of $T_i$ is determined by the encoder mutual
    information $I(X; T_i)$. Sample complexity refers to how many samples you need
    to achieve certain accuracy and generalization.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**X轴**：$T_i$的样本复杂度由编码器的互信息$I(X; T_i)$确定。样本复杂度指的是为达到一定精度和泛化性能需要多少样本。'
- en: '**Y-axis**: The accuracy (generalization error) is determined by the decoder
    mutual information $I(T_i; Y)$.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Y轴**：准确度（泛化误差）由解码器的互信息$I(T_i; Y)$确定。'
- en: '![](../Images/db04b80b8d6b009306ab7a0c1bd15b90.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/db04b80b8d6b009306ab7a0c1bd15b90.png)'
- en: 'Fig. 2\. The encoder vs decoder mutual information of DNN hidden layers of
    50 experiments. Different layers are color-coders, with green being the layer
    right next to the input and the orange being the furthest. There are three snapshots,
    at the initial epoch, 400 epochs and 9000 epochs respectively. (Image source:
    [Shwartz-Ziv and Tishby, 2017](https://arxiv.org/pdf/1703.00810.pdf))'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图2\. DNN隐藏层的编码器与解码器互信息，共50次实验。不同层以颜色区分，绿色为最靠近输入的层，橙色为最远的层。有三个快照，分别为初始时期、400个时期和9000个时期。（图片来源：[Shwartz-Ziv
    and Tishby, 2017](https://arxiv.org/pdf/1703.00810.pdf)）
- en: Each dot in Fig. 2\. marks the encoder/ decoder mutual information of one hidden
    layer of one network simulation (no regularization is applied; no weights decay,
    no dropout, etc.). They move up as expected because the knowledge about the true
    labels is increasing (accuracy increases). At the early stage, the hidden layers
    learn a lot about the input $X$, but later they start to compress to forget some
    information about the input. Tishby believes that *“the most important part of
    learning is actually forgetting”*. Check out this [nice video](https://youtu.be/P1A1yNsxMjc)
    that demonstrates how the mutual information measures of layers are changing in
    epoch time.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2\. 中的每个点标记了一个网络模拟的一个隐藏层的编码器/解码器互信息（没有应用正则化；没有权重衰减，没有丢弃等）。它们如预期地上升，因为对真实标签的知识增加（准确性增加）。在早期阶段，隐藏层学习了关于输入
    $X$ 的很多信息，但后来它们开始压缩以忘记一些关于输入的信息。Tishby 认为*“学习中最重要的部分实际上是遗忘”*。看看这个[不错的视频](https://youtu.be/P1A1yNsxMjc)，展示了层的互信息随时期变化的情况。
- en: '![](../Images/24dbba82727e84ea5c448a5eb0a140c3.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/24dbba82727e84ea5c448a5eb0a140c3.png)'
- en: 'Fig. 3\. Here is an aggregated view of Fig 2\. The compression happens after
    the generalization error becomes very small. (Image source: [Tishby’ talk 15:15](https://youtu.be/bLqJHjXihK8?t=15m15s))'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3\. 这里是图 2\. 的聚合视图。压缩发生在泛化误差变得非常小之后。 (图片来源: [Tishby’ talk 15:15](https://youtu.be/bLqJHjXihK8?t=15m15s))'
- en: Two Optimization Phases
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 两个优化阶段
- en: Tracking the normalized mean and standard deviation of each layer’s weights
    in time also reveals two optimization phases of the training process.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 跟踪每个层权重的归一化均值和标准差随时间的变化也揭示了训练过程的两个优化阶段。
- en: '![](../Images/a46487283321e26a3874a4f56504aab2.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a46487283321e26a3874a4f56504aab2.png)'
- en: 'Fig. 4\. The norm of mean and standard deviation of each layer''s weight gradients
    for each layer as a function of training epochs. Different layers are color-coded.
    (Image source: [Shwartz-Ziv and Tishby, 2017](https://arxiv.org/pdf/1703.00810.pdf))'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '图 4\. 每个层的权重梯度的均值和标准差的范数作为训练时期函数的图示。不同层以颜色编码。 (图片来源: [Shwartz-Ziv and Tishby,
    2017](https://arxiv.org/pdf/1703.00810.pdf))'
- en: Among early epochs, the mean values are three magnitudes larger than the standard
    deviations. After a sufficient number of epochs, the error saturates and the standard
    deviations become much noisier afterward. The further a layer is away from the
    output, the noisier it gets, because the noises can get amplified and accumulated
    through the back-prop process (not due to the width of the layer).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在早期时期，均值比标准差大三个数量级。经过足够数量的时期后，误差饱和，标准差之后变得更加嘈杂。离输出层越远的层，噪音就越大，因为噪音可以通过反向传播过程放大和积累（不是由于层的宽度）。
- en: Learning Theory
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习理论
- en: “Old” Generalization Bounds
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: “旧” 泛化界限
- en: 'The generalization bounds defined by the classic learning theory is:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 经典学习理论定义的泛化界限是：
- en: $$ \epsilon^2 < \frac{\log|H_\epsilon| + \log{1/\delta}}{2m} $$
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \epsilon^2 < \frac{\log|H_\epsilon| + \log{1/\delta}}{2m} $$
- en: '$\epsilon$: The difference between the training error and the generalization
    error. The generalization error measures how accurate the prediction of an algorithm
    is for previously unseen data.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '$\epsilon$: 训练误差和泛化误差之间的差异。泛化误差衡量算法对以前未见数据的预测的准确性。'
- en: '$H_\epsilon$: $\epsilon$-cover of the hypothesis class. Typically we assume
    the size $\vert H_\epsilon \vert \sim (1/\epsilon)^d$.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '$H_\epsilon$: 假设类的 $\epsilon$-覆盖。通常我们假设大小 $\vert H_\epsilon \vert \sim (1/\epsilon)^d$。'
- en: '$\delta$: Confidence.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '$\delta$: 置信度。'
- en: '$m$: The number of training samples.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '$m$: 训练样本的数量。'
- en: '$d$: The VC dimension of the hypothesis.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '$d$: 假设的 VC 维度。'
- en: This definition states that the difference between the training error and the
    generalization error is bounded by a function of the hypothesis space size and
    the dataset size. The bigger the hypothesis space gets, the bigger the generalization
    error becomes. I recommend this tutorial on ML theory, [part1](https://mostafa-samir.github.io/ml-theory-pt1/)
    and [part2](https://mostafa-samir.github.io/ml-theory-pt2/), if you are interested
    in reading more on generalization bounds.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这个定义说明训练误差和泛化误差之间的差异受假设空间大小和数据集大小的函数限制。假设空间越大，泛化误差就越大。如果你对泛化界限感兴趣，我推荐这个关于 ML
    理论的教程，[part1](https://mostafa-samir.github.io/ml-theory-pt1/) 和 [part2](https://mostafa-samir.github.io/ml-theory-pt2/)。
- en: However, it does not work for deep learning. The larger a network is, the more
    parameters it needs to learn. With this generalization bounds, larger networks
    (larger $d$) would have worse bounds. This is contrary to the intuition that larger
    networks are able to achieve better performance with higher expressivity.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这对深度学习不起作用。网络越大，需要学习的参数就越多。根据这些泛化界限，更大的网络（更大的$d$）将具有更差的界限。这与更大的网络能够通过更高的表达能力实现更好性能的直觉相矛盾。
- en: “New” Input compression bound
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: “新”输入压缩界限
- en: To solve this counterintuitive observation, Tishby et al. proposed a new input
    compression bound for DNN.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这一反直觉的观察，Tishby等人提出了DNN的新输入压缩界限。
- en: First let us have $T_\epsilon$ as an $\epsilon$-partition of the input variable
    $X$. This partition compresses the input with respect to the homogeneity to the
    labels into small cells. The cells in total can cover the whole input space. If
    the prediction outputs binary values, we can replace the cardinality of the hypothesis,
    $\vert H_\epsilon \vert$, with $2^{\vert T_\epsilon \vert}$.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 首先让我们将$T_\epsilon$作为输入变量$X$的一个$\epsilon$-分区。这个分区将输入相对于标签的同质性进行压缩到小单元格中。这些单元格总共可以覆盖整个输入空间。如果预测输出是二进制值，我们可以用$2^{\vert
    T_\epsilon \vert}$替换假设的基数$\vert H_\epsilon \vert$。
- en: $$ |H_\epsilon| \sim 2^{|X|} \to 2^{|T_\epsilon|} $$
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: $$ |H_\epsilon| \sim 2^{|X|} \to 2^{|T_\epsilon|} $$
- en: 'When $X$ is large, the size of $X$ is approximately $2^{H(X)}$. Each cell in
    the $\epsilon$-partition is of size $2^{H(X \vert T_\epsilon)}$. Therefore we
    have $\vert T_\epsilon \vert \sim \frac{2^{H(X)}}{2^{H(X \vert T_\epsilon)}} =
    2^{I(T_\epsilon; X)}$. Then the input compression bound becomes:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 当$X$很大时，$X$的大小约为$2^{H(X)}$。$\epsilon$-分区中的每个单元大小约为$2^{H(X \vert T_\epsilon)}$。因此，我们有$\vert
    T_\epsilon \vert \sim \frac{2^{H(X)}}{2^{H(X \vert T_\epsilon)}} = 2^{I(T_\epsilon;
    X)}$。然后输入压缩界限变为：
- en: $$ \epsilon^2 < \frac{2^{I(T_\epsilon; X)} + \log{1/\delta}}{2m} $$![](../Images/9d816171e63c2bf5f8933a29512d4059.png)
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \epsilon^2 < \frac{2^{I(T_\epsilon; X)} + \log{1/\delta}}{2m} $$![](../Images/9d816171e63c2bf5f8933a29512d4059.png)
- en: Fig. 5\. The black line is the optimal achievable information bottleneck (IB)
    limit. The red line corresponds to the upper bound on the out-of-sample IB distortion,
    when trained on a finite sample set. $\Delta C$ is the complexity gap and $\Delta
    G$ is the generalization gap. (Recreated based on [Tishby’ talk 24:50](https://youtu.be/bLqJHjXihK8?t=24m56s))
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图5\. 黑线是最佳可实现的信息瓶颈（IB）极限。红线对应于在有限样本集上训练时的样本外IB失真的上界。$\Delta C$是复杂度差距，$\Delta
    G$是泛化差距。（基于[Tishby的演讲 24:50](https://youtu.be/bLqJHjXihK8?t=24m56s)重新创建）
- en: Network Size and Training Data Size
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 网络大小和训练数据大小
- en: The Benefit of More Hidden Layers
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更多隐藏层的好处
- en: Having more layers give us computational benefits and speed up the training
    process for good generalization.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有更多层次给我们带来计算上的好处，并加快训练过程以获得良好的泛化能力。
- en: '![](../Images/c9adee527ec8529de3bc055469835592.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c9adee527ec8529de3bc055469835592.png)'
- en: 'Fig. 6\. The optimization time is much shorter (fewer epochs) with more hidden
    layers. (Image source: [Shwartz-Ziv and Tishby, 2017](https://arxiv.org/pdf/1703.00810.pdf))'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图6\. 随着隐藏层数增加，优化时间（较少的迭代次数）大大缩短。（图片来源：[Shwartz-Ziv and Tishby, 2017](https://arxiv.org/pdf/1703.00810.pdf)）
- en: '**Compression through stochastic relaxation**: According to the [diffusion
    equation](https://en.wikipedia.org/wiki/Fokker%E2%80%93Planck_equation), the relaxation
    time of layer $k$ is proportional to the exponential of this layer’s compression
    amount $\Delta S_k$: $\Delta t_k \sim \exp(\Delta S_k)$. We can compute the layer
    compression as $\Delta S_k = I(X; T_k) - I(X; T_{k-1})$. Because $\exp(\sum_k
    \Delta S_k) \geq \sum_k \exp(\Delta S_k)$, we would expect an exponential decrease
    in training epochs with more hidden layers (larger $k$).'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**通过随机松弛进行压缩**：根据[扩散方程](https://en.wikipedia.org/wiki/Fokker%E2%80%93Planck_equation)，第$k$层的松弛时间与该层的压缩量$\Delta
    S_k$的指数成正比：$\Delta t_k \sim \exp(\Delta S_k)$。我们可以计算层的压缩量为$\Delta S_k = I(X; T_k)
    - I(X; T_{k-1})$。因为$\exp(\sum_k \Delta S_k) \geq \sum_k \exp(\Delta S_k)$，我们预期随着隐藏层数增加（更大的$k$），训练迭代次数会呈指数减少。'
- en: The Benefit of More Training Samples
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更多训练样本的好处
- en: Fitting more training data requires more information captured by the hidden
    layers. With increased training data size, the decoder mutual information (recall
    that this is directly related to the generalization error), $I(T; Y)$, is pushed
    up and gets closer to the theoretical information bottleneck bound. Tishby emphasized
    that It is the mutual information, not the layer size or the VC dimension, that
    determines generalization, different from standard theories.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 拟合更多的训练数据需要隐藏层捕获更多的信息。随着训练数据规模的增加，解码器的互信息（回想一下，这与泛化误差直接相关），$I(T; Y)$，被推高并接近理论信息瓶颈界限。Tishby强调，决定泛化的是互信息，而不是层大小或VC维度，这与标准理论不同。
- en: '![](../Images/1f76910a5f723427b12b5c88c69b59e4.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1f76910a5f723427b12b5c88c69b59e4.png)'
- en: 'Fig. 7\. The training data of different sizes is color-coded. The information
    plane of multiple converged networks are plotted. More training data leads to
    better generalization. (Image source: [Shwartz-Ziv and Tishby, 2017](https://arxiv.org/pdf/1703.00810.pdf))'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图7。不同大小的训练数据以颜色编码。绘制了多个收敛网络的信息平面。更多的训练数据导致更好的泛化。（图片来源：[Shwartz-Ziv 和 Tishby，2017](https://arxiv.org/pdf/1703.00810.pdf))
- en: '* * *'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 'Cited as:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 引用为：
- en: '[PRE0]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: References
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Naftali Tishby. [Information Theory of Deep Learning](https://youtu.be/bLqJHjXihK8)'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Naftali Tishby. [深度学习的信息理论](https://youtu.be/bLqJHjXihK8)'
- en: '[2] [Machine Learning Theory - Part 1: Introduction](https://mostafa-samir.github.io/ml-theory-pt1/)'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] [机器学习理论 - 第1部分：介绍](https://mostafa-samir.github.io/ml-theory-pt1/)'
- en: '[3] [Machine Learning Theory - Part 2: Generalization Bounds](https://mostafa-samir.github.io/ml-theory-pt2/)'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] [机器学习理论 - 第2部分：泛化界限](https://mostafa-samir.github.io/ml-theory-pt2/)'
- en: '[4] [New Theory Cracks Open the Black Box of Deep Learning](https://www.quantamagazine.org/new-theory-cracks-open-the-black-box-of-deep-learning-20170921/)
    by Quanta Magazine.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] [揭开深度学习黑匣子的新理论](https://www.quantamagazine.org/new-theory-cracks-open-the-black-box-of-deep-learning-20170921/)，作者为Quanta
    Magazine。'
- en: '[5] Naftali Tishby and Noga Zaslavsky. [“Deep learning and the information
    bottleneck principle.”](https://arxiv.org/pdf/1503.02406.pdf) IEEE Information
    Theory Workshop (ITW), 2015.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Naftali Tishby 和 Noga Zaslavsky. [“深度学习与信息瓶颈原理。”](https://arxiv.org/pdf/1503.02406.pdf)
    IEEE信息理论研讨会（ITW），2015。'
- en: '[6] Ravid Shwartz-Ziv and Naftali Tishby. [“Opening the Black Box of Deep Neural
    Networks via Information.”](https://arxiv.org/pdf/1703.00810.pdf) arXiv preprint
    arXiv:1703.00810, 2017.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Ravid Shwartz-Ziv 和 Naftali Tishby. [“通过信息揭开深度神经网络的黑匣子。”](https://arxiv.org/pdf/1703.00810.pdf)
    arXiv预印本 arXiv:1703.00810，2017。'
