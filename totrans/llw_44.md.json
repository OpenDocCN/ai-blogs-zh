["```py\nimport urllib2 from datetime import datetime BASE_URL = \"https://www.google.com/finance/historical?\"  \"output=csv&q={0}&startdate=Jan+1%2C+1980&enddate={1}\" symbol_url = BASE_URL.format(  urllib2.quote('GOOG'), # Replace with any stock you are interested. urllib2.quote(datetime.now().strftime(\"%b+%d,+%Y\"), '+') ) \n```", "```py\ntry:  f = urllib2.urlopen(symbol_url) with open(\"GOOG.csv\", 'w') as fin: print >> fin, f.read() except urllib2.HTTPError:  print \"Fetching Failed: {}\".format(symbol_url) \n```", "```py\nclass RNNConfig():  # ... old ones embedding_size = 3 stock_count = 50 \n```", "```py\n# Mapped to an integer. one label refers to one stock symbol. stock_labels = tf.placeholder(tf.int32, [None, 1]) \n```", "```py\n# NOTE: config = RNNConfig() and it defines hyperparameters. # Convert the integer labels to numeric embedding vectors. embedding_matrix = tf.Variable(  tf.random_uniform([config.stock_count, config.embedding_size], -1.0, 1.0) ) \n```", "```py\nstacked_stock_labels = tf.tile(stock_labels, multiples=[1, config.num_steps]) \n```", "```py\n# stock_label_embeds.get_shape() = (?, num_steps, embedding_size). stock_label_embeds = tf.nn.embedding_lookup(embedding_matrix, stacked_stock_labels) \n```", "```py\n# inputs.get_shape() = (?, num_steps, input_size) # stock_label_embeds.get_shape() = (?, num_steps, embedding_size) # inputs_with_embeds.get_shape() = (?, num_steps, input_size + embedding_size) inputs_with_embeds = tf.concat([inputs, stock_label_embeds], axis=2) \n```", "```py\nfrom sklearn.preprocessing import LabelEncoder label_encoder = LabelEncoder() label_encoder.fit(list_of_symbols) \n```", "```py\nimport csv embedding_metadata_path = os.path.join(your_log_file_folder, 'metadata.csv') with open(embedding_metadata_path, 'w') as fout:  csv_writer = csv.writer(fout) # write the content into the csv file. # for example, csv_writer.writerows([\"GOOG\", \"information_technology\"]) \n```", "```py\nfrom tensorflow.contrib.tensorboard.plugins import projector with tf.Session(graph=lstm_graph) as sess:  summary_writer = tf.summary.FileWriter(your_log_file_folder) summary_writer.add_graph(sess.graph) \n```", "```py\n projector_config = projector.ProjectorConfig() # You can add multiple embeddings. Here we add only one. added_embedding = projector_config.embeddings.add() added_embedding.tensor_name = embedding_matrix.name # Link this tensor to its metadata file. added_embedding.metadata_path = embedding_metadata_path \n```", "```py\n projector.visualize_embeddings(summary_writer, projector_config) \n```", "```py\npython main.py --stock_count=50 --embed_size=3 --input_size=3 --max_epoch=50 --train \n```", "```py\nstock_count = 100\ninput_size = 3\nembed_size = 3\nnum_steps = 30\nlstm_size = 256\nnum_layers = 1\nmax_epoch = 50\nkeep_prob = 0.8\nbatch_size = 64\ninit_learning_rate = 0.05\nlearning_rate_decay = 0.99\ninit_epoch = 5 \n```"]