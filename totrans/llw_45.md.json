["```py\nseq = [np.array(seq[i * self.input_size: (i + 1) * self.input_size])  for i in range(len(seq) // self.input_size)]   # Split into groups of `num_steps` X = np.array([seq[i: i + self.num_steps] for i in range(len(seq) - self.num_steps)]) y = np.array([seq[i + self.num_steps] for i in range(len(seq) - self.num_steps)]) \n```", "```py\n# Configuration is wrapped in one object for easy tracking and passing. class RNNConfig():  input_size=1 num_steps=30 lstm_size=128 num_layers=1 keep_prob=0.8 batch_size = 64 init_learning_rate = 0.001 learning_rate_decay = 0.99 init_epoch = 5 max_epoch = 50   config = RNNConfig() \n```", "```py\nimport tensorflow as tf tf.reset_default_graph() lstm_graph = tf.Graph() \n```", "```py\nwith lstm_graph.as_default(): \n```", "```py\n # Dimension = ( #     number of data examples, #     number of input in one computation step, #     number of numbers in one input # ) # We don't know the number of examples beforehand, so it is None. inputs = tf.placeholder(tf.float32, [None, config.num_steps, config.input_size]) targets = tf.placeholder(tf.float32, [None, config.input_size]) learning_rate = tf.placeholder(tf.float32, None) \n```", "```py\n def _create_one_cell(): return tf.contrib.rnn.LSTMCell(config.lstm_size, state_is_tuple=True) if config.keep_prob < 1.0: return tf.contrib.rnn.DropoutWrapper(lstm_cell, output_keep_prob=config.keep_prob) \n```", "```py\n cell = tf.contrib.rnn.MultiRNNCell( [_create_one_cell() for _ in range(config.num_layers)], state_is_tuple=True ) if config.num_layers > 1 else _create_one_cell() \n```", "```py\n val, _ = tf.nn.dynamic_rnn(cell, inputs, dtype=tf.float32) \n```", "```py\n # Before transpose, val.get_shape() = (batch_size, num_steps, lstm_size) # After transpose, val.get_shape() = (num_steps, batch_size, lstm_size) val = tf.transpose(val, [1, 0, 2]) # last.get_shape() = (batch_size, lstm_size) last = tf.gather(val, int(val.get_shape()[0]) - 1, name=\"last_lstm_output\") \n```", "```py\n weight = tf.Variable(tf.truncated_normal([config.lstm_size, config.input_size])) bias = tf.Variable(tf.constant(0.1, shape=[config.input_size])) prediction = tf.matmul(last, weight) + bias \n```", "```py\n loss = tf.reduce_mean(tf.square(prediction - targets)) optimizer = tf.train.RMSPropOptimizer(learning_rate) minimize = optimizer.minimize(loss) \n```", "```py\nwith tf.Session(graph=lstm_graph) as sess: \n```", "```py\n tf.global_variables_initializer().run() \n```", "```py\nlearning_rates_to_use = [  config.init_learning_rate * ( config.learning_rate_decay ** max(float(i + 1 - config.init_epoch), 0.0) ) for i in range(config.max_epoch)] \n```", "```py\n for epoch_step in range(config.max_epoch): current_lr = learning_rates_to_use[epoch_step]  # Check https://github.com/lilianweng/stock-rnn/blob/master/data_wrapper.py # if you are curious to know what is StockDataSet and how generate_one_epoch() # is implemented. for batch_X, batch_y in stock_dataset.generate_one_epoch(config.batch_size): train_data_feed = { inputs: batch_X, targets: batch_y, learning_rate: current_lr } train_loss, _ = sess.run([loss, minimize], train_data_feed) \n```", "```py\n saver = tf.train.Saver() saver.save(sess, \"your_awesome_model_path_and_name\", global_step=max_epoch_step) \n```", "```py\nwith tf.Session(graph=lstm_graph) as sess:  merged_summary = tf.summary.merge_all() writer = tf.summary.FileWriter(\"location_for_keeping_your_log_files\", sess.graph) writer.add_graph(sess.graph) \n```", "```py\n_summary = sess.run([merged_summary], test_data_feed) writer.add_summary(_summary, global_step=epoch_step)  # epoch_step in range(config.max_epoch) \n```", "```py\nnum_layers=1 keep_prob=0.8 batch_size = 64 init_learning_rate = 0.001 learning_rate_decay = 0.99 init_epoch = 5 max_epoch = 100 num_steps=30 \n```", "```py\npython main.py --stock_symbol=SP500 --train --input_size=1 --lstm_size=128 \n```"]