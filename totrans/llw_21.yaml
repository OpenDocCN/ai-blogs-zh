- en: Curriculum for Reinforcement Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习课程
- en: 原文：[https://lilianweng.github.io/posts/2020-01-29-curriculum-rl/](https://lilianweng.github.io/posts/2020-01-29-curriculum-rl/)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://lilianweng.github.io/posts/2020-01-29-curriculum-rl/](https://lilianweng.github.io/posts/2020-01-29-curriculum-rl/)
- en: '[Updated on 2020-02-03: mentioning [PCG](#pcg) in the “Task-Specific Curriculum”
    section.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[更新于2020-02-03：在“任务特定课程”部分提到[PCG](#pcg)。'
- en: '[Updated on 2020-02-04: Add a new [“curriculum through distillation”](#curriculum-through-distillation)
    section.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[更新于2020-02-04：添加一个新的[“通过蒸馏进行课程”](#curriculum-through-distillation)部分。'
- en: It sounds like an impossible task if we want to teach integral or derivative
    to a 3-year-old who does not even know basic arithmetics. That’s why education
    is important, as it provides a systematic way to break down complex knowledge
    and a nice curriculum for teaching concepts from simple to hard. A curriculum
    makes learning difficult things easier and approachable for us humans. But, how
    about machine learning models? Can we train our models more efficiently with a
    curriculum? Can we design a curriculum to speed up learning?
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要教一个连基本算术都不懂的3岁孩子积分或导数，那听起来像是一项不可能的任务。这就是为什么教育很重要，因为它提供了一个系统的方式来分解复杂知识，并为从简单到困难的概念教学提供了一个良好的课程。课程使学习困难的事物变得更容易和更易接近我们人类。但是，机器学习模型呢？我们能否通过课程更有效地训练我们的模型？我们能否设计一个课程来加快学习速度？
- en: 'Back in 1993, Jeffrey Elman has proposed the idea of training neural networks
    with a curriculum. His early work on learning simple language grammar demonstrated
    the importance of such a strategy: starting with a restricted set of simple data
    and gradually increasing the complexity of training samples; otherwise the model
    was not able to learn at all.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 1993年，杰弗里·埃尔曼提出了用课程来训练神经网络的想法。他在学习简单语法的早期工作中展示了这种策略的重要性：从一组简单的数据开始，逐渐增加训练样本的复杂性；否则模型根本无法学习。
- en: Compared to training without a curriculum, we would expect the adoption of the
    curriculum to expedite the speed of convergence and may or may not improve the
    final model performance. To design an efficient and effective curriculum is not
    easy. Keep in mind that, a bad curriculum may even hamper learning.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 与没有课程的训练相比，我们预期采用课程会加快收敛速度，可能会或可能不会改善最终模型性能。设计一个高效和有效的课程并不容易。请记住，一个糟糕的课程甚至可能阻碍学习。
- en: Next, we will look into several categories of curriculum learning, as illustrated
    in Fig. 1\. Most cases are applied to Reinforcement Learning, with a few exceptions
    on Supervised Learning.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探讨几种课程学习的类别，如图1所示。大多数情况适用于强化学习，少数情况适用于监督学习。
- en: '![](../Images/596013d8c1c10d0fb0818e6a32713b00.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/596013d8c1c10d0fb0818e6a32713b00.png)'
- en: Fig. 1\. Five types of curriculum for reinforcement learning.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 图1。强化学习的五种课程类型。
- en: 'In “The importance of starting small” paper ([Elman 1993](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.128.4487&rep=rep1&type=pdf)),
    I especially like the starting sentences and find them both inspiring and affecting:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在“从小事开始的重要性”论文中（[Elman 1993](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.128.4487&rep=rep1&type=pdf)），我特别喜欢开头的句子，觉得既鼓舞人心又触动人心：
- en: “Humans differ from other species along many dimensions, but two are particularly
    noteworthy. Humans display an exceptional capacity to learn; and humans are remarkable
    for the unusually long time it takes to reach maturity. The adaptive advantage
    of learning is clear, and it may be argued that, through culture, learning has
    created the basis for a non-genetically based transmission of behaviors which
    may accelerate the evolution of our species.”
  id: totrans-11
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “人类在许多方面与其他物种不同，但有两点特别值得注意。人类展示了异常的学习能力；而人类以异常长的时间达到成熟而显著。学习的适应性优势是明显的，可以说，通过文化，学习已经创造了一种非基因传播行为的基础，这可能加速了我们物种的进化。”
- en: Indeed, learning is probably the best superpower we humans have.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，学习可能是我们人类拥有的最好的超能力。
- en: Task-Specific Curriculum
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 任务特定的课程
- en: '[Bengio, et al. (2009)](https://www.researchgate.net/profile/Y_Bengio/publication/221344862_Curriculum_learning/links/546cd2570cf2193b94c577ac/Curriculum-learning.pdf)
    provided a good overview of curriculum learning in the old days. The paper presented
    two ideas with toy experiments using a manually designed task-specific curriculum:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[Bengio等人（2009）](https://www.researchgate.net/profile/Y_Bengio/publication/221344862_Curriculum_learning/links/546cd2570cf2193b94c577ac/Curriculum-learning.pdf)在旧时代提供了课程学习的良好概述。该论文通过使用手动设计的任务特定课程进行玩具实验，提出了两个想法：'
- en: Cleaner Examples may yield better generalization faster.
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更干净的例子可能更快地产生更好的泛化。
- en: Introducing gradually more difficult examples speeds up online training.
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 逐渐引入更难的例子可以加快在线训练速度。
- en: 'It is plausible that some curriculum strategies could be useless or even harmful.
    A good question to answer in the field is: *What could be the general principles
    that make some curriculum strategies work better than others?* The Bengio 2009
    paper hypothesized it would be beneficial to make learning focus on “interesting”
    examples that are neither too hard or too easy.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 有可能一些课程策略是无用的甚至有害的。在这个领域要回答的一个好问题是：*是什么一般原则使一些课程策略比其他更有效？* Bengio 2009 年的论文假设，让学习集中在既不太难也不太容易的“有趣”例子上可能是有益的。
- en: If our naive curriculum is to train the model on samples with a gradually increasing
    level of complexity, we need a way to quantify the difficulty of a task first.
    One idea is to use its minimal loss with respect to another model while this model
    is pretrained on other tasks ([Weinshall, et al. 2018](https://arxiv.org/abs/1802.03796)).
    In this way, the knowledge of the pretrained model can be transferred to the new
    model by suggesting a rank of training samples. Fig. 2 shows the effectiveness
    of the `curriculum` group (green), compared to `control` (random order; yellow)
    and `anti` (reverse the order; red) groups.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们朴素的课程是训练模型逐渐增加复杂性的样本，我们需要一种方法来首先量化任务的难度。一个想法是使用其相对于另一个模型的最小损失，而这个模型是在其他任务上预训练的（[Weinshall,
    et al. 2018](https://arxiv.org/abs/1802.03796)）。通过这种方式，预训练模型的知识可以通过建议训练样本的排名传递给新模型。图
    2 显示了`curriculum`组（绿色）的有效性，与`control`（随机顺序；黄色）和`anti`（反向顺序；红色）组相比。
- en: '![](../Images/45a0b852ba0ba7308a35abd9dcbd54e5.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/45a0b852ba0ba7308a35abd9dcbd54e5.png)'
- en: 'Fig. 2\. Image classification accuracy on test image set (5 member classes
    of "small mammals" in CIFAR100). There are 4 experimental groups, (a) `curriculum`:
    sort the labels by the confidence of another trained classifier (e.g. the margin
    of an SVM); (b) `control-curriculum`: sort the labels randomly; (c) `anti-curriculum`:
    sort the labels reversely; (d) `None`: no curriculum. (Image source: [Weinshall,
    et al. 2018](https://arxiv.org/abs/1802.03796))'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2\. 测试图像集上的图像分类准确性（CIFAR100 中“小型哺乳动物”的 5 个成员类别）。有 4 个实验组，(a) `curriculum`：按另一个训练好的分类器的置信度（例如
    SVM 的边缘）对标签进行排序；(b) `control-curriculum`：随机排序标签；(c) `anti-curriculum`：反向排序标签；(d)
    `None`：无课程。（图片来源：[Weinshall, et al. 2018](https://arxiv.org/abs/1802.03796)）
- en: '[Zaremba & Sutskever (2014)](https://arxiv.org/abs/1410.4615) did an interesting
    experiment on training LSTM to predict the output of a short Python program for
    mathematical ops without actually executing the code. They found curriculum is
    necessary for learning. The program’s complexity is controlled by two parameters,
    `length` ∈ [1, a] and `nesting`∈ [1, b]. Three strategies are considered:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '[Zaremba & Sutskever (2014)](https://arxiv.org/abs/1410.4615) 对训练 LSTM 来预测数学运算的短
    Python 程序输出进行了有趣的实验，而不实际执行代码。他们发现课程对学习是必要的。程序的复杂性由两个参数控制，`length` ∈ [1, a] 和 `nesting`∈
    [1, b]。考虑了三种策略：'
- en: 'Naive curriculum: increase `length` first until reaching `a`; then increase
    `nesting` and reset `length` to 1; repeat this process until both reach maximum.'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 朴素课程：首先增加`length`直到达到`a`；然后增加`nesting`并将`length`重置为1；重复此过程直到两者都达到最大值。
- en: 'Mix curriculum: sample `length` ~ [1, a] and `nesting` ~ [1, b]'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 混合课程：样本`length` ~ [1, a] 和 `nesting` ~ [1, b]
- en: 'Combined: naive + mix.'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 组合：朴素 + 混合。
- en: They noticed that combined strategy always outperformed the naive curriculum
    and would generally (but not always) outperform the mix strategy — indicating
    that it is quite important to mix in easy tasks during training to *avoid forgetting*.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 他们注意到，组合策略总是优于朴素课程，并且通常（但不总是）优于混合策略 — 表明在训练过程中混合易任务是非常重要的，以*避免遗忘*。
- en: Procedural content generation ([PCG](https://en.wikipedia.org/wiki/Procedural_generation))
    is a popular approach for creating video games of various levels of difficulty.
    PCG involves algorithmic randomness and a heavy dose of human expertise in designing
    game elements and dependencies among them. Procedurally generated levels have
    been introduced into several benchmark environments for evaluating whether an
    RL agent can generalize to a new level that it is not trained on ([meta-RL](https://lilianweng.github.io/posts/2019-06-23-meta-rl/)!),
    such as [GVGAI](http://www.gvgai.net/), OpenAI [CoinRun](https://openai.com/blog/quantifying-generalization-in-reinforcement-learning/)
    and [Procgen benchmark](https://openai.com/blog/procgen-benchmark/). Using GVGAI,
    [Justesen, et al. (2018)](https://arxiv.org/abs/1806.10729) demonstrated that
    an RL policy can easily overfit to a specific game but training over a simple
    curriculum that grows the task difficulty together with the model performance
    helps its generalization to new human-designed levels. Similar results are also
    found in CoinRun ([Cobbe, et al. 2018](https://arxiv.org/abs/1812.02341)). POET
    ([Wang et al, 2019](https://arxiv.org/abs/1901.01753)) is another example for
    leveraging evolutionary algorithm and procedural generated game levels to improve
    RL generalization, which I’ve described in details in my [meta-RL post](https://lilianweng.github.io/posts/2019-06-23-meta-rl/#evolutionary-algorithm-on-environment-generation).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 过程内容生成（[PCG](https://en.wikipedia.org/wiki/Procedural_generation)）是创建各种难度视频游戏的流行方法。
    PCG涉及算法随机性和大量人类专业知识在设计游戏元素和它们之间的依赖关系。程序生成的关卡已经被引入几个基准环境中，用于评估RL代理是否能够推广到未经训练的新关卡（[元RL](https://lilianweng.github.io/posts/2019-06-23-meta-rl/)！），如[GVGAI](http://www.gvgai.net/)，OpenAI的[CoinRun](https://openai.com/blog/quantifying-generalization-in-reinforcement-learning/)和[Procgen
    benchmark](https://openai.com/blog/procgen-benchmark/)。使用GVGAI，[Justesen等人（2018）](https://arxiv.org/abs/1806.10729)证明了RL策略可以轻松过拟合到特定游戏，但通过训练一个简单的课程，使任务难度与模型性能一起增加，有助于其推广到新的人类设计关卡。CoinRun中也发现了类似的结果（[Cobbe等人，2018](https://arxiv.org/abs/1812.02341)）。POET（[Wang等人，2019](https://arxiv.org/abs/1901.01753)）是利用进化算法和程序生成的游戏关卡来改善RL泛化的另一个例子，我在我的[元RL文章](https://lilianweng.github.io/posts/2019-06-23-meta-rl/#evolutionary-algorithm-on-environment-generation)中详细描述了这一点。
- en: 'To follow the curriculum learning approaches described above, generally we
    need to figure out two problems in the training procedure:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 要遵循上述描述的课程学习方法，通常我们需要解决培训过程中的两个问题：
- en: Design a metric to quantify how hard a task is so that we can sort tasks accordingly.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设计一个度量标准来量化任务的难度，以便我们可以根据任务进行排序。
- en: Provide a sequence of tasks with an increasing level of difficulty to the model
    during training.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练过程中向模型提供一系列难度逐渐增加的任务。
- en: However, the order of tasks does not have to be sequential. In our Rubik’s cube
    paper ([OpenAI et al, 2019](https://arxiv.org/abs/1910.07113.)), we depended on
    *Automatic domain randomization* (**ADR**) to generate a curriculum by growing
    a distribution of environments with increasing complexity. The difficulty of each
    task (i.e. solving a Rubik’s cube in a set of environments) depends on the randomization
    ranges of various environmental parameters. Even with a simplified assumption
    that all the environmental parameters are uncorrelated, we were able to create
    a decent curriculum for our robot hand to learn the task.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，任务的顺序不一定要是顺序的。在我们的魔方论文（[OpenAI等人，2019](https://arxiv.org/abs/1910.07113)）中，我们依赖*自动领域随机化*（**ADR**）通过增加复杂性生成一系列环境的分布来生成课程。每个任务的难度（即在一组环境中解决魔方）取决于各种环境参数的随机化范围。即使假设所有环境参数都不相关，我们也能为我们的机器手创建一个不错的课程来学习任务。
- en: Teacher-Guided Curriculum
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 教师引导的课程
- en: The idea of *Automatic Curriculum Learning* was proposed by [Graves, et al.
    2017](https://arxiv.org/abs/1704.03003) slightly earlier. It considers a $N$-task
    curriculum as an [$N$-armed bandit](https://lilianweng.github.io/posts/2018-01-23-multi-armed-bandit/)
    problem and an adaptive policy which learns to optimize the returns from this
    bandit.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '*自动课程学习*的概念是由[Graves等人，2017](https://arxiv.org/abs/1704.03003)稍早提出的。它将$N$任务课程视为一个[$N$臂老虎机](https://lilianweng.github.io/posts/2018-01-23-multi-armed-bandit/)问题，并且一个自适应策略学习如何优化这个老虎机的回报。'
- en: 'Two categories of learning signals have been considered in the paper:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 论文中考虑了两类学习信号：
- en: 'Loss-driven progress: the loss function change before and after one gradient
    update. This type of reward signals tracks the speed of the learning process,
    because the greatest task loss decrease is equivalent to the fastest learning.'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 损失驱动的进展：梯度更新前后的损失函数变化。这种奖励信号跟踪学习过程的速度，因为最大的任务损失减少等同于最快的学习。
- en: 'Complex-driven progress: the KL divergence between posterior and prior distribution
    over network weights. This type of learning signals are inspired by the [MDL](https://en.wikipedia.org/wiki/Minimum_description_length)
    principle, “increasing the model complexity by a certain amount is only worthwhile
    if it compresses the data by a greater amount”. The model complexity is therefore
    expected to increase most in response to the model nicely generalizing to training
    examples.'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 复杂驱动的进展：后验分布和先验分布之间的KL散度。这种学习信号的灵感来自于[MDL](https://en.wikipedia.org/wiki/Minimum_description_length)原则，“通过一定量增加模型复杂性只有在它能够以更大量压缩数据时才值得”。因此，模型复杂性预计会在模型很好地泛化到训练示例时增加最多。
- en: This framework of proposing curriculum automatically through another RL agent
    was formalized as *Teacher-Student Curriculum Learning* (**TSCL**; [Matiisen,
    et al. 2017](https://arxiv.org/abs/1707.00183)). In TSCL, a *student* is an RL
    agent working on actual tasks while a *teacher* agent is a policy for selecting
    tasks. The student aims to master a complex task that might be hard to learn directly.
    To make this task easier to learn, we set up the teacher agent to guide the student’s
    training process by picking proper sub-tasks.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 通过另一个RL代理自动提出课程的框架被正式化为*教师-学生课程学习*（**TSCL**；[Matiisen, et al. 2017](https://arxiv.org/abs/1707.00183)）。在TSCL中，*学生*是一个在实际任务上工作的RL代理，而*教师*代理是选择任务的策略。学生的目标是掌握一个可能直接学习困难的复杂任务。为了使这个任务更容易学习，我们设置教师代理来引导学生的训练过程，选择适当的子任务。
- en: '![](../Images/610c37fc84e422f14d069303c977b685.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/610c37fc84e422f14d069303c977b685.png)'
- en: 'Fig. 3\. The setup of teacher-student curriculum learning. (Image source: [Matiisen,
    et al. 2017](https://arxiv.org/abs/1707.00183) + my annotation in red.)'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图3。教师-学生课程学习的设置。（图片来源：[Matiisen, et al. 2017](https://arxiv.org/abs/1707.00183)
    + 我的红色注释。）
- en: 'In the process, the student should learn tasks which:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个过程中，学生应该学习以下任务：
- en: can help the student make fastest learning progress, or
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可以帮助学生取得最快的学习进展，或
- en: are at risk of being forgotten.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有被遗忘的风险。
- en: 'Note: The setup of framing the teacher model as an RL problem feels quite similar
    to Neural Architecture Search (NAS), but differently the RL model in TSCL operates
    on the task space and NAS operates on the main model architecture space.'
  id: totrans-42
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 注意：将教师模型设置为RL问题的框架与神经架构搜索（NAS）非常相似，但TSCL中的RL模型在任务空间上运行，而NAS在主模型架构空间上运行。
- en: 'Training the teacher model is to solve a [POMDP](https://en.wikipedia.org/wiki/Partially_observable_Markov_decision_process)
    problem:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 训练教师模型是为了解决[POMDP](https://en.wikipedia.org/wiki/Partially_observable_Markov_decision_process)问题：
- en: The unobserved $s_t$ is the full state of the student model.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 未观察到的$s_t$是学生模型的完整状态。
- en: The observed $o = (x_t^{(1)}, \dots, x_t^{(N)})$ are a list of scores for $N$
    tasks.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 观察到的$o = (x_t^{(1)}, \dots, x_t^{(N)})$是$N$个任务的分数列表。
- en: The action $a$ is to pick on subtask.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 动作$a$是选择子任务。
- en: The reward per step is the score delta.$r_t = \sum_{i=1}^N x_t^{(i)} - x_{t-1}^{(i)}$
    (i.e., equivalent to maximizing the score of all tasks at the end of the episode).
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每步的奖励是分数差值。$r_t = \sum_{i=1}^N x_t^{(i)} - x_{t-1}^{(i)}$（即，等同于在剧集结束时最大化所有任务的分数）。
- en: The method of estimating learning progress from noisy task scores while balancing
    exploration vs exploitation can be borrowed from the non-stationary multi-armed
    bandit problem — use [ε-greedy](https://lilianweng.github.io/posts/2018-01-23-multi-armed-bandit/#%CE%B5-greedy-algorithm),
    or [Thompson sampling](https://lilianweng.github.io/posts/2018-01-23-multi-armed-bandit/#thompson-sampling).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 从嘈杂的任务分数中估计学习进度的方法，同时平衡探索与开发，可以从非稳态多臂老虎机问题中借鉴——使用[ε-greedy](https://lilianweng.github.io/posts/2018-01-23-multi-armed-bandit/#%CE%B5-greedy-algorithm)，或[汤普森抽样](https://lilianweng.github.io/posts/2018-01-23-multi-armed-bandit/#thompson-sampling)。
- en: The core idea, in summary, is to use one policy to propose tasks for another
    policy to learn better. Interestingly, both works above (in the discrete task
    space) found that uniformly sampling from all tasks is a surprisingly strong benchmark.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 核心思想，总结起来，是使用一个策略为另一个策略提出更好的任务。有趣的是，以上两项工作（在离散任务空间中）发现均匀地从所有任务中抽样是一个令人惊讶地强大的基准。
- en: 'What if the task space is continuous? [Portelas, et al. (2019)](https://arxiv.org/abs/1910.07224)
    studied a continuous teacher-student framework, where the teacher has to sample
    parameters from continuous task space to generate a learning curriculum. Given
    a newly sampled parameter $p$, the absolute learning progress (short for ALP)
    is measured as $\text{ALP}_p = \vert r - r_\text{old} \vert$, where $r$ is the
    episodic reward associated with $p$ and $r_\text{old}$ is the reward associated
    with $p_\text{old}$. Here, $p_\text{old}$ is a previous sampled parameter closest
    to $p$ in the task space, which can be retrieved by nearest neighbor. Note that
    how this ALP score is different from learning signals in [TSCL](#TSCL) or [Grave,
    et al. 2017](#grave-et-al-2017) above: ALP score measures the reward difference
    between two tasks rather than performance at two time steps of the same task.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 如果任务空间是连续的会怎样？[Portelas, et al. (2019)](https://arxiv.org/abs/1910.07224)研究了一个连续的师生框架，其中老师必须从连续的任务空间中采样参数以生成学习课程。给定一个新采样的参数$p$，绝对学习进度（简称ALP）被测量为$\text{ALP}_p
    = \vert r - r_\text{old} \vert$，其中$r$是与$p$相关的一个周期性奖励，$r_\text{old}$是与$p_\text{old}$相关的奖励。这里，$p_\text{old}$是任务空间中最接近$p$的先前采样参数，可以通过最近邻检索。请注意，这个ALP分数如何与[TSCL](#TSCL)或[Grave,
    et al. 2017](#grave-et-al-2017)中的学习信号不同：ALP分数衡量了两个任务之间的奖励差异，而不是同一任务两个时间步的表现。
- en: 'On top of the task parameter space, a Gaussian mixture model is trained to
    fit the distribution of $\text{ALP}_p$ over $p$. ε-greedy is used when sampling
    the tasks: with some probability, sampling a random task; otherwise sampling proportionally
    to ALP score from the GMM model.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在任务参数空间的顶部，训练一个高斯混合模型来拟合$\text{ALP}_p$在$p$上的分布。在采样任务时使用ε-greedy策略：以一定概率随机采样一个任务；否则根据GMM模型中的ALP分数比例采样。
- en: '![](../Images/909382040c59b57d9379dd790a258e6f.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/909382040c59b57d9379dd790a258e6f.png)'
- en: 'Fig. 4\. The algorithm of ALP-GMM (absolute learning progress Gaussian mixture
    model). (Image source: [Portelas, et al., 2019](https://arxiv.org/abs/1910.07224))'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图4\. ALP-GMM（绝对学习进度高斯混合模型）的算法。（图片来源：[Portelas, et al., 2019](https://arxiv.org/abs/1910.07224))
- en: Curriculum through Self-Play
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过自我对弈进行课程设置
- en: Different from the teacher-student framework, two agents are doing very different
    things. The teacher learns to pick a task for the student without any knowledge
    of the actual task content. What if we want to make both train on the main task
    directly? How about even make them compete with each other?
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 与师生框架不同，两个代理在做非常不同的事情。老师学习为学生挑选任务，而不知道实际任务内容。如果我们想让两者直接在主要任务上训练呢？甚至让他们互相竞争呢？
- en: '[Sukhbaatar, et al. (2017)](https://arxiv.org/abs/1703.05407) proposed a framework
    for automatic curriculum learning through **asymmetric self-play**. Two agents,
    Alice and Bob, play the same task with different goals: Alice challenges Bob to
    achieve the same state and Bob attempts to complete it as fast as he can.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '[Sukhbaatar, et al. (2017)](https://arxiv.org/abs/1703.05407)提出了一个通过**非对称自我对弈**进行自动课程学习的框架。两个代理，Alice和Bob，玩同一个任务但有不同的目标：Alice挑战Bob达到相同状态，而Bob则尽可能快地完成。'
- en: '![](../Images/c44693f06fb2a18f7c3faac11cac7943.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c44693f06fb2a18f7c3faac11cac7943.png)'
- en: 'Fig. 5\. Illustration of the self-play setup when training two agents. The
    example task is [MazeBase](https://github.com/facebook/MazeBase): An agent is
    asked to reach a goal flag in a maze with a light switch, a key and a wall with
    a door. Toggling the key switch can open or close the door and Turning off the
    light makes only the glowing light switch available to the agent. (Image source:
    [Sukhbaatar, et al. 2017](https://arxiv.org/abs/1703.05407))'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图5\. 展示了训练两个代理时的自我对弈设置。示例任务是[MazeBase](https://github.com/facebook/MazeBase)：一个代理被要求在一个迷宫中达到一个带有灯开关、钥匙和门的墙的目标旗帜。切换钥匙开关可以打开或关闭门，关闭灯只能让代理使用发光的开关。（图片来源：[Sukhbaatar,
    et al. 2017](https://arxiv.org/abs/1703.05407)）
- en: 'Let us consider Alice and Bob as two separate copies for one RL agent trained
    in the same environment but with different brains. Each of them has independent
    parameters and loss objective. The self-play-driven training consists of two types
    of episodes:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将Alice和Bob视为同一RL代理的两个独立副本，在相同环境中训练，但具有不同的大脑。他们各自具有独立的参数和损失目标。自我对弈驱动的训练包括两种类型的情节：
- en: In the *self-play episode*, Alice alters the state from $s_0$ to $s_t$ and then
    Bob is asked to return the environment to its original state $s_0$ to get an internal
    reward.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在*自我对弈的情节*中，Alice将状态从$s_0$改变到$s_t$，然后Bob被要求将环境恢复到原始状态$s_0$以获得内部奖励。
- en: In the *target task episode*, Bob receives an external reward if he visits the
    target flag.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在*目标任务剧集*中，如果Bob访问目标标志，他将获得外部奖励。
- en: Note that since B has to repeat the actions between the same pair of $(s_0,
    s_t)$ of A, this framework only works in reversible or resettable environments.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，由于B必须在A的相同一对$(s_0, s_t)$之间重复动作，这个框架只适用于可逆或可重置的环境。
- en: 'Alice should learn to push Bob out of his comfort zone, but not give him impossible
    tasks. Bob’s reward is set as $R_B = -\gamma t_B$ and Alice’s reward is $R_A =
    \gamma \max(0, t_B - t_A)$, where $t_B$ is the total time for B to complete the
    task, $t_A$ is the time until Alice performs the STOP action and $\gamma$ is a
    scalar constant to rescale the reward to be comparable with the external task
    reward. If B fails a task, $t_B = t_\max - t_A$. Both policies are goal-conditioned.
    The losses imply:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: Alice应该学会让Bob走出舒适区，但不要给他不可能的任务。Bob的奖励设定为$R_B = -\gamma t_B$，Alice的奖励为$R_A =
    \gamma \max(0, t_B - t_A)$，其中$t_B$是B完成任务的总时间，$t_A$是Alice执行STOP动作的时间，$\gamma$是一个标量常数，用于重新调整奖励，使其可与外部任务奖励进行比较。如果B失败了一个任务，$t_B
    = t_\max - t_A$。两个策略都是目标条件的。损失意味着：
- en: B wants to finish a task asap.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: B希望尽快完成任务。
- en: A prefers tasks that take more time of B.
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: A更喜欢占用B更多时间的任务。
- en: A does not want to take too many steps when B is failing.
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当B失败时，A不想走太多步。
- en: In this way, the interaction between Alice and Bob automatically builds a curriculum
    of increasingly challenging tasks. Meanwhile, as A has done the task herself before
    proposing the task to B, the task is guaranteed to be solvable.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方式，Alice和Bob之间的互动自动构建了一系列越来越具挑战性任务的课程。同时，由于A在向B提出任务之前已经完成了任务，所以可以保证任务是可解的。
- en: The paradigm of A suggesting tasks and then B solving them does sound similar
    to the Teacher-Student framework. However, in asymmetric self-play, Alice, who
    plays a teacher role, also works on the same task to find challenging cases for
    Bob, rather than optimizes B’s learning process explicitly.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: A建议任务，然后B解决任务的范式听起来与师生框架相似。然而，在非对称自我对弈中，扮演老师角色的Alice也在同一个任务上工作，寻找Bob的挑战性案例，而不是明确优化B的学习过程。
- en: Automatic Goal Generation
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自动目标生成
- en: Often RL policy needs to be able to perform over a set of tasks. The goal should
    be carefully chosen so that at every training stage, it would not be too hard
    or too easy for the current policy. A goal $g \in \mathcal{G}$ can be defined
    as a set of states $S^g$ and a goal is considered as achieved whenever an agent
    arrives at any of those states.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 通常RL策略需要能够执行一组任务。目标应该谨慎选择，以便在每个训练阶段，对于当前策略来说既不太难也不太容易。目标$g \in \mathcal{G}$可以定义为一组状态$S^g$，当代理到达这些状态中的任何一个时，目标被视为已实现。
- en: The approach of Generative Goal Learning ([Florensa, et al. 2018](https://arxiv.org/abs/1705.06366))
    relies on a **Goal GAN** to generate desired goals automatically. In their experiment,
    the reward is very sparse, just a binary flag for whether a goal is achieved or
    not and the policy is conditioned on goal,
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 生成目标学习方法（[Florensa, et al. 2018](https://arxiv.org/abs/1705.06366)）的方法依赖于**目标
    GAN**自动生成所需目标。在他们的实验中，奖励非常稀疏，只是一个二进制标志，表示目标是否实现，策略取决于目标，
- en: '$$ \begin{aligned} \pi^{*}(a_t\vert s_t, g) &= \arg\max_\pi \mathbb{E}_{g\sim
    p_g(.)} R^g(\pi) \\ \text{where }R^g(\pi) &= \mathbb{E}_\pi(.\mid s_t, g) \mathbf{1}[\exists
    t \in [1,\dots, T]: s_t \in S^g] \end{aligned} $$'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '$$ \begin{aligned} \pi^{*}(a_t\vert s_t, g) &= \arg\max_\pi \mathbb{E}_{g\sim
    p_g(.)} R^g(\pi) \\ \text{where }R^g(\pi) &= \mathbb{E}_\pi(.\mid s_t, g) \mathbf{1}[\exists
    t \in [1,\dots, T]: s_t \in S^g] \end{aligned} $$'
- en: Here $R^g(\pi)$ is the expected return, also equivalent to the success probability.
    Given sampled trajectories from the current policy, as long as any state belongs
    to the goal set, the return will be positive.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这里$R^g(\pi)$是期望回报，也等同于成功概率。给定当前策略的采样轨迹，只要任何状态属于目标集，回报将为正。
- en: 'Their approach iterates through 3 steps until the policy converges:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 他们的方法通过3个步骤迭代，直到策略收敛：
- en: Label a set of goals based on whether they are at the appropriate level of difficulty
    for the current policy.
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据当前策略是否适当难度，为一组目标打上标签。
- en: The set of goals at the appropriate level of difficulty are named **GOID** (short
    for “Goals of Intermediate Difficulty”).
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 适当难度的目标集被命名为**GOID**（“中等难度目标”的缩写）。
- en: '$\text{GOID}_i := \{g : R_\text{min} \leq R^g(\pi_i) \leq R_\text{max} \} \subseteq
    G$'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '$\text{GOID}_i := \{g : R_\text{min} \leq R^g(\pi_i) \leq R_\text{max} \} \subseteq
    G$'
- en: Here $R_\text{min}$ and $R_\text{max}$ can be interpreted as a minimum and maximum
    probability of reaching a goal over T time-steps.
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这里 $R_\text{min}$ 和 $R_\text{max}$ 可以被解释为在 T 个时间步内达到目标的最小和最大概率。
- en: Train a Goal GAN model using labelled goals from step 1 to produce new goals
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用从步骤 1 中标记的目标训练 Goal GAN 模型以生成新目标。
- en: Use these new goals to train the policy, improving its coverage objective.
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用这些新目标来训练策略，改进其覆盖目标。
- en: 'The Goal GAN generates a curriculum automatically:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: Goal GAN 自动生成课程：
- en: 'Generator $G(z)$: produces a new goal. => expected to be a goal uniformly sampled
    from $GOID$ set.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '生成器 $G(z)$: 产生一个新的目标。 => 预期是从 $GOID$ 集合均匀抽样得到的目标。'
- en: 'Discriminator $D(g)$: evaluates whether a goal can be achieved. => expected
    to tell whether a goal is from $GOID$ set.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '判别器 $D(g)$: 评估一个目标是否可以实现。 => 预期能够判断目标是否来自 $GOID$ 集合。'
- en: 'The Goal GAN is constructed similar to LSGAN (Least-Squared GAN; [Mao et al.,
    (2017)](https://arxiv.org/abs/1611.04076)), which has better stability of learning
    compared to vanilla GAN. According to LSGAN, we should minimize the following
    losses for $D$ and $G$ respectively:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: Goal GAN 的构建类似于 LSGAN（最小二乘 GAN; [Mao 等人，(2017)](https://arxiv.org/abs/1611.04076)），相比于普通
    GAN，具有更好的学习稳定性。根据 LSGAN，我们应分别最小化以下损失函数以用于 $D$ 和 $G$：
- en: $$ \begin{aligned} \mathcal{L}_\text{LSGAN}(D) &= \frac{1}{2} \mathbb{E}_{g
    \sim p_\text{data}(g)} [ (D(g) - b)^2] + \frac{1}{2} \mathbb{E}_{z \sim p_z(z)}
    [ (D(G(z)) - a)^2] \\ \mathcal{L}_\text{LSGAN}(G) &= \frac{1}{2} \mathbb{E}_{z
    \sim p_z(z)} [ (D(G(z)) - c)^2] \end{aligned} $$
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \begin{aligned} \mathcal{L}_\text{LSGAN}(D) &= \frac{1}{2} \mathbb{E}_{g
    \sim p_\text{data}(g)} [ (D(g) - b)^2] + \frac{1}{2} \mathbb{E}_{z \sim p_z(z)}
    [ (D(G(z)) - a)^2] \\ \mathcal{L}_\text{LSGAN}(G) &= \frac{1}{2} \mathbb{E}_{z
    \sim p_z(z)} [ (D(G(z)) - c)^2] \end{aligned} $$
- en: where $a$ is the label for fake data, $b$ for real data, and $c$ is the value
    that $G$ wants $D$ to believe for fake data. In LSGAN paper’s experiments, they
    used $a=-1, b=1, c=0$.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $a$ 是假数据的标签，$b$ 是真实数据的标签，$c$ 是 $G$ 希望 $D$ 相信假数据的值。在 LSGAN 论文的实验中，他们使用了 $a=-1,
    b=1, c=0$。
- en: 'The Goal GAN introduces an extra binary flag $y_b$ indicating whether a goal
    $g$ is real ($y_g = 1$) or fake ($y_g = 0$) so that the model can use negative
    samples for training:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: Goal GAN 引入了额外的二进制标志 $y_b$，指示一个目标 $g$ 是真实的（$y_g = 1$）还是虚假的（$y_g = 0$），以便模型可以使用负样本进行训练：
- en: $$ \begin{aligned} \mathcal{L}_\text{GoalGAN}(D) &= \frac{1}{2} \mathbb{E}_{g
    \sim p_\text{data}(g)} [ (D(g) - b)^2 + (1-y_g) (D(g) - a)^2] + \frac{1}{2} \mathbb{E}_{z
    \sim p_z(z)} [ (D(G(z)) - a)^2] \\ \mathcal{L}_\text{GoalGAN}(G) &= \frac{1}{2}
    \mathbb{E}_{z \sim p_z(z)} [ (D(G(z)) - c)^2] \end{aligned} $$![](../Images/6d4e8e1e646bb9e16e5842b52052d8cf.png)
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \begin{aligned} \mathcal{L}_\text{GoalGAN}(D) &= \frac{1}{2} \mathbb{E}_{g
    \sim p_\text{data}(g)} [ (D(g) - b)^2 + (1-y_g) (D(g) - a)^2] + \frac{1}{2} \mathbb{E}_{z
    \sim p_z(z)} [ (D(G(z)) - a)^2] \\ \mathcal{L}_\text{GoalGAN}(G) &= \frac{1}{2}
    \mathbb{E}_{z \sim p_z(z)} [ (D(G(z)) - c)^2] \end{aligned} $$![](../Images/6d4e8e1e646bb9e16e5842b52052d8cf.png)
- en: 'Fig. 6\. The algorithm of Generative Goal Learning. (Image source: ([Florensa,
    et al. 2018](https://arxiv.org/abs/1705.06366))'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '图 6\. 生成式目标学习算法。 (图片来源: ([Florensa, et al. 2018](https://arxiv.org/abs/1705.06366))'
- en: 'Following the same idea, [Racaniere & Lampinen, et al. (2019)](https://arxiv.org/abs/1909.12892)
    designs a method to make the objectives of goal generator more sophisticated.
    Their method contains three components, same as generative goal learning above:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 遵循相同的思路，[Racaniere & Lampinen, et al. (2019)](https://arxiv.org/abs/1909.12892)
    设计了一种方法，使目标生成器的目标更加复杂。他们的方法包含三个组件，与上述生成式目标学习相同：
- en: '**Solver**/Policy $\pi$: In each episode, the solver gets a goal $g$ at the
    beginning and get a single binary reward $R^g$ at the end.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Solver**/Policy $\pi$: 在每一集中，求解器在开始时获得一个目标 $g$，并在结束时获得单一的二进制奖励 $R^g$。'
- en: '**Judge**/Discriminator $D(.)$: A classifier to predict the binary reward (whether
    goal can be achieved or not); precisely it outputs the logit of a probability
    of achieving the given goal, $\sigma(D(g)) = p(R^g=1\vert g)$, where $\sigma$
    is the sigmoid function.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Judge**/Discriminator $D(.)$: 用于预测二进制奖励的分类器（目标是否可以实现）；准确地输出给定目标实现概率的对数几率，$\sigma(D(g))
    = p(R^g=1\vert g)$，其中 $\sigma$ 是 Sigmoid 函数。'
- en: '**Setter**/Generator $G(.)$: The goal setter takes as input a desired feasibility
    score $f \in \text{Unif}(0, 1)$ and generates $g = G(z, f)$, where the latent
    variable $z$ is sampled by $z \sim \mathcal{N}(0, I)$. The goal generator is designed
    to reversible, so $G^{-1}$ can map backwards from a goal $g$ to a latent $z =
    G^{-1}(g, f)$'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Setter**/Generator $G(.)$: 目标设定者接受期望的可行性得分 $f \in \text{Unif}(0, 1)$ 作为输入，并生成
    $g = G(z, f)$，其中潜变量 $z$ 是由 $z \sim \mathcal{N}(0, I)$ 抽样得到的。目标生成器被设计为可逆的，因此 $G^{-1}$
    可以从目标 $g$ 逆向映射到潜变量 $z = G^{-1}(g, f)$'
- en: 'The generator is optimized with three objectives:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器被优化为三个目标：
- en: 'Goal **validity**: The proposed goal should be achievable by an expert policy.
    The corresponding generative loss is designed to increase the likelihood of generating
    goals that the solver policy has achieved before (like in [HER](https://arxiv.org/abs/1707.01495)).'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 目标**有效性**：提出的目标应该可以由专家策略实现。相应的生成损失旨在增加生成已被解算器策略实现过的目标的可能性（就像在[HER](https://arxiv.org/abs/1707.01495)中一样）。
- en: $\mathcal{L}_\text{val}$ is the negative log-likelihood of generated goals that
    have been solved by the solver in the past.
  id: totrans-96
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\mathcal{L}_\text{val}$ 是过去已被解决的生成目标的负对数似然。
- en: $$ \begin{align*} \mathcal{L}_\text{val} = \mathbb{E}_{\substack{ g \sim \text{
    achieved by solver}, \\ \xi \in \text{Uniform}(0, \delta), \\ f \in \text{Uniform}(0,
    1) }} \big[ -\log p(G^{-1}(g + \xi, f)) \big] \end{align*} $$
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \begin{align*} \mathcal{L}_\text{val} = \mathbb{E}_{\substack{ g \sim \text{
    achieved by solver}, \\ \xi \in \text{Uniform}(0, \delta), \\ f \in \text{Uniform}(0,
    1) }} \big[ -\log p(G^{-1}(g + \xi, f)) \big] \end{align*} $$
- en: 'Goal **feasibility**: The proposed goal should be achievable by the current
    policy; that is, the level of difficulty should be appropriate.'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 目标**可行性**：提出的目标应该可以由当前策略实现；即，难度水平应该适当。
- en: $\mathcal{L}_\text{feas}$ is the output probability by the judge model $D$ on
    the generated goal $G(z, f)$ should match the desired $f$.
  id: totrans-99
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\mathcal{L}_\text{feas}$ 是评判模型 $D$ 对生成目标 $G(z, f)$ 的输出概率应该与期望的 $f$ 匹配。
- en: $$ \begin{align*} \mathcal{L}_\text{feas} = \mathbb{E}_{\substack{ z \in \mathcal{N}(0,
    1), \\ f \in \text{Uniform}(0, 1) }} \big[ D(G(z, f)) - \sigma^{-1}(f)^2 \big]
    \end{align*} $$
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \begin{align*} \mathcal{L}_\text{feas} = \mathbb{E}_{\substack{ z \in \mathcal{N}(0,
    1), \\ f \in \text{Uniform}(0, 1) }} \big[ D(G(z, f)) - \sigma^{-1}(f)^2 \big]
    \end{align*} $$
- en: 'Goal **coverage**: We should maximize the entropy of generated goals to encourage
    diverse goal and to improve the coverage over the goal space.'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 目标**覆盖率**：我们应该最大化生成目标的熵，以鼓励多样化的目标并提高对目标空间的覆盖率。
- en: $$ \begin{align*} \mathcal{L}_\text{cov} = \mathbb{E}_{\substack{ z \in \mathcal{N}(0,
    1), \\ f \in \text{Uniform}(0, 1) }} \big[ \log p(G(z, f)) \big] \end{align*}
    $$
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \begin{align*} \mathcal{L}_\text{cov} = \mathbb{E}_{\substack{ z \in \mathcal{N}(0,
    1), \\ f \in \text{Uniform}(0, 1) }} \big[ \log p(G(z, f)) \big] \end{align*}
    $$
- en: Their experiments showed complex environments require all three losses above.
    When the environment is changing between episodes, both the goal generator and
    the discriminator need to be conditioned on environmental observation to produce
    better results. If there is a desired goal distribution, an additional loss can
    be added to match a desired goal distribution using Wasserstein distance. Using
    this loss, the generator can push the solver toward mastering the desired tasks
    more efficiently.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 他们的实验表明，复杂环境需要上述三种损失。当环境在不同剧集之间变化时，目标生成器和鉴别器都需要根据环境观察进行条件化，以产生更好的结果。如果有一个期望的目标分布，可以添加额外的损失以使用Wasserstein距离匹配期望的目标分布。使用这种损失，生成器可以更有效地推动解算器掌握期望的任务。
- en: '![](../Images/89fdac9f6e04c04aef0553d408bf86c3.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/89fdac9f6e04c04aef0553d408bf86c3.png)'
- en: 'Fig. 7\. Training schematic for the (a) solver/policy, (b) judge/discriminator,
    and (c) setter/goal generator models. (Image source: [Racaniere & Lampinen, et
    al., 2019](https://arxiv.org/abs/1909.12892))'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图7. (a) 解算器/策略，(b) 评判者/鉴别器，和 (c) 设定者/目标生成器模型的训练示意图。 (图片来源：[Racaniere & Lampinen,
    et al., 2019](https://arxiv.org/abs/1909.12892))
- en: Skill-Based Curriculum
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于技能的课程
- en: Another view is to decompose what an agent is able to complete into a variety
    of skills and each skill set could be mapped into a task. Let’s imagine when an
    agent interacts with the environment in an unsupervised manner, is there a way
    to discover useful skills from such interaction and further build into the solutions
    for more complicated tasks through a curriculum?
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个观点是将代理能够完成的任务分解为各种技能，并且每个技能集可以映射到一个任务。想象一下，当一个代理以无监督的方式与环境互动时，是否有一种方法可以从这种互动中发现有用的技能，并进一步构建成更复杂任务的解决方案？
- en: '[Jabri, et al. (2019)](https://arxiv.org/abs/1912.04226) developed an automatic
    curriculum, **CARML** (short for “Curricula for Unsupervised Meta-Reinforcement
    Learning”), by modeling unsupervised trajectories into a latent skill space, with
    a focus on training [meta-RL](https://lilianweng.github.io/posts/2019-06-23-meta-rl/)
    policies (i.e. can transfer to unseen tasks). The setting of training environments
    in CARML is similar to [DIAYN](https://lilianweng.github.io/posts/2019-06-23-meta-rl/#learning-with-random-rewards).
    Differently, CARML is trained on pixel-level observations but DIAYN operates on
    the true state space. An RL algorithm $\pi_\theta$, parameterized by $\theta$,
    is trained via unsupervised interaction formulated as a CMP combined with a learned
    reward function $r$. This setting naturally works for the meta-learning purpose,
    since a customized reward function can be given only at the test time.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '[Jabri等人（2019）](https://arxiv.org/abs/1912.04226)通过将无监督轨迹建模为潜在技能空间，开发了一个自动课程，**CARML**（“无监督元强化学习课程”），专注于训练[元强化学习（meta-RL）](https://lilianweng.github.io/posts/2019-06-23-meta-rl/)策略（即可以转移到未见过的任务）。CARML中的训练环境设置类似于[DIAYN](https://lilianweng.github.io/posts/2019-06-23-meta-rl/#learning-with-random-rewards)。不同的是，CARML在像素级观察上进行训练，而DIAYN在真实状态空间上运行。通过与学习奖励函数
    $r$ 结合的无监督交互形式化为CMP的RL算法 $\pi_\theta$，参数化为 $\theta$，进行训练。这种设置自然适用于元学习目的，因为在测试时可以仅提供定制的奖励函数。'
- en: '![](../Images/b70be6ecd9e5b04bf598eced542efc15.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b70be6ecd9e5b04bf598eced542efc15.png)'
- en: 'Fig. 8\. An illustration of CARML, containing two steps: (1) organizing experiential
    data into the latent skill space; (2) meta-training the policy with the reward
    function constructed from the learned skills. (Image source: [Jabri, et al 2019](https://arxiv.org/abs/1912.04226))'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图8. 展示了CARML的示意图，包含两个步骤：(1) 将经验数据组织到潜在技能空间中；(2) 使用从学习到的技能构建的奖励函数对策略进行元训练。（图片来源：[Jabri等人，2019](https://arxiv.org/abs/1912.04226)）
- en: CARML is framed as a [variational Expectation-Maximization (EM)](https://chrischoy.github.io/research/Expectation-Maximization-and-Variational-Inference/).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: CARML被构建为一个[变分期望最大化（EM）](https://chrischoy.github.io/research/Expectation-Maximization-and-Variational-Inference/)。
- en: '(1) **E-Step**: This is the stage for organizing experiential data. Collected
    trajectories are modeled with a mixture of latent components forming the [basis](https://en.wikipedia.org/wiki/Basis_(linear_algebra))
    of *skills*.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: (1) **E-Step**：这是组织经验数据的阶段。收集到的轨迹被建模为形成*技能*的潜在组件的混合物。
- en: Let $z$ be a latent task variable and $q_\phi$ be a variational distribution
    of $z$, which could be a mixture model with discrete $z$ or a VAE with continuous
    $z$. A variational posterior $q_\phi(z \vert s)$ works like a classifier, predicting
    a skill given a state, and we would like to maximize $q_\phi(z \vert s)$ to discriminate
    between data produced by different skills as much as possible. In E-step, $q_\phi$
    is fitted to a set of trajectories produced by $\pi_\theta$.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 让 $z$ 成为一个潜在任务变量，$q_\phi$ 成为 $z$ 的变分分布，它可以是具有离散 $z$ 的混合模型或具有连续 $z$ 的VAE。变分后验
    $q_\phi(z \vert s)$ 的作用类似于一个分类器，预测给定状态的技能，并且我们希望最大化 $q_\phi(z \vert s)$，尽可能地区分由不同技能产生的数据。在E步中，$q_\phi$
    被拟合到由 $\pi_\theta$ 产生的一组轨迹。
- en: Precisely, given a trajectory $\tau = (s_1,\dots,s_T)$, we would like to find
    $\phi$ such that
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 精确地说，给定一个轨迹 $\tau = (s_1,\dots,s_T)$，我们希望找到 $\phi$，使得
- en: $$ \max_\phi \mathbb{E}_{z\sim q_\phi(z)} \big[ \log q_\phi(\tau \vert z) \big]
    = \max_\phi \mathbb{E}_{z\sim q_\phi(z)} \big[ \sum_{s_i \in \tau} \log q_\phi(s_i
    \vert z) \big] $$
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \max_\phi \mathbb{E}_{z\sim q_\phi(z)} \big[ \log q_\phi(\tau \vert z) \big]
    = \max_\phi \mathbb{E}_{z\sim q_\phi(z)} \big[ \sum_{s_i \in \tau} \log q_\phi(s_i
    \vert z) \big] $$
- en: A simplifying assumption is made here to ignore the order of states in one trajectory.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这里做出了一个简化假设，忽略了一个轨迹中状态的顺序。
- en: '(2) **M-Step**: This is the stage for doing meta-RL training with $\pi_\theta$.
    The learned skill space is considered as a training task distribution. CARML is
    agnostic to the type of meta-RL algorithm for policy parameter updates.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: (2) **M-Step**：这是进行与 $\pi_\theta$ 的元强化学习训练的阶段。学习到的技能空间被视为训练任务分布。CARML对于策略参数更新的元RL算法类型是不可知的。
- en: 'Given a trajectory $\tau$, it makes sense for the policy to maximize the mutual
    information between $\tau$ and $z$, $I(\tau;z) = H(\tau) - H(\tau \vert z)$, because:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个轨迹 $\tau$，策略最大化 $\tau$ 和 $z$ 之间的互信息是有意义的，$I(\tau;z) = H(\tau) - H(\tau \vert
    z)$，因为：
- en: maximizing $H(\tau)$ => diversity in the policy data space; expected to be large.
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最大化 $H(\tau)$ => 策略数据空间中的多样性；预计会很大。
- en: minimizing $H(\tau \vert z)$ => given a certain skill, the behavior should be
    restricted; expected to be small.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最小化$H(\tau \vert z)$ => 给定某个技能，行为应受限制；预期应该很小。
- en: Then we have,
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们有，
- en: $$ \begin{aligned} I(\tau; z) &= \mathcal{H}(z) - \mathcal{H}(z \vert s_1,\dots,
    s_T) \\ &\geq \mathbb{E}_{s \in \tau} [\mathcal{H}(z) - \mathcal{H}(z\vert s)]
    & \scriptstyle{\text{; discard the order of states.}} \\ &= \mathbb{E}_{s \in
    \tau} [\mathcal{H}(s_t) - \mathcal{H}(s\vert z)] & \scriptstyle{\text{; by definition
    of MI.}} \\ &= \mathbb{E}_{z\sim q_\phi(z), s\sim \pi_\theta(s|z)} [\log q_\phi(s|z)
    - \log \pi_\theta(s)] \\ &\approx \mathbb{E}_{z\sim q_\phi(z), s\sim \pi_\theta(s|z)}
    [\color{green}{\log q_\phi(s|z) - \log q_\phi(s)}] & \scriptstyle{\text{; assume
    learned marginal distr. matches policy.}} \end{aligned} $$
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \begin{aligned} I(\tau; z) &= \mathcal{H}(z) - \mathcal{H}(z \vert s_1,\dots,
    s_T) \\ &\geq \mathbb{E}_{s \in \tau} [\mathcal{H}(z) - \mathcal{H}(z\vert s)]
    & \scriptstyle{\text{; 忽略状态的顺序。}} \\ &= \mathbb{E}_{s \in \tau} [\mathcal{H}(s_t)
    - \mathcal{H}(s\vert z)] & \scriptstyle{\text{; 根据互信息的定义。}} \\ &= \mathbb{E}_{z\sim
    q_\phi(z), s\sim \pi_\theta(s|z)} [\log q_\phi(s|z) - \log \pi_\theta(s)] \\ &\approx
    \mathbb{E}_{z\sim q_\phi(z), s\sim \pi_\theta(s|z)} [\color{green}{\log q_\phi(s|z)
    - \log q_\phi(s)}] & \scriptstyle{\text{; 假设学习的边缘分布与策略匹配。}} \end{aligned} $$
- en: 'We can set the reward as $\log q_\phi(s \vert z) - \log q_\phi(s)$, as shown
    in the red part in the equation above. In order to balance between task-specific
    exploration (as in red below) and latent skill matching (as in blue below) , a
    parameter $\lambda \in [0, 1]$ is added. Each realization of $z \sim q_\phi(z)$
    induces a reward function $r_z(s)$ (remember that reward + CMP => MDP) as follows:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将奖励设置为$\log q_\phi(s \vert z) - \log q_\phi(s)$，如上述方程中的红色部分所示。为了在任务特定的探索（如下面的红色部分）和潜在技能匹配（如下面的蓝色部分）之间取得平衡，添加了一个参数$\lambda
    \in [0, 1]$。每个$z \sim q_\phi(z)$的实现引入了一个奖励函数$r_z(s)$（记住奖励 + CMP => MDP）如下：
- en: $$ \begin{aligned} r_z(s) &= \lambda \log q_\phi(s|z) - \log q_\phi(s) \\ &=
    \lambda \log q_\phi(s|z) - \log \frac{q_\phi(s|z) q_\phi(z)}{q_\phi(z|s)} \\ &=
    \lambda \log q_\phi(s|z) - \log q_\phi(s|z) - \log q_\phi(z) + \log q_\phi(z|s)
    \\ &= (\lambda - 1) \log \color{red}{q_\phi(s|z)} + \color{blue}{\log q_\phi(z|s)}
    + C \end{aligned} $$![](../Images/38d4921e21dd996b2759fffbce3f3cb8.png)
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \begin{aligned} r_z(s) &= \lambda \log q_\phi(s|z) - \log q_\phi(s) \\ &=
    \lambda \log q_\phi(s|z) - \log \frac{q_\phi(s|z) q_\phi(z)}{q_\phi(z|s)} \\ &=
    \lambda \log q_\phi(s|z) - \log q_\phi(s|z) - \log q_\phi(z) + \log q_\phi(z|s)
    \\ &= (\lambda - 1) \log \color{red}{q_\phi(s|z)} + \color{blue}{\log q_\phi(z|s)}
    + C \end{aligned} $$![](../Images/38d4921e21dd996b2759fffbce3f3cb8.png)
- en: 'Fig. 9\. The algorithm of CARML. (Image source: [Jabri, et al 2019](https://arxiv.org/abs/1912.04226))'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图9\. CARML的算法。（图片来源：[Jabri, et al 2019](https://arxiv.org/abs/1912.04226)）
- en: Learning a latent skill space can be done in different ways, such as in [Hausman,
    et al. 2018](https://openreview.net/forum?id=rk07ZXZRb). The goal of their approach
    is to learn a task-conditioned policy, $\pi(a \vert s, t^{(i)})$, where $t^{(i)}$
    is from a discrete list of $N$ tasks, $\mathcal{T} = [t^{(1)}, \dots, t^{(N)}]$.
    However, rather than learning $N$ separate solutions, one per task, it would be
    nice to learn a latent skill space so that each task could be represented in a
    distribution over skills and thus skills are *reused between tasks*. The policy
    is defined as $\pi_\theta(a \vert s,t) = \int \pi_\theta(a \vert z,s,t) p_\phi(z
    \vert t)\mathrm{d}z$, where $\pi_\theta$ and $p_\phi$ are policy and embedding
    networks to learn, respectively. If $z$ is discrete, i.e. drawn from a set of
    $K$ skills, then the policy becomes a mixture of $K$ sub-policies. The policy
    training uses [SAC](http://127.0.0.1:4000/lil-log/2018/04/07/policy-gradient-algorithms.html#sac)
    and the dependency on $z$ is introduced in the entropy term.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 学习潜在技能空间可以通过不同的方式完成，比如在[Hausman, et al. 2018](https://openreview.net/forum?id=rk07ZXZRb)中。他们方法的目标是学习一个任务条件的策略，$\pi(a
    \vert s, t^{(i)})$，其中$t^{(i)}$来自于一个包含$N$个任务的离散列表，$\mathcal{T} = [t^{(1)}, \dots,
    t^{(N)}]$。然而，与其学习$N$个单独的解决方案，一个解决方案对应一个任务，学习一个潜在技能空间会更好，这样每个任务可以在技能分布中表示，因此技能在任务之间*得以重复利用*。策略定义为$\pi_\theta(a
    \vert s,t) = \int \pi_\theta(a \vert z,s,t) p_\phi(z \vert t)\mathrm{d}z$，其中$\pi_\theta$和$p_\phi$分别是要学习的策略和嵌入网络。如果$z$是离散的，即从$K$个技能中抽取，那么策略就变成了$K$个子策略的混合。策略训练使用[SAC](http://127.0.0.1:4000/lil-log/2018/04/07/policy-gradient-algorithms.html#sac)，并且对$z$的依赖性是通过熵项引入的。
- en: Curriculum through Distillation
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过蒸馏进行课程设计
- en: '[I was thinking of the name of this section for a while, deciding between cloning,
    inheritance, and distillation. Eventually, I picked distillation because it sounds
    the coolest B-)]'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '[我考虑了一段时间这一部分的名称，考虑在克隆、继承和蒸馏之间做出选择。最终，我选择了蒸馏，因为听起来最酷B-)]'
- en: The motivation for the **progressive neural network** ([Rusu et al. 2016](https://arxiv.org/abs/1606.04671))
    architecture is to efficiently transfer learned skills between different tasks
    and in the meantime avoid catastrophic forgetting. The curriculum is realized
    through a set of progressively stacked neural network towers (or “columns”, as
    in the paper).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '**渐进神经网络**（[Rusu等人，2016](https://arxiv.org/abs/1606.04671)）架构的动机是在不同任务之间有效地转移学习到的技能，同时避免灾难性遗忘。通过一组逐渐堆叠的神经网络塔（或“列”，如论文中所述）来实现课程。'
- en: 'A progressive network has the following structure:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 渐进网络具有以下结构：
- en: It starts with a single column containing $L$ layers of neurons, in which the
    corresponding activation layers are labelled as $h^{(1)}_i, i=1, \dots, L$. We
    first train this single-column network for one task to convergence, achieving
    parameter config $\theta^{(1)}$.
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它始于一个包含$L$层神经元的单列，其中相应的激活层被标记为$h^{(1)}_i, i=1, \dots, L$。我们首先将这个单列网络训练到收敛，实现参数配置$\theta^{(1)}$。
- en: Once switch to the next task, we need to add a new column to adapt to the new
    context while freezing $\theta^{(1)}$ to lock down the learned skills from the
    previous task. The new column has activation layers labelled as $h^{(2)}_i, i=1,
    \dots, L$, and parameters $\theta^{(2)}$.
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦转移到下一个任务，我们需要添加一个新列以适应新的上下文，同时冻结$\theta^{(1)}$以锁定从先前任务中学到的技能。新列具有标记为$h^{(2)}_i,
    i=1, \dots, L$的激活层，以及参数$\theta^{(2)}$。
- en: 'Step 2 can be repeated with every new task. The $i$-th layer activation in
    the $k$-th column depends on the previous activation layers in all the existing
    columns:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第2步可以针对每个新任务重复进行。第$k$列中第$i$层的激活取决于所有现有列中先前激活层的激活：
- en: $$ h^{(k)}_i = f(W^{(k)}_i h^{(k)}_{i-1} + \sum_{j < k} U_i^{(k:j)} h^{(j)}_{i-1})
    $$
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $$ h^{(k)}_i = f(W^{(k)}_i h^{(k)}_{i-1} + \sum_{j < k} U_i^{(k:j)} h^{(j)}_{i-1})
    $$
- en: where $W^{(k)}_i$ is the weight matrix of the layer $i$ in the column $k$; $U_i^{(k:j)},
    j < k$ are the weight matrices for projecting the layer $i-1$ of the column $j$
    to the layer $i$ of column $k$ ($ j < k $). The above weights matrices should
    be learned. $f(.)$ is a non-linear activation function by choice.
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其中$W^{(k)}_i$是第$k$列中第$i$层的权重矩阵；$U_i^{(k:j)}, j < k$是用于将第$j$列的第$i-1$层投影到第$k$列的第$i$层的权重矩阵（$
    j < k $）。上述权重矩阵应该被学习。$f(.)$是一个非线性激活函数的选择。
- en: '![](../Images/773c637938003150b0ca6ebd0321253b.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/773c637938003150b0ca6ebd0321253b.png)'
- en: 'Fig. 10\. The progressive neural network architecture. (Image source: [Rusu,
    et al. 2017](https://arxiv.org/abs/1610.04286))'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图10. 渐进神经网络架构。（图片来源：[Rusu等人，2017](https://arxiv.org/abs/1610.04286))
- en: The paper experimented with Atari games by training a progressive network on
    multiple games to check whether features learned in one game can transfer to another.
    That is indeed the case. Though interestingly, learning a high dependency on features
    in the previous columns does not always indicate good transfer performance on
    the new task. One hypothesis is that features learned from the old task might
    introduce biases into the new task, leading to policy getting trapped in a sub-optimal
    solution. Overall, the progressive network works better than only fine-tuning
    the top layer and can achieve similar transfer performance as fine-tuning the
    entire network.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 该论文通过在多个游戏上训练渐进网络来进行Atari游戏实验，以检查在一个游戏中学到的特征是否可以转移到另一个游戏。事实上是这样的。尽管有趣的是，对先前列中特征的高依赖并不总是表明在新任务上有良好的转移性能。一个假设是，从旧任务学到的特征可能会引入偏见到新任务中，导致策略陷入次优解。总体而言，渐进网络比仅微调顶层效果更好，并且可以实现与微调整个网络相似的转移性能。
- en: One use case for the progressive network is to do sim2real transfer ([Rusu,
    et al. 2017](https://arxiv.org/abs/1610.04286)), in which the first column is
    trained in simulator with a lot of samples and then the additional columns (could
    be for different real-world tasks) are added and trained with a few real data
    samples.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 渐进网络的一个用例是进行sim2real转移（[Rusu等人，2017](https://arxiv.org/abs/1610.04286)），其中第一列在模拟器中用大量样本进行训练，然后添加并用少量真实数据样本训练额外列（可能用于不同的真实世界任务）。
- en: '[Czarnecki, et al. (2018)](https://arxiv.org/abs/1806.01780) proposed another
    RL training framework, **Mix & Match** (short for **M&M**) to provide curriculum
    through coping knowledge between agents. Given a sequence of agents from simple
    to complex, $\pi_1, \dots, \pi_K$, each parameterized with some shared weights
    (e.g. by shared some lower common layers). M&M trains a mixture of agents, but
    only the final performance of the most complex one $\pi_K$ matters.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '[Czarnecki 等人 (2018)](https://arxiv.org/abs/1806.01780) 提出了另一个 RL 训练框架，**Mix
    & Match**（简称 **M&M**），通过在代理之间共享知识提供课程。给定一系列从简单到复杂的代理，$\pi_1, \dots, \pi_K$，每个都使用一些共享权重进行参数化（例如通过共享一些较低的公共层）。M&M
    训练一组代理，但只有最复杂的一个 $\pi_K$ 的最终表现才重要。'
- en: 'In the meantime, M&M learns a categorical distribution $c \sim \text{Categorical}(1,
    \dots, K \vert \alpha)$ with [pmf](https://en.wikipedia.org/wiki/Probability_mass_function)
    $p(c=i) = \alpha_i$ probability to pick which policy to use at a given time. The
    mixed M&M policy is a simple weighted sum: $\pi_\text{mm}(a \vert s) = \sum_{i=1}^K
    \alpha_i \pi_i(a \vert s)$. Curriculum learning is realized by dynamically adjusting
    $\alpha_i$, from $\alpha_K=0$ to $\alpha_K=1$. The tuning of $\alpha$ can be manual
    or through [population-based training](https://lilianweng.github.io/posts/2019-09-05-evolution-strategies/#hyperparameter-tuning-pbt).'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，M&M 学习了一个分类分布 $c \sim \text{Categorical}(1, \dots, K \vert \alpha)$，其 [pmf](https://en.wikipedia.org/wiki/Probability_mass_function)
    为 $p(c=i) = \alpha_i$，表示在特定时间选择使用哪种策略的概率。混合 M&M 策略是一个简单的加权和：$\pi_\text{mm}(a \vert
    s) = \sum_{i=1}^K \alpha_i \pi_i(a \vert s)$。课程学习通过动态调整 $\alpha_i$ 实现，从 $\alpha_K=0$
    调整到 $\alpha_K=1$。调整 $\alpha$ 可以是手动的，也可以通过 [population-based training](https://lilianweng.github.io/posts/2019-09-05-evolution-strategies/#hyperparameter-tuning-pbt)
    进行。
- en: To encourage cooperation rather than competition among policies, besides the
    RL loss $\mathcal{L}_\text{RL}$, another [distillation](https://arxiv.org/abs/1511.06295)-like
    loss $\mathcal{L}_\text{mm}(\theta)$ is added. The knowledge transfer loss $\mathcal{L}_\text{mm}(\theta)$
    measures the KL divergence between two policies, $\propto D_\text{KL}(\pi_{i}(.
    \vert s) | \pi_j(. \vert s))$ for $i < j$. It encourages complex agents to match
    the simpler ones early on. The final loss is $\mathcal{L} = \mathcal{L}_\text{RL}(\theta
    \vert \pi_\text{mm}) + \lambda \mathcal{L}_\text{mm}(\theta)$.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 为了鼓励策略之间的合作而不是竞争，除了 RL 损失 $\mathcal{L}_\text{RL}$ 外，还添加了另一个类似于 [蒸馏](https://arxiv.org/abs/1511.06295)
    的损失 $\mathcal{L}_\text{mm}(\theta)$。知识传输损失 $\mathcal{L}_\text{mm}(\theta)$ 衡量了两个策略之间的
    KL 散度，$\propto D_\text{KL}(\pi_{i}(. \vert s) | \pi_j(. \vert s))$ 对于 $i < j$。它鼓励复杂的代理尽早与简单的代理匹配。最终损失为
    $\mathcal{L} = \mathcal{L}_\text{RL}(\theta \vert \pi_\text{mm}) + \lambda \mathcal{L}_\text{mm}(\theta)$。
- en: '![](../Images/b00b9bf9dcb20c5814cde908f1997c76.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b00b9bf9dcb20c5814cde908f1997c76.png)'
- en: 'Fig. 11\. The Mix & Match architecture for training a mixture of policies.
    (Image source: [Czarnecki, et al., 2018](https://arxiv.org/abs/1806.01780))'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11\. 用于训练一组策略的 Mix & Match 架构。（图片来源：[Czarnecki 等人，2018](https://arxiv.org/abs/1806.01780)）
- en: Citation
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引用
- en: 'Cited as:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 引用为：
- en: Weng, Lilian. (Jan 2020). Curriculum for reinforcement learning. Lil’Log. https://lilianweng.github.io/posts/2020-01-29-curriculum-rl/.
  id: totrans-147
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Weng, Lilian.（2020年1月）。强化学习的课程。Lil’Log。https://lilianweng.github.io/posts/2020-01-29-curriculum-rl/。
- en: Or
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 或
- en: '[PRE0]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: References
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Jeffrey L. Elman. [“Learning and development in neural networks: The importance
    of starting small.”](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.128.4487&rep=rep1&type=pdf)
    Cognition 48.1 (1993): 71-99.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Jeffrey L. Elman。[“Learning and development in neural networks: The importance
    of starting small.”](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.128.4487&rep=rep1&type=pdf)
    Cognition 48.1 (1993): 71-99。'
- en: '[2] Yoshua Bengio, et al. [“Curriculum learning.”](https://www.researchgate.net/profile/Y_Bengio/publication/221344862_Curriculum_learning/links/546cd2570cf2193b94c577ac/Curriculum-learning.pdf)
    ICML 2009.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Yoshua Bengio 等人。[“Curriculum learning.”](https://www.researchgate.net/profile/Y_Bengio/publication/221344862_Curriculum_learning/links/546cd2570cf2193b94c577ac/Curriculum-learning.pdf)
    ICML 2009。'
- en: '[3] Daphna Weinshall, Gad Cohen, and Dan Amir. [“Curriculum learning by transfer
    learning: Theory and experiments with deep networks.”](https://arxiv.org/abs/1802.03796)
    ICML 2018.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Daphna Weinshall, Gad Cohen 和 Dan Amir。[“Curriculum learning by transfer
    learning: Theory and experiments with deep networks.”](https://arxiv.org/abs/1802.03796)
    ICML 2018。'
- en: '[4] Wojciech Zaremba and Ilya Sutskever. [“Learning to execute.”](https://arxiv.org/abs/1410.4615)
    arXiv preprint arXiv:1410.4615 (2014).'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Wojciech Zaremba 和 Ilya Sutskever。[“Learning to execute.”](https://arxiv.org/abs/1410.4615)
    arXiv 预印本 arXiv:1410.4615 (2014)。'
- en: '[5] Tambet Matiisen, et al. [“Teacher-student curriculum learning.”](https://arxiv.org/abs/1707.00183)
    IEEE Trans. on neural networks and learning systems (2017).'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Tambet Matiisen 等人。[“Teacher-student curriculum learning.”](https://arxiv.org/abs/1707.00183)
    IEEE Trans. on neural networks and learning systems (2017)。'
- en: '[6] Alex Graves, et al. [“Automated curriculum learning for neural networks.”](https://arxiv.org/abs/1704.03003)
    ICML 2017.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Alex Graves等人，《神经网络的自动课程学习》，ICML 2017。'
- en: '[7] Remy Portelas, et al. [Teacher algorithms for curriculum learning of Deep
    RL in continuously parameterized environments](https://arxiv.org/abs/1910.07224).
    CoRL 2019.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] Remy Portelas等人，《连续参数化环境中深度RL课程学习的教师算法》，CoRL 2019。'
- en: '[8] Sainbayar Sukhbaatar, et al. [“Intrinsic Motivation and Automatic Curricula
    via Asymmetric Self-Play.”](https://arxiv.org/abs/1703.05407) ICLR 2018.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] Sainbayar Sukhbaatar等人，《通过不对称自我对弈实现内在动机和自动课程设置》，ICLR 2018。'
- en: '[9] Carlos Florensa, et al. [“Automatic Goal Generation for Reinforcement Learning
    Agents”](https://arxiv.org/abs/1705.06366) ICML 2019.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] Carlos Florensa等人，《强化学习代理的自动生成目标》，ICML 2019。'
- en: '[10] Sebastien Racaniere & Andrew K. Lampinen, et al. [“Automated Curriculum
    through Setter-Solver Interactions”](https://arxiv.org/abs/1909.12892) ICLR 2020.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] Sebastien Racaniere & Andrew K. Lampinen等人，《通过Setter-Solver互动实现自动课程设置》，ICLR
    2020。'
- en: '[11] Allan Jabri, et al. [“Unsupervised Curricula for Visual Meta-Reinforcement
    Learning”](https://arxiv.org/abs/1912.04226) NeuriPS 2019.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] Allan Jabri等人，《视觉元强化学习的无监督课程设置》，NeuriPS 2019。'
- en: '[12] Karol Hausman, et al. [“Learning an Embedding Space for Transferable Robot
    Skills “](https://openreview.net/forum?id=rk07ZXZRb) ICLR 2018.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '[12] Karol Hausman等人，《学习可转移机器人技能的嵌入空间》，ICLR 2018。'
- en: '[13] Josh Merel, et al. [“Reusable neural skill embeddings for vision-guided
    whole body movement and object manipulation”](https://arxiv.org/abs/1911.06636)
    arXiv preprint arXiv:1911.06636 (2019).'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '[13] Josh Merel等人，《用于视觉引导全身运动和物体操作的可重用神经技能嵌入》，arXiv预印本arXiv:1911.06636（2019）。'
- en: '[14] OpenAI, et al. [“Solving Rubik’s Cube with a Robot Hand.”](https://arxiv.org/abs/1910.07113)
    arXiv preprint arXiv:1910.07113 (2019).'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '[14] OpenAI等人，《用机器人手解决魔方问题》，arXiv预印本arXiv:1910.07113（2019）。'
- en: '[15] Niels Justesen, et al. [“Illuminating Generalization in Deep Reinforcement
    Learning through Procedural Level Generation”](https://arxiv.org/abs/1806.10729)
    NeurIPS 2018 Deep RL Workshop.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '[15] Niels Justesen等人，《通过程序级别生成揭示深度强化学习中的泛化能力》，NeurIPS 2018深度RL研讨会。'
- en: '[16] Karl Cobbe, et al. [“Quantifying Generalization in Reinforcement Learning”](https://arxiv.org/abs/1812.02341)
    arXiv preprint arXiv:1812.02341 (2018).'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '[16] Karl Cobbe等人，《量化强化学习中的泛化能力》，arXiv预印本arXiv:1812.02341（2018）。'
- en: '[17] Andrei A. Rusu et al. [“Progressive Neural Networks”](https://arxiv.org/abs/1606.04671)
    arXiv preprint arXiv:1606.04671 (2016).'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '[17] Andrei A. Rusu等人，《渐进式神经网络》，arXiv预印本arXiv:1606.04671（2016）。'
- en: '[18] Andrei A. Rusu et al. [“Sim-to-Real Robot Learning from Pixels with Progressive
    Nets.”](https://arxiv.org/abs/1610.04286) CoRL 2017.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '[18] Andrei A. Rusu等人，《从像素到真实机器人学习的渐进式网络》，CoRL 2017。'
- en: '[19] Wojciech Marian Czarnecki, et al. [“Mix & Match – Agent Curricula for
    Reinforcement Learning.”](https://arxiv.org/abs/1806.01780) ICML 2018.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '[19] Wojciech Marian Czarnecki等人，《混合与匹配 - 强化学习代理课程设置》，ICML 2018。'
