- en: How to Train Really Large Models on Many GPUs?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何在许多GPU上训练真正大型模型？
- en: 原文：[https://lilianweng.github.io/posts/2021-09-25-train-large/](https://lilianweng.github.io/posts/2021-09-25-train-large/)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://lilianweng.github.io/posts/2021-09-25-train-large/](https://lilianweng.github.io/posts/2021-09-25-train-large/)
- en: '[Updated on 2022-03-13: add [expert choice routing](#ec).]'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[更新于2022-03-13：添加[专家选择路由](#ec)。]'
- en: '[Updated on 2022-06-10]: [Greg](https://gregbrockman.com/) and I wrote a shorted
    and upgraded version of this post, published on OpenAI Blog: [“Techniques for
    Training Large Neural Networks”](https://openai.com/blog/techniques-for-training-large-neural-networks/)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[更新于2022-06-10]：[Greg](https://gregbrockman.com/)和我写了这篇文章的缩短和升级版本，发表在OpenAI博客上：[“训练大型神经网络的技术”](https://openai.com/blog/techniques-for-training-large-neural-networks/)'
- en: In recent years, we are seeing better results on many NLP benchmark tasks with
    larger pre-trained [language models](https://lilianweng.github.io/posts/2019-01-31-lm/).
    How to train large and deep neural networks is challenging, as it demands a large
    amount of GPU memory and a long horizon of training time.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，我们在许多NLP基准任务上看到了更好的结果，使用了更大的预训练[语言模型](https://lilianweng.github.io/posts/2019-01-31-lm/)。如何训练大型和深度神经网络是具有挑战性的，因为它需要大量的GPU内存和长时间的训练。
- en: However an individual GPU worker has limited memory and the sizes of many large
    models have grown beyond a single GPU. There are several parallelism paradigms
    to enable model training across multiple GPUs, as well as a variety of model architecture
    and memory saving designs to help make it possible to train *very large* neural
    networks.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，个别GPU工作者的内存有限，许多大型模型的大小已经超出了单个GPU的范围。有几种并行性范式可以实现跨多个GPU的模型训练，以及各种模型架构和节省内存设计，以帮助训练*非常大*的神经网络。
- en: Training Parallelism
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练并行性
- en: The main bottleneck for training very large neural network models is the intense
    demand for a large amount of GPU memory, way above what can be hosted on an individual
    GPU machine. Besides the model weights (e.g. tens of billions of floating point
    numbers), it is usually even more expensive to store intermediate computation
    outputs such as gradients and optimizer states (e.g. momentums & variations in
    Adam). Additionally training a large model often pairs with a large training corpus
    and thus a single process may just take forever.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 训练非常大的神经网络模型的主要瓶颈是对大量GPU内存的强烈需求，远远超出个别GPU机器所能承载的范围。除了模型权重（例如数十亿的浮点数），通常存储中间计算输出（如梯度和优化器状态，例如Adam中的动量和变化）的成本更高。此外，训练大型模型通常需要大量的训练语料库，因此单个进程可能需要很长时间。
- en: As a result, parallelism is necessary. Parallelism can happen at different dimensions,
    including data, model architecture, and tensor operation.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，并行性是必要的。并行性可以在不同的维度上发生，包括数据、模型架构和张量操作。
- en: Data Parallelism
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据并行性
- en: The most naive way for **Data parallelism (DP)** is to copy the same model weights
    into multiple workers and assign a fraction of data to each worker to be processed
    at the same time.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据并行性（DP）**最朴素的方法是将相同的模型权重复制到多个工作者中，并将数据的一部分分配给每个工作者同时处理。'
- en: Naive DP cannot work well if the model size is larger than a single GPU node’s
    memory. Methods like *GeePS* ([Cui et al. 2016](https://www.pdl.cmu.edu/PDL-FTP/CloudComputing/GeePS-cui-eurosys16.pdf))
    offload temporarily unused parameters back to CPU to work with limited GPU memory
    when the model is too big to fit into one machine. The data swapping transfer
    should happen at the backend and not interfere with training computation.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 如果模型大小大于单个GPU节点的内存，则朴素的DP无法很好地工作。像*GeePS*（[Cui等人，2016](https://www.pdl.cmu.edu/PDL-FTP/CloudComputing/GeePS-cui-eurosys16.pdf)）这样的方法会将暂时未使用的参数传输回CPU，以便在模型太大无法适应一台机器时，使用有限的GPU内存。数据交换传输应该在后端进行，不干扰训练计算。
- en: At the end of each minibatch, workers need to synchronize gradients or weights
    to avoid staleness. There are two main synchronization approaches and both have
    clear pros & cons.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个小批次结束时，工作者需要同步梯度或权重以避免陈旧。有两种主要的同步方法，两者都有明显的优缺点。
- en: '*Bulk synchronous parallels (BSP)*: Workers sync data at the end of every minibatch.
    It prevents model weights staleness and good learning efficiency but each machine
    has to halt and wait for others to send gradients.'
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*批量同步并行（BSP）*：工作者在每个小批次结束时同步数据。它可以防止模型权重陈旧，具有良好的学习效率，但每台机器必须停止并等待其他机器发送梯度。'
- en: '*Asynchronous parallel (ASP)*: Every GPU worker processes the data asynchronously,
    no waiting or stalling. However, it can easily lead to stale weights being used
    and thus lower the statistical learning efficiency. Even though it increases the
    computation time, it may not speed up training time to convergence.'
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*异步并行（ASP）*：每个GPU工作人员异步处理数据，无需等待或停顿。然而，这很容易导致使用过时的权重，从而降低统计学习效率。即使增加了计算时间，也可能不会加快收敛的训练时间。'
- en: Somewhere in the middle is to synchronize gradients globally once every $x$
    iterations ($x > 1$). This feature is called “gradient accumulation” in Distribution
    Data Parallel ([DDP](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html))
    since Pytorch v1.5 ([Li et al. 2021](https://arxiv.org/abs/2006.15704)). Bucketing
    gradients avoid immediate `AllReduce` operations but instead buckets multiple
    gradients into one `AllReduce` to improve throughput. Computation and communication
    scheduling optimization can be made based on the computation graph.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在中间某处是每$x$次迭代全局同步梯度一次（$x > 1$）。这个特性在Pytorch v1.5（[Li等人2021](https://arxiv.org/abs/2006.15704)）中被称为“梯度累积”在分布式数据并行（[DDP](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html)）中。梯度分桶避免立即进行`AllReduce`操作，而是将多个梯度分桶到一个`AllReduce`中以提高吞吐量。可以根据计算图进行计算和通信调度优化。
- en: '![](../Images/34e8644725d2af58db26fd6952864d73.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/34e8644725d2af58db26fd6952864d73.png)'
- en: 'Fig. 1\. Pseudo code for Pytorch DDP. (Image source: [Li et al. 2021](https://arxiv.org/abs/2006.15704))'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图1。Pytorch DDP的伪代码。（图片来源：[Li等人2021](https://arxiv.org/abs/2006.15704)）
- en: Model Parallelism
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型并行
- en: '**Model parallelism (MP)** aims to solve the case when the model weights cannot
    fit into a single node. The computation and model parameters are partitioned across
    multiple machines. Different from data parallelism where each worker hosts a full
    copy of the entire model, MP only allocates a fraction of model parameters on
    one worker and thus both the memory usage and the computation are reduced.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '**模型并行（MP）** 旨在解决模型权重无法适应单个节点的情况。计算和模型参数被分割到多台机器上。与数据并行不同，其中每个工作人员托管整个模型的完整副本，MP仅在一个工作人员上分配部分模型参数，因此内存使用和计算都减少了。'
- en: Since deep neural networks usually contain a stack of vertical layers, it feels
    straightforward to split a large model by layer, where a small consecutive set
    of layers are grouped into one partition on one worker. However, a naive implementation
    for running every data batch through multiple such workers with sequential dependency
    leads to big bubbles of waiting time and severe under-utilization of computation
    resources.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 由于深度神经网络通常包含一堆垂直层，因此通过层来分割大型模型似乎很直接，其中一小组连续的层被分组到一个工作人员的一个分区中。然而，通过多个这样的工作人员运行每个数据批次的天真实现会导致等待时间的大量“气泡”和计算资源的严重低效利用。
- en: '![](../Images/7a90f3f6fd6afd22c299d17120d6e29e.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7a90f3f6fd6afd22c299d17120d6e29e.png)'
- en: 'Fig. 2\. A naive model parallelism setup where the model is vertically split
    into 4 partitions. Data is processed by one worker at a time due to sequential
    dependency, leading to large “bubbles” of idle time. (Image source: [Huang et
    al. 2019](https://arxiv.org/abs/1811.06965))'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图2。一个天真的模型并行设置，其中模型被垂直分割为4个分区。由于顺序依赖性，数据一次由一个工作人员处理，导致大量空闲时间的“气泡”。（图片来源：[Huang等人2019](https://arxiv.org/abs/1811.06965)）
- en: Pipeline Parallelism
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 管道并行
- en: '**Pipeline parallelism (PP)** combines model parallelism with data parallelism
    to reduce inefficient time “bubbles’’. The main idea is to split one minibatch
    into multiple microbatches and enable each stage worker to process one microbatch
    simultaneously. Note that every microbatch needs two passes, one forward and one
    backward. Inter-worker communication only transfers activations (forward) and
    gradients (backward). How these passes are scheduled and how the gradients are
    aggregated vary in different approaches. The number of partitions (workers) is
    also known as *pipeline depth*.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**管道并行（PP）** 将模型并行和数据并行结合起来，以减少低效的时间“气泡”。其主要思想是将一个小批量分成多个微批量，并使每个阶段的工作人员同时处理一个微批量。需要注意的是，每个微批量需要进行两次传递，一次前向传递和一次反向传递。工作人员之间的通信仅传输激活（前向）和梯度（反向）。这些传递如何安排以及梯度如何聚合在不同方法中有所不同。分区（工作人员）的数量也被称为*管道深度*。'
- en: 'In *GPipe* ([Huang et al. 2019](https://arxiv.org/abs/1811.06965)) gradients
    from multiple microbatches are aggregated and applied synchronously at the end.
    The synchronous gradient descent guarantees learning consistency and efficiency
    irrespective of the number of workers. As shown in Fig. 3, bubbles still exist
    but are much smaller than what’s in Fig. 2\. Given $m$ evenly split microbatches
    and $d$ partitions, assuming both forward and backward per microbatch take one
    unit of time, the fraction of bubble is:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在*GPipe*（[Huang等人，2019](https://arxiv.org/abs/1811.06965)）中，来自多个微批次的梯度在最后同步聚合和应用。同步梯度下降保证了学习的一致性和效率，不受工作者数量的影响。如图3所示，泡沫仍然存在，但比图2中的要小得多。假设均匀分割的微批次数为$m$，分区数为$d$，假设每个微批次的前向和后向传递都需要一单位时间，泡沫的比例为：
- en: $$ 1 - \frac{2md}{(2m + 2(d-1))d} = \frac{d-1}{m+d-1} $$
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: $$ 1 - \frac{2md}{(2m + 2(d-1))d} = \frac{d-1}{m+d-1} $$
- en: The GPipe paper observed that the bubble overhead is almost negligible if the
    number of microbatches is more than 4x the number of partitions $m > 4d$ (when
    [activation recomputation](#activation-recomputation) is applied).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: GPipe论文观察到，如果微批次的数量大于分区数量的4倍以上 $m > 4d$（当应用[激活重计算](#activation-recomputation)时），泡沫开销几乎可以忽略不计。
- en: '![](../Images/ca000b631b8049fab1780b05714d5a10.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ca000b631b8049fab1780b05714d5a10.png)'
- en: 'Fig. 3\. Illustration of pipeline parallelism in GPipe with 4 microbatches
    and 4 partitions. GPipe aggregates and updates gradients across devices synchronously
    at the end of every batch. (Image source: [Huang et al. 2019](https://arxiv.org/abs/1811.06965))'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图3。GPipe中管道并行的示意图，有4个微批次和4个分区。GPipe在每个批次结束时同步跨设备聚合和更新梯度。（图片来源：[Huang等人，2019](https://arxiv.org/abs/1811.06965)）
- en: GPipe achieves almost linear speedup in throughput with the number of devices,
    although it is not always guaranteed if the model parameters are not evenly distributed
    across workers.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: GPipe几乎实现了与设备数量成线性增长的吞吐量，尽管如果模型参数在工作者之间分布不均匀，则不能始终保证。
- en: '*PipeDream* ([Narayanan et al. 2019](https://cs.stanford.edu/~matei/papers/2019/sosp_pipedream.pdf))
    schedules each worker to alternatively process the forward and backward passes
    (`1F1B`). PipeDream names each model partition “stage” and each stage worker can
    have multiple replicas to run data parallelism. In this process, PipeDream uses
    a deterministic round-robin load balancing strategy to assign work among multiple
    replicas of stages to ensure that the forward and backward passes for the same
    minibatch happen on the same replica.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '*PipeDream*（[Narayanan等人，2019](https://cs.stanford.edu/~matei/papers/2019/sosp_pipedream.pdf)）安排每个工作者交替处理前向和后向传递（`1F1B`）。PipeDream将每个模型分区命名为“阶段”，每个阶段工作者可以有多个副本来运行数据并行。在这个过程中，PipeDream使用确定性的轮询负载平衡策略来分配工作在多个阶段的副本之间，以确保同一微批次的前向和后向传递发生在同一个副本上。'
- en: '![](../Images/7e0dc94eb2bfc9ba07e13e0bbade56d0.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7e0dc94eb2bfc9ba07e13e0bbade56d0.png)'
- en: 'Fig. 4\. Illustration of `1F1B` microbatch scheduling in PipeDream. (Image
    source: [Harlap et al. 2018](https://arxiv.org/abs/1806.03377))'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图4。PipeDream中`1F1B`微批次调度的示意图。（图片来源：[Harlap等人，2018](https://arxiv.org/abs/1806.03377))
- en: 'Since PipeDream does not have an end-of-batch global gradient sync across all
    the workers, an native implementation of 1F1B can easily lead to the forward and
    backward passes of one microbatch using different versions of model weights, thus
    lowering the learning efficiency. PipeDream proposed a few designs to tackle this
    issue:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 由于PipeDream没有跨所有工作者进行批次结束全局梯度同步，1F1B的本地实现很容易导致一个微批次的前向和后向传递使用不同版本的模型权重，从而降低学习效率。PipeDream提出了一些设计来解决这个问题：
- en: '*Weight stashing*: Each worker keeps track of several model versions and makes
    sure that the same version of weights are used in the forward and backward passes
    given one data batch.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*权重存储*：每个工作者跟踪几个模型版本，并确保在给定一个数据批次时，前向和后向传递中使用相同版本的权重。'
- en: '*Vertical sync* (Optional): The version of model weights flows between stage
    workers together with activations and gradients. Then the computation adopts the
    corresponding stashed version propagated from the previous worker. This process
    keeps version consistency across workers. Note that it is asynchronous, different
    from GPipe.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*垂直同步*（可选）：模型权重的版本与阶段工作者之间的流动，连同激活和梯度一起。然后计算采用从上一个工作者传播的相应存储版本。这个过程保持了工作者之间的版本一致性。请注意，这是异步的，与GPipe不同。'
- en: At the beginning of a training run, PipeDream first profiles the computation
    memory cost and time of each layer in the model and then optimizes a solution
    for partitioning layers into stages, which is a dynamic programming problem.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练开始时，PipeDream 首先对模型中每个层的计算内存成本和时间进行分析，然后优化将层分割成阶段的解决方案，这是一个动态规划问题。
- en: '![](../Images/dafb25dca4576b21030d0f19cdab7da3.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dafb25dca4576b21030d0f19cdab7da3.png)'
- en: 'Fig. 5\. Results for VGG16 on ILSVRC12\. (Top) Accuracy vs time. The integer
    marks the number of stage workers. ASP = Asynchronous parallel & BSP = Bulk synchronous
    parallels. (Bottom) Training time speedup for different parallelism configurations.
    Straight pipeline refers to pipeline parallelism without data parallelism. (Image
    source: [Harlap et al. 2018](https://arxiv.org/abs/1806.03377))'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图5. 在 ILSVRC12 上的 VGG16 结果。 （上）准确率 vs 时间。整数表示阶段工作者的数量。 ASP = 异步并行 & BSP = 批量同步并行。
    （下）不同并行配置的训练时间加速。直线管道指的是没有数据并行的管道并行。 （图片来源：[Harlap 等人 2018](https://arxiv.org/abs/1806.03377)）
- en: Two variations of PipeDream were later proposed to reduce the memory footprint
    by stashed model versions ([Narayanan et al. 2021](https://arxiv.org/abs/2006.09503)).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 后来提出了 PipeDream 的两种变体，通过存储模型版本来减少内存占用（[Narayanan 等人 2021](https://arxiv.org/abs/2006.09503)）。
- en: '*PipeDream-flush* adds a globally synchronized pipeline flush periodically,
    just like GPipe. In this way, it greatly reduces the memory footprint (i.e. only
    maintain a single version of model weights) by sacrificing a little throughput.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '*PipeDream-flush* 定期添加全局同步的管道刷新，就像 GPipe 一样。这样一来，通过牺牲一点吞吐量，大大减少了内存占用（即只维护一个模型权重版本）。'
- en: '![](../Images/76daa0863fb7d70fc46fe3d6f216917a.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/76daa0863fb7d70fc46fe3d6f216917a.png)'
- en: 'Fig. 6\. Illustration of pipeline scheduling in PipeDream-flush. (Image source:
    ([Narayanan et al. 2021](https://arxiv.org/abs/2006.09503))'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图6. PipeDream-flush 中的管道调度示意图。 （图片来源：([Narayanan 等人 2021](https://arxiv.org/abs/2006.09503)）
- en: '*PipeDream-2BW* maintains only two versions of model weights, where “2BW” is
    short for “double-buffered weights”. It generates a new model version every $k$
    microbatches and $k$ should be larger than the pipeline depth $d$, $k > d$. A
    newly updated model version cannot fully replace the old version immediately since
    some leftover backward passes still depend on the old version. In total only two
    versions need to be saved so the memory cost is much reduced.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '*PipeDream-2BW* 仅维护两个版本的模型权重，其中“2BW” 是“双缓冲权重”的缩写。它每 $k$ 个微批次生成一个新的模型版本，$k$
    应该大于管道深度 $d$，$k > d$。由于一些残留的反向传播仍依赖于旧版本，新更新的模型版本不能立即完全替换旧版本。总共只需要保存两个版本，因此内存成本大大降低。'
- en: '![](../Images/26685230f71d59f0b6ece033cf10706c.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/26685230f71d59f0b6ece033cf10706c.png)'
- en: 'Fig. 7\. Illustration of pipeline scheduling in PipeDream-2BW. (Image source:
    ([Narayanan et al. 2021](https://arxiv.org/abs/2006.09503))'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图7. PipeDream-2BW 中的管道调度示意图。 （图片来源：([Narayanan 等人 2021](https://arxiv.org/abs/2006.09503)）
- en: Tensor Parallelism
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 张量并行
- en: Both model and pipeline parallelisms split a model vertically. OTOH we can horizontally
    partition the computation for one tensor operation across multiple devices, named
    **Tensor parallelism (TP)**.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 模型和管道并行都是垂直拆分模型。另一方面，我们可以横向分割一个张量操作的计算到多个设备上，称为**张量并行（TP）**。
- en: Let’s take the transformer as an example given its popularity. The transformer
    model mainly consists of layers of MLP and self-attention blocks. *Megatron-LM*
    ([Shoeybi et al. 2020](https://arxiv.org/abs/1909.08053)) adopts a simple way
    to parallelize intra-layer computation for MLP and self-attention.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以 transformer 为例，考虑到其受欢迎程度。transformer 模型主要由 MLP 层和自注意力块组成。*Megatron-LM*（[Shoeybi
    等人 2020](https://arxiv.org/abs/1909.08053)）采用了一种简单的方式来并行化 MLP 和自注意力的层内计算。
- en: 'A MLP layer in a transformer contains a GEMM (General matrix multiply) followed
    by an non-linear GeLU transfer. Let’s split weight matrix $A$ by column:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 transformer 中的 MLP 层包含一个 GEMM（通用矩阵乘法），后跟一个非线性的 GeLU 转换。让我们按列拆分权重矩阵 $A$：
- en: $$ \begin{aligned} \text{Split }A &= [A_1, A_2] \\ Y &=\text{GeLU}(XA) \\ [Y_1,
    Y_2] &= [\text{GeLU}(XA_1), \text{GeLU}(XA_2)] \end{aligned} $$
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \begin{aligned} \text{拆分 }A &= [A_1, A_2] \\ Y &=\text{GeLU}(XA) \\ [Y_1,
    Y_2] &= [\text{GeLU}(XA_1), \text{GeLU}(XA_2)] \end{aligned} $$
- en: The attention block runs GEMM with query ($Q$), key ($K$), and value weights
    ($V$) according to the above partitioning in parallel and then combines them with
    another GEMM to produce the attention head results.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力块使用查询（$Q$）、键（$K$）和值（$V$）权重进行 GEMM 运算，根据上述分区并行运行，然后将它们与另一个 GEMM 结合以生成注意力头结果。
- en: $$ \text{Attention}(X, Q, K, V) = \text{softmax}(\frac{(XQ) (XK)^\top}{\sqrt{d_k}})
    XV $$![](../Images/8aa32f72c1ab42850e313bb0523a3bce.png)
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \text{Attention}(X, Q, K, V) = \text{softmax}(\frac{(XQ) (XK)^\top}{\sqrt{d_k}})
    XV $$![](../Images/8aa32f72c1ab42850e313bb0523a3bce.png)
- en: 'Fig. 8\. Illustration of tensor parallelism for key transformer components
    proposed in Megatron-LM. (Image source: [Shoeybi et al. 2020](https://arxiv.org/abs/1909.08053))'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图8. Megatron-LM中提出的关键变压器组件的张量并行示意图。（图片来源：[Shoeybi等人，2020](https://arxiv.org/abs/1909.08053)）
- en: '[Narayanan et al. (2021)](https://arxiv.org/abs/2104.04473) combined pipeline,
    tensor and data parallelism with a new pipeline scheduling strategy and named
    their approach *PTD-P*. Instead of only positioning a continuous set of layers
    (“model chunk”) on a device, each worker can be assigned with multiple chunks
    of smaller continuous subsets of layers (e.g. device 1 has layers 1, 2, 9, 10;
    device 2 has layers 3, 4, 11, 12; each has two model chunks). The number of microbatches
    in one batch should be exactly divided by the number of workers ($m % d = 0$).
    If there are $v$ model chunks per worker, the pipeline bubble time can be reduced
    by a multiplier of $v$ compared to a GPipe scheduling.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '[Narayanan等人（2021）](https://arxiv.org/abs/2104.04473)将管道、张量和数据并行与新的管道调度策略相结合，并将其方法命名为*PTD-P*。每个工作者不仅可以在设备上定位一组连续的层（“模型块”），而且还可以分配多个较小连续层子集的多个块（例如，设备1有层1、2、9、10；设备2有层3、4、11、12；每个有两个模型块）。一个批次中的微批次数量应该被工作者数量整除（$m
    % d = 0$）。如果每个工作者有$v$个模型块，与GPipe调度相比，管道气泡时间可以通过$v$的倍数减少。'
- en: '![](../Images/51fc1bb09f79a52a4ed5e25c2821e2c7.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/51fc1bb09f79a52a4ed5e25c2821e2c7.png)'
- en: 'Fig. 9\. (Top) Default `1F1B` pipeline schedule as in PipeDream-flush. (Bottom)
    Interleaved 1F1B pipeline schedule. First model chunks are in dark colors and
    second chunks are in light colors. (Image source: [Narayanan et al. 202)](https://arxiv.org/abs/2104.04473))'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.（顶部）PipeDream-flush中的默认`1F1B`管道调度。（底部）交错的1F1B管道调度。第一个模型块为深色，第二个模块为浅色。（图片来源：[Narayanan等人，202)](https://arxiv.org/abs/2104.04473)）
- en: Mixture-of-Experts (MoE)
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 专家混合（MoE）
- en: 'The **Mixture-of-Experts (MoE)** approach attracts a lot of attention recently
    as researchers (mainly from Google) try to push the limit of model size. The core
    of the idea is [ensembling learning](https://en.wikipedia.org/wiki/Ensemble_learning):
    *Combination of multiple weak learners gives you a strong learner!*'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '**专家混合（MoE）**方法最近吸引了很多关注，因为研究人员（主要来自谷歌）试图推动模型规模的极限。这个想法的核心是[集成学习](https://en.wikipedia.org/wiki/Ensemble_learning)：*多个弱学习器的组合会给你一个强学习器！*'
- en: Within one deep neural network, ensembling can be implemented with a gating
    mechanism connecting multiple experts ([Shazeer et al., 2017](https://arxiv.org/abs/1701.06538)).
    The gating mechanism controls which subset of the network (e.g. which experts)
    should be activated to produce outputs. The paper named it “sparsely gated mixture-of-experts”
    (MoE) layer.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个深度神经网络中，集成可以通过一个门控机制来实现，连接多个专家（[Shazeer等人，2017](https://arxiv.org/abs/1701.06538)）。门控机制控制着网络的哪个子集（例如哪些专家）应该被激活以产生输出。该论文将其命名为“稀疏门控专家混合”（MoE）层。
- en: Precisely one MoE layer contains
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 精确地说，一个MoE层包含
- en: $n$ feed-forward networks as experts $\{E_i\}^n_{i=1}$
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $n$个前馈网络作为专家$\{E_i\}^n_{i=1}$
- en: A trainable gating network $G$ to learn a probability distribution over $n$
    experts so as to route the traffic to a few selected experts.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个可训练的门控网络$G$，学习$n$个专家之间的概率分布，以便将流量路由到少数选定的专家。
- en: Depending on the gating outputs, not every expert has to be evaluated. When
    the number of experts is too large, we can consider using a two-level hierarchical
    MoE.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 根据门控输出，不必评估每个专家。当专家数量过多时，我们可以考虑使用两级分层MoE。
- en: '![](../Images/0c9daa61b841bfac6ccff6a8aac3ffbb.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0c9daa61b841bfac6ccff6a8aac3ffbb.png)'
- en: 'Fig. 10\. Illustration of a mixture-of-experts (MoE) layer. Only 2 out of $n$
    experts are selected and activated by the gating network. (Image source: [Shazeer
    et al., 2017](https://arxiv.org/abs/1701.06538))'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图10. 专家混合（MoE）层的示意图。只有$n$个专家中的2个被门控网络选中并激活。（图片来源：[Shazeer等人，2017](https://arxiv.org/abs/1701.06538)）
- en: 'A simple choice of $G$ is to multiply the input with a trainable weight matrix
    $G_g$ and then do softmax: $G_\sigma (x) = \text{softmax}(x W_g)$. However, this
    produces a dense control vector for gating and does not help save computation
    resources because we don’t need to evaluate an expert only when $G^{(i)}(x)=0$.
    Thus the MoE layer only keeps the top $k$ values. It also adds tunable Gaussian
    noise into $G$ to improve load balancing. This mechanism is called *noisy top-k
    gating*.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: $G$的一个简单选择是将输入与可训练的权重矩阵$G_g$相乘，然后进行softmax：$G_\sigma (x) = \text{softmax}(x
    W_g)$。然而，这会产生一个密集的控制向量用于门控，并且不会帮助节省计算资源，因为我们只有在$G^{(i)}(x)=0$时才需要评估一个专家。因此，MoE层仅保留前$k$个值。它还向$G$中添加可调节的高斯噪声以改善负载平衡。这种机制称为*有噪声的top-k门控*。
- en: $$ \begin{aligned} G(x) &= \text{softmax}( \text{topk}(H(x), k)) \\ H^{(i)}(x)
    &= (xW_g)^{(i)} + \epsilon \cdot \text{softplus}((xW_\text{noise})^{(i)} ); \quad
    \epsilon \sim \mathcal{N}(0, \mathbf{1}) \\ \text{topk}^{(i)}(v, k) &= \begin{cases}
    v^{(i)} & \text{if }v^{(i)}\text{ is in the top }k\text{ elements of }v \\ -\infty
    & \text{otherwise} \end{cases} \end{aligned} $$
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \begin{aligned} G(x) &= \text{softmax}( \text{topk}(H(x), k)) \\ H^{(i)}(x)
    &= (xW_g)^{(i)} + \epsilon \cdot \text{softplus}((xW_\text{noise})^{(i)} ); \quad
    \epsilon \sim \mathcal{N}(0, \mathbf{1}) \\ \text{topk}^{(i)}(v, k) &= \begin{cases}
    v^{(i)} & \text{if }v^{(i)}\text{ is in the top }k\text{ elements of }v \\ -\infty
    & \text{otherwise} \end{cases} \end{aligned} $$
- en: where the superscript $v^{(i)}$ denotes the i-th dimension of the vector $v$.
    The function $\text{topk}(., k)$ selected the top $k$ dimensions with highest
    values by setting other dimensions to $-\infty$.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 其中上标$v^{(i)}$表示向量$v$的第i维。函数$\text{topk}(., k)$通过将其他维度设置为$-\infty$来选择具有最高值的前$k$个维度。
- en: To avoid the self-reinforcing effect that the gating network may favor a few
    strong experts all the time, [Shazeer et al. (2017)](https://arxiv.org/abs/1701.06538)
    proposed a soft constraint via an additional importance loss to encourage all
    the experts to have the same weights. It is equivalent to the square of the [coefficient
    of variation](https://en.wikipedia.org/wiki/Coefficient_of_variation) of batchwise
    average value per expert.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免门控网络可能始终偏爱少数强专家的自我强化效应，[Shazeer等人（2017）](https://arxiv.org/abs/1701.06538)提出了通过额外的重要性损失软约束来鼓励所有专家具有相同的权重。这等同于每个专家的批次平均值的[变异系数](https://en.wikipedia.org/wiki/Coefficient_of_variation)的平方。
- en: $$ L_\text{aux} = w_\text{aux} \cdot \text{CV}(\sum_{x \in X} G(x))^2 $$
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: $$ L_\text{aux} = w_\text{aux} \cdot \text{CV}(\sum_{x \in X} G(x))^2 $$
- en: where $ \text{CV}$ is the coefficient of variation and the loss weight $w_\text{aux}$
    is a hyperparameter to tune.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\text{CV}$是变异系数，损失权重$w_\text{aux}$是一个需要调整的超参数。
- en: Because every expert network only gets a fraction of training samples (“The
    shrinking batch problem”), we should try to use a batch size as large as possible
    in MoE. However, it is restricted by GPU memory. Data parallelism and model parallelism
    can be applied to improve the throughput.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 由于每个专家网络只能获得训练样本的一部分（“缩小的批次问题”），我们应该尽量使用尽可能大的批次大小在MoE中。然而，这受限于GPU内存。数据并行和模型并行可以应用于提高吞吐量。
- en: '![](../Images/50080a3b615434ae0b725afa32a8b411.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/50080a3b615434ae0b725afa32a8b411.png)'
- en: 'Fig. 11\. Test perplexity on 1-Billion-Word language modeling benchmark. (Left)
    The model capacity increases from left to right, containing 4, 32, 256, 256, 1024
    and 4096 experts. (Right) Performance of the 4 billion parameters MoE model, the
    largest one in the left figure, under different computation budgets. (Image source:
    [Shazeer et al., 2017](https://arxiv.org/abs/1701.06538))'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图11\. 在1亿字语言建模基准测试中的测试困惑度。（左）模型容量从左到右增加，包含4、32、256、256、1024和4096个专家。（右）4亿参数MoE模型的性能，是左图中最大的模型，在不同计算预算下的表现。（图片来源：[Shazeer
    et al., 2017](https://arxiv.org/abs/1701.06538)）
- en: '**GShard** ([Lepikhin et al., 2020](https://arxiv.org/abs/2006.16668)) scales
    the MoE transformer model up to 600 billion parameters with sharding. The MoE
    transformer replaces every other feed forward layer with a MoE layer. The *sharded
    MoE transformer* only has the MoE layers sharded across multiple machines, while
    other layers are simply duplicated.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '**GShard**（[Lepikhin et al., 2020](https://arxiv.org/abs/2006.16668)）通过分片将MoE变压器模型扩展到6000亿参数。MoE变压器用MoE层替换每个其他前馈层。*分片MoE变压器*只在多台机器上分片MoE层，而其他层只是简单地复制。'
- en: 'There are several improved designs for the gating function $G$ in GShard:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种改进的设计用于GShard中的门控函数$G$：
- en: '*Expert capacity*: The amount of tokens going through one expert should not
    go above a threshold, named “expert capacity”. If a token is routed to experts
    that have reached their capacity, the token would be marked “overflowed” and the
    gating output is changed to a zero vector.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*专家容量*：通过一个专家的令牌数量不应超过一个名为“专家容量”的阈值。如果一个令牌被路由到已达到容量的专家，该令牌将被标记为“溢出”，并且门控输出将更改为零向量。'
- en: '*Local group dispatching*: Tokens are evenly partitioned into multiple local
    groups and the expert capacity is enforced on the group level.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*本地组调度*：令牌被均匀分成多个本地组，并在组级别上强制执行专家容量。'
- en: '*Auxiliary loss*: The motivation is similar to the original MoE aux loss. They
    add an auxiliary loss to minimize the mean square of the fraction of data routed
    to each expert.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*辅助损失*：动机与原始MoE辅助损失类似。他们添加了一个辅助损失，以最小化路由到每个专家的数据比例的均方。'
- en: '*Random routing*: The 2nd-best expert is selected with a probability proportional
    to its weight; otherwise, GShard follows a random routing, so as to add some randomness.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*随机路由*：以与其权重成比例的概率选择第二优秀的专家；否则，GShard将遵循随机路由，以增加一些随机性。'
- en: '![](../Images/8d32c3abda389ef5575d5678ad0968b0.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8d32c3abda389ef5575d5678ad0968b0.png)'
- en: 'Fig. 12\. Pseudo code of the group-level top-2 gating mechanism with auxiliary
    loss in GShard. (Image source: [Lepikhin et al., 2020](https://arxiv.org/abs/2006.16668))'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图12. GShard中带有辅助损失的组级前2门控机制的伪代码。（图片来源：[Lepikhin等人，2020](https://arxiv.org/abs/2006.16668)）
- en: '**Switch Transformer** ([Fedus et al. 2021](https://arxiv.org/abs/2101.03961))
    scales the model size up to trillions of parameters (!!) by replacing the dense
    feed forward layer with a *sparse switch FFN layer* in which each input is only
    routed to *one* expert network. The auxiliary loss for load balancing is $\text{loss}_\text{aux}
    = w_\text{aux} \sum_{i=1}^n f_i p_i$ given $n$ experts, where $f_i$ is the fraction
    of tokens routed to the $i$-th expert and $p_i$ is the routing probability for
    expert $i$ predicted by the gating network.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '**Switch Transformer**（[Fedus等人，2021](https://arxiv.org/abs/2101.03961)）通过将密集前馈层替换为*稀疏开关FFN层*，将模型规模扩展到了数万亿个参数（!!）。在这种结构中，每个输入仅路由到*一个*专家网络。用于负载平衡的辅助损失为$\text{loss}_\text{aux}
    = w_\text{aux} \sum_{i=1}^n f_i p_i$，其中$n$为专家数量，$f_i$为路由到第$i$个专家的令牌比例，$p_i$为由门控网络预测的专家$i$的路由概率。'
- en: '![](../Images/42291aab3e0b05e8606849b6fe2ab36c.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/42291aab3e0b05e8606849b6fe2ab36c.png)'
- en: 'Fig. 13\. Switch transformer. The sparse switch FFN layer is in the blue boxes.
    (Image source: [Fedus et al. 2021](https://arxiv.org/abs/2101.03961))'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图13. Switch Transformer。稀疏开关FFN层位于蓝色框中。（图片来源：[Fedus等人，2021](https://arxiv.org/abs/2101.03961)）
- en: 'To improve training stability, switch transformer incorporates the following
    designs:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高训练稳定性，Switch Transformer融入了以下设计：
- en: '*Selective precision*. They showed that selectively casting only a local part
    of the model to FP32 precision improves stability, while avoiding the expensive
    communication cost of FP32 tensors. The FP32 precision is only used within the
    body of the router function and the results are recast to FP16.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*选择性精度*。他们表明，仅将模型的局部部分选择性地转换为FP32精度可以提高稳定性，同时避免FP32张量的昂贵通信成本。FP32精度仅在路由器函数的主体中使用，并且结果被重新转换为FP16。'
- en: '*Smaller initialization*. The initialization of weight matrices is sampled
    from a truncated normal distribution with mean $\mu=0$ and stdev $\sigma = \sqrt{s/n}$.
    They also recommended reducing the transformer initialization scale parameter
    $s=1$ to $s=0.1$.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*更小的初始化*。权重矩阵的初始化是从截断的正态分布中采样，均值为$\mu=0$，标准差为$\sigma = \sqrt{s/n}$。他们还建议将变压器初始化比例参数$s=1$减少到$s=0.1$。'
- en: '*Use higher expert dropout*. Fine-tuning often works with a small dataset.
    To avoid overfitting, the dropout rate within each expert is increased by a significant
    amount. Interestingly they found that increasing dropout across all layers lead
    to poor performance. In the paper, they used a dropout rate 0.1 at non-expert
    layers but 0.4 within expert FF layers.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用更高的专家丢弃率*。微调通常适用于小数据集。为避免过拟合，每个专家内的丢弃率增加了相当多。有趣的是，他们发现增加所有层的丢弃率会导致性能下降。在论文中，他们在非专家层使用了0.1的丢弃率，但在专家FF层内使用了0.4的丢弃率。'
- en: 'The switch transformer paper summarized different data and model parallelism
    strategies for training large models with a nice illustration:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: Switch Transformer论文总结了用于训练大型模型的不同数据和模型并行策略，并配有一张精美的插图：
- en: '![](../Images/2a115ad0c35c2984c893ca84c7aefe31.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2a115ad0c35c2984c893ca84c7aefe31.png)'
- en: 'Fig. 14\. An illustration of various parallelism strategies on how (Top) model
    weights and (Bottom) data are split over multiple GPU cores. In the top row, each
    color denotes a unique weight matrix. In the bottom row, different colors indicate
    different sets of tokens. (Image source: [Fedus et al. 2021](https://arxiv.org/abs/2101.03961))'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 图14. 各种并行策略的示意图，展示了如何（顶部）将模型权重和（底部）数据分割到多个GPU核心上。在顶部一行，每种颜色代表一个独特的权重矩阵。在底部一行，不同颜色表示不同的令牌集合。（图片来源：[Fedus等人，2021](https://arxiv.org/abs/2101.03961)）
- en: Both GShard top-2 and Switch Transformer top-1 depend on *token choice*, where
    each token picks the best one or two experts to route through. They both adopt
    an auxiliary loss to encourage more balanced load allocation but it does not guarantee
    the best performance. Furthermore, the expert capacity limit may lead to wasted
    tokens as they would be discarded if an expert reaches its capacity limit.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: GShard top-2和Switch Transformer top-1都依赖于*令牌选择*，其中每个令牌选择最佳的一个或两个专家进行路由。它们都采用辅助损失来鼓励更平衡的负载分配，但这并不保证最佳性能。此外，专家容量限制可能导致令牌浪费，因为如果专家达到其容量限制，这些令牌将被丢弃。
- en: '**Export Choice (EC)** ([Zhou et al. 2022](https://arxiv.org/abs/2202.09368))
    routing instead enables each expert to select the top-$k$ tokens. In this way,
    each expert naturally guarantees a fixed capacity and each token may be routed
    to multiple experts. EC can achieve perfect load balancing and is shown to improve
    training convergence by 2x.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '**导出选择（EC）**（[周等人，2022](https://arxiv.org/abs/2202.09368)）路由使每个专家选择前$k$个令牌。通过这种方式，每个专家自然保证了固定的容量，每个令牌可能被路由到多个专家。EC可以实现完美的负载平衡，并且据显示可以将训练收敛速度提高2倍。'
- en: 'Given $e$ experts and an input matrix $X \in \mathbb{R}^{n \times d}$, the
    token-to-expert affinity scores are computed by: $$ S = \text{softmax}(X \cdot
    W_g), \text{where } W_g \in \mathbb{R}^{d \times e}, S \in \mathbb{R}^{n \times
    e} $$'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 给定$e$个专家和输入矩阵$X \in \mathbb{R}^{n \times d}$，通过以下方式计算令牌与专家的关联分数：$$ S = \text{softmax}(X
    \cdot W_g), \text{其中 } W_g \in \mathbb{R}^{d \times e}, S \in \mathbb{R}^{n \times
    e} $$
- en: A token-to-expert assignment is represented by three matrices, $I, G \in \mathbb{R}^{e\times
    k}$ and $P \in \mathbb{R}^{e \times k \times n}$. $I[i,j]$ annotates which token
    is the $j$-th selection by the $i$-th expert. The gating matrix $G$ stores the
    routing weights of selected tokens. $P$ is the one-hot version of $I$, used to
    produce the input matrix ($P \cdot X \in \mathbb{R}^{e \times k \times d}$) for
    the gated FFN layer. $$ G, I = \text{top-k}(S^\top, k)\quad P = \text{one-hot}(I)
    $$
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 令牌与专家的分配由三个矩阵表示，$I, G \in \mathbb{R}^{e\times k}$和$P \in \mathbb{R}^{e \times
    k \times n}$。$I[i,j]$注释第$i$个专家选择的第$j$个令牌。门控矩阵$G$存储所选令牌的路由权重。$P$是$I$的独热版本，用于生成门控FFN层的输入矩阵（$P
    \cdot X \in \mathbb{R}^{e \times k \times d}$）。$$ G, I = \text{top-k}(S^\top,
    k)\quad P = \text{one-hot}(I) $$
- en: One regularization that export choice routing explored is to limit the maximum
    number of experts per token.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 导出选择路由探索的一种正则化是限制每个令牌的最大专家数量。
- en: '$$ \begin{aligned} & \max_A \langle S^\top, A\rangle + \lambda H(A) \\ \text{s.t.}
    & \forall i: \sum_{j''} A[i, j''] = k,\quad \forall j: \sum_{i''} A[i'', j] \leq
    b,\quad \forall i,j: 0 \leq A[i,j] \leq 1 \end{aligned} $$'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '$$ \begin{aligned} & \max_A \langle S^\top, A\rangle + \lambda H(A) \\ \text{s.t.}
    & \forall i: \sum_{j''} A[i, j''] = k,\quad \forall j: \sum_{i''} A[i'', j] \leq
    b,\quad \forall i,j: 0 \leq A[i,j] \leq 1 \end{aligned} $$'
- en: where each entry $A[i,j]$ in $A \in \mathbb{R}^{e \times n}$ marks whether the
    $i$-the expert selects the $j$-th token. Solving this is non-trivial. The paper
    used [Dykstra’s algorithm](https://projecteuclid.org/journals/annals-of-probability/volume-13/issue-3/An-Iterative-Procedure-for-Obtaining-I-Projections-onto-the-Intersection/10.1214/aop/1176992918.full)
    that runs a sequence of multiple iterative computation steps. Capped expert choice
    results in a slight decrease in the fine-tuning performance in the experiments.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 其中矩阵$A \in \mathbb{R}^{e \times n}$中的每个条目$A[i,j]$标记第$i$个专家是否选择第$j$个令牌。解决这个问题并不容易。该论文使用[Dykstra算法](https://projecteuclid.org/journals/annals-of-probability/volume-13/issue-3/An-Iterative-Procedure-for-Obtaining-I-Projections-onto-the-Intersection/10.1214/aop/1176992918.full)运行一系列多次迭代计算步骤。在实验中，限制专家选择会导致微调性能略微下降。
- en: The parameter $k$ is determined by $k=nc/e$, where $n$ is the total number of
    tokens in one batch and $c$ is a capacity factor indicating the average number
    of experts used by one token. The paper used $c=2$ in most experiments, but EC
    with $c=1$ still outperforms the top-1 token choice gating. Interestingly, $c=0.5$
    only marginally hurts the training performance.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 参数$k$由$k=nc/e$确定，其中$n$是一个批次中的总标记数，$c$是一个容量因子，表示一个标记平均使用的专家数量。在大多数实验中，论文使用$c=2$，但是$c=1$的EC仍然优于最佳的top-1标记选择门控。有趣的是，$c=0.5$只会轻微影响训练性能。
- en: One big drawback of EC is that it does not work when the batch size is too small,
    neither for auto-regressive text generation, because it needs to know the future
    tokens to do the top-$k$ selection.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: EC的一个很大的缺点是，当批量大小太小时，它无法工作，也不适用于自回归文本生成，因为它需要知道未来的标记才能进行top-$k$选择。
- en: Other Memory Saving Designs
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 其他节省内存的设计
- en: CPU Offloading
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CPU卸载
- en: When the GPU memory is full, one option is to offload temporarily unused data
    to CPU and read them back when needed later ([Rhu et al. 2016](https://arxiv.org/abs/1602.08124)).
    The idea of **CPU offloading** is straightforward but is less popular in recent
    years due to the slowdown it brings into the training time.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 当GPU内存已满时，一个选择是将暂时未使用的数据卸载到CPU，并在以后需要时读取它们（[Rhu等人，2016](https://arxiv.org/abs/1602.08124)）。**CPU卸载**的想法很简单，但由于它带来的训练时间延迟，近年来变得不那么流行。
- en: Activation Recomputation
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 激活重新计算
- en: '**Activation recomputation** (also known as “activation checkpointing” or “gradient
    checkpointing”; [Chen et al. 2016](https://arvix.org/abs/1604.06174)) is a smart
    yet simple idea to reduce memory footprint at the cost of computation time. It
    reduces the memory cost of training a $\ell$ layer deep neural net to $O(\sqrt{\ell})$,
    which only additionally consumes an extra forward pass computation per batch.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '**激活重新计算**（也称为“激活检查点”或“梯度检查点”；[Chen等人，2016](https://arvix.org/abs/1604.06174)）是一种聪明而简单的想法，可以减少内存占用，但会增加计算时间。它将训练一个$\ell$层深度神经网络的内存成本降低到$O(\sqrt{\ell})$，每批次只额外消耗一个额外的前向传播计算。'
- en: 'Let’s say, we evenly divide an $\ell$-layer network into $d$ partitions. Only
    activations at partition boundaries are saved and communicated between workers.
    Intermediate activations at intra-partition layers are still needed for computing
    gradients so they are recomputed during backward passes. With activation recomputation,
    the memory cost for training $M(\ell)$ is:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们将一个$\ell$层网络均匀分成$d$个分区。只有在分区边界处保存并在工作节点之间传递激活。在分区内层的中间激活仍然需要用于计算梯度，因此在反向传播期间重新计算。通过激活重新计算，训练$M(\ell)$的内存成本为：
- en: $$ M(\ell) =\max_{i=1,\dots,k} \underbrace{\text{cost-of-one-partition}(i)}_\text{cost
    of back-propagation on the i-th partition} + \underbrace{O(d)}_\text{store intermediate
    outputs} = O(\frac{\ell}{d}) + O(d) $$
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: $$ M(\ell) =\max_{i=1,\dots,k} \underbrace{\text{cost-of-one-partition}(i)}_\text{第i个分区的反向传播成本}
    + \underbrace{O(d)}_\text{存储中间输出} = O(\frac{\ell}{d}) + O(d) $$
- en: The minimum cost is $O(\sqrt{\ell})$ at $d=\sqrt{\ell}$.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 最小成本为$d=\sqrt{\ell}$时的$O(\sqrt{\ell})$。
- en: Activation recompuation trick can give sublinear memory cost with respect to
    the model size.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 激活重新计算技巧可以相对于模型大小给出次线性的内存成本。
- en: '![](../Images/afc5c46368ab59520fafcfae83854497.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/afc5c46368ab59520fafcfae83854497.png)'
- en: 'Fig. 15\. The memory cost of different memory saving algorithms. Sharing: Memory
    used by intermediate results is recycled when no longer needed. Inplace: Save
    the output directly into memory of an input value. (Image source: [Chen et al.
    2016](https://arvix.org/abs/1604.06174))'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图15\. 不同内存节省算法的内存成本。共享：当不再需要中间结果时，中间结果使用的内存被回收。原地：直接将输出保存到输入值的内存中。（图片来源：[Chen等人，2016](https://arvix.org/abs/1604.06174)）
- en: Mixed Precision Training
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 混合精度训练
- en: '[Narang & Micikevicius et al. (2018)](https://arxiv.org/abs/1710.03740) introduced
    a method to train models using half-precision floating point (FP16) numbers without
    losing model accuracy.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '[Narang & Micikevicius等人（2018）](https://arxiv.org/abs/1710.03740)提出了一种使用半精度浮点（FP16）数字训练模型的方法，而不会丢失模型准确性。'
- en: '![](../Images/82fba07ad959cc2ac2d8917f526db4fc.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/82fba07ad959cc2ac2d8917f526db4fc.png)'
- en: 'Fig. 16\. The procedure of mixed precision training at one layer. (Image source:
    [Narang & Micikevicius, et al. 2018](https://arxiv.org/abs/1710.03740))'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图16\. 一个层的混合精度训练过程。（图片来源：[Narang & Micikevicius等人，2018](https://arxiv.org/abs/1710.03740)）
- en: 'Three techniques to avoid losing critical information at half-precision:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 避免在半精度下丢失关键信息的三种技术：
- en: '*Full-precision master copy of weights*. Maintain a full precision (FP32) copy
    of model weights that accumulates gradients. The numbers are rounded up to half-precision
    for forward & backward passes. The motivation is that each gradient update (i.e.
    gradient times the learning rate) might be too small to be fully contained within
    the FP16 range (i.e. $2^{-24}$ becomes zero in FP16).'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*完整精度的权重主副本*。维护一个完整精度（FP32）的模型权重副本，用于累积梯度。这些数字在前向和反向传递中被舍入为半精度。其动机是每次梯度更新（即梯度乘以学习率）可能太小，无法完全包含在
    FP16 范围内（即 $2^{-24}$ 在 FP16 中变为零）。'
- en: '*Loss scaling*. Scale up the loss to better handle gradients with small magnitudes
    (See Fig. 16). Scaling up the gradients helps shift them to occupy a larger section
    towards the right section (containing larger values) of the representable range,
    preserving values that are otherwise lost.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*损失缩放*。将损失放大以更好地处理梯度幅度较小的情况（见图 16）。放大梯度有助于将它们移动到占据表示范围右侧（包含较大值）的较大部分，保留否则会丢失的值。'
- en: '*Arithmetic precision*. For common network arithmetic (e.g. vector dot-product,
    reduction by summing up vector elements), we can accumulate the partial results
    in FP32 and then save the final output as FP16 before saving into memory. Point-wise
    operations can be executed in either FP16 or FP32.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*算术精度*。对于常见的网络算术运算（例如向量点积，通过对向量元素求和来减少），我们可以在 FP32 中累积部分结果，然后在保存到内存之前将最终输出保存为
    FP16。逐点操作可以在 FP16 或 FP32 中执行。'
- en: '![](../Images/9193e47bb29dc4e8f26ce7e4ef390ca8.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9193e47bb29dc4e8f26ce7e4ef390ca8.png)'
- en: 'Fig. 17\. The histogram of gradients in full precision. The left part up to
    $2^{-24}$ will be zero-ed off once the model switches to FP16\. (Image source:
    [Narang & Micikevicius, et al. 2018](https://arxiv.org/abs/1710.03740))'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17\. 完整精度梯度的直方图。一旦模型切换到 FP16，左侧部分直到 $2^{-24}$ 将被清零。 （图片来源：[Narang & Micikevicius
    等人，2018](https://arxiv.org/abs/1710.03740)）
- en: In their experiments, loss scaling is not needed for some networks (e.g. image
    classification, Faster R-CNN), but necessary for others (e.g. Multibox SSD, big
    LSTM language model).
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在他们的实验中，某些网络（例如图像分类，Faster R-CNN）不需要损失缩放，但对于其他网络（例如 Multibox SSD，大型 LSTM 语言模型）则是必要的。
- en: Compression
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 压缩
- en: Intermediate results often consume a lot of memory, although they are only needed
    in one forward pass and one backward pass. There is a noticeable temporal gap
    between these two uses. Thus [Jain et al. (2018)](https://www.microsoft.com/en-us/research/uploads/prod/2018/04/fiddle-gist-isca18.pdf)
    proposed a data encoding strategy to compress the intermediate results after the
    first use in the first pass and then decode it back for back-propagation later.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 中间结果通常消耗大量内存，尽管它们只在一个前向传递和一个反向传递中需要。这两次使用之间存在明显的时间间隔。因此[Jain 等人（2018）](https://www.microsoft.com/en-us/research/uploads/prod/2018/04/fiddle-gist-isca18.pdf)提出了一种数据编码策略，在第一次使用后压缩中间结果，然后在后续的反向传播中解码回来。
- en: 'Their system *Gist* incorporates two encoding schemes: *Layer-specific lossless
    encoding*; focus on ReLU-Pool (“Binarize”) and ReLU-Conv (“Sparse storage and
    dense computation”) patterns. *Aggressive lossy encoding*; use delayed precision
    reduction (DPR). They observed that the first immediate use of feature maps should
    be kept at high precision but the second use can tolerate lower precision.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 他们的系统*Gist*包含两种编码方案：*特定层无损编码*；专注于 ReLU-Pool（“二值化”）和 ReLU-Conv（“稀疏存储和密集计算”）模式。*激进的有损编码*；使用延迟精度降低（DPR）。他们观察到，特征图的第一次即时使用应保持高精度，但第二次使用可以容忍较低精度。
- en: The experiments showed that Gist can reduce the memory cost by 2x across 5 SOTA
    image classification DNNs, with an average of 1.8x with only 4% performance overhead.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 实验表明，Gist 可以在 5 个 SOTA 图像分类 DNN 中将内存成本降低 2 倍，平均降低 1.8 倍，仅有 4% 的性能开销。
- en: Memory Efficient Optimizer
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 内存高效优化器
- en: Optimizers are eager for memory consumption. Take the popular Adam optimizer
    as an example, it internally needs to maintain momentums and variances, both at
    the same scale as gradients and model parameters. All out of a sudden, we need
    to save 4x the memory of model weights.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 优化器对内存消耗渴望。以流行的 Adam 优化器为例，内部需要维护动量和方差，都与梯度和模型参数在相同的规模上。突然之间，我们需要保存模型权重的 4 倍内存。
- en: Several optimizers have been proposed to reduce the memory footprint. For example,
    instead of storing the full momentums and variations as in Adam, *Adafactor* ([Shazeer
    et al. 2018](https://arxiv.org/abs/1804.04235)) only tracks the per-row and per-column
    sums of the moving averages and then estimates the second moments based on these
    sums. *SM3* ([Anil et al. 2019](https://arxiv.org/abs/1901.11150)) describes a
    different adaptive optimization method, leading to largely reduced memory as well.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 已经提出了几种优化器来减少内存占用。例如，Adafactor（[Shazeer等人2018](https://arxiv.org/abs/1804.04235)）不像Adam那样存储完整的动量和变化，而是仅跟踪移动平均值的每行和每列总和，然后基于这些总和估计第二时刻。SM3（[Anil等人2019](https://arxiv.org/abs/1901.11150)）描述了一种不同的自适应优化方法，也大大减少了内存。
- en: '*ZeRO* (*Zero Redundancy Optimizer*; [Rajbhandari et al. 2019](https://arxiv.org/abs/1910.02054))
    optimizes the memory used for training large models based on the observation about
    two major memory consumption of large model training:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '*ZeRO*（零冗余优化器；[Rajbhandari等人2019](https://arxiv.org/abs/1910.02054)）根据对大型模型训练的两个主要内存消耗的观察进行了优化：'
- en: The majority is occupied by *model states*, including optimizer states (e.g.
    Adam momentums and variances), gradients and parameters. Mixed-precision training
    demands a lot of memory since the optimizer needs to keep a copy of FP32 parameters
    and other optimizer states, besides the FP16 version.
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 大部分被*模型状态*占据，包括优化器状态（例如Adam动量和方差）、梯度和参数。混合精度训练需要大量内存，因为优化器需要保留FP32参数和其他优化器状态的副本，除了FP16版本。
- en: The remaining is consumed by activations, temporary buffers and unusable fragmented
    memory (named *residual states* in the paper).
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 剩余部分被激活、临时缓冲区和无法使用的碎片化内存（在论文中称为*残余状态*）消耗。
- en: ZeRO combines two approaches, *ZeRO-DP* and *ZeRO-R*. ZeRO-DP is an enhanced
    data parallelism to avoid simple redundancy over model states. It partitions optimizer
    state, gradients and parameters across multiple data parallel processes via a
    dynamic communication schedule to minimize the communication volume. ZeRO-R optimizes
    the memory consumption of residual states, using partitioned activation recomputation,
    constant buffer size and on-the-fly memory defragmentation.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: ZeRO结合了两种方法，*ZeRO-DP*和*ZeRO-R*。ZeRO-DP是一种增强的数据并行ism，以避免模型状态上的简单冗余。它通过动态通信时间表将优化器状态、梯度和参数分区到多个数据并行进程中，以最小化通信量。ZeRO-R通过使用分区激活重计算、恒定缓冲区大小和即时内存碎片整理来优化残余状态的内存消耗。
- en: Citation
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引用
- en: 'Cited as:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 引用为：
- en: Weng, Lilian. (Sep 2021). How to train really large models on many GPUs? Lil’Log.
    https://lilianweng.github.io/posts/2021-09-25-train-large/.
  id: totrans-138
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Weng，Lilian。 （2021年9月）。 如何在许多GPU上训练非常大的模型？ Lil’Log。 https://lilianweng.github.io/posts/2021-09-25-train-large/。
- en: Or
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 或
- en: '[PRE0]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: References
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Li et al. [“PyTorch Distributed: Experiences on Accelerating Data Parallel
    Training”](https://arxiv.org/abs/2006.15704) VLDB 2020.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Li等人。[“PyTorch分布式：加速数据并行训练的经验”](https://arxiv.org/abs/2006.15704) VLDB
    2020。'
- en: '[2] Cui et al. [“GeePS: Scalable deep learning on distributed GPUs with a GPU-specialized
    parameter server”](https://www.pdl.cmu.edu/PDL-FTP/CloudComputing/GeePS-cui-eurosys16.pdf)
    EuroSys 2016'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Cui等人。[“GeePS: 在分布式GPU上进行可扩展的深度学习，带有GPU专用参数服务器”](https://www.pdl.cmu.edu/PDL-FTP/CloudComputing/GeePS-cui-eurosys16.pdf)
    EuroSys 2016。'
- en: '[3] Shoeybi et al. [“Megatron-LM: Training Multi-Billion Parameter Language
    Models Using Model Parallelism.”](https://arxiv.org/abs/1909.08053) arXiv preprint
    arXiv:1909.08053 (2019).'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Shoeybi等人。[“Megatron-LM：使用模型并行ism训练多十亿参数语言模型。”](https://arxiv.org/abs/1909.08053)
    arXiv预印本arXiv:1909.08053（2019年）。'
- en: '[4] Narayanan et al. [“Efficient Large-Scale Language Model Training on GPU
    Clusters Using Megatron-LM.”](https://arxiv.org/abs/2104.04473) arXiv preprint
    arXiv:2104.04473 (2021).'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Narayanan等人。[“在GPU集群上使用Megatron-LM进行高效的大规模语言模型训练。”](https://arxiv.org/abs/2104.04473)
    arXiv预印本arXiv:2104.04473（2021年）。'
- en: '[5] Huang et al. [“GPipe: Efficient Training of Giant Neural Networks using
    Pipeline Parallelism.”](https://arxiv.org/abs/1811.06965) arXiv preprint arXiv:1811.06965
    (2018).'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Huang等人。[“GPipe: 使用管道并行性高效训练巨型神经网络。”](https://arxiv.org/abs/1811.06965)
    arXiv预印本arXiv:1811.06965（2018年）。'
- en: '[6] Narayanan et al. [“PipeDream: Generalized Pipeline Parallelism for DNN
    Training.”](https://cs.stanford.edu/~matei/papers/2019/sosp_pipedream.pdf) SOSP
    2019.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Narayanan等人。[“PipeDream: 用于DNN训练的广义管道并行性。”](https://cs.stanford.edu/~matei/papers/2019/sosp_pipedream.pdf)
    SOSP 2019。'
- en: '[7] Narayanan et al. [“Memory-Efficient Pipeline-Parallel DNN Training.”](https://arxiv.org/abs/2006.09503)
    ICML 2021.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] Narayanan等人。[“内存高效的管道并行DNN训练。”](https://arxiv.org/abs/2006.09503) ICML
    2021。'
- en: '[8] Shazeer et al. [“The Sparsely-Gated Mixture-of-Experts Layer Noam.”](https://arxiv.org/abs/1701.06538)
    arXiv preprint arXiv:1701.06538 (2017).'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] Shazeer等人。[“稀疏门控专家混合层Noam。”](https://arxiv.org/abs/1701.06538) arXiv预印本arXiv:1701.06538（2017年）。'
- en: '[9] Lepikhin et al. [“GShard: Scaling Giant Models with Conditional Computation
    and Automatic Sharding.”](https://arxiv.org/abs/2006.16668) arXiv preprint arXiv:2006.16668
    (2020).'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] Lepikhin等人。[“GShard：使用条件计算和自动分片扩展巨型模型。”](https://arxiv.org/abs/2006.16668)
    arXiv预印本arXiv:2006.16668（2020年）。'
- en: '[10] Fedus et al. [“Switch Transformers: Scaling to Trillion Parameter Models
    with Simple and Efficient Sparsity.”](https://arxiv.org/abs/2101.03961) arXiv
    preprint arXiv:2101.03961 (2021).'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] Fedus等人。[“Switch Transformers：使用简单高效的稀疏性扩展到万亿参数模型。”](https://arxiv.org/abs/2101.03961)
    arXiv预印本arXiv:2101.03961（2021年）。'
- en: '[11] Narang & Micikevicius, et al. [“Mixed precision training.”](https://arxiv.org/abs/1710.03740)
    ICLR 2018.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] Narang & Micikevicius等人。[“混合精度训练。”](https://arxiv.org/abs/1710.03740)
    ICLR 2018。'
- en: '[12] Chen et al. 2016 [“Training Deep Nets with Sublinear Memory Cost.”](https://arxiv.org/abs/1604.06174)
    arXiv preprint arXiv:1604.06174 (2016).'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '[12] Chen等人。2016年[“使用次线性内存成本训练深度网络。”](https://arxiv.org/abs/1604.06174) arXiv预印本arXiv:1604.06174（2016年）。'
- en: '[13] Jain et al. [“Gist: Efficient data encoding for deep neural network training.”](https://www.microsoft.com/en-us/research/uploads/prod/2018/04/fiddle-gist-isca18.pdf)
    ISCA 2018.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '[13] Jain等人。[“Gist：用于深度神经网络训练的高效数据编码。”](https://www.microsoft.com/en-us/research/uploads/prod/2018/04/fiddle-gist-isca18.pdf)
    ISCA 2018。'
- en: '[14] Shazeer & Stern. [“Adafactor: Adaptive learning rates with sublinear memory
    cost.”](https://arxiv.org/abs/1804.04235) arXiv preprint arXiv:1804.04235 (2018).'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '[14] Shazeer & Stern。[“Adafactor：具有次线性内存成本的自适应学习率。”](https://arxiv.org/abs/1804.04235)
    arXiv预印本arXiv:1804.04235（2018年）。'
- en: '[15] Anil et al. [“Memory-Efficient Adaptive Optimization.”](https://arxiv.org/abs/1901.11150)
    arXiv preprint arXiv:1901.11150 (2019).'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '[15] Anil等人。[“内存高效的自适应优化。”](https://arxiv.org/abs/1901.11150) arXiv预印本arXiv:1901.11150（2019年）。'
- en: '[16] Rajbhandari et al. [“ZeRO: Memory Optimization Towards Training A Trillion
    Parameter Models Samyam.”](https://arxiv.org/abs/1910.02054) arXiv preprint arXiv:1910.02054
    (2019).'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '[16] Rajbhandari等人。[“ZeRO：面向训练万亿参数模型的内存优化Samyam。”](https://arxiv.org/abs/1910.02054)
    arXiv预印本arXiv:1910.02054（2019年）。'
- en: '[17] Zhou et al. [“Mixture-of-Experts with Expert Choice Routing”](https://arxiv.org/abs/2202.09368)
    arXiv preprint arXiv:2202.09368 (2022).'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '[17] Zhou等人。[“具有专家选择路由的专家混合模型”](https://arxiv.org/abs/2202.09368) arXiv预印本arXiv:2202.09368（2022年）。'
