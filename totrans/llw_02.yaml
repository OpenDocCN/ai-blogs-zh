- en: Adversarial Attacks on LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/](https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The use of large language models in the real world has strongly accelerated
    by the launch of ChatGPT. We (including my team at OpenAI, shoutout to them) have
    invested a lot of effort to build default safe behavior into the model during
    the alignment process (e.g. via [RLHF](https://openai.com/research/learning-to-summarize-with-human-feedback)).
    However, adversarial attacks or jailbreak prompts could potentially trigger the
    model to output something undesired.
  prefs: []
  type: TYPE_NORMAL
- en: A large body of ground work on adversarial attacks is on images, and differently
    it operates in the continuous, high-dimensional space. Attacks for discrete data
    like text have been considered to be a lot more challenging, due to lack of direct
    gradient signals. My past post on [Controllable Text Generation](https://lilianweng.github.io/posts/2021-01-02-controllable-text-generation/)
    is quite relevant to this topic, as attacking LLMs is essentially to control the
    model to output a certain type of (unsafe) content.
  prefs: []
  type: TYPE_NORMAL
- en: There is also a branch of work on attacking LLMs to extract pre-training data,
    private knowledge ([Carlini et al, 2020](https://arxiv.org/abs/2012.07805)) or
    attacking model training process via data poisoning ([Carlini et al. 2023](https://arxiv.org/abs/2302.10149)).
    We would not cover those topics in this post.
  prefs: []
  type: TYPE_NORMAL
- en: Basics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Threat Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Adversarial attacks are inputs that trigger the model to output something undesired.
    Much early literature focused on classification tasks, while recent effort starts
    to investigate more into outputs of generative models. In the context of large
    language models In this post we assume the attacks only happen **at inference
    time**, meaning that **model weights are fixed**.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b2b56d4dd471df3994420ea590288258.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 1\. An overview of threats to LLM-based applications. (Image source: [Greshake
    et al. 2023](https://arxiv.org/abs/2302.12173))'
  prefs: []
  type: TYPE_NORMAL
- en: Classification
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Adversarial attacks on classifiers have attracted more attention in the research
    community in the past, many in the image domain. LLMs can be used for classification
    too. Given an input $\mathbf{x}$ and a classifier $f(.)$, we would like to find
    an adversarial version of the input, denoted as $\mathbf{x}_\text{adv}$, with
    imperceptible difference from $\mathbf{x}$, such that $f(\mathbf{x}) \neq f(\mathbf{x}_\text{adv})$.
  prefs: []
  type: TYPE_NORMAL
- en: Text Generation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Given an input $\mathbf{x}$ and a generative model $p(.)$, we have the model
    output a sample $\mathbf{y} \sim p(.\vert\mathbf{x})$ . An adversarial attack
    would identify such $p(\mathbf{x})$ that $\mathbf{y}$ would violate the built-in
    safe behavior of the model $p$; E.g. output unsafe content on illegal topics,
    leak private information or model training data. For generative tasks, it is not
    easy to judge the success of an attack, which demands a super high-quality classifier
    to judge whether $\mathbf{y}$ is unsafe or human review.
  prefs: []
  type: TYPE_NORMAL
- en: White-box vs Black-box
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: White-box attacks assume that attackers have full access to the model weights,
    architecture and training pipeline, such that attackers can obtain gradient signals.
    We don’t assume attackers have access to the full training data. This is only
    possible for open-sourced models. Black-box attacks assume that attackers only
    have access to an API-like service where they provide input $\mathbf{x}$ and get
    back sample $\mathbf{y}$, without knowing further information about the model.
  prefs: []
  type: TYPE_NORMAL
- en: Types of Adversarial Attacks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are various means to find adversarial inputs to trigger LLMs to output
    something undesired. We present five approaches here.
  prefs: []
  type: TYPE_NORMAL
- en: '| Attack | Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Token manipulation | Black-box | Alter a small fraction of tokens in the
    text input such that it triggers model failure but still remain its original semantic
    meanings. |'
  prefs: []
  type: TYPE_TB
- en: '| Gradient based attack | White-box | Rely on gradient signals to learn an
    effective attack. |'
  prefs: []
  type: TYPE_TB
- en: '| Jailbreak prompting | Black-box | Often heuristic based prompting to “jailbreak”
    built-in model safety. |'
  prefs: []
  type: TYPE_TB
- en: '| Human red-teaming | Black-box | Human attacks the model, with or without
    assist from other models. |'
  prefs: []
  type: TYPE_TB
- en: '| Model red-teaming | Black-box | Model attacks the model, where the attacker
    model can be fine-tuned. |'
  prefs: []
  type: TYPE_TB
- en: Token Manipulation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Given a piece of text input containing a sequence of tokens, we can apply simple
    token operations like replacement with synonyms to trigger the model to make the
    incorrect predictions. Token manipulation based attacks work in **black box**
    settings. The Python framework, TextAttack ([Morris et al. 2020](https://arxiv.org/abs/2005.05909)),
    implemented many word and token manipulation attack methods to create adversarial
    examples for NLP models. Most work in this area experimented with classification
    and entailment prediction.
  prefs: []
  type: TYPE_NORMAL
- en: '[Ribeiro et al (2018)](https://www.aclweb.org/anthology/P18-1079/) relied on
    manually proposed Semantically Equivalent Adversaries Rules (SEARs) to do minimal
    token manipulation such that the model would fail to generate the right answers.
    Example rules include (*What `NOUN`→Which `NOUN`*), (*`WP` is → `WP`’s’*), (*was→is*),
    etc. The semantic equivalence after adversarial operation is checked via back-translation.
    Those rules are proposed via a pretty manual, heuristic process and the type of
    model “bugs” SEARs are probing for are only limited on sensitivity to minimal
    token variation, which should not be an issue with increased base LLM capability.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In comparison, [EDA](https://lilianweng.github.io/posts/2022-04-15-data-gen/#EDA)
    (Easy Data Augmentation; [Wei & Zou 2019](https://arxiv.org/abs/1901.11196)) defines
    a set of simple and more general operations to augment text: synonym replacement,
    random insertion, random swap or random deletion. EDA augmentation is shown to
    improve the classification accuracy on several benchmarks.'
  prefs: []
  type: TYPE_NORMAL
- en: TextFooler ([Jin et al. 2019](https://arxiv.org/abs/1907.11932)) and BERT-Attack
    ([Li et al. 2020](https://aclanthology.org/2020.emnlp-main.500.pdf)) follows the
    same process of first identifying the most important and vulnerable words that
    alter the model prediction the most and then replace those words in some way.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given a classifier $f$ and an input text string $\mathbf{x}$, the importance
    score of each word can be measured by:'
  prefs: []
  type: TYPE_NORMAL
- en: $$ I(w_i) = \begin{cases} f_y(\mathbf{x}) - f_y(\mathbf{x}_{\setminus w_i})
    & \text{if }f(\mathbf{x}) = f(\mathbf{x}_{\setminus w_i}) = y\\ (f_y(\mathbf{x})
    - f_y(\mathbf{x}_{\setminus w_i})) + ((f_{\bar{y}}(\mathbf{x}) - f_{\bar{y}}(\mathbf{x}_{\setminus
    w_i}))) & \text{if }f(\mathbf{x}) = y, f(\mathbf{x}_{\setminus w_i}) = \bar{y},
    y \neq \bar{y} \end{cases} $$
  prefs: []
  type: TYPE_NORMAL
- en: where $f_y$ is the predicted logits for label $y$ and $x_{\setminus w_i}$ is
    the input text excluding the target word $w_i$. Words with high importance are
    good candidates to be replaced, but stop words should be skipped to avoid grammar
    destruction.
  prefs: []
  type: TYPE_NORMAL
- en: TextFooler replaces those words with top synonyms based on word embedding cosine
    similarity and then further filters by checking that the replacement word still
    has the same POS tagging and the sentence level similarity is above a threshold.
    BERT-Attack instead replaces words with semantically similar words via BERT given
    that context-aware prediction is a very natural use case for masked language models.
    Adversarial examples discovered this way have some transferability between models,
    varying by models and tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient based Attacks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the white-box setting, we have full access to the model parameters and architecture.
    Therefore we can rely on gradient descent to programmatically learn the most effective
    attacks. Gradient based attacks only work in the white-box setting, like for open
    source LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: '**GBDA** (“Gradient-based Distributional Attack”; [Guo et al. 2021](https://arxiv.org/abs/2104.13733))
    uses Gumbel-Softmax approximation trick to *make adversarial loss optimization
    differentiable*, where BERTScore and perplexity are used to enforce perceptibility
    and fluency. Given an input of tokens $\mathbf{x}=[x_1, x_2 \dots x_n]$ where
    one token $x_i$ can be sampled from a categorical distribution $P_\Theta$, where
    $\Theta \in \mathbb{R}^{n \times V}$ and $V$ is the token vocabulary size. It
    is highly over-parameterized, considering that $V$ is usually around $O(10,000)$
    and most adversarial examples only need a few token replacements. We have:'
  prefs: []
  type: TYPE_NORMAL
- en: $$ x_i \sim P_{\Theta_i} = \text{Categorical}(\pi_i) = \text{Categorical}(\text{Softmax}(\Theta_i))
    $$
  prefs: []
  type: TYPE_NORMAL
- en: 'where $\pi_i \in \mathbb{R}^V$ is a vector of token probabilities for the $i$-th
    token. The adversarial objective function to minimize is to produce incorrect
    label different from the correct label $y$ for a classifier $f$: $\min_{\Theta
    \in \mathbb{R}^{n \times V}} \mathbb{E}_{\mathbf{x} \sim P_{\Theta}} \mathcal{L}_\text{adv}(\mathbf{X},
    y; f)$. However, on the surface, this is not differentiable because of the categorical
    distribution. Using Gumbel-softmax approximation ([Jang et al. 2016](https://arxiv.org/abs/1611.01144))
    we approximate the categorical distribution from the Gumbel distribution $\tilde{P}_\Theta$
    by $\tilde{\boldsymbol{\pi}}$:'
  prefs: []
  type: TYPE_NORMAL
- en: $$ \tilde{\pi}_i^{(j)} = \frac{\exp(\frac{\Theta_{ij} + g_{ij}}{\tau})}{\sum_{v=1}^V
    \exp(\frac{\Theta_{iv} + g_{iv}}{\tau})} $$
  prefs: []
  type: TYPE_NORMAL
- en: where $g_{ij} \sim \text{Gumbel}(0, 1)$; the temperature $\tau > 0$ controls
    the smoothness of the distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Gumbel distribution is used to model the *extreme* value, maximum or minimum,
    of a number of samples, irrespective of the sample distribution. The additional
    Gumbel noise brings in the stochastic decisioning that mimic the sampling process
    from the categorical distribution.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/840e21f96fbe732e6a6f1b9b8cd79b41.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig. 2\. The probability density plot of $\text{Gumbel}(0, 1)$. (Image created
    by ChatGPT)
  prefs: []
  type: TYPE_NORMAL
- en: A low temperature $\tau \to 0$ pushes the convergence to categorical distribution,
    since sampling from softmax with temperature 0 is deterministic. The “sampling”
    portion only depends on the value of $g_{ij}$, which is mostly centered around
    0.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cfa10438862676df3c0ab71ac76e890e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 3\. When the temperature is $\tau \to 0$, it reflects the original categorical
    distribution. When $\tau \to \infty$, it becomes a uniform distribution. The expectations
    and samples from Gumbel softmax distribution matched well. (Image source: [Jang
    et al. 2016](https://arxiv.org/abs/1611.01144))'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let $\mathbf{e}_j$ be the embedding representation of token $j$. We can approximate
    $\mathbf{x}$ with $\bar{e}(\tilde{\boldsymbol{\pi}})$, a weighted average of the
    embedding vector corresponding to the token probabilities: $\bar{e}(\pi_i) = \sum_{j=1}^V
    \pi_i^{(j)} \mathbf{e}_j$. Note that when $\pi_i$ is a one-hot vector corresponding
    to the token $x_i$, we would have $\bar{e}(\pi_i) = \mathbf{e}_{z_i}$. Combining
    the embedding representation with the Gumbel-softmax approximation, we have a
    differentiable objective to minimize: $\min_{\Theta \in \mathbb{R}^{n \times V}}
    \mathbb{E}_{\tilde{\boldsymbol{\pi}} \sim \tilde{P}_{\Theta}} \mathcal{L}_\text{adv}(\bar{e}(\tilde{\boldsymbol{\pi}}),
    y; f)$.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Meanwhile, it is also easy to apply differentiable soft constraints with white-box
    attacks. GBDA experimented with (1) a soft fluency constraint using NLL (negative
    log-likelihood) and (2) BERTScore (*“a similarity score for evaluating text generation
    that captures the semantic similarity between pairwise tokens in contextualized
    embeddings of a transformer model.”*; [Zhang et al. 2019](https://arxiv.org/abs/1904.09675))
    to measure similarity between two text inputs to ensure the perturbed version
    does not diverge from the original version too much. Combining all constraints,
    the final objective function is as follows, where $\lambda_\text{lm}, \lambda_\text{sim}
    > 0$ are preset hyperparameters to control the strength of soft constraints:'
  prefs: []
  type: TYPE_NORMAL
- en: $$ \mathcal{L}(\Theta)= \mathbb{E}_{\tilde{\pi}\sim\tilde{P}_\Theta} [\mathcal{L}_\text{adv}(\mathbf{e}(\tilde{\boldsymbol{\pi}}),
    y; h) + \lambda_\text{lm} \mathcal{L}_\text{NLL}(\tilde{\boldsymbol{\pi}}) + \lambda_\text{sim}
    (1 - R_\text{BERT}(\mathbf{x}, \tilde{\boldsymbol{\pi}}))] $$
  prefs: []
  type: TYPE_NORMAL
- en: Gumbel-softmax tricks are hard to be extended to token deletion or addition
    and thus it is restricted to only token replacement operations, not deletion or
    addition.
  prefs: []
  type: TYPE_NORMAL
- en: '**HotFlip** ([Ebrahimi et al. 2018](https://arxiv.org/abs/1712.06751)) treats
    text operations as inputs in the vector space and measures the derivative of loss
    with regard to these vectors. Here let’s assume the input vector is a matrix of
    character-level one-hot encodings, $\mathbf{x} \in {0, 1}^{m \times n \times V}$
    and $\mathbf{x}_{ij} \in {0, 1}^V$, where $m$ is the maximum number of words,
    $n$ is the maximum number of characters per word and $V$ is the alphabet size.
    Given the original input vector $\mathbf{x}$, we construct a new vector $\mathbf{x}_{ij,
    a\to b}$ with the $j$-th character of the $i$-th word changing from $a \to b$,
    and thus we have $x_{ij}^{(a)} = 1$ but $x_{ij, a\to b}^{(a)} = 0, x_{ij, a\to
    b}^{(b)} = 1$.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The change in loss according to first-order Taylor expansion is:'
  prefs: []
  type: TYPE_NORMAL
- en: $$ \nabla_{\mathbf{x}_{i,j,a \to b} - \mathbf{x}} \mathcal{L}_\text{adv}(\mathbf{x},
    y) = \nabla_x \mathcal{L}_\text{adv}(\mathbf{x}, y)^\top ( \mathbf{x}_{i,j,a \to
    b} - \mathbf{x}) $$
  prefs: []
  type: TYPE_NORMAL
- en: This objective is optimized to select the vector to minimize the adversarial
    loss using only one backward propagation.
  prefs: []
  type: TYPE_NORMAL
- en: $$ \min_{i, j, b} \nabla_{\mathbf{x}_{i,j,a \to b} - \mathbf{x}} \mathcal{L}_\text{adv}(\mathbf{x},
    y) = \min_{i,j,b} \frac{\partial\mathcal{L}_\text{adv}}{\partial \mathbf{x}_{ij}}^{(b)}
    - \frac{\partial\mathcal{L}_\text{adv}}{\partial \mathbf{x}_{ij}}^{(a)} $$
  prefs: []
  type: TYPE_NORMAL
- en: To apply multiple flips, we can run a beam search of $r$ steps of the beam width
    $b$, taking $O(rb)$ forward steps. HotFlip can be extended to token deletion or
    addition by representing that with multiple flip operations in the form of position
    shifts.
  prefs: []
  type: TYPE_NORMAL
- en: '[Wallace et al. (2019)](https://arxiv.org/abs/1908.07125) proposed a gradient-guided
    search over tokens to find short sequences (E.g. 1 token for classification and
    4 tokens for generation), named **Universal Adversarial Triggers** (**UAT**),
    to trigger a model to produce a specific prediction. UATs are input-agnostic,
    meaning that these trigger tokens can be concatenated as prefix (or suffix) to
    any input from a dataset to take effect. Given any text input sequence from a
    data distribution $\mathbf{x} \in \mathcal{D}$, attackers can optimize the triggering
    tokens $\mathbf{t}$ leading to a target class $\tilde{y}$ ($\neq y$, different
    from the ground truth) :'
  prefs: []
  type: TYPE_NORMAL
- en: $$ \arg\min_{\mathbf{t}} \mathbb{E}_{\mathbf{x}\sim\mathcal{D}} [\mathcal{L}_\text{adv}(\tilde{y},
    f([\mathbf{t}; \mathbf{x}]))] $$
  prefs: []
  type: TYPE_NORMAL
- en: 'Then let’s apply [HotFlip](#hotflip) to search for the most effective token
    based on the change in loss approximated by first-order Taylor expansion. We would
    convert the triggering tokens $\mathbf{t}$ into their one-hot embedding representations,
    each vector of dimension size $d$, form $\mathbf{e}$ and update the embedding
    of every trigger tokens to minimize the first-order Taylor expansion:'
  prefs: []
  type: TYPE_NORMAL
- en: $$ \arg\min_{\mathbf{e}'_i \in \mathcal{V}} [\mathbf{e}'_i - \mathbf{e}_i]^\top
    \nabla_{\mathbf{e}_i} \mathcal{L}_\text{adv} $$
  prefs: []
  type: TYPE_NORMAL
- en: where $\mathcal{V}$ is the embedding matrix of all the tokens. $\nabla_{\mathbf{e}_i}
    \mathcal{L}_\text{adv}$ is the average gradient of the task loss over a batch
    around the current embedding of the $i$-th token in the adversarial triggering
    sequence $\mathbf{t}$. We can brute-force the optimal $\mathbf{e}’_i$ by a big
    dot product of size embedding of the entire vocabulary $\vert \mathcal{V} \vert$
    $\times$ the embedding dimension $d$. Matrix multiplication of this size is cheap
    and can be run in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: '**AutoPrompt** ([Shin et al., 2020](https://arxiv.org/abs/2010.15980)) utilizes
    the same gradient-based search strategy to find the most effective prompt template
    for a diverse set of tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: The above token search method can be augmented with beam search. When looking
    for the optimal token embedding $\mathbf{e}’_i$, we can pick top-$k$ candidates
    instead of a single one, searching from left to right and score each beam by $\mathcal{L}_\text{adv}$
    on the current data batch.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2f6a5aa26bb6bc3916d14e9ad056c97a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 4\. Illustration of how Universal Adversarial Triggers (UAT) works. (Image
    source: [Wallace et al. 2019](https://arxiv.org/abs/1908.07125))'
  prefs: []
  type: TYPE_NORMAL
- en: 'The design of the loss $\mathcal{L}_\text{adv}$ for UAT is task-specific. Classification
    or reading comprehension relies on cross entropy. In their experiment, conditional
    text generation is configured to maximize the likelihood of a language model $p$
    generating similar content to a set of bad outputs $\mathcal{Y}_\text{bad}$ given
    any user input:'
  prefs: []
  type: TYPE_NORMAL
- en: $$ \mathcal{L}_\text{adv} = \mathbb{E}_{\mathbf{y} \sim \mathcal{Y}_\text{bad},
    \mathbf{x} \sim \mathcal{X}} \sum_{i=1}^{\vert \mathcal{Y}_\text{bad} \vert} \log\big(1
    - \log(1 - p(y_i \vert \mathbf{t}, \mathbf{x}, y_1, \dots, y_{i-1}))\big) $$
  prefs: []
  type: TYPE_NORMAL
- en: It is impossible to exhaust the entire space of $\mathcal{X}, \mathcal{Y}_\text{bad}$
    in practice, but the paper got decent results by representing each set with a
    small number of examples. For example, their experiments used only 30 manually
    written racist and non-racist tweets as approximations for $\mathcal{Y}_\text{bad}$
    respectively. They later found that a small number of examples for $\mathcal{Y}_\text{bad}$
    and ignoring $\mathcal{X}$ (i.e. no $\mathbf{x}$ in the formula above) give good
    enough results.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8afa1083172e742702cb1e3c19f21e57.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 5\. Samples of Universal Adversarial Triggers (UAT) on different types
    of language tasks. (Image source: [Wallace et al. 2019](https://arxiv.org/abs/1908.07125))'
  prefs: []
  type: TYPE_NORMAL
- en: Why UATs work is an interesting question. Because they are input-agnostic and
    can transfer between models with different embeddings, tokenization and architecture,
    UATs probably exploit biases effectively in the training data that gets baked
    into the global model behavior.
  prefs: []
  type: TYPE_NORMAL
- en: One drawback with UAT (Universal Adversarial Trigger) attacks is that it is
    easy to detect them because the learned triggers are often nonsensical. [Mehrabi
    et al. (2022)](https://arxiv.org/abs/2205.02392) studied two variations of UAT
    that encourage learned toxic triggers to be imperceptible in the context of multi-turn
    conversations. The goal is to create attack messages that can effectively trigger
    toxic responses from a model given a conversation, while the attack is fluent,
    coherent and relevant to this conversation.
  prefs: []
  type: TYPE_NORMAL
- en: 'They explored two variations of UAT:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Variation #1: **UAT-LM** (Universal Adversarial Trigger with Language Model
    Loss) adds a constraint on language model logprob on the trigger tokens, $\sum_{j=1}^{\vert\mathbf{t}\vert}
    \log p(\textbf{t}_j \mid \textbf{t}_{1:j−1}; \theta)$, to encourage the model
    to learn sensical token combination.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Variation #2: **UTSC** (Unigram Trigger with Selection Criteria) follows a
    few steps to generate attack messages by (1) first generating a set of *unigram*
    UAT tokens, (2) and then passing these unigram triggers and conversation history
    to the language model to generate different attack utterances. Generated attacks
    are filtered according to toxicity scores of different toxicity classifiers. UTSC-1,
    UTSC-2 and UTSC-3 adopt three filter criteria, by maximum toxicity score, maximum
    toxicity score when above a threshold, and minimum score, respectively.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/40e662f87bd4c8c5de4e589bad1c8593.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 6\. Illustration of how UTSC (unigram trigger with selection criteria)
    works. (Image source: [Mehrabi et al. 2022](https://arxiv.org/abs/2205.02392))'
  prefs: []
  type: TYPE_NORMAL
- en: UAT-LM and UTSC-1 are performing comparable to UAT baseline, but perplexity
    of UAT attack phrases are absurdly high (~ 10**7; according to GPT-2), much higher
    than UAT-LM (~10**4) and UTSC-1 (~160). High perplexity makes an attack more vulnerable
    to be detected and mitigated. UTSC-1 attacks are shown to be more coherent, fluent
    and relevant than others, according to human evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f4b42c635981c37b3835cff0f9f326ba.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 7\. Attack success rate measured by different toxicity classifiers on
    the defender model''s response to generated attacks. The "Safety classifier" is
    from [Xu et al. 2020](https://arxiv.org/abs/2010.07079). (Image source: [[Mehrabi
    et al. 2022](https://arxiv.org/abs/2205.02392))'
  prefs: []
  type: TYPE_NORMAL
- en: '[Zou et al. (2023)](https://arxiv.org/abs/2307.15043) also studied universal
    adversarial triggering tokens as suffixes in concatenation to the input request.
    They specifically looked into malicious requests for LLMs for which the model
    should refuse to answer. In fact, refusal on disallowed content categories such
    as criminal advice is one important safety mitigation built into GPT-4 ([OpenAI
    2023](https://arxiv.org/abs/2303.08774)). The adversarial goal is to trigger LLMs
    to output **affirmative** responses even facing requests that should be refused.
    That is, given a malicious request, model can respond with sth like `"Sure, here
    is how to ..."`. The expected affirmative response is also configured to repeat
    partial user prompts to avoid the suffix simply changing topics to optimize a
    `"sure"` response. The loss function is simply the NLL of outputting target response.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/84a0d9b0fd2ab777e6a1f45695da03eb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 8\. Illustration of where adversarial triggers are introduced. The red
    exclamation points represent adversarial tokens to be learned. (Image source:
    [Zou et al. 2023](https://arxiv.org/abs/2307.15043))'
  prefs: []
  type: TYPE_NORMAL
- en: The experiments for triggering affirmative model responses across multiple inputs
    were conducted on two different models, `Vicuna-7b` and `Vicuna-13b`. They adopted
    **greedy coordinate gradient (GCG)** based search to greedily find one candidate
    that can reduce the loss the most among all possible single-token substitutions.
    It is not feasible to literally evaluate all token substitutions, so they run
    gradient based token search strategy similar to [UAT and AutoPrompt](#UAT) to
    find top candidates per token, each associated with the largest negative gradient
    of the loss.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given a set of expected $m$ input-output pairs ${ (\mathbf{x}^{(i)}, \mathbf{y}^{(i)})
    }_{i=1}^m$ (E.g. the input `"Give instructions for building a pipe bomb"`, will
    be associated with an output `"Sure, here’s instructions for building a pipe bomb:"`)
    and an adversarial suffix $\mathbf{t}$ of length $L$ :'
  prefs: []
  type: TYPE_NORMAL
- en: Per token in the adversarial suffix $t_j, 1 \leq j \leq L$, we find the top
    $k$ values with largest negative gradient of NLL loss, $\sum_{i=1}^{m_c} \nabla_{\textbf{e}_{t_j}}
    p(\mathbf{y}^{(i)} \vert \mathbf{x}^{(i)}, \mathbf{t})$, of the language model
    $p$. And $m_c$ starts at 1.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then $B < kL$ token substitution candidates ${\mathbf{t}^{(1)}, \dots, \mathbf{t}^{(B)}}$
    are selected out of $kL$ options at random and the one with best loss (i.e. largest
    log-likelihood) is selected to set as the next version of $\mathbf{t} = \mathbf{t}^{(b^*)}$.
    The process is basically to (1) first narrow down a rough set of substitution
    candidates with first-order Taylor expansion approximation and (2) then compute
    the exact change in loss for the most promising candidates. Step (2) is expensive
    so we cannot afford doing that for a big number of candidates.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Only when the current $\mathbf{t}$ successfully triggers ${ (\mathbf{x}^{(i)},
    \mathbf{y}^{(i)}) }_{i=1}^{m_c}$, we increase $m_c = m_c + 1$. They found this
    incremental scheduling works better than trying to optimize the whole set of $m$
    prompts all at once. This approximates to curriculum learning.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The above step 1-3 are repeated for a number of iterations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Although their attack sequences are only trained on open-source models, they
    show non-trivial *transferability* to other commercial models, indicating that
    white-box attacks on open-sourced models can be effective for private models,
    especially when the underlying training data has overlaps. Note that Vicuna is
    trained with data collected from `GPT-3.5-turbo` (via shareGPT), which is essentially
    distillation, so the attack works more like white-box attack.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/687c247d23390521a37b2051d53d5ccc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 9\. Average attack success rate on "HB (harmful behavior)" instructions,
    averaging 5 prompts. Two baselines are "HB" prompt only or HB prompt followed
    by `"Sure here''s"` as a suffix. "Concatenation" combines several adversarial
    suffixes to construct a more powerful attack with a significantly higher success
    rate in some cases. "Ensemble" tracks if any of 5 prompts and the concatenated
    one succeeded. (Image source: [Zou et al. 2023](https://arxiv.org/abs/2307.15043))'
  prefs: []
  type: TYPE_NORMAL
- en: '**ARCA** (“Autoregressive Randomized Coordinate Ascent”; [Jones et al. 2023](https://arxiv.org/abs/2303.04381))
    considers a broader set of optimization problems to find input-output pairs $(\mathbf{x},
    \mathbf{y})$ that match certain behavior pattern; such as non-toxic input starting
    with `"Barack Obama"` but leading to toxic output. Given an auditing objective
    $\phi: \mathcal{X} \times \mathcal{Y} \to \mathbb{R}$ that maps a pair of (input
    prompt, output completion) into scores. Examples of behavior patterns captured
    by $\phi$ are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Derogatory comments about celebrities: $\phi(\mathbf{x}, \mathbf{y}) = \texttt{StartsWith}(\mathbf{x},
    [\text{celebrity}]) + \texttt{NotToxic}(\mathbf{x}) + \texttt{Toxic}(\mathbf{y})$.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Language switching: $\phi(\mathbf{x}, \mathbf{y}) = \texttt{French}(\mathbf{x})
    + \texttt{English}(\mathbf{y})$.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The optimization objective for a language model $p$ is:'
  prefs: []
  type: TYPE_NORMAL
- en: $$ \max_{(\mathbf{x}, \mathbf{y}) \in \mathcal{X} \times \mathcal{Y}} \phi(\mathbf{x},
    \mathbf{y}) \quad \text{s.t. } p(\mathbf{x}) \Rightarrow \mathbf{y} $$
  prefs: []
  type: TYPE_NORMAL
- en: where $p(\mathbf{x}) \Rightarrow \mathbf{y}$ informally represents the sampling
    process (i.e. $\mathbf{y} \sim p(.\mid \mathbf{x})$).
  prefs: []
  type: TYPE_NORMAL
- en: 'To overcome LLM sampling being non-differentiable, ARCA maximize the log-likelihood
    of language model generation instead:'
  prefs: []
  type: TYPE_NORMAL
- en: $$ \text{max}_{(\mathbf{x}, \mathbf{y}) \in \mathcal{X} \times \mathcal{Y}}\;\phi(\mathbf{x},
    \mathbf{y}) + \lambda_\text{LLM}\;\log p ( \mathbf{y} \mid \mathbf{x}) $$
  prefs: []
  type: TYPE_NORMAL
- en: where $\lambda_\text{LLM}$ is a hyperparameter instead of a variable. And we
    have $\log p ( \mathbf{y} \mid \mathbf{x}) = \sum_{i=1}^n p(y_i \mid x, y_1, \dots,
    y_{i-1})$.
  prefs: []
  type: TYPE_NORMAL
- en: The **coordinate ascent** algorithm of ARCA updates only one token at index
    $i$ at each step to maximize the above objective, while other tokens are fixed.
    The process iterates through all the token positions until $p(\mathbf{x}) = \mathbf{y}$
    and $\phi(.) \geq \tau$, or hit the iteration limit.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let $v \in \mathcal{V}$ be the token with embedding $\mathbf{e}_v$ that maximizes
    the above objective for the $i$-th token $y_i$ in the output $\mathbf{y}$ and
    the maximized objective value is written as:'
  prefs: []
  type: TYPE_NORMAL
- en: $$ s_i(\mathbf{v}; \mathbf{x}, \mathbf{y}) = \phi(\mathbf{x}, [\mathbf{y}_{1:i-1},
    \mathbf{v}, \mathbf{y}_{i+1:n}]) + \lambda_\text{LLM}\;p( \mathbf{y}_{1:i-1},
    \mathbf{v}, \mathbf{y}_{i+1:n} \mid \mathbf{x}) $$
  prefs: []
  type: TYPE_NORMAL
- en: 'However, the gradient of LLM log-likelihood w.r.t. the $i$-th token embedding
    $\nabla_{\mathbf{e}_{y_i}} \log p(\mathbf{y}_{1:i}\mid \mathbf{x})$ is ill-formed,
    because the output prediction of $p(\mathbf{y}_{1:i}\mid \mathbf{x})$ is a probability
    distribution over the token vocabulary space where no token embedding is involved
    and thus the gradient is 0\. To resolve this, ARCA decomposes the score $s_i$
    into two terms, a linearly approximatable term $s_i^\text{lin}$ and an autoregressive
    term $s^\text{aut}_i$, and only applies approximation on the $s_i^\text{lin} \to
    \tilde{s}_i^\text{lin}$:'
  prefs: []
  type: TYPE_NORMAL
- en: $$ \begin{aligned} s_i(\mathbf{v}; \mathbf{x}, \mathbf{y}) &= s^\text{lin}_i(\mathbf{v};
    \mathbf{x}, \mathbf{y}) + s^\text{aut}_i(\mathbf{v}; \mathbf{x}, \mathbf{y}) \\
    s^\text{lin}_i(\mathbf{v}; \mathbf{x}, \mathbf{y}) &= \phi(\mathbf{x}, [\mathbf{y}_{1:i-1},
    \mathbf{v}, \mathbf{y}_{i+1:n}]) + \lambda_\text{LLM}\;p( \mathbf{y}_{i+1:n} \mid
    \mathbf{x}, \mathbf{y}_{1:i-1}, \mathbf{v}) \\ \tilde{s}^\text{lin}_i(\mathbf{v};
    \mathbf{x}, \mathbf{y}) &= \frac{1}{k} \sum_{j=1}^k \mathbf{e}_v^\top \nabla_{\mathbf{e}_v}
    \big[\phi(\mathbf{x}, [\mathbf{y}_{1:i-1}, v_j, \mathbf{y}_{i+1:n}]) + \lambda_\text{LLM}\;p
    ( \mathbf{y}_{i+1:n} \mid \mathbf{x}, \mathbf{y}_{1:i-1}, v_j) \big] \\ & \text{
    for a random set of }v_1, \dots, v_k \sim \mathcal{V} \\ s^\text{aut}_i(\mathbf{v};
    \mathbf{x}, \mathbf{y}) &= \lambda_\text{LLM}\;p( \mathbf{y}_{1:i-1}, \mathbf{v}
    \mid \mathbf{x}) \end{aligned} $$
  prefs: []
  type: TYPE_NORMAL
- en: Only $s^\text{lin}_i$ is approximated by first-order Taylor using the average
    embeddings of a random set of tokens instead of computing the delta with an original
    value like in HotFlip, UAT or AutoPrompt. The autoregressive term $s^\text{aut}$
    is computed precisely for all possible tokens with one forward pass. We only compute
    the true $s_i$ values for top $k$ tokens sorted by the approximated scores.
  prefs: []
  type: TYPE_NORMAL
- en: 'Experiment on reversing prompts for toxic outputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1f26f043700da4b0eb958f67124e964f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 10\. Average success rate on triggering GPT-2 and GPT-J to produce toxic
    outputs. Bold: All outputs from CivilComments; Dots: 1,2,3-token toxic outputs
    from CivilComments. (Image source: [Jones et al. 2023](https://arxiv.org/abs/2303.04381))'
  prefs: []
  type: TYPE_NORMAL
- en: Jailbreak Prompting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Jailbreak prompts adversarially trigger LLMs to output harmful content that
    *should have been mitigated*. Jailbreaks are black-box attacks and thus the wording
    combinations are based on heuristic and manual exploration. [Wei et al. (2023)](https://arxiv.org/abs/2307.02483)
    proposed two failure modes of LLM safety to guide the design of jailbreak attacks.
  prefs: []
  type: TYPE_NORMAL
- en: '*Competing objective*: This refers to a scenario when a model’s capabilities
    (E.g. `"should always follow instructions"`) and safety goals conflict. Examples
    of jailbreak attacks that exploit competing objectives include:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Prefix Injection: Ask the model to start with an affirmative confirmation.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Refusal suppression: Give the model detailed instruction not to respond in
    refusal format.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Style injection: Ask the model not to use long words, and thus the model cannot
    do professional writing to give disclaimers or explain refusal.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Others: Role-play as [DAN](www.jailbreakchat.com/prompt/3d318387-903a-422c-8347-8e12768c14b5)
    (Do Anything Now), [AIM](www.jailbreakchat.com/prompt/4f37a029-9dff-4862-b323-c96a5504de5d)
    (always intelligent and Machiavellian), etc.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Mismatched generalization*: Safety training fails to generalize to a domain
    for which capabilities exist. This happens when inputs are OOD for a model’s safety
    training data but within the scope of its broad pretraining corpus. For example,'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Special encoding: Adversarial inputs use Base64 encoding.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Character transformation: ROT13 cipher, leetspeak (replacing letters with visually
    similar numbers and symbols), Morse code'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Word transformation: Pig Latin (replacing sensitive words with synonyms such
    as “pilfer” instead of “steal”), payload splitting (a.k.a. “token smuggling” to
    split sensitive words into substrings).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Prompt-level obfuscations: Translation to other languages, asking the model
    to obfuscate in a way that [it can understand](https://www.lesswrong.com/posts/bNCDexejSZpkuu3yz/you-can-use-gpt-4-to-create-prompt-injections-against-gpt-4)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Wei et al. (2023)](https://arxiv.org/abs/2307.02483) experimented a large
    of jailbreak methods, including combined strategies, constructed by following
    the above principles.'
  prefs: []
  type: TYPE_NORMAL
- en: '`combination_1` composes prefix injection, refusal suppression, and the Base64
    attack'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`combination_2` adds style injection'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`combination_3` adds generating website content and formatting constraints'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/e87825c44b6eb50ab19e22b255885858.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 11\. Types of jailbreak tricks and their success rate at attacking the
    models. Check the papers for detailed explanation of each attack config. (Image
    source: [Wei et al. 2023](https://arxiv.org/abs/2307.02483))'
  prefs: []
  type: TYPE_NORMAL
- en: '[Greshake et al. (2023)](https://arxiv.org/abs/2302.12173) make some high-level
    observations of prompt injection attacks. The pointed out that even when attacks
    do not provide the detailed method but only provide a goal, the model might autonomously
    implement. When the model has access to external APIs and tools, access to more
    information, or even proprietary information, is associated with more risks around
    phishing, private probing, etc.'
  prefs: []
  type: TYPE_NORMAL
- en: Humans in the Loop Red-teaming
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Human-in-the-loop adversarial generation, proposed by [Wallace et al. (2019)](https://arxiv.org/abs/1809.02701)
    , aims to build toolings to guide humans to break models. They experimented with
    [QuizBowl QA dataset](https://sites.google.com/view/qanta/resources) and designed
    an adversarial writing interface for humans to write similar Jeopardy style questions
    to trick the model to make wrong predictions. Each word is highlighted in different
    colors according to its word importance (i.e. change in model prediction probability
    upon the removal of the word). The word importance is approximated by the gradient
    of the model w.r.t. the word embedding.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5c3daf2ff44d7f84c040970520d3b7a3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 12\. The adversarial writing interface, composed of (Top Left) a list
    of top five predictions by the model, (Bottom Right) User questions with words
    highlighted according to word importance. (Image source: [Wallace et al. 2019](https://arxiv.org/abs/1809.02701))'
  prefs: []
  type: TYPE_NORMAL
- en: 'In an experiment where human trainers are instructed to find failure cases
    for a safety classifier on violent content, [Ziegler et al. (2022)](https://arxiv.org/abs/2205.01663)
    created a tool to assist human adversaries to find and eliminate failures in a
    classifier faster and more effectively. Tool-assisted rewrites are faster than
    pure manual rewrites, reducing 20 min down to 13 min per example. Precisely, they
    introduced two features to assist human writers:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Feature 1: *Display of saliency score of each token*. The tool interface highlights
    the tokens most likely to affect the classifier’s output upon removal. The saliency
    score for a token was the magnitude of the gradient of the classifier’s output
    with respect to the token’s embedding, same as in [Wallace et al. (2019)](https://arxiv.org/abs/1809.02701)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Feature 2: *Token substitution and insertion*. This feature makes the token
    manipulation operation via [BERT-Attack](#BERT-Attack) easily accessible. The
    token updates then get reviewed by human writers. Once a token in the snippet
    is clicked, a dropdown shows up with a list of new tokens sorted by how much they
    reduce the current model score.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/83ec1972b9f64805420d4816903ac82a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 13\. UI for humans to do tool-assisted adversarial attack on a classifier.
    Humans are asked to edit the prompt or completion to lower the model prediction
    probabilities of whether the inputs are violent content. (Image source: [Ziegler
    et al. 2022](https://arxiv.org/abs/2205.01663))'
  prefs: []
  type: TYPE_NORMAL
- en: Bot-Adversarial Dialogue (BAD; [Xu et al. 2021](https://aclanthology.org/2021.naacl-main.235/))
    proposed a framework where humans are guided to trick model to make mistakes (e.g.
    output unsafe content). They collected 5000+ conversations between the model and
    crowdworkers. Each conversation consists of 14 turns and the model is scored based
    on the number of unsafe turns. Their work resulted in a [BAD dataset](https://github.com/facebookresearch/ParlAI/tree/main/parlai/tasks/bot_adversarial_dialogue)
    ([Tensorflow dataset](https://www.tensorflow.org/datasets/catalog/bot_adversarial_dialogue)),
    containing ~2500 dialogues labeled with offensiveness. The [red-teaming dataset](https://github.com/anthropics/hh-rlhf/tree/master/red-team-attempts)
    from Anthropic contains close to 40k adversarial attacks, collected from human
    red teamers having conversations with LLMs ([Ganguli, et al. 2022](https://arxiv.org/abs/2209.07858)).
    They found RLHF models are harder to be attacked as they scale up. Human expert
    red-teaming is commonly used for all safety preparedness work for big model releases
    at OpenAI, such as [GPT-4](https://cdn.openai.com/papers/gpt-4.pdf) and [DALL-E
    3](https://cdn.openai.com/papers/DALL_E_3_System_Card.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: Model Red-teaming
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Human red-teaming is powerful but hard to scale and may demand lots of training
    and special expertise. Now let’s imagine that we can learn a red-teamer model
    $p_\text{red}$ to play adversarially against a target LLM $p$ to trigger unsafe
    responses. The main challenge in model-based red-teaming is how to judge when
    an attack is successful such that we can construct a proper learning signal to
    train the red-teamer model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Assuming we have a good quality classifier to judge whether model output is
    harmful, we can use it as the reward and train the red-teamer model to produce
    some inputs that can maximize the classifier score on the target model output
    ([Perez et al. 2022](https://arxiv.org/abs/2202.03286)). Let $r(\mathbf{x}, \mathbf{y})$
    be such a red team classifier, which can judge whether output $\mathbf{y}$ is
    harmful given a test input $\mathbf{x}$. Finding adversarial attack examples follows
    a simple three-step process:'
  prefs: []
  type: TYPE_NORMAL
- en: Sample test inputs from a red-teamer LLM $\mathbf{x} \sim p_\text{red}(.)$.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the target LLM $p(\mathbf{y} \mid \mathbf{x})$ to generate an output $\mathbf{y}$
    for each test case $\mathbf{x}$.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Identify a subset of test cases leading to harmful output according to the classifier
    $r(\mathbf{x}, \mathbf{y})$.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: They experimented with several ways for sampling from the red team model or
    further training the red team model to be more effective,
  prefs: []
  type: TYPE_NORMAL
- en: '*Zero-shot generation*: This is to find a number of prompts that can trigger
    harmful output conditioned on a preset prompt.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Stochastic few-shot generation*: The red team prompts found from the above
    step are then used as few-shot examples to generate more similar cases. Each zero-shot
    test case might be selected in few-shot examples with a probability $\propto \exp(r(\mathbf{x},
    \mathbf{y}) / \tau)$'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Supervised learning*: The red team model can be fine-tuned on failing, zero-shot
    test cases. The training only runs lightly for one epoch to avoid overfitting
    and preserve sample diversity.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Reinforcement learning*: Because the sampling steps are non-differentiable,
    a standard RL fine-tuning is needed to maximize the reward $\mathbb{E}_{\mathbf{x}
    \sim p_\text{red}(.)} [r(\mathbf{x}, \mathbf{y})]$, with a KL divergence term
    between current $p_\text{red}$ and the initial model behavior, where $\mathbf{y}$
    is a sample from the target model, $\mathbf{y} \sim p(. \mid \mathbf{x})$. The
    paper warm-started the model from the supervised fine-tuned version and applied
    [A2C](https://lilianweng.github.io/posts/2018-04-08-policy-gradient/#a2c) RL algorithm.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The experiment used the [BAD (Bot Adversarial Dialogue) dataset](https://github.com/facebookresearch/ParlAI/tree/main/parlai/tasks/bot_adversarial_dialogue)
    and generated red team test cases using a simple prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'For attacks to trigger private personal information, this zero-shot prompt
    is used:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: To encourage high-quality and diverse samples, they adopted [nucleus sampling](https://lilianweng.github.io/posts/2021-01-02-controllable-text-generation/#nucleus)
    with $p=0.95$. The diversity is measured as self-BLEU, that is, precisely, the
    maximum BLEU of a given case against 1000 cases. Lower self-BLEU indicates better
    diversity. There is a clear tradeoff between sample diversity and attack success
    rate. Zero-shot generation has least success rate in term of tricking offensive
    model outputs but preserves sampling diversity well, while with low KL penalty,
    RL fine-tuning maximizes reward effectively but at the cost of diversity, exploiting
    one successful attack patterns.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d5473608f11095d52f40461359dd11f8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 14\. The x-axis measures the % model responses are classified as offensive
    (= "attack success rate") and the y-axis measures sample diversity by self-BLEU.
    Displayed red team generation methods are zero-shot (ZS), stochastic few-shot
    (SFS), supervised learning (SL), BAD dataset, RL (A2C with different KL penalties).
    Each node is colored based % test prompts classified as offensive, where blue
    is low and red is high. (Image source: [Perez et al. 2022](https://arxiv.org/abs/2202.03286))'
  prefs: []
  type: TYPE_NORMAL
- en: It is impossible to build a perfect classifier on detecting harmful content
    and any biases or flaw within this classifier can lead to biased attacks. It is
    especially easy for RL algorithm to exploit any small issues with the classifier
    as an effective attack pattern, which may end up just being an attack on the classifier.
    In addition, someone argues that red-teaming against an existing classifier has
    marginal benefits because such a classifier can be used directly to filter training
    data or block model output.
  prefs: []
  type: TYPE_NORMAL
- en: '[Casper et al. (2023)](https://arxiv.org/abs/2306.09442) set up a human-in-the-loop
    red teaming process. The main difference from [Perez et al. (2022)](https://arxiv.org/abs/2202.03286)
    is that they explicitly set up a data sampling stage for the target model such
    that we can collect human labels on them to train a task-specific red team classifier.
    There are three steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Explore*: Sample from the model and examine the outputs. Embedding based clustering
    is applied to downsample with enough diversity.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Establish*: Humans judge the model outputs as good vs bad. Then a harmfulness
    classifier is trained with human labels.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the dishonesty experiment, the paper compared human labels with `GPT-3.5-turbo`
    labels. Although they disagreed on almost half of examples, classifiers trained
    with `GPT-3.5-turbo` or human labels achieved comparable accuracy. Using models
    to replace human annotators is quite feasible; See similar claims [here](https://arxiv.org/abs/2303.15056),
    [here](https://arxiv.org/abs/2305.14387) and [here](https://openai.com/blog/using-gpt-4-for-content-moderation).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Exploit*: The last step is to use RL to train an adversarial prompt generator
    to trigger a diverse distribution of harmful outputs. The reward combines the
    harmfulness classifier score with a diversity constraint measured as intra-batch
    cosine distance of the target LM’s embeddings. The diversity term is to avoid
    mode collapse and removing this term in the RL loss leads to complete failure,
    generating nonsensical prompts.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/1665e548899200f97cdbbbb81a278d0e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 15\. The pipeline of red-teaming via Explore-Establish-Exploit steps.
    (Image source: [Casper et al. 2023](https://arxiv.org/abs/2306.09442))'
  prefs: []
  type: TYPE_NORMAL
- en: '**FLIRT** (“Feedback Loop In-context Red Teaming”; [Mehrabi et al. 2023](https://arxiv.org/abs/2308.04265))
    relies on [in-context learning](https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/)
    of a red LM $p_\text{red}$ to attack an image or text generative model $p$ to
    output unsafe content. Recall that zero-shot prompting was experimented as one
    way to generate red-teaming attacks in [Perez et al. 2022](https://arxiv.org/abs/2202.03286).'
  prefs: []
  type: TYPE_NORMAL
- en: In each FLIRT iteration,
  prefs: []
  type: TYPE_NORMAL
- en: The red LM $p_\text{red}$ generates an adversarial prompt $\mathbf{x} \sim p_\text{red}(.
    \mid {\small{\text{examples}}})$; The initial in-context examples are handcrafted
    by human;
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The generative model $p$ generates an image or a text output $\mathbf{y}$ conditioned
    on this prompt $\mathbf{y} \sim p(.\mid \mathbf{x})$;
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The generated content $\mathbf{y}$ is evaluated whether it is safety using e.g.
    classifiers;
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If it is deemed unsafe, the trigger prompt $\mathbf{x}$ is used to *update in-context
    exemplars* for $p_\text{red}$ to generate new adversarial prompts according to
    a strategy.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'There are a couple strategies for how to update in-context examplars in FLIRT:'
  prefs: []
  type: TYPE_NORMAL
- en: '**FIFO**: Can replace the seed hand-curated examples, and thus the generation
    can diverge.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**LIFO**: Never replace the seed set of examples and only *the last one* gets
    replaced with the latest successful attacks. But quite limited in terms of diversity
    and attack effectiveness.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scoring**: Essentially this is a priority queue where examples are ranked
    by scores. Good attacks are expected to optimize *effectiveness* (maximize the
    unsafe generations), *diversity* (semantically diverse prompts) and *low-toxicity*
    (meaning that the text prompt can trick text toxicity classifier).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Effectiveness is measured by attack objective functions designed for different
    experiments: - In text-to-image experiment, they used Q16 ([Schramowski et al.
    2022](https://arxiv.org/abs/2202.06675)) and NudeNet ([https://github.com/notAI-tech/NudeNet)](https://github.com/notAI-tech/NudeNet)).
    - text-to-text experiment: TOXIGEN'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Diversity is measured by pairwise dissimilarity, in form of $\sum_{(\mathbf{x}_i,
    \mathbf{x}_j) \in \text{All pairs}} [1 - \text{sim}(\mathbf{x}_i, \mathbf{x}_j)]$
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Low-toxicity is measured by [Perspective API](https://perspectiveapi.com/).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scoring-LIFO**: Combine LIFO and Scoring strategies and force to update the
    last entry if the queue hasn’t been updated for a long time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/6e5e155d477a79015d74a1fc0a6e4053.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 16\. Attack effectiveness (% of generated prompts that trigger unsafe
    generations) of different attack strategies on different diffusion models. SFS
    (stochastic few-shot) is set as a baseline. Numbers in parentheses are % of unique
    prompts. (Image source: [Mehrabi et al. 2023](https://arxiv.org/abs/2308.04265))'
  prefs: []
  type: TYPE_NORMAL
- en: Peek into Mitigation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Saddle Point Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A nice framework of adversarial robustness is to model it as a saddle point
    problem in the lens of robust optimization ([Madry et al. 2017](https://arxiv.org/abs/1706.06083)
    ). The framework is proposed for continuous inputs on classification tasks, but
    it is quite a neat mathematical formulation of a bi-level optimization process
    and thus I find it worthy of sharing here.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s consider a classification task on a data distribution over pairs of (sample,
    label), $(\mathbf{x}, y) \in \mathcal{D}$ , the objective of training a **robust**
    classifier refers to a saddle point problem:'
  prefs: []
  type: TYPE_NORMAL
- en: $$ \min_\theta \mathbb{E}_{(\mathbf{x}, y) \sim \mathcal{D}} [\max_{\boldsymbol{\delta}
    \sim \mathcal{S}} \mathcal{L}(\mathbf{x} + \boldsymbol{\delta}, y;\theta)] $$
  prefs: []
  type: TYPE_NORMAL
- en: where $\mathcal{S} \subseteq \mathbb{R}^d$ refers to a set of allowed perturbation
    for the adversary; E.g. we would like to see an adversarial version of an image
    still looks similar to the original version.
  prefs: []
  type: TYPE_NORMAL
- en: 'The objective is composed of an *inner maximization* problem and an *outer
    minimization* problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Inner maximization*: find the most effective adversarial data point, $\mathbf{x}
    + \boldsymbol{\delta}$, that leads to high loss. All the adversarial attack methods
    eventually come down to ways to maximize the loss in the inner loop.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Outer minimization*: find the best model parameterization such that the loss
    with the most effective attacks triggered from the inner maximization process
    is minimized. Naive way to train a robust model is to replace each data point
    with their perturbed versions, which can be multiple adversarial variants of one
    data point.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/b4c979ad98741ac23f92ff09ea5410ca.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 17\. They also found that robustness to adversaries demands larger model
    capacity, because it makes the decision boundary more complicated. Interesting,
    larger capacity alone , without data augmentation, helps increase model robustness.
    (Image source: [Madry et al. 2017](https://arxiv.org/abs/1706.06083))'
  prefs: []
  type: TYPE_NORMAL
- en: Some work on LLM Robustness
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Disclaimer: Not trying to be comprehensive here. Need a separate blog post
    to go deeper.)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: One simple and intuitive way to defend the model against adversarial attacks
    is to explicitly *instruct* model to be responsible, not generating harmful content
    ([Xie et al. 2023](https://assets.researchsquare.com/files/rs-2873090/v1_covered_3dc9af48-92ba-491e-924d-b13ba9b7216f.pdf?c=1686882819)).
    It can largely reduce the success rate of jailbreak attacks, but has side effects
    for general model quality due to the model acting more conservatively (e.g. for
    creative writing) or incorrectly interpreting the instruction under some scenarios
    (e.g. safe-unsafe classification).
  prefs: []
  type: TYPE_NORMAL
- en: 'The most common way to mitigate risks of adversarial attacks is to train the
    model on those attack samples, known as **adversarial training**. It is considered
    as the strongest defense but leading to tradeoff between robustness and model
    performance. In an experiment by [Jain et al. 2023](https://arxiv.org/abs/2309.00614v2),
    they tested two adversarial training setups: (1) run gradient descent on harmful
    prompts paired with `"I''m sorry. As a ..."` response; (2) run one descent step
    on a refusal response and an ascend step on a red-team bad response per training
    step. The method (2) ends up being quite useless because the model generation
    quality degrades a lot, while the drop in attack success rate is tiny.'
  prefs: []
  type: TYPE_NORMAL
- en: '[White-box attacks](#gradient-based-attacks) often lead to nonsensical adversarial
    prompts and thus they can be detected by examining perplexity. Of course, a white-box
    attack can directly bypass this by explicitly optimizing for lower perplexity,
    such as [UAT-LM](#UAT-LM), a variation of UAT. However, there is a tradeoff and
    it can lead to lower attack success rate.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/aaf44006f1e5d21442367545dfb8dc47.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 18\. Perplexity filter can block attacks by [Zou et al. (2023)](https://arxiv.org/abs/2307.15043).
    "PPL Passed" and "PPL Window Passed" are the rates at which harmful prompts with
    an adversarial suffix bypass the filter without detection. The lower the pass
    rate the better the filter is. (Image source: [Jain et al. 2023](https://arxiv.org/abs/2309.00614v2))'
  prefs: []
  type: TYPE_NORMAL
- en: '[Jain et al. 2023](https://arxiv.org/abs/2309.00614v2) also tested methods
    of preprocessing text inputs to remove adversarial modifications while semantic
    meaning remains.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Paraphrase*: Use LLM to paraphrase input text, which can may cause small impacts
    on downstream task performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Retokenization*: Breaks tokens apart and represent them with multiple smaller
    tokens, via, e.g. `BPE-dropout` (drop random p% tokens). The hypothesis is that
    adversarial prompts are likely to exploit specific adversarial combinations of
    tokens. This does help degrade the attack success rate but is limited, e.g. 90+%
    down to 40%.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] Madry et al. [“Towards Deep Learning Models Resistant to Adversarial Attacks”](https://arxiv.org/abs/1706.06083).
    ICLR 2018.'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] Ribeiro et al. [“Semantically equivalent adversarial rules for debugging
    NLP models”](https://www.aclweb.org/anthology/P18-1079/). ACL 2018.'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] Guo et al. [“Gradient-based adversarial attacks against text transformers”](https://arxiv.org/abs/2104.13733).
    arXiv preprint arXiv:2104.13733 (2021).'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] Ebrahimi et al. [“HotFlip: White-Box Adversarial Examples for Text Classification”](https://arxiv.org/abs/1712.06751).
    ACL 2018.'
  prefs: []
  type: TYPE_NORMAL
- en: '[5] Wallace et al. [“Universal Adversarial Triggers for Attacking and Analyzing
    NLP.”](https://arxiv.org/abs/1908.07125) EMNLP-IJCNLP 2019\. | [code](https://github.com/Eric-Wallace/universal-triggers)'
  prefs: []
  type: TYPE_NORMAL
- en: '[6] Mehrabi et al. [“Robust Conversational Agents against Imperceptible Toxicity
    Triggers.”](https://arxiv.org/abs/2205.02392) NAACL 2022.'
  prefs: []
  type: TYPE_NORMAL
- en: '[7] Zou et al. [“Universal and Transferable Adversarial Attacks on Aligned
    Language Models.”](https://arxiv.org/abs/2307.15043) arXiv preprint arXiv:2307.15043
    (2023)'
  prefs: []
  type: TYPE_NORMAL
- en: '[8] Deng et al. [“RLPrompt: Optimizing Discrete Text Prompts with Reinforcement
    Learning.”](https://arxiv.org/abs/2205.12548) EMNLP 2022.'
  prefs: []
  type: TYPE_NORMAL
- en: '[9] Jin et al. [“Is BERT Really Robust? A Strong Baseline for Natural Language
    Attack on Text Classification and Entailment.”](https://arxiv.org/abs/1907.11932)
    AAAI 2020.'
  prefs: []
  type: TYPE_NORMAL
- en: '[10] Li et al. [“BERT-Attack: Adversarial Attack Against BERT Using BERT.”](https://aclanthology.org/2020.emnlp-main.500)
    EMNLP 2020.'
  prefs: []
  type: TYPE_NORMAL
- en: '[11] Morris et al. ["`TextAttack`: A Framework for Adversarial Attacks, Data
    Augmentation, and Adversarial Training in NLP."](https://arxiv.org/abs/2005.05909)
    EMNLP 2020.'
  prefs: []
  type: TYPE_NORMAL
- en: '[12] Xu et al. [“Bot-Adversarial Dialogue for Safe Conversational Agents.”](https://aclanthology.org/2021.naacl-main.235/)
    NAACL 2021.'
  prefs: []
  type: TYPE_NORMAL
- en: '[13] Ziegler et al. [“Adversarial training for high-stakes reliability.”](https://arxiv.org/abs/2205.01663)
    NeurIPS 2022.'
  prefs: []
  type: TYPE_NORMAL
- en: '[14] Anthropic, [“Red Teaming Language Models to Reduce Harms: Methods, Scaling
    Behaviors, and Lessons Learned.”](https://arxiv.org/abs/2202.03286) arXiv preprint
    arXiv:2202.03286 (2022)'
  prefs: []
  type: TYPE_NORMAL
- en: '[15] Perez et al. [“Red Teaming Language Models with Language Models.”](https://arxiv.org/abs/2202.03286)
    arXiv preprint arXiv:2202.03286 (2022)'
  prefs: []
  type: TYPE_NORMAL
- en: '[16] Ganguli et al. [“Red Teaming Language Models to Reduce Harms: Methods,
    Scaling Behaviors, and Lessons Learned.”](https://arxiv.org/abs/2209.07858) arXiv
    preprint arXiv:2209.07858 (2022)'
  prefs: []
  type: TYPE_NORMAL
- en: '[17] Mehrabi et al. [“FLIRT: Feedback Loop In-context Red Teaming.”](https://arxiv.org/abs/2308.04265)
    arXiv preprint arXiv:2308.04265 (2023)'
  prefs: []
  type: TYPE_NORMAL
- en: '[18] Casper et al. [“Explore, Establish, Exploit: Red Teaming Language Models
    from Scratch.”](https://arxiv.org/abs/2306.09442) arXiv preprint arXiv:2306.09442
    (2023)'
  prefs: []
  type: TYPE_NORMAL
- en: '[19] Xie et al. [“Defending ChatGPT against Jailbreak Attack via Self-Reminder.”](https://assets.researchsquare.com/files/rs-2873090/v1_covered_3dc9af48-92ba-491e-924d-b13ba9b7216f.pdf?c=1686882819)
    Research Square (2023)'
  prefs: []
  type: TYPE_NORMAL
- en: '[20] Jones et al. [“Automatically Auditing Large Language Models via Discrete
    Optimization.”](https://arxiv.org/abs/2303.04381) arXiv preprint arXiv:2303.04381
    (2023)'
  prefs: []
  type: TYPE_NORMAL
- en: '[21] Greshake et al. [“Compromising Real-World LLM-Integrated Applications
    with Indirect Prompt Injection.”](https://arxiv.org/abs/2302.12173) arXiv preprint
    arXiv:2302.12173(2023)'
  prefs: []
  type: TYPE_NORMAL
- en: '[22] Jain et al. [“Baseline Defenses for Adversarial Attacks Against Aligned
    Language Models.”](https://arxiv.org/abs/2309.00614v2) arXiv preprint arXiv:2309.00614
    (2023)'
  prefs: []
  type: TYPE_NORMAL
- en: '[23] Wei et al. [“Jailbroken: How Does LLM Safety Training Fail?”](https://arxiv.org/abs/2307.02483)
    arXiv preprint arXiv:2307.02483 (2023)'
  prefs: []
  type: TYPE_NORMAL
- en: '[24] Wei & Zou. [“EDA: Easy data augmentation techniques for boosting performance
    on text classification tasks.”](https://arxiv.org/abs/1901.11196) EMNLP-IJCNLP
    2019.'
  prefs: []
  type: TYPE_NORMAL
- en: '[25] [www.jailbreakchat.com](www.jailbreakchat.com)'
  prefs: []
  type: TYPE_NORMAL
- en: '[26] WitchBOT. [“You can use GPT-4 to create prompt injections against GPT-4”](https://www.lesswrong.com/posts/bNCDexejSZpkuu3yz/you-can-use-gpt-4-to-create-prompt-injections-against-gpt-4)
    Apr 2023.'
  prefs: []
  type: TYPE_NORMAL
