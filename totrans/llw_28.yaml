- en: 'Object Detection Part 4: Fast Detection Models'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://lilianweng.github.io/posts/2018-12-27-object-recognition-part-4/](https://lilianweng.github.io/posts/2018-12-27-object-recognition-part-4/)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In [Part 3](https://lilianweng.github.io/posts/2017-12-31-object-recognition-part-3/),
    we have reviewed models in the R-CNN family. All of them are region-based object
    detection algorithms. They can achieve high accuracy but could be too slow for
    certain applications such as autonomous driving. In Part 4, we only focus on fast
    object detection models, including SSD, RetinaNet, and models in the YOLO family.
  prefs: []
  type: TYPE_NORMAL
- en: 'Links to all the posts in the series: [[Part 1](https://lilianweng.github.io/posts/2017-10-29-object-recognition-part-1/)]
    [[Part 2](https://lilianweng.github.io/posts/2017-12-15-object-recognition-part-2/)]
    [[Part 3](https://lilianweng.github.io/posts/2017-12-31-object-recognition-part-3/)]
    [[Part 4](https://lilianweng.github.io/posts/2018-12-27-object-recognition-part-4/)].'
  prefs: []
  type: TYPE_NORMAL
- en: Two-stage vs One-stage Detectors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Models in the R-CNN family are all region-based. The detection happens in two
    stages: (1) First, the model proposes a set of regions of interests by select
    search or regional proposal network. The proposed regions are sparse as the potential
    bounding box candidates can be infinite. (2) Then a classifier only processes
    the region candidates.'
  prefs: []
  type: TYPE_NORMAL
- en: The other different approach skips the region proposal stage and runs detection
    directly over a dense sampling of possible locations. This is how a one-stage
    object detection algorithm works. This is faster and simpler, but might potentially
    drag down the performance a bit.
  prefs: []
  type: TYPE_NORMAL
- en: All the models introduced in this post are one-stage detectors.
  prefs: []
  type: TYPE_NORMAL
- en: 'YOLO: You Only Look Once'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **YOLO** model (**“You Only Look Once”**; [Redmon et al., 2016](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf))
    is the very first attempt at building a fast real-time object detector. Because
    YOLO does not undergo the region proposal step and only predicts over a limited
    number of bounding boxes, it is able to do inference super fast.
  prefs: []
  type: TYPE_NORMAL
- en: Workflow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Pre-train** a CNN network on image classification task.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Split an image into $S \times S$ cells. If an object’s center falls into a cell,
    that cell is “responsible” for detecting the existence of that object. Each cell
    predicts (a) the location of $B$ bounding boxes, (b) a confidence score, and (c)
    a probability of object class conditioned on the existence of an object in the
    bounding box.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The **coordinates** of bounding box are defined by a tuple of 4 values, (center
    x-coord, center y-coord, width, height) — $(x, y, w, h)$, where $x$ and $y$ are
    set to be offset of a cell location. Moreover, $x$, $y$, $w$ and $h$ are normalized
    by the image width and height, and thus all between (0, 1].
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A **confidence score** indicates the likelihood that the cell contains an object:
    `Pr(containing an object) x IoU(pred, truth)`; where `Pr` = probability and `IoU`
    = interaction under union.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If the cell contains an object, it predicts a **probability** of this object
    belonging to every class $C_i, i=1, \dots, K$: `Pr(the object belongs to the class
    C_i | containing an object)`. At this stage, the model only predicts one set of
    class probabilities per cell, regardless of the number of bounding boxes, $B$.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: In total, one image contains $S \times S \times B$ bounding boxes, each box
    corresponding to 4 location predictions, 1 confidence score, and K conditional
    probabilities for object classification. The total prediction values for one image
    is $S \times S \times (5B + K)$, which is the tensor shape of the final conv layer
    of the model.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The final layer of the pre-trained CNN is modified to output a prediction tensor
    of size $S \times S \times (5B + K)$.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/e2eb281fd51e55b215a29cb7d09fe3c0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 1\. The workflow of YOLO model. (Image source: [original paper](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf))'
  prefs: []
  type: TYPE_NORMAL
- en: Network Architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The base model is similar to [GoogLeNet](https://www.cs.unc.edu/~wliu/papers/GoogLeNet.pdf)
    with inception module replaced by 1x1 and 3x3 conv layers. The final prediction
    of shape $S \times S \times (5B + K)$ is produced by two fully connected layers
    over the whole conv feature map.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a94d308f05e47f644244bbab129b1a7e.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig. 2\. The network architecture of YOLO.
  prefs: []
  type: TYPE_NORMAL
- en: Loss Function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The loss consists of two parts, the *localization loss* for bounding box offset
    prediction and the *classification loss* for conditional class probabilities.
    Both parts are computed as the sum of squared errors. Two scale parameters are
    used to control how much we want to increase the loss from bounding box coordinate
    predictions ($\lambda_\text{coord}$) and how much we want to decrease the loss
    of confidence score predictions for boxes without objects ($\lambda_\text{noobj}$).
    Down-weighting the loss contributed by background boxes is important as most of
    the bounding boxes involve no instance. In the paper, the model sets $\lambda_\text{coord}
    = 5$ and $\lambda_\text{noobj} = 0.5$.
  prefs: []
  type: TYPE_NORMAL
- en: $$ \begin{aligned} \mathcal{L}_\text{loc} &= \lambda_\text{coord} \sum_{i=0}^{S^2}
    \sum_{j=0}^B \mathbb{1}_{ij}^\text{obj} [(x_i - \hat{x}_i)^2 + (y_i - \hat{y}_i)^2
    + (\sqrt{w_i} - \sqrt{\hat{w}_i})^2 + (\sqrt{h_i} - \sqrt{\hat{h}_i})^2 ] \\ \mathcal{L}_\text{cls}
    &= \sum_{i=0}^{S^2} \sum_{j=0}^B \big( \mathbb{1}_{ij}^\text{obj} + \lambda_\text{noobj}
    (1 - \mathbb{1}_{ij}^\text{obj})\big) (C_{ij} - \hat{C}_{ij})^2 + \sum_{i=0}^{S^2}
    \sum_{c \in \mathcal{C}} \mathbb{1}_i^\text{obj} (p_i(c) - \hat{p}_i(c))^2\\ \mathcal{L}
    &= \mathcal{L}_\text{loc} + \mathcal{L}_\text{cls} \end{aligned} $$
  prefs: []
  type: TYPE_NORMAL
- en: 'NOTE: In the original YOLO paper, the loss function uses $C_i$ instead of $C_{ij}$
    as confidence score. I made the correction based on my own understanding, since
    every bounding box should have its own confidence score. Please kindly let me
    if you do not agree. Many thanks.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: where,
  prefs: []
  type: TYPE_NORMAL
- en: '$\mathbb{1}_i^\text{obj}$: An indicator function of whether the cell i contains
    an object.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '$\mathbb{1}_{ij}^\text{obj}$: It indicates whether the j-th bounding box of
    the cell i is “responsible” for the object prediction (see Fig. 3).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '$C_{ij}$: The confidence score of cell i, `Pr(containing an object) * IoU(pred,
    truth)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '$\hat{C}_{ij}$: The predicted confidence score.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '$\mathcal{C}$: The set of all classes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '$p_i(c)$: The conditional probability of whether cell i contains an object
    of class $c \in \mathcal{C}$.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '$\hat{p}_i(c)$: The predicted conditional class probability.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/6cd5297ca41572fd4f26d5411d0196a0.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig. 3\. At one location, in cell i, the model proposes B bounding box candidates
    and the one that has highest overlap with the ground truth is the "responsible"
    predictor.
  prefs: []
  type: TYPE_NORMAL
- en: The loss function only penalizes classification error if an object is present
    in that grid cell, $\mathbb{1}_i^\text{obj} = 1$. It also only penalizes bounding
    box coordinate error if that predictor is “responsible” for the ground truth box,
    $\mathbb{1}_{ij}^\text{obj} = 1$.
  prefs: []
  type: TYPE_NORMAL
- en: As a one-stage object detector, YOLO is super fast, but it is not good at recognizing
    irregularly shaped objects or a group of small objects due to a limited number
    of bounding box candidates.
  prefs: []
  type: TYPE_NORMAL
- en: 'SSD: Single Shot MultiBox Detector'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **Single Shot Detector** (**SSD**; [Liu et al, 2016](https://arxiv.org/abs/1512.02325))
    is one of the first attempts at using convolutional neural network’s pyramidal
    feature hierarchy for efficient detection of objects of various sizes.
  prefs: []
  type: TYPE_NORMAL
- en: Image Pyramid
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: SSD uses the [VGG-16](https://arxiv.org/abs/1409.1556) model pre-trained on
    ImageNet as its base model for extracting useful image features. On top of VGG16,
    SSD adds several conv feature layers of decreasing sizes. They can be seen as
    a *pyramid representation* of images at different scales. Intuitively large fine-grained
    feature maps at earlier levels are good at capturing small objects and small coarse-grained
    feature maps can detect large objects well. In SSD, the detection happens in every
    pyramidal layer, targeting at objects of various sizes.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/652ffe91e1c8de38b5aeb0387ded78c0.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig. 4\. The model architecture of SSD.
  prefs: []
  type: TYPE_NORMAL
- en: Workflow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Unlike YOLO, SSD does not split the image into grids of arbitrary size but predicts
    offset of predefined *anchor boxes* (this is called “default boxes” in the paper)
    for every location of the feature map. Each box has a fixed size and position
    relative to its corresponding cell. All the anchor boxes tile the whole feature
    map in a convolutional manner.
  prefs: []
  type: TYPE_NORMAL
- en: Feature maps at different levels have different receptive field sizes. The anchor
    boxes on different levels are rescaled so that one feature map is only responsible
    for objects at one particular scale. For example, in Fig. 5 the dog can only be
    detected in the 4x4 feature map (higher level) while the cat is just captured
    by the 8x8 feature map (lower level).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2c48c66b1f7a7353016d5979b54ec791.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 5\. The SSD framework. (a) The training data contains images and ground
    truth boxes for every object. (b) In a fine-grained feature maps (8 x 8), the
    anchor boxes of different aspect ratios correspond to smaller area of the raw
    input. (c) In a coarse-grained feature map (4 x 4), the anchor boxes cover larger
    area of the raw input. (Image source: [original paper](https://arxiv.org/abs/1512.02325))'
  prefs: []
  type: TYPE_NORMAL
- en: The width, height and the center location of an anchor box are all normalized
    to be (0, 1). At a location $(i, j)$ of the $\ell$-th feature layer of size $m
    \times n$, $i=1,\dots,n, j=1,\dots,m$, we have a unique linear scale proportional
    to the layer level and 5 different box aspect ratios (width-to-height ratios),
    in addition to a special scale (why we need this? the paper didn’t explain. maybe
    just a heuristic trick) when the aspect ratio is 1\. This gives us 6 anchor boxes
    in total per feature cell.
  prefs: []
  type: TYPE_NORMAL
- en: '$$ \begin{aligned} \text{level index: } &\ell = 1, \dots, L \\ \text{scale
    of boxes: } &s_\ell = s_\text{min} + \frac{s_\text{max} - s_\text{min}}{L - 1}
    (\ell - 1) \\ \text{aspect ratio: } &r \in \{1, 2, 3, 1/2, 1/3\}\\ \text{additional
    scale: } & s''_\ell = \sqrt{s_\ell s_{\ell + 1}} \text{ when } r = 1 \text{thus,
    6 boxes in total.}\\ \text{width: } &w_\ell^r = s_\ell \sqrt{r} \\ \text{height:
    } &h_\ell^r = s_\ell / \sqrt{r} \\ \text{center location: } & (x^i_\ell, y^j_\ell)
    = (\frac{i+0.5}{m}, \frac{j+0.5}{n}) \end{aligned} $$![](../Images/c83971b9eef5c4b682572971417840f4.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Fig. 6\. An example of how the anchor box size is scaled up with the layer index
    $\ell$ for $L=6, s\_\text{min} = 0.2, s\_\text{max} = 0.9$. Only the boxes of
    aspect ratio $r=1$ are illustrated.
  prefs: []
  type: TYPE_NORMAL
- en: At every location, the model outputs 4 offsets and $c$ class probabilities by
    applying a $3 \times 3 \times p$ conv filter (where $p$ is the number of channels
    in the feature map) for every one of $k$ anchor boxes. Therefore, given a feature
    map of size $m \times n$, we need $kmn(c+4)$ prediction filters.
  prefs: []
  type: TYPE_NORMAL
- en: Loss Function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Same as YOLO, the loss function is the sum of a localization loss and a classification
    loss.
  prefs: []
  type: TYPE_NORMAL
- en: $\mathcal{L} = \frac{1}{N}(\mathcal{L}_\text{cls} + \alpha \mathcal{L}_\text{loc})$
  prefs: []
  type: TYPE_NORMAL
- en: where $N$ is the number of matched bounding boxes and $\alpha$ balances the
    weights between two losses, picked by cross validation.
  prefs: []
  type: TYPE_NORMAL
- en: The *localization loss* is a [smooth L1 loss](https://github.com/rbgirshick/py-faster-rcnn/files/764206/SmoothL1Loss.1.pdf)
    between the predicted bounding box correction and the true values. The coordinate
    correction transformation is same as what [R-CNN](https://lilianweng.github.io/posts/2017-12-31-object-recognition-part-3/#r-cnn)
    does in [bounding box regression](https://lilianweng.github.io/posts/2017-12-31-object-recognition-part-3/#bounding-box-regression).
  prefs: []
  type: TYPE_NORMAL
- en: $$ \begin{aligned} \mathcal{L}_\text{loc} &= \sum_{i,j} \sum_{m\in\{x, y, w,
    h\}} \mathbb{1}_{ij}^\text{match} L_1^\text{smooth}(d_m^i - t_m^j)^2\\ L_1^\text{smooth}(x)
    &= \begin{cases} 0.5 x^2 & \text{if } \vert x \vert < 1\\ \vert x \vert - 0.5
    & \text{otherwise} \end{cases} \\ t^j_x &= (g^j_x - p^i_x) / p^i_w \\ t^j_y &=
    (g^j_y - p^i_y) / p^i_h \\ t^j_w &= \log(g^j_w / p^i_w) \\ t^j_h &= \log(g^j_h
    / p^i_h) \end{aligned} $$
  prefs: []
  type: TYPE_NORMAL
- en: where $\mathbb{1}_{ij}^\text{match}$ indicates whether the $i$-th bounding box
    with coordinates $(p^i_x, p^i_y, p^i_w, p^i_h)$ is matched to the $j$-th ground
    truth box with coordinates $(g^j_x, g^j_y, g^j_w, g^j_h)$ for any object. $d^i_m,
    m\in\{x, y, w, h\}$ are the predicted correction terms. See [this](https://lilianweng.github.io/posts/2017-12-31-object-recognition-part-3/#bounding-box-regression)
    for how the transformation works.
  prefs: []
  type: TYPE_NORMAL
- en: 'The *classification loss* is a softmax loss over multiple classes ([softmax_cross_entropy_with_logits](https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits)
    in tensorflow):'
  prefs: []
  type: TYPE_NORMAL
- en: $$ \mathcal{L}_\text{cls} = -\sum_{i \in \text{pos}} \mathbb{1}_{ij}^k \log(\hat{c}_i^k)
    - \sum_{i \in \text{neg}} \log(\hat{c}_i^0)\text{, where }\hat{c}_i^k = \text{softmax}(c_i^k)
    $$
  prefs: []
  type: TYPE_NORMAL
- en: 'where $\mathbb{1}_{ij}^k$ indicates whether the $i$-th bounding box and the
    $j$-th ground truth box are matched for an object in class $k$. $\text{pos}$ is
    the set of matched bounding boxes ($N$ items in total) and $\text{neg}$ is the
    set of negative examples. SSD uses [hard negative mining](https://lilianweng.github.io/posts/2017-12-31-object-recognition-part-3/#common-tricks)
    to select easily misclassified negative examples to construct this $\text{neg}$
    set: Once all the anchor boxes are sorted by objectiveness confidence score, the
    model picks the top candidates for training so that neg:pos is at most 3:1.'
  prefs: []
  type: TYPE_NORMAL
- en: YOLOv2 / YOLO9000
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**YOLOv2** ([Redmon & Farhadi, 2017](https://arxiv.org/abs/1612.08242)) is
    an enhanced version of YOLO. **YOLO9000** is built on top of YOLOv2 but trained
    with joint dataset combining the COCO detection dataset and the top 9000 classes
    from ImageNet.'
  prefs: []
  type: TYPE_NORMAL
- en: YOLOv2 Improvement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A variety of modifications are applied to make YOLO prediction more accurate
    and faster, including:'
  prefs: []
  type: TYPE_NORMAL
- en: '**1\. BatchNorm helps**: Add *batch norm* on all the convolutional layers,
    leading to significant improvement over convergence.'
  prefs: []
  type: TYPE_NORMAL
- en: '**2\. Image resolution matters**: Fine-tuning the base model with *high resolution*
    images improves the detection performance.'
  prefs: []
  type: TYPE_NORMAL
- en: '**3\. Convolutional anchor box detection**: Rather than predicts the bounding
    box position with fully-connected layers over the whole feature map, YOLOv2 uses
    *convolutional layers* to predict locations of *anchor boxes*, like in faster
    R-CNN. The prediction of spatial locations and class probabilities are decoupled.
    Overall, the change leads to a slight decrease in mAP, but an increase in recall.'
  prefs: []
  type: TYPE_NORMAL
- en: '**4\. K-mean clustering of box dimensions**: Different from faster R-CNN that
    uses hand-picked sizes of anchor boxes, YOLOv2 runs k-mean clustering on the training
    data to find good priors on anchor box dimensions. The distance metric is designed
    to *rely on IoU scores*:'
  prefs: []
  type: TYPE_NORMAL
- en: $$ \text{dist}(x, c_i) = 1 - \text{IoU}(x, c_i), i=1,\dots,k $$
  prefs: []
  type: TYPE_NORMAL
- en: where $x$ is a ground truth box candidate and $c_i$ is one of the centroids.
    The best number of centroids (anchor boxes) $k$ can be chosen by the [elbow method](https://en.wikipedia.org/wiki/Elbow_method_(clustering)).
  prefs: []
  type: TYPE_NORMAL
- en: The anchor boxes generated by clustering provide better average IoU conditioned
    on a fixed number of boxes.
  prefs: []
  type: TYPE_NORMAL
- en: '**5\. Direct location prediction**: YOLOv2 formulates the bounding box prediction
    in a way that it would *not diverge* from the center location too much. If the
    box location prediction can place the box in any part of the image, like in regional
    proposal network, the model training could become unstable.'
  prefs: []
  type: TYPE_NORMAL
- en: Given the anchor box of size $(p_w, p_h)$ at the grid cell with its top left
    corner at $(c_x, c_y)$, the model predicts the offset and the scale, $(t_x, t_y,
    t_w, t_h)$ and the corresponding predicted bounding box $b$ has center $(b_x,
    b_y)$ and size $(b_w, b_h)$. The confidence score is the sigmoid ($\sigma$) of
    another output $t_o$.
  prefs: []
  type: TYPE_NORMAL
- en: $$ \begin{aligned} b_x &= \sigma(t_x) + c_x\\ b_y &= \sigma(t_y) + c_y\\ b_w
    &= p_w e^{t_w}\\ b_h &= p_h e^{t_h}\\ \text{Pr}(\text{object}) &\cdot \text{IoU}(b,
    \text{object}) = \sigma(t_o) \end{aligned} $$![](../Images/91e23778f5ee93891da12b61a4a57b87.png)
  prefs: []
  type: TYPE_NORMAL
- en: 'Fig. 7\. YOLOv2 bounding box location prediction. (Image source: [original
    paper](https://arxiv.org/abs/1612.08242))'
  prefs: []
  type: TYPE_NORMAL
- en: '**6\. Add fine-grained features**: YOLOv2 adds a passthrough layer to bring
    *fine-grained features* from an earlier layer to the last output layer. The mechanism
    of this passthrough layer is similar to *identity mappings in ResNet* to extract
    higher-dimensional features from previous layers. This leads to 1% performance
    increase.'
  prefs: []
  type: TYPE_NORMAL
- en: '**7\. Multi-scale training**: In order to train the model to be robust to input
    images of different sizes, a *new size* of input dimension is *randomly sampled*
    every 10 batches. Since conv layers of YOLOv2 downsample the input dimension by
    a factor of 32, the newly sampled size is a multiple of 32.'
  prefs: []
  type: TYPE_NORMAL
- en: '**8\. Light-weighted base model**: To make prediction even faster, YOLOv2 adopts
    a light-weighted base model, DarkNet-19, which has 19 conv layers and 5 max-pooling
    layers. The key point is to insert avg poolings and 1x1 conv filters between 3x3
    conv layers.'
  prefs: []
  type: TYPE_NORMAL
- en: 'YOLO9000: Rich Dataset Training'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Because drawing bounding boxes on images for object detection is much more expensive
    than tagging images for classification, the paper proposed a way to combine small
    object detection dataset with large ImageNet so that the model can be exposed
    to a much larger number of object categories. The name of YOLO9000 comes from
    the top 9000 classes in ImageNet. During joint training, if an input image comes
    from the classification dataset, it only backpropagates the classification loss.
  prefs: []
  type: TYPE_NORMAL
- en: The detection dataset has much fewer and more general labels and, moreover,
    labels cross multiple datasets are often not mutually exclusive. For example,
    ImageNet has a label “Persian cat” while in COCO the same image would be labeled
    as “cat”. Without mutual exclusiveness, it does not make sense to apply softmax
    over all the classes.
  prefs: []
  type: TYPE_NORMAL
- en: In order to efficiently merge ImageNet labels (1000 classes, fine-grained) with
    COCO/PASCAL (< 100 classes, coarse-grained), YOLO9000 built a hierarchical tree
    structure with reference to [WordNet](https://wordnet.princeton.edu/) so that
    general labels are closer to the root and the fine-grained class labels are leaves.
    In this way, “cat” is the parent node of “Persian cat”.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3c13d804fb372ab68020163ff3ac5ca6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 8\. The WordTree hierarchy merges labels from COCO and ImageNet. Blue
    nodes are COCO labels and red nodes are ImageNet labels. (Image source: [original
    paper](https://arxiv.org/abs/1612.08242))'
  prefs: []
  type: TYPE_NORMAL
- en: 'To predict the probability of a class node, we can follow the path from the
    node to the root:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Note that `Pr(contain a "physical object")` is the confidence score, predicted
    separately in the bounding box detection pipeline. The path of conditional probability
    prediction can stop at any step, depending on which labels are available.
  prefs: []
  type: TYPE_NORMAL
- en: RetinaNet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **RetinaNet** ([Lin et al., 2018](https://arxiv.org/abs/1708.02002)) is
    a one-stage dense object detector. Two crucial building blocks are *featurized
    image pyramid* and the use of *focal loss*.
  prefs: []
  type: TYPE_NORMAL
- en: Focal Loss
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One issue for object detection model training is an extreme imbalance between
    background that contains no object and foreground that holds objects of interests.
    **Focal loss** is designed to assign more weights on hard, easily misclassified
    examples (i.e. background with noisy texture or partial object) and to down-weight
    easy examples (i.e. obviously empty background).
  prefs: []
  type: TYPE_NORMAL
- en: Starting with a normal cross entropy loss for binary classification,
  prefs: []
  type: TYPE_NORMAL
- en: $$ \text{CE}(p, y) = -y\log p - (1-y)\log(1-p) $$
  prefs: []
  type: TYPE_NORMAL
- en: where $y \in \{0, 1\}$ is a ground truth binary label, indicating whether a
    bounding box contains a object, and $p \in [0, 1]$ is the predicted probability
    of objectiveness (aka confidence score).
  prefs: []
  type: TYPE_NORMAL
- en: For notational convenience,
  prefs: []
  type: TYPE_NORMAL
- en: $$ \text{let } p_t = \begin{cases} p & \text{if } y = 1\\ 1-p & \text{otherwise}
    \end{cases}, \text{then } \text{CE}(p, y)=\text{CE}(p_t) = -\log p_t $$
  prefs: []
  type: TYPE_NORMAL
- en: Easily classified examples with large $p_t \gg 0.5$, that is, when $p$ is very
    close to 0 (when y=0) or 1 (when y=1), can incur a loss with non-trivial magnitude.
    Focal loss explicitly adds a weighting factor $(1-p_t)^\gamma, \gamma \geq 0$
    to each term in cross entropy so that the weight is small when $p_t$ is large
    and therefore easy examples are down-weighted.
  prefs: []
  type: TYPE_NORMAL
- en: $$ \text{FL}(p_t) = -(1-p_t)^\gamma \log p_t $$![](../Images/98a50b58a985bc86b9cffbdcfb01e4bc.png)
  prefs: []
  type: TYPE_NORMAL
- en: 'Fig. 9\. The focal loss focuses less on easy examples with a factor of $(1-p\_t)^\gamma$.
    (Image source: [original paper](https://arxiv.org/abs/1708.02002))'
  prefs: []
  type: TYPE_NORMAL
- en: For a better control of the shape of the weighting function (see Fig. 10.),
    RetinaNet uses an $\alpha$-balanced variant of the focal loss, where $\alpha=0.25,
    \gamma=2$ works the best.
  prefs: []
  type: TYPE_NORMAL
- en: $$ \text{FL}(p_t) = -\alpha (1-p_t)^\gamma \log p_t $$![](../Images/b490046e27975c8b2e9ac2325bea5b87.png)
  prefs: []
  type: TYPE_NORMAL
- en: Fig. 10\. The plot of focal loss weights $\alpha (1-p\_t)^\gamma$ as a function
    of $p\_t$, given different values of $\alpha$ and $\gamma$.
  prefs: []
  type: TYPE_NORMAL
- en: Featurized Image Pyramid
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **featurized image pyramid** ([Lin et al., 2017](https://arxiv.org/abs/1612.03144))
    is the backbone network for RetinaNet. Following the same approach by [image pyramid](#image-pyramid)
    in SSD, featurized image pyramids provide a basic vision component for object
    detection at different scales.
  prefs: []
  type: TYPE_NORMAL
- en: The key idea of feature pyramid network is demonstrated in Fig. 11\. The base
    structure contains a sequence of *pyramid levels*, each corresponding to one network
    *stage*. One stage contains multiple convolutional layers of the same size and
    the stage sizes are scaled down by a factor of 2\. Let’s denote the last layer
    of the $i$-th stage as $C_i$.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bffb8c6833265029ccb29c1f154e247e.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig. 11\. The illustration of the featurized image pyramid module. (Replot based
    on figure 3 in [FPN paper](https://arxiv.org/abs/1612.03144))
  prefs: []
  type: TYPE_NORMAL
- en: 'Two pathways connect conv layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Bottom-up pathway** is the normal feedforward computation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Top-down pathway** goes in the inverse direction, adding coarse but semantically
    stronger feature maps back into the previous pyramid levels of a larger size via
    lateral connections.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: First, the higher-level features are upsampled spatially coarser to be 2x larger.
    For image upscaling, the paper used nearest neighbor upsampling. While there are
    many [image upscaling algorithms](https://en.wikipedia.org/wiki/Image_scaling#Algorithms)
    such as using [deconv](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d_transpose),
    adopting another image scaling method might or might not improve the performance
    of RetinaNet.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The larger feature map undergoes a 1x1 conv layer to reduce the channel dimension.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, these two feature maps are merged by element-wise addition.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The lateral connections only happen at the last layer in stages, denoted as
    $\{C_i\}$, and the process continues until the finest (largest) merged feature
    map is generated. The prediction is made out of every merged map after a 3x3 conv
    layer, $\{P_i\}$.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'According to ablation studies, the importance rank of components of the featurized
    image pyramid design is as follows: **1x1 lateral connection** > detect object
    across multiple layers > top-down enrichment > pyramid representation (compared
    to only check the finest layer).'
  prefs: []
  type: TYPE_NORMAL
- en: Model Architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The featurized pyramid is constructed on top of the ResNet architecture. Recall
    that [ResNet](TBA) has 5 conv blocks (= network stages / pyramid levels). The
    last layer of the $i$-th pyramid level, $C_i$, has resolution $2^i$ lower than
    the raw input dimension.
  prefs: []
  type: TYPE_NORMAL
- en: 'RetinaNet utilizes feature pyramid levels $P_3$ to $P_7$:'
  prefs: []
  type: TYPE_NORMAL
- en: $P_3$ to $P_5$ are computed from the corresponding ResNet residual stage from
    $C_3$ to $C_5$. They are connected by both top-down and bottom-up pathways.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $P_6$ is obtained via a 3×3 stride-2 conv on top of $C_5$
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $P_7$ applies ReLU and a 3×3 stride-2 conv on $P_6$.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adding higher pyramid levels on ResNet improves the performance for detecting
    large objects.
  prefs: []
  type: TYPE_NORMAL
- en: Same as in SSD, detection happens in all pyramid levels by making a prediction
    out of every merged feature map. Because predictions share the same classifier
    and the box regressor, they are all formed to have the same channel dimension
    d=256.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are A=9 anchor boxes per level:'
  prefs: []
  type: TYPE_NORMAL
- en: The base size corresponds to areas of $32^2$ to $512^2$ pixels on $P_3$ to $P_7$
    respectively. There are three size ratios, $\{2^0, 2^{1/3}, 2^{2/3}\}$.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For each size, there are three aspect ratios {1/2, 1, 2}.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As usual, for each anchor box, the model outputs a class probability for each
    of $K$ classes in the classification subnet and regresses the offset from this
    anchor box to the nearest ground truth object in the box regression subnet. The
    classification subnet adopts the focal loss introduced above.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4003b7a9a0a1c5e27d6579ecea1ddf40.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 12\. The RetinaNet model architecture uses a [FPN](https://arxiv.org/abs/1612.03144)
    backbone on top of ResNet. (Image source: the [FPN](https://arxiv.org/abs/1612.03144)
    paper)'
  prefs: []
  type: TYPE_NORMAL
- en: YOLOv3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[YOLOv3](https://pjreddie.com/media/files/papers/YOLOv3.pdf) is created by
    applying a bunch of design tricks on YOLOv2\. The changes are inspired by recent
    advances in the object detection world.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are a list of changes:'
  prefs: []
  type: TYPE_NORMAL
- en: '**1\. Logistic regression for confidence scores**: YOLOv3 predicts an confidence
    score for each bounding box using *logistic regression*, while YOLO and YOLOv2
    uses sum of squared errors for classification terms (see the [loss function](#loss-function)
    above). Linear regression of offset prediction leads to a decrease in mAP.'
  prefs: []
  type: TYPE_NORMAL
- en: '**2\. No more softmax for class prediction**: When predicting class confidence,
    YOLOv3 uses *multiple independent logistic classifier* for each class rather than
    one softmax layer. This is very helpful especially considering that one image
    might have multiple labels and not all the labels are guaranteed to be mutually
    exclusive.'
  prefs: []
  type: TYPE_NORMAL
- en: '**3\. Darknet + ResNet as the base model**: The new Darknet-53 still relies
    on successive 3x3 and 1x1 conv layers, just like the original dark net architecture,
    but has residual blocks added.'
  prefs: []
  type: TYPE_NORMAL
- en: '**4\. Multi-scale prediction**: Inspired by image pyramid, YOLOv3 adds several
    conv layers after the base feature extractor model and makes prediction at three
    different scales among these conv layers. In this way, it has to deal with many
    more bounding box candidates of various sizes overall.'
  prefs: []
  type: TYPE_NORMAL
- en: '**5\. Skip-layer concatenation**: YOLOv3 also adds cross-layer connections
    between two prediction layers (except for the output layer) and earlier finer-grained
    feature maps. The model first up-samples the coarse feature maps and then merges
    it with the previous features by concatenation. The combination with finer-grained
    information makes it better at detecting small objects.'
  prefs: []
  type: TYPE_NORMAL
- en: Interestingly, focal loss does not help YOLOv3, potentially it might be due
    to the usage of $\lambda_\text{noobj}$ and $\lambda_\text{coord}$ — they increase
    the loss from bounding box location predictions and decrease the loss from confidence
    predictions for background boxes.
  prefs: []
  type: TYPE_NORMAL
- en: Overall YOLOv3 performs better and faster than SSD, and worse than RetinaNet
    but 3.8x faster.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e30cfb033268041e1f11978a171a7485.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 13\. The comparison of various fast object detection models on speed and
    mAP performance. (Image source: [focal loss](https://arxiv.org/abs/1708.02002)
    paper with additional labels from the [YOLOv3](https://pjreddie.com/media/files/papers/YOLOv3.pdf)
    paper.)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 'Cited as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Reference
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] Joseph Redmon, et al. [“You only look once: Unified, real-time object detection.”](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf)
    CVPR 2016.'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] Joseph Redmon and Ali Farhadi. [“YOLO9000: Better, Faster, Stronger.”](http://openaccess.thecvf.com/content_cvpr_2017/papers/Redmon_YOLO9000_Better_Faster_CVPR_2017_paper.pdf)
    CVPR 2017.'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] Joseph Redmon, Ali Farhadi. [“YOLOv3: An incremental improvement.”](https://pjreddie.com/media/files/papers/YOLOv3.pdf).'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] Wei Liu et al. [“SSD: Single Shot MultiBox Detector.”](https://arxiv.org/abs/1512.02325)
    ECCV 2016.'
  prefs: []
  type: TYPE_NORMAL
- en: '[5] Tsung-Yi Lin, et al. [“Feature Pyramid Networks for Object Detection.”](https://arxiv.org/abs/1612.03144)
    CVPR 2017.'
  prefs: []
  type: TYPE_NORMAL
- en: '[6] Tsung-Yi Lin, et al. [“Focal Loss for Dense Object Detection.”](https://arxiv.org/abs/1708.02002)
    IEEE transactions on pattern analysis and machine intelligence, 2018.'
  prefs: []
  type: TYPE_NORMAL
- en: '[7] [“What’s new in YOLO v3?”](https://towardsdatascience.com/yolo-v3-object-detection-53fb7d3bfe6b)
    by Ayoosh Kathuria on “Towards Data Science”, Apr 23, 2018.'
  prefs: []
  type: TYPE_NORMAL
