- en: 'Object Detection for Dummies Part 2: CNN, DPM and Overfeat'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对于小白的目标检测第二部分：CNN，DPM 和 Overfeat
- en: 原文：[https://lilianweng.github.io/posts/2017-12-15-object-recognition-part-2/](https://lilianweng.github.io/posts/2017-12-15-object-recognition-part-2/)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://lilianweng.github.io/posts/2017-12-15-object-recognition-part-2/](https://lilianweng.github.io/posts/2017-12-15-object-recognition-part-2/)
- en: '[Part 1](https://lilianweng.github.io/posts/2017-10-29-object-recognition-part-1/)
    of the “Object Detection for Dummies” series introduced: (1) the concept of image
    gradient vector and how HOG algorithm summarizes the information across all the
    gradient vectors in one image; (2) how the image segmentation algorithm works
    to detect regions that potentially contain objects; (3) how the Selective Search
    algorithm refines the outcomes of image segmentation for better region proposal.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[“小白的目标检测”系列的第一部分](https://lilianweng.github.io/posts/2017-10-29-object-recognition-part-1/)介绍了：（1）图像梯度向量的概念以及
    HOG 算法如何总结一幅图像中所有梯度向量的信息；（2）图像分割算法如何检测潜在包含对象的区域；（3）选择性搜索算法如何优化图像分割的结果以获得更好的区域提议。'
- en: In Part 2, we are about to find out more on the classic convolution neural network
    architectures for image classification. They lay the ***foundation*** for further
    progress on the deep learning models for object detection. Go check [Part 3](https://lilianweng.github.io/posts/2017-12-31-object-recognition-part-3/)
    if you want to learn more on R-CNN and related models.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二部分，我们将更深入地了解经典卷积神经网络架构用于图像分类。它们为进一步发展目标检测的深度学习模型奠定了***基础***。如果想了解更多关于 R-CNN
    和相关模型的内容，请查看[第三部分](https://lilianweng.github.io/posts/2017-12-31-object-recognition-part-3/)。
- en: 'Links to all the posts in the series: [[Part 1](https://lilianweng.github.io/posts/2017-10-29-object-recognition-part-1/)]
    [[Part 2](https://lilianweng.github.io/posts/2017-12-15-object-recognition-part-2/)]
    [[Part 3](https://lilianweng.github.io/posts/2017-12-31-object-recognition-part-3/)]
    [[Part 4](https://lilianweng.github.io/posts/2018-12-27-object-recognition-part-4/)].'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 系列中所有帖子的链接：[[第一部分](https://lilianweng.github.io/posts/2017-10-29-object-recognition-part-1/)]
    [[第二部分](https://lilianweng.github.io/posts/2017-12-15-object-recognition-part-2/)]
    [[第三部分](https://lilianweng.github.io/posts/2017-12-31-object-recognition-part-3/)]
    [[第四部分](https://lilianweng.github.io/posts/2018-12-27-object-recognition-part-4/)]。
- en: CNN for Image Classification
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图像分类的CNN
- en: CNN, short for “**Convolutional Neural Network**”, is the go-to solution for
    computer vision problems in the deep learning world. It was, to some extent, [inspired](https://lilianweng.github.io/posts/2017-06-21-overview/#convolutional-neural-network)
    by how human visual cortex system works.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: CNN，即“**卷积神经网络**”，是深度学习领域中处理计算机视觉问题的首选解决方案。在某种程度上，它受到了人类视觉皮层系统运作方式的[启发](https://lilianweng.github.io/posts/2017-06-21-overview/#convolutional-neural-network)。
- en: Convolution Operation
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 卷积操作
- en: I strongly recommend this [guide](https://arxiv.org/pdf/1603.07285.pdf) to convolution
    arithmetic, which provides a clean and solid explanation with tons of visualizations
    and examples. Here let’s focus on two-dimensional convolution as we are working
    with images in this post.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我强烈推荐这篇[指南](https://arxiv.org/pdf/1603.07285.pdf)来学习卷积算术，它提供了清晰而扎实的解释，配有大量可视化和示例。在本文中，让我们专注于二维卷积，因为我们处理的是图像。
- en: In short, convolution operation slides a predefined [kernel](https://en.wikipedia.org/wiki/Kernel_(image_processing))
    (also called “filter”) on top of the input feature map (matrix of image pixels),
    multiplying and adding the values of the kernel and partial input features to
    generate the output. The values form an output matrix, as usually, the kernel
    is much smaller than the input image.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，卷积操作将预定义的[核](https://en.wikipedia.org/wiki/Kernel_(image_processing))（也称为“滤波器”）滑动在输入特征图（图像像素矩阵）上，将核和部分输入特征的值相乘相加以生成输出。这些值形成一个输出矩阵，通常情况下，核远小于输入图像。
- en: '![](../Images/01d4c042e809ce81d1ae6b9bd8f99952.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/01d4c042e809ce81d1ae6b9bd8f99952.png)'
- en: 'Fig. 1\. An illustration of applying a kernel on the input feature map to generate
    the output. (Image source: [River Trail documentation](http://intellabs.github.io/RiverTrail/tutorial/))'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1\. 展示了在输入特征图上应用核以生成输出的示意图。（图片来源：[River Trail documentation](http://intellabs.github.io/RiverTrail/tutorial/)）
- en: Figure 2 showcases two real examples of how to convolve a 3x3 kernel over a
    5x5 2D matrix of numeric values to generate a 3x3 matrix. By controlling the padding
    size and the stride length, we can generate an output matrix of a certain size.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2 展示了如何通过在一个 5x5 的二维数值矩阵上卷积一个 3x3 的核来生成一个 3x3 的矩阵的两个真实示例。通过控制填充大小和步长，我们可以生成特定大小的输出矩阵。
- en: '![](../Images/6208a1261b1d80318eb8bf0e780c36f8.png) ![](../Images/5b1ed46db91a7c5e83ce80098ed32534.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6208a1261b1d80318eb8bf0e780c36f8.png) ![](../Images/5b1ed46db91a7c5e83ce80098ed32534.png)'
- en: 'Fig. 2\. Two examples of 2D convolution operation: (top) no padding and 1x1
    strides; (bottom) 1x1 border zeros padding and 2x2 strides. (Image source: [deeplearning.net](http://deeplearning.net/software/theano_versions/dev/tutorial/conv_arithmetic.html))'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图2\. 2D卷积操作的两个示例：（顶部）无填充和1x1步幅；（底部）1x1边界零填充和2x2步幅。（图片来源：[deeplearning.net](http://deeplearning.net/software/theano_versions/dev/tutorial/conv_arithmetic.html)）
- en: AlexNet (Krizhevsky et al, 2012)
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AlexNet（Krizhevsky等人，2012）
- en: 5 convolution [+ optional max pooling] layers + 2 MLP layers + 1 LR layer
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 5个卷积[+可选最大池化]层 + 2个MLP层 + 1个LR层
- en: Use data augmentation techniques to expand the training dataset, such as image
    translations, horizontal reflections, and patch extractions.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用数据增强技术扩展训练数据集，如图像平移、水平反射和补丁提取。
- en: '![](../Images/de2692e9c6d28e2c4f8489b4c22cf078.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/de2692e9c6d28e2c4f8489b4c22cf078.png)'
- en: 'Fig. 3\. The architecture of AlexNet. (Image source: [link](http://vision03.csail.mit.edu/cnn_art/index.html))'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图3\. AlexNet的架构。（图片来源：[link](http://vision03.csail.mit.edu/cnn_art/index.html)）
- en: VGG (Simonyan and Zisserman, 2014)
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VGG（Simonyan和Zisserman，2014）
- en: The network is considered as “very deep” at its time; 19 layers
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该网络在当时被认为是“非常深”的；19层
- en: The architecture is extremely simplified with only 3x3 convolutional layers
    and 2x2 pooling layers. The stacking of small filters simulates a larger filter
    with fewer parameters.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该架构非常简化，只有3x3卷积层和2x2池化层。小滤波器的堆叠模拟了具有更少参数的大滤波器。
- en: ResNet (He et al., 2015)
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ResNet（He等人，2015）
- en: The network is indeed very deep; 152 layers of simple architecture.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络确实非常深；152层简单的架构。
- en: '**Residual Block**: Some input of a certain layer can be passed to the component
    two layers later. Residual blocks are essential for keeping a deep network trainable
    and eventually work. Without residual blocks, the training loss of a plain network
    does not monotonically decrease as the number of layers increases due to [vanishing
    and exploding gradients](http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/).'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**残差块**：某一层的某些输入可以传递到两层后的组件。残差块对于保持深度网络可训练并最终起作用至关重要。没有残差块，普通网络的训练损失随着层数增加不会单调递减，这是由于[梯度消失和梯度爆炸](http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/)。'
- en: '![](../Images/98029e385bce47c73cb2f72229d7d384.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/98029e385bce47c73cb2f72229d7d384.png)'
- en: 'Fig. 4\. An illustration of the residual block of ResNet. In some way, we can
    say the design of residual blocks is inspired by V4 getting input directly from
    V1 in the human visual cortex system. (left image source: [Wang et al., 2017](https://arxiv.org/pdf/1312.6229.pdf))'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图4\. ResNet的残差块示意图。在某种程度上，我们可以说残差块的设计受到了人类视觉皮层系统中V1直接从V1获取输入的启发。（左图来源：[Wang等人，2017](https://arxiv.org/pdf/1312.6229.pdf)）
- en: 'Evaluation Metrics: mAP'
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估指标：mAP
- en: A common evaluation metric used in many object recognition and detection tasks
    is “**mAP**”, short for “**mean average precision**”. It is a number from 0 to
    100; higher value is better.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 许多目标识别和检测任务中使用的常见评估指标是“**mAP**”，即“**平均精度均值**”。它是一个从0到100的数字；数值越高越好。
- en: Combine all detections from all test images to draw a precision-recall curve
    (PR curve) for each class; The “average precision” (AP) is the area under the
    PR curve.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将所有测试图像中的所有检测结果组合在一起，为每个类别绘制一个精度-召回曲线（PR曲线）；“平均精度”（AP）是PR曲线下的面积。
- en: Given that target objects are in different classes, we first compute AP separately
    for each class, and then average over classes.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 鉴于目标对象属于不同类别，我们首先分别计算每个类别的AP，然后对类别求平均。
- en: A detection is a true positive if it has **“intersection over union” (IoU)**
    with a ground-truth box greater than some threshold (usually 0.5; if so, the metric
    is “[mAP@0.5](mailto:mAP@0.5)”)
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果检测结果与地面实况框的“交并比”（IoU）大于某个阈值（通常为0.5），则检测结果为真阳性；如果是这样，指标为“[mAP@0.5](mailto:mAP@0.5)”。
- en: Deformable Parts Model
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可变部件模型
- en: 'The Deformable Parts Model (DPM) ([Felzenszwalb et al., 2010](http://people.cs.uchicago.edu/~pff/papers/lsvm-pami.pdf))
    recognizes objects with a mixture graphical model (Markov random fields) of deformable
    parts. The model consists of three major components:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 可变部件模型（DPM）（[Felzenszwalb等人，2010](http://people.cs.uchicago.edu/~pff/papers/lsvm-pami.pdf)）使用可变部件的混合图模型（马尔可夫随机场）识别对象。该模型由三个主要组件组成：
- en: A coarse ***root filter*** defines a detection window that approximately covers
    an entire object. A filter specifies weights for a region feature vector.
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个粗糙的***根滤波器***定义了一个大致覆盖整个对象的检测窗口。滤波器为区域特征向量指定权重。
- en: Multiple ***part filters*** that cover smaller parts of the object. Parts filters
    are learned at twice resolution of the root filter.
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 多个***部件滤波器***覆盖对象的较小部分。部件滤波器在根滤波器的两倍分辨率处学习。
- en: A ***spatial model*** for scoring the locations of part filters relative to
    the root.
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个***空间模型***用于评分部件滤波器相对于根部的位置。
- en: '![](../Images/0df4397cf7ee2b4ec3115e158d699c0c.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0df4397cf7ee2b4ec3115e158d699c0c.png)'
- en: Fig. 5\. The DPM model contains (a) a root filter, (b) multiple part filters
    at twice the resolution, and (c) a model for scoring the location and deformation
    of parts.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图5。DPM 模型包含（a）一个根滤波器，（b）两倍分辨率的多个部件滤波器，以及（c）用于评分部件位置和变形的模型。
- en: 'The quality of detecting an object is measured by the score of filters minus
    the deformation costs. The matching score $f$, in laymen’s terms, is:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 检测对象的质量由滤波器得分减去变形成本来衡量。通俗地说，匹配得分 $f$ 是：
- en: $$ f(\text{model}, x) = f(\beta_\text{root}, x) + \sum_{\beta_\text{part} \in
    \text{part filters}} \max_y [f(\beta_\text{part}, y) - \text{cost}(\beta_\text{part},
    x, y)] $$
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: $$ f(\text{model}, x) = f(\beta_\text{root}, x) + \sum_{\beta_\text{part} \in
    \text{part filters}} \max_y [f(\beta_\text{part}, y) - \text{cost}(\beta_\text{part},
    x, y)] $$
- en: in which,
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，
- en: $x$ is an image with a specified position and scale;
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $x$ 是一个具有指定位置和比例的图像；
- en: $y$ is a sub region of $x$.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $y$ 是 $x$ 的一个子区域。
- en: $\beta_\text{root}$ is the root filter.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\beta_\text{root}$ 是根滤波器。
- en: $\beta_\text{part}$ is one part filter.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\beta_\text{part}$ 是一个部件滤波器。
- en: cost() measures the penalty of the part deviating from its ideal location relative
    to the root.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: cost() 衡量部件偏离其理想位置相对于根部的惩罚。
- en: 'The basic score model is the dot product between the filter $\beta$ and the
    region feature vector $\Phi(x)$: $f(\beta, x) = \beta \cdot \Phi(x)$. The feature
    set $\Phi(x)$ can be defined by HOG or other similar algorithms.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 基本得分模型是滤波器 $\beta$ 与区域特征向量 $\Phi(x)$ 的点积：$f(\beta, x) = \beta \cdot \Phi(x)$。特征集
    $\Phi(x)$ 可以由 HOG 或其他类似算法定义。
- en: A root location with high score detects a region with high chances to contain
    an object, while the locations of the parts with high scores confirm a recognized
    object hypothesis. The paper adopted latent SVM to model the classifier.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 具有高得分的根位置检测到具有高概率包含对象的区域，而具有高得分的部件位置确认了一个识别对象的假设。该论文采用潜在 SVM 来建模分类器。
- en: '![](../Images/a65a1e2362b06aef57b0f99d366a0115.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a65a1e2362b06aef57b0f99d366a0115.png)'
- en: 'Fig. 6\. The matching process by DPM. (Image source: [Felzenszwalb et al.,
    2010](http://people.cs.uchicago.edu/~pff/papers/lsvm-pami.pdf))'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图6。DPM 的匹配过程。（图片来源：[Felzenszwalb et al., 2010](http://people.cs.uchicago.edu/~pff/papers/lsvm-pami.pdf)）
- en: The author later claimed that DPM and CNN models are not two distinct approaches
    to object recognition. Instead, a DPM model can be formulated as a CNN by unrolling
    the DPM inference algorithm and mapping each step to an equivalent CNN layer.
    (Check the details in [Girshick et al., 2015](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Girshick_Deformable_Part_Models_2015_CVPR_paper.pdf)!)
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 作者后来声称，DPM 和 CNN 模型并不是目标识别的两种不同方法。相反，可以通过展开 DPM 推理算法并将每个步骤映射到等效的 CNN 层来将 DPM
    模型构建为 CNN。（查看[Girshick et al., 2015](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Girshick_Deformable_Part_Models_2015_CVPR_paper.pdf)中的详细信息！）
- en: Overfeat
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Overfeat
- en: Overfeat [[paper](https://pdfs.semanticscholar.org/f2c2/fbc35d0541571f54790851de9fcd1adde085.pdf)][[code](https://github.com/sermanet/OverFeat)]
    is a pioneer model of integrating the object detection, localization and classification
    tasks all into one convolutional neural network. The main idea is to (i) do image
    classification at different locations on regions of multiple scales of the image
    in a sliding window fashion, and (ii) predict the bounding box locations with
    a regressor trained on top of the same convolution layers.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: Overfeat [[paper](https://pdfs.semanticscholar.org/f2c2/fbc35d0541571f54790851de9fcd1adde085.pdf)][[code](https://github.com/sermanet/OverFeat)]
    是将目标检测、定位和分类任务集成到一个卷积神经网络中的先驱模型。其主要思想是（i）以滑动窗口方式在图像的多个尺度区域上进行不同位置的图像分类，以及（ii）通过在相同卷积层之上训练的回归器预测边界框位置。
- en: 'The Overfeat model architecture is very similar to [AlexNet](#alexnet-krizhevsky-et-al-2012).
    It is trained as follows:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: Overfeat 模型架构与[AlexNet](#alexnet-krizhevsky-et-al-2012)非常相似。它的训练如下：
- en: '![](../Images/152d3b3b7e960f44f42829738d0b796b.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/152d3b3b7e960f44f42829738d0b796b.png)'
- en: 'Fig. 7\. The training stages of the Overfeat model. (Image source: [link](http://vision.stanford.edu/teaching/cs231b_spring1415/slides/overfeat_eric.pdf))'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7\. Overfeat 模型的训练阶段。（图片来源：[链接](http://vision.stanford.edu/teaching/cs231b_spring1415/slides/overfeat_eric.pdf)）
- en: Train a CNN model (similar to AlexNet) on the image classification task.
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在图像分类任务上训练一个 CNN 模型（类似于 AlexNet）。
- en: Then, we replace the top classifier layers by a regression network and train
    it to predict object bounding boxes at each spatial location and scale. The regressor
    is class-specific, each generated for one image class.
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们通过一个回归网络替换顶部分类器层，并训练它以预测每个空间位置和尺度的对象边界框。回归器是特定于类别的，为每个图像类别生成一个。
- en: 'Input: Images with classification and bounding box.'
  id: totrans-60
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入：带有分类和边界框的图像。
- en: 'Output: $(x_\text{left}, x_\text{right}, y_\text{top}, y_\text{bottom})$, 4
    values in total, representing the coordinates of the bounding box edges.'
  id: totrans-61
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出：$(x_\text{left}, x_\text{right}, y_\text{top}, y_\text{bottom})$，总共 4 个值，表示边界框边缘的坐标。
- en: 'Loss: The regressor is trained to minimize $l2$ norm between generated bounding
    box and the ground truth for each training example.'
  id: totrans-62
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 损失：回归器经过训练，以最小化每个训练示例的生成边界框与实际情况之间的 $l2$ 范数。
- en: At the detection time,
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在检测时，
- en: Perform classification at each location using the pretrained CNN model.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用预训练的 CNN 模型在每个位置执行分类。
- en: Predict object bounding boxes on all classified regions generated by the classifier.
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预测分类器生成的所有分类区域上的对象边界框。
- en: Merge bounding boxes with sufficient overlap from localization and sufficient
    confidence of being the same object from the classifier.
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 合并来自定位的具有足够重叠的边界框和来自分类器的足够置信度的相同对象。
- en: '* * *'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 'Cited as:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 引用为：
- en: '[PRE0]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Reference
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Vincent Dumoulin and Francesco Visin. [“A guide to convolution arithmetic
    for deep learning.”](https://arxiv.org/pdf/1603.07285.pdf) arXiv preprint arXiv:1603.07285
    (2016).'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Vincent Dumoulin 和 Francesco Visin. [“深度学习中的卷积算术指南。”](https://arxiv.org/pdf/1603.07285.pdf)
    arXiv 预印本 arXiv:1603.07285（2016）。'
- en: '[2] Haohan Wang, Bhiksha Raj, and Eric P. Xing. [“On the Origin of Deep Learning.”](https://arxiv.org/pdf/1702.07800.pdf)
    arXiv preprint arXiv:1702.07800 (2017).'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Haohan Wang、Bhiksha Raj 和 Eric P. Xing。[“深度学习的起源。”](https://arxiv.org/pdf/1702.07800.pdf)
    arXiv 预印本 arXiv:1702.07800（2017）。'
- en: '[3] Pedro F. Felzenszwalb, Ross B. Girshick, David McAllester, and Deva Ramanan.
    [“Object detection with discriminatively trained part-based models.”](http://people.cs.uchicago.edu/~pff/papers/lsvm-pami.pdf)
    IEEE transactions on pattern analysis and machine intelligence 32, no. 9 (2010):
    1627-1645.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Pedro F. Felzenszwalb, Ross B. Girshick, David McAllester 和 Deva Ramanan。[“使用经过区分训练的基于部件的模型进行目标检测。”](http://people.cs.uchicago.edu/~pff/papers/lsvm-pami.pdf)
    IEEE 模式分析与机器智能交易 32，第 9 期（2010）：1627-1645。'
- en: '[4] Ross B. Girshick, Forrest Iandola, Trevor Darrell, and Jitendra Malik.
    [“Deformable part models are convolutional neural networks.”](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Girshick_Deformable_Part_Models_2015_CVPR_paper.pdf)
    In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), pp. 437-446\.
    2015.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Ross B. Girshick, Forrest Iandola, Trevor Darrell 和 Jitendra Malik。[“可变形部件模型是卷积神经网络。”](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Girshick_Deformable_Part_Models_2015_CVPR_paper.pdf)
    在 IEEE 计算机视觉与模式识别（CVPR）会议论文集中，第 437-446 页。2015年。'
- en: '[5] Sermanet, Pierre, David Eigen, Xiang Zhang, Michaël Mathieu, Rob Fergus,
    and Yann LeCun. [“OverFeat: Integrated Recognition, Localization and Detection
    using Convolutional Networks”](https://pdfs.semanticscholar.org/f2c2/fbc35d0541571f54790851de9fcd1adde085.pdf)
    arXiv preprint arXiv:1312.6229 (2013).'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Sermanet, Pierre, David Eigen, Xiang Zhang, Michaël Mathieu, Rob Fergus
    和 Yann LeCun。[“OverFeat：使用卷积网络进行集成识别、定位和检测”](https://pdfs.semanticscholar.org/f2c2/fbc35d0541571f54790851de9fcd1adde085.pdf)
    arXiv 预印本 arXiv:1312.6229（2013）。'
