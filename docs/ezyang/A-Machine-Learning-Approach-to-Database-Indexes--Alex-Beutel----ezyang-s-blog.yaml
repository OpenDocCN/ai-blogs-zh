- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-07-01 18:16:59'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-07-01 18:16:59
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'A Machine Learning Approach to Database Indexes (Alex Beutel) : ezyang’s blog'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据库索引的机器学习方法（Alex Beutel）：ezyang’s 博客
- en: 来源：[http://blog.ezyang.com/2017/12/a-machine-learning-approach-to-database-indexes-alex-beutel/](http://blog.ezyang.com/2017/12/a-machine-learning-approach-to-database-indexes-alex-beutel/)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[http://blog.ezyang.com/2017/12/a-machine-learning-approach-to-database-indexes-alex-beutel/](http://blog.ezyang.com/2017/12/a-machine-learning-approach-to-database-indexes-alex-beutel/)
- en: The below is a transcript of a talk by [Alex Beutel](http://alexbeutel.com/)
    on [machine learning database indexes](https://arxiv.org/abs/1712.01208), at the
    [ML Systems Workshop](https://nips.cc/Conferences/2017/Schedule?showEvent=8774)
    at NIPS'17.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是对 [Alex Beutel](http://alexbeutel.com/) 在 [machine learning database indexes](https://arxiv.org/abs/1712.01208)
    上的讲话的转录，于 [ML Systems Workshop](https://nips.cc/Conferences/2017/Schedule?showEvent=8774)
    在 NIPS'17 上进行。
- en: '* * *'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: DB researchers think about there research differently. You have a system that
    needs to work for all cases. Where as in ML, we have a unique circumstance, I'll
    build a model that works well. In DB, you have to fit all.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 数据库研究人员以不同的方式思考他们的研究。你有一个需要适用于所有情况的系统。而在机器学习中，我们有一个独特的情况，我会建立一个效果良好的模型。在数据库中，你必须适应所有情况。
- en: To give an example of this is a B-tree. A B-tree works for range queries. We
    have records, key, we want to find all records for range of keys. 0-1000, you
    build tree on top of sorted array. To quickly look up starting point in range.
    What if all my data, all of the keys, from zero to million... it becomes clear,
    you don't need the whole tree above. You can use the key itself as an offset into
    the array. Your lookup is O(1), O(1) memory, no need for extra data structure.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 举个例子，这是一个 B-树。B-树适用于范围查询。我们有记录，关键字，我们想要找到所有关键字范围内的记录。0-1000，你在排序数组上构建树。为了快速查找范围内的起始点。如果我的所有数据，所有的关键字，从零到百万……变得清晰，你不需要整个树顶部。你可以将关键字本身用作数组中的偏移量。你的查找是
    O(1)，O(1) 内存，不需要额外的数据结构。
- en: Now, we can't go for each app, we can't make a custom implementation to make
    use of some pattern. DB scale to any application, we don't want to rebuild it
    any time.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们不能为每个应用程序都进行定制实现以利用某种模式。数据库可以扩展到任何应用程序，我们不希望每次都重新构建它。
- en: But ML excels in this situation. It works well for a wide variety of distributions,
    learn and make use of them effectively.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 但是在这种情况下，机器学习表现出色。它对各种分布都能很好地工作，学习并有效地利用它们。
- en: This is the key insight we came to. Traditional data structures make no assumptions
    about your data. They work under any distribution, and generally scale O(n). Interestingly,
    learning, these data distributions, can offer a huge win. What we're trying to
    go to, is instead of scaling to size of data, we scale to complexity of it. With
    linear data, it's O(1). For other distributions, can we leverage this?
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们得出的关键见解。传统的数据结构不对数据做任何假设。它们适用于任何分布，并且通常在 O(n) 的规模上扩展。有趣的是，学习这些数据分布可以带来巨大的收益。我们尝试的是，不再按数据大小扩展，而是按其复杂性扩展。对于线性数据，复杂度是
    O(1)。对于其他分布，我们能否利用这一点？
- en: There are three dat structures underlying databases. There are B-Trees; range
    queries, similarity search. Main index. Hash maps for point lookups; individual
    records. This is more common throughout CS. And bloom filters, are really common
    for set-inclusion queries. Do I have a key. If your record is stored on disk,
    checking first if there's a record with that key is worthwhile. We're going to
    focus entirely on B-trees.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 数据库的底层有三种数据结构。有 B-树；范围查询，相似性搜索。主索引。哈希映射用于点查找；单个记录。这在计算机科学中更为常见。而布隆过滤器，在集合包含查询中非常常见。我有一个关键。如果你的记录存储在磁盘上，首先检查是否有这个键的记录是值得的。我们将完全专注于
    B-树。
- en: B-trees take a tree like structure with high branching factor. What makes it
    really effective is that it's cache efficient. You can store top level nodes in
    your cache where it's fast to look it up, maybe others in main memory, and the
    actual memory on disk. By caching the hierarchy appropriately, it makes it efficiently.
    At a high level, a B-tree maps a key to a page, some given place in memory. Once
    it finds that page, it will do some local search to find the particular range
    of that key. That could be a scan or binary search; we know the range will be
    the position from start of page to page size.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: B-树采用高分支因子的树形结构。其真正有效之处在于它的高缓存效率。你可以将顶层节点存储在缓存中，快速查找，可能将其他节点存储在主内存中，实际内存在磁盘上。通过适当缓存层次结构，使其高效。在高层次上，B-树将一个关键字映射到一个页，内存中的某个给定位置。一旦找到该页，它将进行一些局部搜索，以找到该关键字的特定范围。这可以是扫描或二分搜索；我们知道范围将是从页面起始到页面大小的位置。
- en: An abstract level, the Btree is just a model. It's taking the position of the
    key, and trying to estimate the position. What we have in this case, we want to
    search in this error range to find the ultimate record. At a high level, it would
    mean that we can't use any model. We need err_min and err_max. But we have all
    the data. If you have all the data, you know at index construction time, you know
    all the data you're executing against, and you can calculate what the model's
    min and max error is.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在抽象级别上，B 树只是一个模型。它采用关键点的位置，并尝试估计位置。在这种情况下，我们希望在此误差范围内搜索以找到最终记录。在高层次上，这意味着我们无法使用任何模型。我们需要
    err_min 和 err_max。但我们拥有所有数据。如果在索引构建时，您已经知道了要执行的所有数据，并且可以计算出模型的最小和最大误差。
- en: One interesting thing is this is just a regression problem. What you're really
    modeling is just the CDF. On the X axis on this plot here, the X axis is your
    keys, Ys your position. This is modeling where your probability mass is located;
    where your data is in the keyspace. CDFs are studied somewhat, but not a ton,
    in the literature. This is a nice new implication of research.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，这只是一个回归问题。你真正建模的是 CDF。在这个图的 X 轴上，X 轴是您的键，Y 轴是您的位置。这在哪里建模您的概率质量的位置；您的数据在键空间中的位置。CDF
    在文献中有些研究，但不多。这是研究的一个新的有趣的含义。
- en: We thought, OK, let's try this out straightaway. Train a model, see how fast
    it is. We looked at 200M server logs, timestamp key, 2 layer NN, 32-width, relatively
    small by ML. We train to predict position, square error. A B-Tree executes in
    300ns. Unfortunately, with the model, it takes 80000ns. By most ML model speeds,
    this is great. If you're looking at executing on server, great. But this doesn't
    work for a database.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想，好吧，让我们立即尝试一下。训练一个模型，看看它有多快。我们查看了2亿个服务器日志，时间戳键，2层NN，32宽度，相对较小的机器学习模型。我们训练以预测位置，平方误差。一个
    B 树执行需要300纳秒。不幸的是，用模型来执行需要80000纳秒。按大多数机器学习模型的速度来看，这很好。如果您打算在服务器上执行，那很好。但这对数据库不适用。
- en: There are a bunch of problems baked into this. TF is really designed for large
    models. Think about translation or superresolution images; these are hefty tasks.
    We need to make this fast for database level speed. Second, b-trees are great
    for overfitting. There's no risk of over-fitting in this context. They're also
    cache efficient; that's not looked at in ML. The last thing is local search in
    the end. Is that really the most effective way of ultimately finding that key?
    I'm skipping that part because it's fairly detailed, I'll focus on first three.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里有一堆问题。TF 确实是为大型模型设计的。想想翻译或超分辨率图像；这些任务都很重。我们需要将其速度提高到数据库级别的速度。其次，B 树在过拟合方面表现出色。在这种情况下不存在过拟合的风险。它们还具有高效的缓存效率；这在机器学习中并没有得到重视。最后是最后的局部搜索。这真的是最终找到关键点的最有效方法吗？我跳过了那部分，因为它相当详细，我将专注于前三个问题。
- en: The first part is just the raw speed fo execution of ML model. This was built
    really by Tim, this Learning Index Framework program. What it does is it lets
    you create different indexes under different configurations. For one thing, it
    lets you do code compilation for TF, ideas from Tupleware, where you can take
    a linear model and execute it extremely quickly. We can also train simple models.
    Use TF for more complex gradient descent based learning; extract weights, and
    have inference graph be codegenned. And we can do a lot of autotuning, to find
    what the best model architecture is. We know ahead of time what the best training
    is. We can make pretty smart decisions about what works best.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 第一部分只是机器学习模型执行的原始速度。这确实是由 Tim 构建的 Learning Index Framework 程序。它可以让您在不同配置下创建不同的索引。首先，它可以为
    TF 进行代码编译，借鉴了 Tupleware 的思想，您可以快速执行线性模型。我们还可以训练简单的模型。使用 TF 进行更复杂的基于梯度下降的学习；提取权重，并且推理图被代码生成。我们还可以进行大量的自动调整，以找到最佳的模型架构。我们提前知道什么是最佳的训练。我们可以做出关于哪种方法最有效的相当明智的决策。
- en: The next problem is accuracy and sepeed. If I have 100M records, I narrow down
    quickly from 1.5M to 24K, with each step down this tree. Each one of those steps
    is 50-60 cycles to look through that page, and to find what the right branch is.
    So we have to get to an accurracy of 12000, within 500 mul/add, to beat these
    levels of hierarchy, which are in cache. This is a steep task. The question is
    what is the right model? a really wide network? Single hidden layer? This scales
    nicely, we can fit in 256 layer reasonably. We could go deeper... the challenge
    is we have width^2, which need to be parallelized somehow. The challenge is, how
    do we effectively scale this. We want to add capacity to the model, make it more
    and more accurate, with increased size, without becoming to.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个问题是准确性和速度。如果我有 100M 记录，我从 1.5M 快速缩小到 24K，每一步都在这棵树上。这些步骤中的每一个都是查找那一页，找到正确分支的
    50-60 个周期。因此，我们必须在缓存中达到 12000 的精度，在 500 乘加内，才能超过这些层次的水平。这是一个艰巨的任务。问题是什么是正确的模型？一个非常宽的网络？单隐藏层？这样的规模很好，我们可以合理地放入
    256 层。我们可以更深入......挑战是我们有宽度^2，需要以某种方式并行化。挑战是，我们如何有效地扩展这个。我们希望向模型增加容量，使其更加精确，增加大小，而不至于变得。
- en: We took a different approach, based on mixed experts. We'll have a key, have
    a really simple classifier. We get an estimate. Then we can use that estimate
    to find it at the next stage. Narrow down the CDF range, and try to be more accurate
    in the subset of space. It will still get key as input; given key, give position,
    but more narrow space of keys. We build this down, and we'll walk down this hierarchy.
    This decouples model size and complexity. We have a huge model, overfitting, but
    we don't have to execute all of the sparsity that you would have to do from a
    pure ML view. We can decouple it usefully. The nice thing we can do is fall back
    to B-trees for subsets that are difficult to learn in a model. The LIF framework
    lets us substitute it in easily. In the worst case, B-tree. Best case, more efficient.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们采取了一种基于混合专家的不同方法。我们会有一个关键点，有一个非常简单的分类器。我们得到一个估计值。然后我们可以使用那个估计值在下一个阶段找到它。缩小
    CDF 范围，并在空间子集中尝试更精确。它仍然会以关键点作为输入；给定关键点，给出位置，但关键点的空间更窄。我们构建这一点，并且我们将沿着这个层次结构向下走。这解耦了模型大小和复杂性。我们有一个庞大的模型，过度拟合，但我们不必执行所有从纯
    ML 视图中必须执行的稀疏化。我们可以有用地解耦它。我们可以做的好事是为在模型中难以学习的子集退回到 B 树。LIF 框架让我们能够轻松替代它。在最坏的情况下，是
    B 树。最好的情况下，更高效。
- en: The quick results version here, is we find we have four different data sets.
    Most are integer data sets; last one is string data set. We're trying to save
    memory and speed; we save memory hugely; these are really simple models. Linear
    with simple layer, with possibly two stages. We're able to get a significant speedup
    in these cases. Server logs one is interesting. It looks at a high level very
    linear, but there's actually daily patterns to this data accessed. Maps is more
    linear; it's longitudes of spaces. We created synthetic data that's log normal,
    and here we see we can model it effectively. Strings is an interesting challenge
    going forward; your data is larger and more complicated, building models that
    are efficient over a really long string is different; the overall patterns are
    harder to have intuition about. One thing really worth noting here, it's not using
    GPUs or TPUs; it's pureely CPU comparison. Apples-to-apples.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的快速结果版本是，我们发现我们有四种不同的数据集。大多数是整数数据集；最后一个是字符串数据集。我们试图节省内存和速度；我们极大地节省内存；这些都是非常简单的模型。线性与简单层，可能有两个阶段。在这些情况下，我们能够获得显著的加速。服务器日志是有趣的。从高层看似乎是非常线性的，但实际上这些数据访问有每日模式。地图更线性；是空间的经度。我们创建了对数正态的合成数据，在这里我们看到我们可以有效地建模。字符串是一个有趣的挑战；您的数据更大更复杂，构建在长字符串上高效的模型是不同的；整体模式更难以直觉理解。这里真正值得注意的一点是，它不使用
    GPU 或 TPU；这纯粹是 CPU 对比。一比一。
- en: This is mostly going into the B-tree part. This is a regression model looking
    at CDF of data. We can use these exact same models for hash maps. With bloom filters,
    you can use binary classifiers. I have a bunch of results in the poster in the
    back.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这主要涉及到 B 树部分。这是一个回归模型，看数据的 CDF。我们可以用这些完全相同的模型用于哈希映射。用布隆过滤器，你可以使用二进制分类器。我在后面的海报上有一堆结果。
- en: A few minutes to talk about rooms for improvement. There are a bunch of directions
    that we're excited to explore. Obvious one is GPUs/TPUs. It's cPUs because that's
    when B-trees are most effective; but scaling is all about ML. Improving throughput
    and latency for models with GPUs, exciting going forward. Modeling themselves;
    there's no reason to believe hierarchy of models is the right or best choice;
    it's interesting to build model structures that match your hardware. Memory efficient,
    underlying architecture of GPUs. In the scale of ns we need for database. Multidimensional
    indexes; ML excels in high numbers of dimension; most things are not looking at
    a single integer feature. There's interesting question about how you map to multidimensional
    indexes that are difficult to scale. If we have a CDF, you can approximately sort
    it right there. And inserts and updates, assumed read-only databases. Large class
    of systems, but we get more data. How do we balance overfitting with accuracy;
    can we add some extra auxiliary data structures to balance this out?
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 几分钟来谈谈改进的空间。有很多我们很兴奋要探索的方向。显而易见的一个是GPU/TPU。它是CPU，因为在B树最有效的时候；但是扩展性全都是关于机器学习的。改进GPU模型的吞吐量和延迟，前景充满挑战。建模本身；没有理由相信模型层次结构是正确或最佳选择；构建与硬件匹配的模型结构是很有趣的。内存高效，GPU的底层架构。在数据库所需的ns级别的规模上。多维索引；机器学习在高维数上表现出色；大多数事物不是在单个整数特征上进行观察。如何将难以扩展的多维索引映射到的有趣问题。如果有一个CDF，你可以在那里近似地对其进行排序。和插入和更新，假设只读数据库。大类系统，但我们获取更多的数据。如何在过拟合与准确性之间取得平衡；我们能够添加一些额外的辅助数据结构来平衡这一点吗？
- en: 'Q: One thing is that when... this problem, we solved pretty well without ML.
    When we introduce ML, we should introduce new metrics. We shouldn''t make our
    system more fragile, because distribution changes. What would be the worst case
    when distribution changes?'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 'Q: 有一件事是，当... 这个问题时，我们在没有机器学习的情况下解决得非常好。当我们引入机器学习时，我们应该引入新的度量标准。我们不应该让我们的系统更加脆弱，因为分布会改变。当分布改变时，最坏的情况会是什么？'
- en: 'A: As the data becomes updated... in the case of inference and updates, there''s
    a question about generalization. I think you could look at it from the ML point
    of view: statistically, test model today on tomorrows inserts. (It''s a method.
    If I use this method, and then train it with data that I don''t yet have... and
    do.) The typical extrapolation to future generalization of ML. Guarantees are
    hard. There will be a worst case that is awful... but the flip side, that''s the
    ML side... generalization. There''s also a point of view, I couple this with classic
    data structure. we coupled modeling with classic data structures: search, bloom
    filter case, so you don''t actually have this work. You catch worst case.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 'A: 随着数据的更新... 在推断和更新的情况下，有一个关于泛化的问题。我认为你可以从机器学习的角度来看待它：统计上，今天测试模型，明天的插入数据。
    （这是一种方法。如果我使用这种方法，然后用我还没有的数据来训练它... 并且...）典型的推广到未来机器学习的泛化。保证是困难的。会有一个最糟糕的情况是糟糕的...
    但另一面，那是机器学习的一面... 泛化。还有一个观点，我将这与经典数据结构相结合。我们将建模与经典数据结构相结合：搜索、布隆过滤器案例，所以你实际上没有这项工作。你抓住了最坏的情况。'
- en: Let me add to that. If you assume that the inserts follow the same distribution
    as trained model, then the inserts become all one operation. They're even better.
    Suppose they don't follow the same distribution? you can still do delta indexing.
    Most systems do do delta indexing. So inserts are not a big problem.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 让我补充一下。如果你假设插入数据遵循与训练模型相同的分布，那么插入数据就成为一个操作。它们甚至更好。假设它们不遵循相同的分布？你仍然可以做增量索引。大多数系统确实做增量索引。所以插入数据不是一个大问题。
- en: 'Q: (Robert) Most of the inputs were one or two real numbers, and outputs are
    a single real number. how does it work if you use a low degree polynomial, or
    a piecewise linear classifier on the different digits?'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 'Q: （罗伯特）大部分输入是一个或两个实数，输出是一个单一的实数。如果你在不同的数字上使用低次多项式，或者分段线性分类器，它是如何工作的？'
- en: 'A: In the case of strings, it''s not a single input. (Treat it as integer?)
    Well, it''s possibly a thousand characters long. It''s not the best representation.
    Different representations work really well. The last thing I want to say, piecewise
    linear could work, but when you run 10k, 100k submodels, it''s slow. Hierarchy
    helps. Polynomials are interesting, depends on data source.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 'A: 对于字符串来说，它不是单一的输入。（把它当作整数处理？）嗯，它可能长达一千个字符。这不是最好的表示方式。不同的表示方式确实效果很好。我想说的最后一件事是，分段线性可能有效，但当你运行10k、100k个子模型时，速度会很慢。层次结构有助于。多项式很有趣，取决于数据来源。'
- en: 'Q: Can you comment how bad your worst case is? Average numbers?'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 'Q: 你能评论一下你们的最坏情况有多糟吗？平均数是多少？'
- en: 'A: We specifically always have a spillover. The worst case is defaulting to
    typical database. We haven''t had a case where you do worse, because we''ll default
    to B-tree. (Deterministic execution?) Not inference time.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 'A: 我们总是会有溢出的情况。最坏的情况是默认到典型的数据库。我们还没有遇到更糟的情况，因为我们会默认使用B树。（确定性执行？）不是推断时间。'
