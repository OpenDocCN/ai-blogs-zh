- en: Learning Word Embedding
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习词嵌入
- en: 原文：[https://lilianweng.github.io/posts/2017-10-15-word-embedding/](https://lilianweng.github.io/posts/2017-10-15-word-embedding/)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://lilianweng.github.io/posts/2017-10-15-word-embedding/](https://lilianweng.github.io/posts/2017-10-15-word-embedding/)
- en: Human vocabulary comes in free text. In order to make a machine learning model
    understand and process the natural language, we need to transform the free-text
    words into numeric values. One of the simplest transformation approaches is to
    do a one-hot encoding in which each distinct word stands for one dimension of
    the resulting vector and a binary value indicates whether the word presents (1)
    or not (0).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 人类词汇以自由文本形式存在。为了使机器学习模型理解和处理自然语言，我们需要将自由文本单词转换为数值。最简单的转换方法之一是进行一位有效编码，其中每个不同的单词代表结果向量的一个维度，二进制值表示单词是否存在（1）或不存在（0）。
- en: However, one-hot encoding is impractical computationally when dealing with the
    entire vocabulary, as the representation demands hundreds of thousands of dimensions.
    Word embedding represents words and phrases in vectors of (non-binary) numeric
    values with much lower and thus denser dimensions. An intuitive assumption for
    good word embedding is that they can approximate the similarity between words
    (i.e., “cat” and “kitten” are similar words, and thus they are expected to be
    close in the reduced vector space) or disclose hidden semantic relationships (i.e.,
    the relationship between “cat” and “kitten” is an analogy to the one between “dog”
    and “puppy”). Contextual information is super useful for learning word meaning
    and relationship, as similar words may appear in the similar context often.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在处理整个词汇表时，使用一位有效编码在计算上是不切实际的，因为表示需要数十万维度。词嵌入用更低维度和更密集的（非二进制）数值向量表示单词和短语。一个对良好词嵌入的直观假设是它们可以近似单词之间的相似性（即，“猫”和“小猫”是相似的单词，因此它们在降维向量空间中应该接近）或揭示隐藏的语义关系（即，“猫”和“小猫”之间的关系类似于“狗”和“小狗”之间的关系）。上下文信息对于学习单词含义和关系非常有用，因为相似的单词经常出现在相似的上下文中。
- en: There are two main approaches for learning word embedding, both relying on the
    contextual knowledge.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 学习词嵌入有两种主要方法，都依赖于上下文知识。
- en: '**Count-based**: The first one is unsupervised, based on matrix factorization
    of a global word co-occurrence matrix. Raw co-occurrence counts do not work well,
    so we want to do smart things on top.'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于计数**：第一种是无监督的，基于全局单词共现矩阵的矩阵分解。原始共现计数效果不佳，因此我们希望在此基础上做一些聪明的事情。'
- en: '**Context-based**: The second approach is supervised. Given a local context,
    we want to design a model to predict the target words and in the meantime, this
    model learns the efficient word embedding representation.'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于上下文**：第二种方法是监督的。给定一个局部上下文，我们希望设计一个模型来预测目标单词，同时，该模型学习高效的词嵌入表示。'
- en: Count-Based Vector Space Model
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于计数的向量空间模型
- en: Count-based vector space models heavily rely on the word frequency and co-occurrence
    matrix with the assumption that words in the same contexts share similar or related
    semantic meanings. The models map count-based statistics like co-occurrences between
    neighboring words down to a small and dense word vectors. PCA, topic models, and
    neural probabilistic language models are all good examples of this category.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 基于计数的向量空间模型严重依赖于单词频率和共现矩阵，假设在相同上下文中的单词具有相似或相关的语义含义。这些模型将基于计数的统计数据（如相邻单词之间的共现）映射到小而密集的单词向量中。PCA、主题模型和神经概率语言模型都是这一类别的良好示例。
- en: '* * *'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Different from the count-based approaches, context-based methods build predictive
    models that directly target at predicting a word given its neighbors. The dense
    word vectors are part of the model parameters. The best vector representation
    of each word is learned during the model training process.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 与基于计数的方法不同，基于上下文的方法构建预测模型，直接针对预测给定其邻居的单词。密集的单词向量是模型参数的一部分。在模型训练过程中学习每个单词的最佳向量表示。
- en: 'Context-Based: Skip-Gram Model'
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于上下文：Skip-Gram 模型
- en: 'Suppose that you have a sliding window of a fixed size moving along a sentence:
    the word in the middle is the “target” and those on its left and right within
    the sliding window are the context words. The skip-gram model ([Mikolov et al.,
    2013](https://arxiv.org/pdf/1301.3781.pdf)) is trained to predict the probabilities
    of a word being a context word for the given target.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你有一个固定大小的滑动窗口沿着句子移动：中间的词是“目标”，在其左右范围内的词是上下文词。跳字模型（[Mikolov et al., 2013](https://arxiv.org/pdf/1301.3781.pdf)）被训练以预测给定目标的单词作为上下文词的概率。
- en: The following example demonstrates multiple pairs of target and context words
    as training samples, generated by a 5-word window sliding along the sentence.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例演示了多对目标和上下文单词作为训练样本，由一个5个单词窗口沿着句子滑动生成。
- en: “The man who passes the sentence should swing the sword.” – Ned Stark
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “宣判者应该挥动剑。” – 尼德·史塔克
- en: '| Sliding window (size = 5) | Target word | Context |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| 滑动窗口（大小=5） | 目标词 | 上下文 |'
- en: '| --- | --- | --- |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| [The man who] | the | man, who |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| [宣判者] | 宣判者 | 宣判者 |'
- en: '| [The man who passes] | man | the, who, passes |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| [宣判者] | 宣判者 | 宣判者 |'
- en: '| [The man who passes the] | who | the, man, passes, the |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| [宣判者] | 宣判者 | 宣判者 |'
- en: '| [man who passes the sentence] | passes | man, who, the, sentence |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| [宣判者] | 宣判者 | 宣判者 |'
- en: '| … | … | … |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| … | … | … |'
- en: '| [sentence should swing the sword] | swing | sentence, should, the, sword
    |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| [宣判者应该挥动剑] | 挥动 | 宣判者，应该，the，剑 |'
- en: '| [should swing the sword] | the | should, swing, sword |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| [应该挥动剑] | the | 应该，挥动，剑 |'
- en: '| [swing the sword] | sword | swing, the |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| [挥动剑] | 挥动 | 挥动，the |'
- en: '| {:.info} |  |  |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| {:.info} |  |  |'
- en: 'Each context-target pair is treated as a new observation in the data. For example,
    the target word “swing” in the above case produces four training samples: (“swing”,
    “sentence”), (“swing”, “should”), (“swing”, “the”), and (“swing”, “sword”).'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 每个上下文-目标对被视为数据中的一个新观察。例如，上述情况中目标词“挥动”产生四个训练样本：（“挥动”，“宣判者”），（“挥动”，“应该”），（“挥动”，“the”），和（“挥动”，“剑”）。
- en: '![](../Images/e5151d376bc1aa711c3570622dbea589.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e5151d376bc1aa711c3570622dbea589.png)'
- en: Fig. 1\. The skip-gram model. Both the input vector $\mathbf{x}$ and the output
    $\mathbf{y}$ are one-hot encoded word representations. The hidden layer is the
    word embedding of size $N$.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图1\. 跳字模型。输入向量$\mathbf{x}$和输出$\mathbf{y}$都是独热编码的单词表示。隐藏层是大小为$N$的单词嵌入。
- en: Given the vocabulary size $V$, we are about to learn word embedding vectors
    of size $N$. The model learns to predict one context word (output) using one target
    word (input) at a time.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 给定词汇量$V$，我们将学习大小为$N$的单词嵌入向量。该模型学会一次使用一个目标词（输入）来预测一个上下文词（输出）。
- en: According to Fig. 1,
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 根据图1，
- en: Both input word $w_i$ and the output word $w_j$ are one-hot encoded into binary
    vectors $\mathbf{x}$ and $\mathbf{y}$ of size $V$.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入词$w_i$和输出词$w_j$都被独热编码为大小为$V$的二进制向量$\mathbf{x}$和$\mathbf{y}$。
- en: 'First, the multiplication of the binary vector $\mathbf{x}$ and the word embedding
    matrix $W$ of size $V \times N$ gives us the embedding vector of the input word
    $w_i$: the i-th row of the matrix $W$.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，二进制向量$\mathbf{x}$和大小为$V \times N$的单词嵌入矩阵$W$的乘积给出了输入词$w_i$的嵌入向量：矩阵$W$的第i行。
- en: This newly discovered embedding vector of dimension $N$ forms the hidden layer.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个新发现的维度为$N$的嵌入向量形成了隐藏层。
- en: The multiplication of the hidden layer and the word context matrix $W’$ of size
    $N \times V$ produces the output one-hot encoded vector $\mathbf{y}$.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 隐藏层和大小为$N \times V$的单词上下文矩阵$W’$的乘积产生输出独热编码向量$\mathbf{y}$。
- en: 'The output context matrix $W’$ encodes the meanings of words as context, different
    from the embedding matrix $W$. NOTE: Despite the name, $W’$ is independent of
    $W$, not a transpose or inverse or whatsoever.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出上下文矩阵$W’$将单词的含义编码为上下文，与嵌入矩阵$W$不同。注意：尽管名字是$W’$，但它与$W$是独立的，不是转置或逆之类的。
- en: 'Context-Based: Continuous Bag-of-Words (CBOW)'
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于上下文：连续词袋（CBOW）
- en: The Continuous Bag-of-Words (CBOW) is another similar model for learning word
    vectors. It predicts the target word (i.e. “swing”) from source context words
    (i.e., “sentence should the sword”).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 连续词袋（CBOW）是另一个类似的学习单词向量的模型。它从源上下文单词（即“宣判者应该the剑”）中预测目标单词（即“挥动”）。
- en: '![](../Images/c2bbdbb91902ffebc2ad8e5da68ddd3a.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c2bbdbb91902ffebc2ad8e5da68ddd3a.png)'
- en: Fig. 2\. The CBOW model. Word vectors of multiple context words are averaged
    to get a fixed-length vector as in the hidden layer. Other symbols have the same
    meanings as in Fig 1.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2\. CBOW 模型。多个上下文单词的词向量被平均以获得一个固定长度的向量，就像隐藏层中那样。其他符号的含义与图 1 中相同。
- en: Because there are multiple contextual words, we average their corresponding
    word vectors, constructed by the multiplication of the input vector and the matrix
    $W$. Because the averaging stage smoothes over a lot of the distributional information,
    some people believe the CBOW model is better for small dataset.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 因为有多个上下文单词，我们对它们对应的单词向量进行平均，由输入向量和矩阵 $W$ 的乘积构成。由于平均阶段平滑了许多分布信息，一些人认为 CBOW 模型对小数据集更好。
- en: Loss Functions
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 损失函数
- en: Both the skip-gram model and the CBOW model should be trained to minimize a
    well-designed loss/objective function. There are several loss functions we can
    incorporate to train these language models. In the following discussion, we will
    use the skip-gram model as an example to describe how the loss is computed.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 跳字模型和 CBOW 模型都应该经过训练以最小化一个精心设计的损失/目标函数。我们可以结合几种损失函数来训练这些语言模型。在接下来的讨论中，我们将以跳字模型为例来描述损失是如何计算的。
- en: Full Softmax
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 完全 Softmax
- en: 'The skip-gram model defines the embedding vector of every word by the matrix
    $W$ and the context vector by the output matrix $W’$. Given an input word $w_I$,
    let us label the corresponding row of $W$ as vector $v_{w_I}$ (embedding vector)
    and its corresponding column of $W’$ as $v’_{w_I}$ (context vector). The final
    output layer applies softmax to compute the probability of predicting the output
    word $w_O$ given $w_I$, and therefore:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 跳字模型通过矩阵 $W$ 定义每个单词的嵌入向量，通过输出矩阵 $W'$ 定义上下文向量。给定一个输入单词 $w_I$，让我们将 $W$ 的对应行标记为向量
    $v_{w_I}$（嵌入向量），将 $W'$ 的对应列标记为 $v'_{w_I}$（上下文向量）。最终输出层应用 softmax 来计算给定 $w_I$ 预测输出单词
    $w_O$ 的概率，因此：
- en: $$ p(w_O \vert w_I) = \frac{\exp({v'_{w_O}}^{\top} v_{w_I})}{\sum_{i=1}^V \exp({v'_{w_i}}^{\top}
    v_{w_I})} $$
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: $$ p(w_O \vert w_I) = \frac{\exp({v'_{w_O}}^{\top} v_{w_I})}{\sum_{i=1}^V \exp({v'_{w_i}}^{\top}
    v_{w_I})} $$
- en: This is accurate as presented in Fig. 1\. However, when $V$ is extremely large,
    calculating the denominator by going through all the words for every single sample
    is computationally impractical. The demand for more efficient conditional probability
    estimation leads to the new methods like *hierarchical softmax*.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这在图 1 中的呈现是准确的。然而，当 $V$ 非常大时，为每个样本计算分母通过遍历所有单词是计算上不可行的。对更高效的条件概率估计的需求导致了像*分层
    Softmax*这样的新方法的出现。
- en: Hierarchical Softmax
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分层 Softmax
- en: Morin and Bengio ([2005](https://www.iro.umontreal.ca/~lisa/pointeurs/hierarchical-nnlm-aistats05.pdf))
    proposed hierarchical softmax to make the sum calculation faster with the help
    of a binary tree structure. The hierarchical softmax encodes the language model’s
    output softmax layer into a tree hierarchy, where each leaf is one word and each
    internal node stands for relative probabilities of the children nodes.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: Morin 和 Bengio（[2005](https://www.iro.umontreal.ca/~lisa/pointeurs/hierarchical-nnlm-aistats05.pdf)）提出了分层
    Softmax，通过二叉树结构使求和计算更快。分层 Softmax 将语言模型的输出 softmax 层编码成树形层次结构，其中每个叶子代表一个单词，每个内部节点代表子节点的相对概率。
- en: '![](../Images/e738e537d3a5018f75cf104afc353a85.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e738e537d3a5018f75cf104afc353a85.png)'
- en: 'Fig. 3\. An illustration of the hierarchical softmax binary tree. The leaf
    nodes in white are words in the vocabulary. The gray inner nodes carry information
    on the probabilities of reaching its child nodes. One path starting from the root
    to the leaf $w\_i$. $n(w\_i, j)$ denotes the j-th node on this path. (Image source:
    [word2vec Parameter Learning Explained](https://arxiv.org/pdf/1411.2738.pdf))'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3\. 分层 Softmax 二叉树的示意图。白色的叶子节点是词汇中的单词。灰色的内部节点携带到达其子节点的概率信息。从根节点到叶子 $w\_i$
    的一条路径。$n(w\_i, j)$ 表示这条路径上的第 j 个节点。（图片来源：[word2vec 参数学习解释](https://arxiv.org/pdf/1411.2738.pdf)）
- en: Each word $w_i$ has a unique path from the root down to its corresponding leaf.
    The probability of picking this word is equivalent to the probability of taking
    this path from the root down through the tree branches. Since we know the embedding
    vector $v_n$ of the internal node $n$, the probability of getting the word can
    be computed by the product of taking left or right turn at every internal node
    stop.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 每个单词 $w_i$ 从根节点到其对应叶子节点有一条唯一路径。选择该单词的概率等同于从根节点通过树枝向下走到该路径的概率。由于我们知道内部节点 $n$
    的嵌入向量 $v_n$，得到该单词的概率可以通过在每个内部节点停止时取左转或右转的乘积来计算。
- en: 'According to Fig. 3, the probability of one node is ($\sigma$ is the sigmoid
    function):'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 根据图3，一个节点的概率是（$\sigma$是sigmoid函数）：
- en: $$ \begin{align} p(\text{turn right} \to \dots w_I \vert n) &= \sigma({v'_n}^{\top}
    v_{w_I})\\ p(\text{turn left } \to \dots w_I \vert n) &= 1 - p(\text{turn right}
    \vert n) = \sigma(-{v'_n}^{\top} v_{w_I}) \end{align} $$
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \begin{align} p(\text{向右转} \to \dots w_I \vert n) &= \sigma({v'_n}^{\top}
    v_{w_I})\\ p(\text{向左转} \to \dots w_I \vert n) &= 1 - p(\text{向右转} \vert n) =
    \sigma(-{v'_n}^{\top} v_{w_I}) \end{align} $$
- en: 'The final probability of getting a context word $w_O$ given an input word $w_I$
    is:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 给定输入词$w_I$时获得上下文词$w_O$的最终概率是：
- en: $$ p(w_O \vert w_I) = \prod_{k=1}^{L(w_O)} \sigma(\mathbb{I}_{\text{turn}}(n(w_O,
    k), n(w_O, k+1)) \cdot {v'_{n(w_O, k)}}^{\top} v_{w_I}) $$
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: $$ p(w_O \vert w_I) = \prod_{k=1}^{L(w_O)} \sigma(\mathbb{I}_{\text{turn}}(n(w_O,
    k), n(w_O, k+1)) \cdot {v'_{n(w_O, k)}}^{\top} v_{w_I}) $$
- en: where $L(w_O)$ is the depth of the path leading to the word $w_O$ and $\mathbb{I}_{\text{turn}}$
    is a specially indicator function which returns 1 if $n(w_O, k+1)$ is the left
    child of $n(w_O, k)$ otherwise -1\. The internal nodes’ embeddings are learned
    during the model training. The tree structure helps greatly reduce the complexity
    of the denominator estimation from O(V) (vocabulary size) to O(log V) (the depth
    of the tree) at the training time. However, at the prediction time, we still to
    compute the probability of every word and pick the best, as we don’t know which
    leaf to reach for in advance.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$L(w_O)$是通向单词$w_O$的路径的深度，$\mathbb{I}_{\text{turn}}$是一个特殊的指示函数，如果$n(w_O, k+1)$是$n(w_O,
    k)$的左子节点，则返回1，否则返回-1。内部节点的嵌入是在模型训练期间学习的。树结构在训练时极大地减少了分母估计的复杂度，从O(V)（词汇量大小）降低到O(log
    V)（树的深度）。然而，在预测时，我们仍然需要计算每个单词的概率并选择最佳的，因为我们事先不知道要到达哪个叶子节点。
- en: 'A good tree structure is crucial to the model performance. Several handy principles
    are: group words by frequency like what is implemented by Huffman tree for simple
    speedup; group similar words into same or close branches (i.e. use predefined
    word clusters, WordNet).'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 一个良好的树结构对模型性能至关重要。几个方便的原则是：按频率分组单词，就像哈夫曼树为简单加速所实现的那样；将相似的单词分组到相同或相近的分支中（即使用预定义的单词簇，WordNet）。
- en: Cross Entropy
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 交叉熵
- en: Another approach completely steers away from the softmax framework. Instead,
    the loss function measures the cross entropy between the predicted probabilities
    $p$ and the true binary labels $\mathbf{y}$.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法完全摆脱了softmax框架。相反，损失函数衡量了预测概率$p$和真实二进制标签$\mathbf{y}$之间的交叉熵。
- en: First, let’s recall that the cross entropy between two distributions $p$ and
    $q$ is measured as $ H(p, q) = -\sum_x p(x) \log q(x) $. In our case, the true
    label $y_i$ is 1 only when $w_i$ is the output word; $y_j$ is 0 otherwise. The
    loss function $\mathcal{L}_\theta$ of the model with parameter config $\theta$
    aims to minimize the cross entropy between the prediction and the ground truth,
    as lower cross entropy indicates high similarity between two distributions.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们回顾一下两个分布$p$和$q$之间的交叉熵是如何衡量的，即$ H(p, q) = -\sum_x p(x) \log q(x)$。在我们的情况下，当$w_i$是输出词时，真实标签$y_i$为1；否则$y_j$为0。模型的损失函数$\mathcal{L}_\theta$旨在最小化预测与真实之间的交叉熵，因为较低的交叉熵表示两个分布之间的高相似性。
- en: $$ \mathcal{L}_\theta = - \sum_{i=1}^V y_i \log p(w_i | w_I) = - \log p(w_O
    \vert w_I) $$
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \mathcal{L}_\theta = - \sum_{i=1}^V y_i \log p(w_i | w_I) = - \log p(w_O
    \vert w_I) $$
- en: Recall that,
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 请回忆，
- en: $$ p(w_O \vert w_I) = \frac{\exp({v'_{w_O}}^{\top} v_{w_I})}{\sum_{i=1}^V \exp({v'_{w_i}}^{\top}
    v_{w_I})} $$
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: $$ p(w_O \vert w_I) = \frac{\exp({v'_{w_O}}^{\top} v_{w_I})}{\sum_{i=1}^V \exp({v'_{w_i}}^{\top}
    v_{w_I})} $$
- en: Therefore,
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，
- en: $$ \mathcal{L}_{\theta} = - \log \frac{\exp({v'_{w_O}}^{\top}{v_{w_I}})}{\sum_{i=1}^V
    \exp({v'_{w_i}}^{\top}{v_{w_I} })} = - {v'_{w_O}}^{\top}{v_{w_I} } + \log \sum_{i=1}^V
    \exp({v'_{w_i} }^{\top}{v_{w_I}}) $$
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \mathcal{L}_{\theta} = - \log \frac{\exp({v'_{w_O}}^{\top}{v_{w_I}})}{\sum_{i=1}^V
    \exp({v'_{w_i}}^{\top}{v_{w_I} })} = - {v'_{w_O}}^{\top}{v_{w_I} } + \log \sum_{i=1}^V
    \exp({v'_{w_i} }^{\top}{v_{w_I}}) $$
- en: To start training the model using back-propagation with SGD, we need to compute
    the gradient of the loss function. For simplicity, let’s label $z_{IO} = {v’_{w_O}}^{\top}{v_{w_I}}$.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始使用反向传播和SGD训练模型，我们需要计算损失函数的梯度。为简单起见，让我们标记$z_{IO} = {v’_{w_O}}^{\top}{v_{w_I}}$。
- en: $$ \begin{align} \nabla_\theta \mathcal{L}_{\theta} &= \nabla_\theta\big( -
    z_{IO} + \log \sum_{i=1}^V e^{z_{Ii}} \big) \\ &= - \nabla_\theta z_{IO} + \nabla_\theta
    \big( \log \sum_{i=1}^V e^{z_{Ii}} \big) \\ &= - \nabla_\theta z_{IO} + \frac{1}{\sum_{i=1}^V
    e^{z_{Ii}}} \sum_{i=1}^V e^{z_{Ii}} \nabla_\theta z_{Ii} \\ &= - \nabla_\theta
    z_{IO} + \sum_{i=1}^V \frac{e^{z_{Ii}}}{\sum_{i=1}^V e^{z_{Ii}}} \nabla_\theta
    z_{Ii} \\ &= - \nabla_\theta z_{IO} + \sum_{i=1}^V p(w_i \vert w_I) \nabla_\theta
    z_{Ii} \\ &= - \nabla_\theta z_{IO} + \mathbb{E}_{w_i \sim Q(\tilde{w})} \nabla_\theta
    z_{Ii} \end{align} $$
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \begin{align} \nabla_\theta \mathcal{L}_{\theta} &= \nabla_\theta\big( -
    z_{IO} + \log \sum_{i=1}^V e^{z_{Ii}} \big) \\ &= - \nabla_\theta z_{IO} + \nabla_\theta
    \big( \log \sum_{i=1}^V e^{z_{Ii}} \big) \\ &= - \nabla_\theta z_{IO} + \frac{1}{\sum_{i=1}^V
    e^{z_{Ii}}} \sum_{i=1}^V e^{z_{Ii}} \nabla_\theta z_{Ii} \\ &= - \nabla_\theta
    z_{IO} + \sum_{i=1}^V \frac{e^{z_{Ii}}}{\sum_{i=1}^V e^{z_{Ii}}} \nabla_\theta
    z_{Ii} \\ &= - \nabla_\theta z_{IO} + \sum_{i=1}^V p(w_i \vert w_I) \nabla_\theta
    z_{Ii} \\ &= - \nabla_\theta z_{IO} + \mathbb{E}_{w_i \sim Q(\tilde{w})} \nabla_\theta
    z_{Ii} \end{align} $$
- en: where $Q(\tilde{w})$ is the distribution of noise samples.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $Q(\tilde{w})$ 是噪声样本的分布。
- en: According to the formula above, the correct output word has a positive reinforcement
    according to the first term (the larger $\nabla_\theta z_{IO}$ the better loss
    we have), while other words have a negative impact as captured by the second term.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 根据上述公式，正确的输出单词根据第一项有正向强化（$\nabla_\theta z_{IO}$ 越大，我们的损失就越好），而其他单词则受到第二项的负面影响。
- en: How to estimate $\mathbb{E}_{w_i \sim Q(\tilde{w})} \nabla_\theta {v’_{w_i}}^{\top}{v_{w_I}}$
    with a sample set of noise words rather than scanning through the entire vocabulary
    is the key of using cross-entropy-based sampling approach.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 如何估计 $\mathbb{E}_{w_i \sim Q(\tilde{w})} \nabla_\theta {v’_{w_i}}^{\top}{v_{w_I}}$，而不是通过扫描整个词汇表来使用基于交叉熵的抽样方法，这是关键。
- en: Noise Contrastive Estimation (NCE)
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 噪声对比估计（NCE）
- en: The Noise Contrastive Estimation (NCE) metric intends to differentiate the target
    word from noise samples using a logistic regression classifier ([Gutmann and Hyvärinen,
    2010](http://proceedings.mlr.press/v9/gutmann10a/gutmann10a.pdf)).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 噪声对比估计（NCE）指标旨在使用 logistic 回归分类器（[Gutmann and Hyvärinen, 2010](http://proceedings.mlr.press/v9/gutmann10a/gutmann10a.pdf)）区分目标单词和噪声样本。
- en: Given an input word $w_I$, the correct output word is known as $w$. In the meantime,
    we sample $N$ other words from the noise sample distribution $Q$, denoted as $\tilde{w}_1,
    \tilde{w}_2, \dots, \tilde{w}_N \sim Q$. Let’s label the decision of the binary
    classifier as $d$ and $d$$ can only take a binary value.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 给定输入单词 $w_I$，正确的输出单词称为 $w$。同时，我们从噪声样本分布 $Q$ 中抽样 $N$ 个其他单词，表示为 $\tilde{w}_1,
    \tilde{w}_2, \dots, \tilde{w}_N \sim Q$。让我们将二元分类器的决策标记为 $d$，$d$ 只能取二进制值。
- en: $$ \mathcal{L}_\theta = - [ \log p(d=1 \vert w, w_I) + \sum_{i=1, \tilde{w}_i
    \sim Q}^N \log p(d=0|\tilde{w}_i, w_I) ] $$
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \mathcal{L}_\theta = - [ \log p(d=1 \vert w, w_I) + \sum_{i=1, \tilde{w}_i
    \sim Q}^N \log p(d=0|\tilde{w}_i, w_I) ] $$
- en: When $N$ is big enough, according to [the Law of large numbers](https://en.wikipedia.org/wiki/Law_of_large_numbers),
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 当 $N$ 足够大时，根据[大数定律](https://en.wikipedia.org/wiki/Law_of_large_numbers)，
- en: $$ \mathcal{L}_\theta = - [ \log p(d=1 \vert w, w_I) + N\mathbb{E}_{\tilde{w}_i
    \sim Q} \log p(d=0|\tilde{w}_i, w_I)] $$
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \mathcal{L}_\theta = - [ \log p(d=1 \vert w, w_I) + N\mathbb{E}_{\tilde{w}_i
    \sim Q} \log p(d=0|\tilde{w}_i, w_I)] $$
- en: To compute the probability $p(d=1 \vert w, w_I)$, we can start with the joint
    probability $p(d, w \vert w_I)$. Among $w, \tilde{w}_1, \tilde{w}_2, \dots, \tilde{w}_N$,
    we have 1 out of (N+1) chance to pick the true word $w$, which is sampled from
    the conditional probability $p(w \vert w_I)$; meanwhile, we have N out of (N+1)
    chances to pick a noise word, each sampled from $q(\tilde{w}) \sim Q$. Thus,
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算概率 $p(d=1 \vert w, w_I)$，我们可以从联合概率 $p(d, w \vert w_I)$ 开始。在 $w, \tilde{w}_1,
    \tilde{w}_2, \dots, \tilde{w}_N$ 中，我们有 1/(N+1) 的机会选择真实单词 $w$，它是从条件概率 $p(w \vert
    w_I)$ 中抽样得到的；同时，我们有 N/(N+1) 的机会选择一个噪声单词，每个都是从 $q(\tilde{w}) \sim Q$ 中抽样得到的。因此，
- en: $$ p(d, w | w_I) = \begin{cases} \frac{1}{N+1} p(w \vert w_I) & \text{if } d=1
    \\ \frac{N}{N+1} q(\tilde{w}) & \text{if } d=0 \end{cases} $$
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: $$ p(d, w | w_I) = \begin{cases} \frac{1}{N+1} p(w \vert w_I) & \text{if } d=1
    \\ \frac{N}{N+1} q(\tilde{w}) & \text{if } d=0 \end{cases} $$
- en: 'Then we can figure out $p(d=1 \vert w, w_I)$ and $p(d=0 \vert w, w_I)$:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以计算出 $p(d=1 \vert w, w_I)$ 和 $p(d=0 \vert w, w_I)$：
- en: $$ \begin{align} p(d=1 \vert w, w_I) &= \frac{p(d=1, w \vert w_I)}{p(d=1, w
    \vert w_I) + p(d=0, w \vert w_I)} &= \frac{p(w \vert w_I)}{p(w \vert w_I) + Nq(\tilde{w})}
    \end{align} $$$$ \begin{align} p(d=0 \vert w, w_I) &= \frac{p(d=0, w \vert w_I)}{p(d=1,
    w \vert w_I) + p(d=0, w \vert w_I)} &= \frac{Nq(\tilde{w})}{p(w \vert w_I) + Nq(\tilde{w})}
    \end{align} $$
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \begin{align} p(d=1 \vert w, w_I) &= \frac{p(d=1, w \vert w_I)}{p(d=1, w
    \vert w_I) + p(d=0, w \vert w_I)} &= \frac{p(w \vert w_I)}{p(w \vert w_I) + Nq(\tilde{w})}
    \end{align} $$$$ \begin{align} p(d=0 \vert w, w_I) &= \frac{p(d=0, w \vert w_I)}{p(d=1,
    w \vert w_I) + p(d=0, w \vert w_I)} &= \frac{Nq(\tilde{w})}{p(w \vert w_I) + Nq(\tilde{w})}
    \end{align} $$
- en: 'Finally the loss function of NCE’s binary classifier becomes:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，NCE的二元分类器的损失函数变为：
- en: $$ \begin{align} \mathcal{L}_\theta & = - [ \log p(d=1 \vert w, w_I) + \sum_{\substack{i=1
    \\ \tilde{w}_i \sim Q}}^N \log p(d=0|\tilde{w}_i, w_I)] \\ & = - [ \log \frac{p(w
    \vert w_I)}{p(w \vert w_I) + Nq(\tilde{w})} + \sum_{\substack{i=1 \\ \tilde{w}_i
    \sim Q}}^N \log \frac{Nq(\tilde{w}_i)}{p(w \vert w_I) + Nq(\tilde{w}_i)}] \end{align}
    $$
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \begin{align} \mathcal{L}_\theta & = - [ \log p(d=1 \vert w, w_I) + \sum_{\substack{i=1
    \\ \tilde{w}_i \sim Q}}^N \log p(d=0|\tilde{w}_i, w_I)] \\ & = - [ \log \frac{p(w
    \vert w_I)}{p(w \vert w_I) + Nq(\tilde{w})} + \sum_{\substack{i=1 \\ \tilde{w}_i
    \sim Q}}^N \log \frac{Nq(\tilde{w}_i)}{p(w \vert w_I) + Nq(\tilde{w}_i)}] \end{align}
    $$
- en: 'However, $p(w \vert w_I)$ still involves summing up the entire vocabulary in
    the denominator. Let’s label the denominator as a partition function of the input
    word, $Z(w_I)$. A common assumption is $Z(w) \approx 1$ given that we expect the
    softmax output layer to be normalized ([Minh and Teh, 2012](https://www.cs.toronto.edu/~amnih/papers/ncelm.pdf)).
    Then the loss function is simplified to:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，$p(w \vert w_I)$仍然涉及将整个词汇表求和作为分母。让我们将分母标记为输入单词的分区函数，$Z(w_I)$。一个常见的假设是$Z(w)
    \approx 1$，因为我们期望softmax输出层被归一化（[Minh and Teh, 2012](https://www.cs.toronto.edu/~amnih/papers/ncelm.pdf)）。然后损失函数简化为：
- en: $$ \mathcal{L}_\theta = - [ \log \frac{\exp({v'_w}^{\top}{v_{w_I}})}{\exp({v'_w}^{\top}{v_{w_I}})
    + Nq(\tilde{w})} + \sum_{\substack{i=1 \\ \tilde{w}_i \sim Q}}^N \log \frac{Nq(\tilde{w}_i)}{\exp({v'_w}^{\top}{v_{w_I}})
    + Nq(\tilde{w}_i)}] $$
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \mathcal{L}_\theta = - [ \log \frac{\exp({v'_w}^{\top}{v_{w_I}})}{\exp({v'_w}^{\top}{v_{w_I}})
    + Nq(\tilde{w})} + \sum_{\substack{i=1 \\ \tilde{w}_i \sim Q}}^N \log \frac{Nq(\tilde{w}_i)}{\exp({v'_w}^{\top}{v_{w_I}})
    + Nq(\tilde{w}_i)}] $$
- en: 'The noise distribution $Q$ is a tunable parameter and we would like to design
    it in a way so that:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 噪声分布$Q$是一个可调参数，我们希望设计它的方式是：
- en: intuitively it should be very similar to the real data distribution; and
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 直观上应该与真实数据分布非常相似；以及
- en: it should be easy to sample from.
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应该容易从中进行采样。
- en: For example, the sampling implementation ([log_uniform_candidate_sampler](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/candidate_sampling_ops.py#L83))
    of NCE loss in tensorflow assumes that such noise samples follow a log-uniform
    distribution, also known as [Zipfian’s law](https://en.wikipedia.org/wiki/Zipf%27s_law).
    The probability of a given word in logarithm is expected to be reversely proportional
    to its rank, while high-frequency words are assigned with lower ranks. In this
    case, $q(\tilde{w}) = \frac{1}{ \log V}(\log (r_{\tilde{w}} + 1) - \log r_{\tilde{w}})$,
    where $r_{\tilde{w}} \in [1, V]$ is the rank of a word by frequency in descending
    order.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，tensorflow中NCE损失的采样实现([log_uniform_candidate_sampler](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/candidate_sampling_ops.py#L83))假设这样的噪声样本遵循对数均匀分布，也被称为[齐夫定律](https://en.wikipedia.org/wiki/Zipf%27s_law)。对数概率中给定单词的概率预计与其排名成反比，而高频词被赋予较低的排名。在这种情况下，$q(\tilde{w})
    = \frac{1}{ \log V}(\log (r_{\tilde{w}} + 1) - \log r_{\tilde{w}})$，其中$r_{\tilde{w}}
    \in [1, V]$是按频率降序排列的单词的排名。
- en: Negative Sampling (NEG)
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 负采样（NEG）
- en: The Negative Sampling (NEG) proposed by Mikolov et al. ([2013](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf))
    is a simplified variation of NCE loss. It is especially famous for training Google’s
    [word2vec](https://code.google.com/archive/p/word2vec/) project. Different from
    NCE Loss which attempts to approximately maximize the log probability of the softmax
    output, negative sampling did further simplification because it focuses on learning
    high-quality word embedding rather than modeling the word distribution in natural
    language.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: Mikolov等人提出的负采样（NEG）是NCE损失的简化变体。它尤其以训练Google的[word2vec](https://code.google.com/archive/p/word2vec/)项目而闻名。与试图近似最大化softmax输出的对数概率的NCE
    Loss不同，负采样进行了进一步简化，因为它专注于学习高质量的词嵌入，而不是对自然语言中的词分布进行建模。
- en: 'NEG approximates the binary classifier’s output with sigmoid functions as follows:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: NEG用sigmoid函数近似二元分类器的输出如下：
- en: $$ \begin{align} p(d=1 \vert w_, w_I) &= \sigma({v'_{w}}^\top v_{w_I}) \\ p(d=0
    \vert w, w_I) &= 1 - \sigma({v'_{w}}^\top v_{w_I}) = \sigma(-{v'_{w}}^\top v_{w_I})
    \end{align} $$
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \begin{align} p(d=1 \vert w_, w_I) &= \sigma({v'_{w}}^\top v_{w_I}) \\ p(d=0
    \vert w, w_I) &= 1 - \sigma({v'_{w}}^\top v_{w_I}) = \sigma(-{v'_{w}}^\top v_{w_I})
    \end{align} $$
- en: 'The final NCE loss function looks like:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的NCE损失函数如下所示：
- en: $$ \mathcal{L}_\theta = - [ \log \sigma({v'_{w}}^\top v_{w_I}) + \sum_{\substack{i=1
    \\ \tilde{w}_i \sim Q}}^N \log \sigma(-{v'_{\tilde{w}_i}}^\top v_{w_I})] $$
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \mathcal{L}_\theta = - [ \log \sigma({v'_{w}}^\top v_{w_I}) + \sum_{\substack{i=1
    \\ \tilde{w}_i \sim Q}}^N \log \sigma(-{v'_{\tilde{w}_i}}^\top v_{w_I})] $$
- en: Other Tips for Learning Word Embedding
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习词嵌入的其他提示
- en: Mikolov et al. ([2013](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf))
    suggested several helpful practices that could result in good word embedding learning
    outcomes.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: Mikolov等人建议了几种有助于获得良好词嵌入学习结果的实践方法。
- en: '**Soft sliding window**. When pairing the words within the sliding window,
    we could assign less weight to more distant words. One heuristic is — given a
    maximum window size parameter defined, $s_{\text{max}}$, the actual window size
    is randomly sampled between 1 and $s_{\text{max}}$ for every training sample.
    Thus, each context word has the probability of 1/(its distance to the target word)
    being observed, while the adjacent words are always observed.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**软滑动窗口**。在滑动窗口内配对单词时，我们可以给距离较远的单词分配较少的权重。一个启发式方法是——给定一个定义的最大窗口大小参数，$s_{\text{max}}$，实际窗口大小对于每个训练样本都在1和$s_{\text{max}}$之间随机抽样。因此，每个上下文单词被观察到的概率为1/(它到目标单词的距离)，而相邻单词总是被观察到。'
- en: '**Subsampling frequent words**. Extremely frequent words might be too general
    to differentiate the context (i.e. think about stopwords). While on the other
    hand, rare words are more likely to carry distinct information. To balance the
    frequent and rare words, Mikolov et al. proposed to discard words $w$ with probability
    $1-\sqrt{t/f(w)}$ during sampling. Here $f(w)$ is the word frequency and $t$ is
    an adjustable threshold.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对频繁单词进行子采样**。极其频繁的单词可能过于普通，无法区分上下文（即考虑停用词）。另一方面，罕见单词更有可能携带独特信息。为了平衡频繁和罕见单词，Mikolov等人建议在采样过程中以概率$1-\sqrt{t/f(w)}$丢弃单词$w$。这里$f(w)$是单词频率，$t$是可调阈值。'
- en: '**Learning phrases first**. A phrase often stands as a conceptual unit, rather
    than a simple composition of individual words. For example, we cannot really tell
    “New York” is a city name even we know the meanings of “new” and “york”. Learning
    such phrases first and treating them as word units before training the word embedding
    model improves the outcome quality. A simple data-driven approach is based on
    unigram and bigram counts: $s_{\text{phrase}} = \frac{C(w_i w_j) - \delta}{ C(w_i)C(w_j)}$,
    where $C(.)$ is simple count of an unigram $w_i$ or bigram $w_i w_j$ and $\delta$
    is a discounting threshold to prevent super infrequent words and phrases. Higher
    scores indicate higher chances of being phrases. To form phrases longer than two
    words, we can scan the vocabulary multiple times with decreasing score cutoff
    values.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**首先学习短语**。短语通常作为一个概念单元存在，而不是简单的个别单词组合。例如，即使我们知道“new”和“york”的含义，我们也无法确定“New
    York”是一个城市名。在训练词嵌入模型之前，首先学习这些短语并将它们视为单词单元可以提高结果质量。一个简单的数据驱动方法基于单字和双字计数：$s_{\text{phrase}}
    = \frac{C(w_i w_j) - \delta}{ C(w_i)C(w_j)}$，其中$C(.)$是单字$w_i$或双字$w_i w_j$的简单计数，$\delta$是一个折扣阈值，用于防止过于罕见的单词和短语。得分越高表示成为短语的可能性越大。为了形成超过两个单词的短语，我们可以多次扫描词汇表，逐渐降低得分截断值。'
- en: 'GloVe: Global Vectors'
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GloVe：全局向量
- en: The Global Vector (GloVe) model proposed by Pennington et al. ([2014](http://www.aclweb.org/anthology/D14-1162))
    aims to combine the count-based matrix factorization and the context-based skip-gram
    model together.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: Pennington等人提出的全局向量（GloVe）模型旨在将基于计数的矩阵分解和基于上下文的跳字模型结合在一起。
- en: 'We all know the counts and co-occurrences can reveal the meanings of words.
    To distinguish from $p(w_O \vert w_I)$ in the context of a word embedding word,
    we would like to define the co-ocurrence probability as:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们都知道计数和共现可以揭示单词的含义。为了区分在词嵌入词的上下文中的$p(w_O \vert w_I)$，我们想定义共现概率为：
- en: $$ p_{\text{co}}(w_k \vert w_i) = \frac{C(w_i, w_k)}{C(w_i)} $$
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: $$ p_{\text{co}}(w_k \vert w_i) = \frac{C(w_i, w_k)}{C(w_i)} $$
- en: $C(w_i, w_k)$ counts the co-occurrence between words $w_i$ and $w_k$.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: $C(w_i, w_k)$ 计算了单词$w_i$和$w_k$的共现次数。
- en: Say, we have two words, $w_i$=“ice” and $w_j$=“steam”. The third word $\tilde{w}_k$=“solid”
    is related to “ice” but not “steam”, and thus we expect $p_{\text{co}}(\tilde{w}_k
    \vert w_i)$ to be much larger than $p_{\text{co}}(\tilde{w}_k \vert w_j)$ and
    therefore $\frac{p_{\text{co}}(\tilde{w}_k \vert w_i)}{p_{\text{co}}(\tilde{w}_k
    \vert w_j)}$ to be very large. If the third word $\tilde{w}_k$ = “water” is related
    to both or $\tilde{w}_k$ = “fashion” is unrelated to either of them, $\frac{p_{\text{co}}(\tilde{w}_k
    \vert w_i)}{p_{\text{co}}(\tilde{w}_k \vert w_j)}$ is expected to be close to
    one.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 假设，我们有两个单词，$w_i$=“ice”和$w_j$=“steam”。第三个单词$\tilde{w}_k$=“solid”与“ice”相关但与“steam”无关，因此我们期望$p_{\text{co}}(\tilde{w}_k
    \vert w_i)$远大于$p_{\text{co}}(\tilde{w}_k \vert w_j)$，因此$\frac{p_{\text{co}}(\tilde{w}_k
    \vert w_i)}{p_{\text{co}}(\tilde{w}_k \vert w_j)}$应该非常大。如果第三个单词$\tilde{w}_k$=“water”与两者相关或$\tilde{w}_k$=“fashion”与两者都不相关，则$\frac{p_{\text{co}}(\tilde{w}_k
    \vert w_i)}{p_{\text{co}}(\tilde{w}_k \vert w_j)}$预计接近于一。
- en: 'The intuition here is that the word meanings are captured by the ratios of
    co-occurrence probabilities rather than the probabilities themselves. The global
    vector models the relationship between two words regarding to the third context
    word as:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的直觉是，单词含义由共现概率的比率而不是概率本身捕获。全局向量模型了解两个单词之间关于第三个上下文单词的关系如下：
- en: $$ F(w_i, w_j, \tilde{w}_k) = \frac{p_{\text{co}}(\tilde{w}_k \vert w_i)}{p_{\text{co}}(\tilde{w}_k
    \vert w_j)} $$
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: $$ F(w_i, w_j, \tilde{w}_k) = \frac{p_{\text{co}}(\tilde{w}_k \vert w_i)}{p_{\text{co}}(\tilde{w}_k
    \vert w_j)} $$
- en: 'Further, since the goal is to learn meaningful word vectors, $F$ is designed
    to be a function of the linear difference between two words $w_i - w_j$:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，由于目标是学习有意义的单词向量，$F$被设计为两个单词$w_i - w_j$之间的线性差异的函数：
- en: $$ F((w_i - w_j)^\top \tilde{w}_k) = \frac{p_{\text{co}}(\tilde{w}_k \vert w_i)}{p_{\text{co}}(\tilde{w}_k
    \vert w_j)} $$
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: $$ F((w_i - w_j)^\top \tilde{w}_k) = \frac{p_{\text{co}}(\tilde{w}_k \vert w_i)}{p_{\text{co}}(\tilde{w}_k
    \vert w_j)} $$
- en: With the consideration of $F$ being symmetric between target words and context
    words, the final solution is to model $F$ as an **exponential** function. Please
    read the original paper ([Pennington et al., 2014](http://www.aclweb.org/anthology/D14-1162))
    for more details of the equations.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到目标词和上下文词之间对称的$F$，最终的解决方案是将$F$建模为一个**指数**函数。请阅读原始论文（[Pennington et al., 2014](http://www.aclweb.org/anthology/D14-1162)）以获取更多方程的细节。
- en: $$ \begin{align} F({w_i}^\top \tilde{w}_k) &= \exp({w_i}^\top \tilde{w}_k) =
    p_{\text{co}}(\tilde{w}_k \vert w_i) \\ F((w_i - w_j)^\top \tilde{w}_k) &= \exp((w_i
    - w_j)^\top \tilde{w}_k) = \frac{\exp(w_i^\top \tilde{w}_k)}{\exp(w_j^\top \tilde{w}_k)}
    = \frac{p_{\text{co}}(\tilde{w}_k \vert w_i)}{p_{\text{co}}(\tilde{w}_k \vert
    w_j)} \end{align} $$
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \begin{align} F({w_i}^\top \tilde{w}_k) &= \exp({w_i}^\top \tilde{w}_k) =
    p_{\text{co}}(\tilde{w}_k \vert w_i) \\ F((w_i - w_j)^\top \tilde{w}_k) &= \exp((w_i
    - w_j)^\top \tilde{w}_k) = \frac{\exp(w_i^\top \tilde{w}_k)}{\exp(w_j^\top \tilde{w}_k)}
    = \frac{p_{\text{co}}(\tilde{w}_k \vert w_i)}{p_{\text{co}}(\tilde{w}_k \vert
    w_j)} \end{align} $$
- en: Finally,
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，
- en: $$ {w_i}^\top \tilde{w}_k = \log p_{\text{co}}(\tilde{w}_k \vert w_i) = \log
    \frac{C(w_i, \tilde{w}_k)}{C(w_i)} = \log C(w_i, \tilde{w}_k) - \log C(w_i) $$
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: $$ {w_i}^\top \tilde{w}_k = \log p_{\text{co}}(\tilde{w}_k \vert w_i) = \log
    \frac{C(w_i, \tilde{w}_k)}{C(w_i)} = \log C(w_i, \tilde{w}_k) - \log C(w_i) $$
- en: Since the second term $-\log C(w_i)$ is independent of $k$, we can add bias
    term $b_i$ for $w_i$ to capture $-\log C(w_i)$. To keep the symmetric form, we
    also add in a bias $\tilde{b}_k$ for $\tilde{w}_k$.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 由于第二项$-\log C(w_i)$与$k$无关，我们可以为$w_i$添加偏置项$b_i$来捕获$-\log C(w_i)$。为了保持对称形式，我们还为$\tilde{w}_k$添加偏置$\tilde{b}_k$。
- en: $$ \log C(w_i, \tilde{w}_k) = {w_i}^\top \tilde{w}_k + b_i + \tilde{b}_k $$
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \log C(w_i, \tilde{w}_k) = {w_i}^\top \tilde{w}_k + b_i + \tilde{b}_k $$
- en: 'The loss function for the GloVe model is designed to preserve the above formula
    by minimizing the sum of the squared errors:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: GloVe模型的损失函数旨在通过最小化平方误差的总和来保留上述公式：
- en: $$ \mathcal{L}_\theta = \sum_{i=1, j=1}^V f(C(w_i,w_j)) ({w_i}^\top \tilde{w}_j
    + b_i + \tilde{b}_j - \log C(w_i, \tilde{w}_j))^2 $$
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \mathcal{L}_\theta = \sum_{i=1, j=1}^V f(C(w_i,w_j)) ({w_i}^\top \tilde{w}_j
    + b_i + \tilde{b}_j - \log C(w_i, \tilde{w}_j))^2 $$
- en: The weighting schema $f(c)$ is a function of the co-occurrence of $w_i$ and
    $w_j$ and it is an adjustable model configuration. It should be close to zero
    as $c \to 0$; should be non-decreasing as higher co-occurrence should have more
    impact; should saturate when $c$ become extremely large. The paper proposed the
    following weighting function.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 权重模式$f(c)$是$w_i$和$w_j$的共现的函数，是一个可调整的模型配置。当$c \to 0$时，应接近于零；随着更高的共现，应该是非递减的；当$c$变得极端大时应该饱和。论文提出了以下权重函数。
- en: $$ f(c) = \begin{cases} (\frac{c}{c_{\max}})^\alpha & \text{if } c < c_{\max}
    \text{, } c_{\max} \text{ is adjustable.} \\ 1 & \text{if } \text{otherwise} \end{cases}
    $$
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: $$ f(c) = \begin{cases} (\frac{c}{c_{\max}})^\alpha & \text{如果 } c < c_{\max}
    \text{，} c_{\max} \text{是可调整的。} \\ 1 & \text{如果 } \text{其他情况} \end{cases} $$
- en: 'Examples: word2vec on “Game of Thrones”'
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 例子：Game of Thrones上的word2vec
- en: After reviewing all the theoretical knowledge above, let’s try a little experiment
    in word embedding extracted from “the Games of Thrones corpus”. The process is
    super straightforward using [gensim](https://radimrehurek.com/gensim/models/word2vec.html).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在回顾了上述所有理论知识之后，让我们尝试从“权力的游戏语料库”中提取的词嵌入进行一点实验。使用[gensim](https://radimrehurek.com/gensim/models/word2vec.html)的过程非常简单。
- en: '**Step 1: Extract words**'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤1：提取单词**'
- en: '[PRE0]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '**Step 2: Feed a word2vec model**'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤2：输入word2vec模型**'
- en: '[PRE1]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '**Step 3: Check the results**'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤3：检查结果**'
- en: 'In the GoT word embedding space, the top similar words to “king” and “queen”
    are:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在GoT词嵌入空间中，“国王”和“王后”的相似词排名如下：
- en: '| `model.most_similar(''king'', topn=10)` (word, similarity with ‘king’) |
    `model.most_similar(''queen'', topn=10)` (word, similarity with ‘queen’) |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| `model.most_similar(''king'', topn=10)`（与‘国王’相似的词） | `model.most_similar(''queen'',
    topn=10)`（与‘王后’相似的词） |'
- en: '| --- | --- |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| (‘kings’, 0.897245) | (‘cersei’, 0.942618) |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| (‘国王’, 0.897245) | (‘瑟曦’, 0.942618) |'
- en: '| (‘baratheon’, 0.809675) | (‘joffrey’, 0.933756) |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| (‘拜拉席恩’, 0.809675) | (‘乔佛里’, 0.933756) |'
- en: '| (‘son’, 0.763614) | (‘margaery’, 0.931099) |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| (‘儿子’, 0.763614) | (‘玛格丽’, 0.931099) |'
- en: '| (‘robert’, 0.708522) | (‘sister’, 0.928902) |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| (‘罗伯特’, 0.708522) | (‘姐妹’, 0.928902) |'
- en: '| (’lords’, 0.698684) | (‘prince’, 0.927364) |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| (‘领主们’, 0.698684) | (‘王子’, 0.927364) |'
- en: '| (‘joffrey’, 0.696455) | (‘uncle’, 0.922507) |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| (‘乔佛里’, 0.696455) | (‘叔叔’, 0.922507) |'
- en: '| (‘prince’, 0.695699) | (‘varys’, 0.918421) |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| (‘王子’, 0.695699) | (‘瓦里斯’, 0.918421) |'
- en: '| (‘brother’, 0.685239) | (’ned’, 0.917492) |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| (‘兄弟’, 0.685239) | (‘尼德’, 0.917492) |'
- en: '| (‘aerys’, 0.684527) | (‘melisandre’, 0.915403) |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| (‘艾利斯’, 0.684527) | (‘梅丽珊卓’, 0.915403) |'
- en: '| (‘stannis’, 0.682932) | (‘robb’, 0.915272) |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| (‘史坦尼斯’, 0.682932) | (‘罗伯’, 0.915272) |'
- en: '* * *'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 'Cited as:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 引用为：
- en: '[PRE2]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: References
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Tensorflow Tutorial [Vector Representations of Words](https://www.tensorflow.org/tutorials/word2vec).'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Tensorflow教程 [单词的向量表示](https://www.tensorflow.org/tutorials/word2vec)。'
- en: '[2] [“Word2Vec Tutorial - The Skip-Gram Model”](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)
    by Chris McCormick.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] [“Word2Vec教程 - Skip-Gram模型”](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)
    by Chris McCormick.'
- en: '[3] [“On word embeddings - Part 2: Approximating the Softmax”](http://ruder.io/word-embeddings-softmax/)
    by Sebastian Ruder.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] [“关于词嵌入 - 第2部分：逼近Softmax”](http://ruder.io/word-embeddings-softmax/) by
    Sebastian Ruder.'
- en: '[4] Xin Rong. [word2vec Parameter Learning Explained](https://arxiv.org/pdf/1411.2738.pdf)'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Xin Rong. [word2vec参数学习解释](https://arxiv.org/pdf/1411.2738.pdf)'
- en: '[5] Mikolov, Tomas, Kai Chen, Greg Corrado, and Jeffrey Dean. [“Efficient estimation
    of word representations in vector space.”](https://arxiv.org/pdf/1301.3781.pdf)
    arXiv preprint arXiv:1301.3781 (2013).'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Mikolov, Tomas, Kai Chen, Greg Corrado, and Jeffrey Dean. [“在向量空间中高效估计词表示”](https://arxiv.org/pdf/1301.3781.pdf)
    arXiv预印本 arXiv:1301.3781 (2013).'
- en: '[6] Frederic Morin and Yoshua Bengio. [“Hierarchical Probabilistic Neural Network
    Language Model.”](https://www.iro.umontreal.ca/~lisa/pointeurs/hierarchical-nnlm-aistats05.pdf)
    Aistats. Vol. 5\. 2005.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Frederic Morin and Yoshua Bengio. [“分层概率神经网络语言模型”](https://www.iro.umontreal.ca/~lisa/pointeurs/hierarchical-nnlm-aistats05.pdf)
    Aistats。Vol. 5。2005年。'
- en: '[7] Michael Gutmann and Aapo Hyvärinen. [“Noise-contrastive estimation: A new
    estimation principle for unnormalized statistical models.”](http://proceedings.mlr.press/v9/gutmann10a/gutmann10a.pdf)
    Proc. Intl. Conf. on Artificial Intelligence and Statistics. 2010.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] Michael Gutmann and Aapo Hyvärinen. [“噪声对比估计：非归一化统计模型的新估计原则”](http://proceedings.mlr.press/v9/gutmann10a/gutmann10a.pdf)
    人工智能和统计学国际会议论文集。2010年。'
- en: '[8] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean.
    [“Distributed representations of words and phrases and their compositionality.”](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)
    Advances in neural information processing systems. 2013.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean.
    [“单词和短语的分布式表示及其组合性”](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)
    神经信息处理系统的进展。2013年。'
- en: '[9] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. [“Efficient estimation
    of word representations in vector space.”](https://arxiv.org/pdf/1301.3781.pdf)
    arXiv preprint arXiv:1301.3781 (2013).'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. [“在向量空间中高效估计词表示”](https://arxiv.org/pdf/1301.3781.pdf)
    arXiv预印本 arXiv:1301.3781 (2013).'
- en: '[10] Marco Baroni, Georgiana Dinu, and Germán Kruszewski. [“Don’t count, predict!
    A systematic comparison of context-counting vs. context-predicting semantic vectors.”](http://anthology.aclweb.org/P/P14/P14-1023.pdf)
    ACL (1). 2014.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] Marco Baroni, Georgiana Dinu, and Germán Kruszewski. [“不要计数，要预测！上下文计数与上下文预测语义向量的系统比较.”](http://anthology.aclweb.org/P/P14/P14-1023.pdf)
    2014年 ACL (1) 会议论文集。'
- en: '[11] Jeffrey Pennington, Richard Socher, and Christopher Manning. [“Glove:
    Global vectors for word representation.”](http://www.aclweb.org/anthology/D14-1162)
    Proc. Conf. on empirical methods in natural language processing (EMNLP). 2014.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] Jeffrey Pennington, Richard Socher, and Christopher Manning. [“Glove:
    全局词向量表示.”](http://www.aclweb.org/anthology/D14-1162) 2014年 Empirical Methods in
    Natural Language Processing (EMNLP) 会议论文集.'
